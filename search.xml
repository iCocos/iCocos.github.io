<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[大数据之平台建设]]></title>
    <url>%2F2020%2F04%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%B9%B3%E5%8F%B0%E5%BB%BA%E8%AE%BE%2F</url>
    <content type="text"><![CDATA[分布式系统基础架构Hadoop Hadoop详细介绍 Hadoop是一个分布式系统基础架构，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。Hadoop实现了一个分布式文件系统（Hadoop Distributed File System），简称HDFS。HDFS有着高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上。而且它提供高传输率（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求（requirements）这样可以流的形式访问（streaming access）文件系统中的数据。 Hadoop体系结构 分布式文件系统HDFS Hadoop Distributed File System，简称HDFS，是一个分布式文件系统。HDFS有着高容错性（fault-tolerent）的特点，并且设计用来部署在低廉的（low-cost）硬件上。而且它提供高吞吐量（high throughput）来访问应用程序的数据，适合那些有着超大数据集（large data set）的应用程序。HDFS放宽了（relax）POSIX的要求（requirements）这样可以实现流的形式访问（streaming access）文件系统中的数据。HDFS开始是为开源的apache项目nutch的基础结构而创建，HDFS是hadoop项目的一部分，而hadoop又是lucene的一部分。 大规模数据集软件架构MapReduce MapReduce是Google提出的一个软件架构，用于大规模数据集（大于1TB）的并行运算。概念”Map（映射）”和”Reduce（化简）”，和他们的主要思想，都是从函数式编程语言借来的，还有从矢量编程语言借来的特性。 当前的软件实现是指定一个Map（映射）函数，用来把一组键值对映射成一组新的键值对，指定并发的Reduce（化简）函数，用来保证所有映射的键值对中的每一个共享相同的键组。 Hadoop资源管理器YARN YAEN详细介绍YARN是新一代Hadoop资源管理器，通过YARN,用户可以运行和管理同一个物理集群机上的多种作业，例如MapReduce批处理和图形处理作业。这样不仅可以巩固一个组织管理的系统数目，而且可以对相同的数据进行不同类型的数据分析。某些情况下，整个数据流可以执行在同一个集群机上。 数据仓库平台Hive Hive详细介绍 Hive是一个基于Hadoop的数据仓库平台。通过hive，我们可以方便地进行ETL的工作。hive定义了一个类似于SQL的查询语言：HQL，能 够将用户编写的QL转化为相应的Mapreduce程序基于Hadoop执行。 Hive是Facebook 2008年8月刚开源的一个数据仓库框架，其系统目标与 Pig 有相似之处，但它有一些Pig目前还不支持的机制，比如：更丰富的类型系统、更类似SQL的查询语言、Table/Partition元数据的持久化等。 数据表和存储管理服务HCatalog Hcatalog 详细介绍 Apache HCatalog是基于Apache Hadoop之上的数据表和存储管理服务。 包括: 提供一个共享的模式和数据类型的机制。 抽象出表，使用户不必关心他们的数据怎么存储。 提供可操作的跨数据处理工具，如Pig，MapReduce，Streaming，和Hive。 大规模数据分析平台Pig Pig详细介绍 Pig是一个基于Hadoop的大规模数据分析平台，它提供的SQL-LIKE语言叫Pig Latin，该语言的编译器会把类SQL的数据分析请求转换为一系列经过优化处理的MapReduce运算。Pig为复杂的海量数据并行计算提供了一个简单的操作和编程接口。 Hadoop管理监控工具Apache Ambari Apache Ambari 详细介绍 Apache Ambari是一个基于Web的Apache Hadoop集群的供应、管理和监控。Ambari目前已支持大多数Hadoop组件，包括HDFS、MapReduce、Hive、Pig、 Hbase、Zookeper、Sqoop和Hcatalog等。 Apache Ambari 支持HDFS、MapReduce、Hive、Pig、Hbase、Zookeper、Sqoop和Hcatalog等的集中管理。也是5个顶级hadoop管理工具之一。 Ambari主要取得了以下成绩: 通过一步步的安装向导简化了集群供应。 预先配置好关键的运维指标（metrics），可以直接查看Hadoop Core（HDFS和MapReduce）及相关项目（如HBase、Hive和HCatalog）是否健康。 支持作业与任务执行的可视化与分析，能够更好地查看依赖和性能。 通过一个完整的RESTful API把监控信息暴露出来，集成了现有的运维工具。 用户界面非常直观，用户可以轻松有效地查看信息并控制集群。 Ambari使用Ganglia收集度量指标，用Nagios支持系统报警，当需要引起管理员的关注时（比如，节点停机或磁盘剩余空间不足等问题），系统将向其发送邮件。 此外，Ambari能够安装安全的（基于Kerberos）Hadoop集群，以此实现了对Hadoop 安全的支持，提供了基于角色的用户认证、授权和审计功能，并为用户管理集成了LDAP[轻量目录访问协议]和Active Directory。 分布式监控系统Ganglia Ganglia详细介绍 Ganglia是用于高性能计算系统（如集群和网格）的可扩展分布式监控系统。它基于针对集群联盟的分层设计。它利用广泛使用的技术，例如用于数据表示的XML，用于紧凑型，便携式数据传输的XDR和用于数据存储和可视化的RRDtool。它使用精心设计的数据结构和算法来实现非常低的每节点开销和高并发性。该实现是强大的，已被移植到广泛的操作系统和处理器架构，目前正在世界各地的数千个集群中使用。它已经被用来连接大学校园和世界各地的群集，并且可以扩展到处理具有2000个节点的群集。 监控系统Nagios Nagios Nagios是一个监视系统运行状态和网络信息的监视系统。Nagios能监视所指定的本地或远程主机以及服务，同时提供异常通知功能等 Nagios可运行在Linux/Unix平台之上，同时提供一个可选的基于浏览器的WEB界面以方便系统管理人员查看网络状态，各种系统问题，以及日志等等。 Nagios 有一个 Windows 下的客户端： http://www.oschina.net/p/nsclientppNagios的主要功能特点： 监视网络服务 (SMTP, POP3, HTTP, NNTP, PING等) 监视主机资源 (进程, 磁盘等) 简单的插件设计可以轻松扩展Nagios的监视功能 服务等监视的并发处理 错误通知功能 (通过email, pager, 或其他用户自定义方法) 可指定自定义的事件处理控制器 可选的基于浏览器的WEB界面以方便系统管理人员查看网络状态，各种系统问题，以及日志等等 可以通过手机查看系统监控信息 开源集群计算环境Apache Spark Apache Spark详细介绍 Apache Spark 是一种与 Hadoop 相似的开源集群计算环境，但是两者之间还存在一些不同之处，这些有用的不同之处使 Spark 在某些工作负载方面表现得更加优越，换句话说，Spark 启用了内存分布数据集，除了能够提供交互式查询外，它还可以优化迭代工作负载。 Spark 是在 Scala 语言中实现的，它将 Scala 用作其应用程序框架。与 Hadoop 不同，Spark 和 Scala 能够紧密集成，其中的 Scala 可以像操作本地集合对象一样轻松地操作分布式数据集。 尽管创建 Spark 是为了支持分布式数据集上的迭代作业，但是实际上它是对 Hadoop 的补充，可以在 Hadoo 文件系统中并行运行。通过名为 Mesos 的第三方集群框架可以支持此行为。Spark 由加州大学伯克利分校 AMP 实验室 (Algorithms, Machines, and People Lab) 开发，可用来构建大型的、低延迟的数据分析应用程序。 基于Hadoop的实时查询Cloudera Impala Cloudera Impala详细的介绍 Cloudera 发布实时查询开源项目 Impala (黑斑羚)！多款产品实测表明，比原来基于MapReduce的Hive SQL查询速度提升3～90倍。Impala是Google Dremel的模仿，但在SQL功能上青出于蓝胜于蓝。 Impala采用与Hive相同的元数据、SQL语法、ODBC驱动程序和用户接口(Hue Beeswax)，这样在使用CDH产品时，批处理和实时查询的平台是统一的。目前支持的文件格式是文本文件和SequenceFiles（可以压缩为Snappy、GZIP和BZIP，前者性能最好）。其他格式如Avro, RCFile, LZO文本和Doug Cutting的Trevni将在正式版中支持。 Hadoop柱状存储格式Parquet Parquet详细介绍 Parquet是一种面向列存存储的文件格式，Cloudera的大数据在线分析（OLAP）项目Impala中使用该格式作为列存储。 Apache Parquet 是一个列存储格式，主要用于 Hadoop 生态系统。对数据处理框架、数据模型和编程语言无关。 OLAP分析引擎Apache Kylin Apache Kylin详细介绍 Apache Kylin 是一个开源的分布式的 OLAP 分析引擎，来自 eBay 公司开发，基于 Hadoop 提供 SQL 接口和 OLAP 接口，支持 TB 到 PB 级别的数据量。 Apache kylin是: 超级快的OLAP引擎，具备可伸缩性 为Hadoop提供ANSI-SQL接口 交互式查询能力 MOLAP Cube 可与其他BI工具无缝集成，如Tableau,而Microstrategy和Excel将很快推出 Apache kylin总结的特点 通过空间换时间-&gt;实现了亚秒级别延迟——&gt;提供了一个交互式的查询 预计算，计算结果保存在HBase中，基于行的关系模式转换为基于键值对的列式模式 维度组合，查询访问不需要扫描表 提供SQL接口 其他值得关注的特性包括： 作业管理和监控 压缩和编码的支持 Cube 的增量更新 Leverage HBase Coprocessor for query latency Approximate Query Capability for distinct Count (HyperLogLog) 易用的 Web 管理、构建、监控和查询 Cube 的接口 Security capability to set ACL at Cube/Project Level 支持 LDAP 集成 分布式实时计算系统Apache Storm Apache Storm详细介绍 Apache Storm 的前身是 Twitter Storm 平台，目前已经归于 Apache 基金会管辖。 Apache Storm 是一个免费开源的分布式实时计算系统。简化了流数据的可靠处理，像 Hadoop 一样实现实时批处理。Storm 很简单，可用于任意编程语言。Apache Storm 采用 Clojure 开发。 Storm 有很多应用场景，包括实时数据分析、联机学习、持续计算、分布式 RPC、ETL 等。Storm 速度非常快，一个测试在单节点上实现每秒一百万的组处理。 目前已经有包括阿里百度在内的数家大型互联网公司在使用该平台。 分布式系统协调Zookeeper Zookeeper详细介绍 ZooKeeper是Hadoop的正式子项目，它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。 Zookeeper是Google的Chubby一个开源的实现.是高有效和可靠的协同工作系统.Zookeeper能够用来leader选举,配置信息维护等.在一个分布式的环境中,我们需要一个Master实例或存储一些配置信息,确保文件写入的一致性等.Zookeeper能够保证如下3点: Watches are ordered with respect to other events, other watches, andasynchronous replies. The ZooKeeper client libraries ensures thateverything is dispatched in order. A client will see a watch event for a znode it is watching before seeing the new data that corresponds to that znode. The order of watch events from ZooKeeper corresponds to the order of the updates as seen by the ZooKeeper service. 在Zookeeper中,znode是一个跟Unix文件系统路径相似的节点,可以往这个节点存储或获取数据.如果在创建znode时Flag设置 为EPHEMERAL,那么当这个创建这个znode的节点和Zookeeper失去连接后,这个znode将不再存在在Zookeeper 里.Zookeeper使用Watcher察觉事件信息,当客户端接收到事件信息,比如连接超时,节点数据改变,子节点改变,可以调用相应的行为来处理数 据.Zookeeper的Wiki页面展示了如何使用Zookeeper来处理事件通知,队列,优先队列,锁,共享锁,可撤销的共享锁,两阶段提交. 那么Zookeeper能帮我们作什么事情呢?简单的例子:假设我们我们有个20个搜索引擎的服务器(每个负责总索引中的一部分的搜索任务)和一个 总服务器(负责向这20个搜索引擎的服务器发出搜索请求并合并结果集),一个备用的总服务器(负责当总服务器宕机时替换总服务器),一个web的 cgi(向总服务器发出搜索请求).搜索引擎的服务器中的15个服务器现在提供搜索服务,5个服务器正在生成索引.这20个搜索引擎的服务器经常要让正在 提供搜索服务的服务器停止提供服务开始生成索引,或生成索引的服务器已经把索引生成完成可以搜索提供服务了.使用Zookeeper可以保证总服务器自动 感知有多少提供搜索引擎的服务器并向这些服务器发出搜索请求,备用的总服务器宕机时自动启用备用的总服务器,web的cgi能够自动地获知总服务器的网络 地址变化.这些又如何做到呢? 提供搜索引擎的服务器都在Zookeeper中创建znode,zk.create(“/search/nodes/node1”,“hostname”.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateFlags.EPHEMERAL); 总服务器可以从Zookeeper中获取一个znode的子节点的列表,zk.getChildren(“/search/nodes”, true); 总服务器遍历这些子节点,并获取子节点的数据生成提供搜索引擎的服务器列表. 当总服务器接收到子节点改变的事件信息,重新返回第二步. 总服务器在Zookeeper中创建节点,zk.create(“/search/master”, “hostname”.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateFlags.EPHEMERAL); 备用的总服务器监控Zookeeper中的”/search/master”节点.当这个znode的节点数据改变时,把自己启动变成总服务器,并把自己的网络地址数据放进这个节点. web的cgi从Zookeeper中”/search/master”节点获取总服务器的网络地址数据并向其发送搜索请求. web的cgi监控Zookeeper中的”/search/master”节点,当这个znode的节点数据改变时,从这个节点获取总服务器的网络地址数据,并改变当前的总服务器的网络地址. Hadoop和数据库数据迁移工具Sqoop Sqoop详细介绍 Sqoop是一个用来将Hadoop和关系型数据库中的数据相互转移的工具，可以将一个关系型数据库（例如 ： MySQL ,Oracle ,Postgres等）中的数据导入到Hadoop的HDFS中，也可以将HDFS的数据导入到关系型数据库中。 日志服务器Apache Flume Apache Flume详细介绍 Flume 是一个分布式、可靠和高可用的服务，用于收集、聚合以及移动大量日志数据，使用一个简单灵活的架构，就流数据模型。这是一个可靠、容错的服务。 分布式发布订阅消息系统kafka Kafka详细介绍 Kafka是一种高吞吐量的分布式发布订阅消息系统，她有如下特性： 通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 高吞吐量：即使是非常普通的硬件kafka也可以支持每秒数十万的消息。 支持通过kafka服务器和消费机集群来分区消息。 支持Hadoop并行数据加载。 kafka的目的是提供一个发布订阅解决方案，它可以处理消费者规模的网站中的所有动作流数据。 这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。 这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。 对于像Hadoop的一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群机来提供实时的消费。 开源计算框架Apache Tez Apache Tez详细介绍 Tez 是 Apache 最新的支持 DAG 作业的开源计算框架，它可以将多个有依赖的作业转换为一个作业从而大幅提升DAG作业的性能。Tez并不直接面向最终用户——事实上它允许开发者为最终用户构建性能更快、扩展性更好的应用程序。Hadoop传统上是一个大量数据批处理平台。但是，有很多用例需要近乎实时的查询处理性能。还有一些工作则不太适合MapReduce，例如机器学习。Tez的目的就是帮助Hadoop处理这些用例场景。 开源工作流引擎Oozie Oozie 详细介绍 ozie 是一个开源的工作流和协作服务引擎，基于 Apache Hadoop 的数据处理任务。Oozie 是可扩展的、可伸缩的面向数据的服务，运行在Hadoop 平台上。 Oozie 包括一个离线的Hadoop处理的工作流解决方案，以及一个查询处理 API。 分布式文档存储数据库MongoDB MongoDB详细介绍 MongoDB是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。他支持的数据结构非常松散，是类似json的bjson格式，因此可以存储比较复杂的数据类型。Mongo最大的特点是他支持的查询语言非常强大，其语法有点类似于面向对象的查询语言，几乎可以实现类似关系数据库单表查询的绝大部分功能，而且还支持对数据建立索引。 内部架构 它的特点是高性能、易部署、易使用，存储数据非常方便。主要功能特性有： 面向集合存储，易存储对象类型的数据。 模式自由 支持动态查询 支持完全索引，包含内部对象。 支持查询。 支持复制和故障恢复。 使用高效的二进制数据存储，包括大型对象（如视频等）。 自动处理碎片，以支持云计算层次的扩展性 支持RUBY，PYTHON，JAVA，C++，PHP等多种语言。 文件存储格式为BSON（一种JSON的扩展） 可通过网络访问 所谓“面向集合”（Collenction-Orented），意思是数据被分组存储在数据集中，被称为一个集合（Collenction)。每个 集合在数据库中都有一个唯一的标识名，并且可以包含无限数目的文档。集合的概念类似关系型数据库（RDBMS）里的表（table），不同的是它不需要定 义任何模式（schema)。模式自由（schema-free)，意味着对于存储在mongodb数据库中的文件，我们不需要知道它的任何结构定义。如果需要的话，你完全可以把不同结构的文件存储在同一个数据库里。存储在集合中的文档，被存储为键-值对的形式。键用于唯一标识一个文档，为字符串类型，而值则可以是各中复杂的文件类型。我们称这种存储形式为BSON（Binary Serialized dOcument Format）。 高性能的NoSQL图形数据库Neo4j Neo4j详细介绍 Neo4j是一个网络——面向网络的数据库——也就是说，它是一个嵌入式的、基于磁盘的、具备完全的事务特性的Java持久化引擎，但是它将结构化数据存储在网络上而不是表中。网络（从数学角度叫做图）是一个灵活的数据结构，可以应用更加敏捷和快速的开发模式。 你可以把Neo4j看作是一个高性能的图引擎，该引擎具有成熟和健壮的数据库的所有特性。程序员工作在一个面向对象的、灵活的网络结构下而不是严格、静态的表中——但是他们可以享受到具备完全的事务特性、企业级的数据库的所有好处。 数据序列化系统Apache Avro Apache Avro详细介绍 Avro（读音类似于[ævrə]）是Hadoop的一个子项目，由Hadoop的 创始人Doug Cutting（也是Lucene，Nutch等项目的创始人）牵头开发。Avro是一个数据序列化系统，设计用于支持大 批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro提供的机制使动态语言可以方便地处理 Avro数据。 容器集群管理系统Kubernetes Kubernetes详细介绍 Kubernetes是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制。 Kubernetes一个核心的特点就是能够自主的管理容器来保证云平台中的容器按照用户的期望状态运行着（比如用户想让apache一直运行，用户不需要关心怎么去做，Kubernetes会自动去监控，然后去重启，新建，总之，让apache一直提供服务），管理员可以加载一个微型服务，让规划器来找到合适的位置，同时，Kubernetes也系统提升工具以及人性化方面，让用户能够方便的部署自己的应用（就像canary deployments）。 现在Kubenetes着重于不间断的服务状态（比如web服务器或者缓存服务器）和原生云平台应用（Nosql）,在不久的将来会支持各种生产云平台中的各种服务，例如，分批，工作流，以及传统数据库。 在Kubenetes中，所有的容器均在Pod中运行,一个Pod可以承载一个或者多个相关的容器，在后边的案例中，同一个Pod中的容器会部署在同一个物理机器上并且能够共享资源。一个Pod也可以包含O个或者多个磁盘卷组（volumes）,这些卷组将会以目录的形式提供给一个容器，或者被所有Pod中的容器共享，对于用户创建的每个Pod,系统会自动选择那个健康并且有足够容量的机器，然后创建类似容器的容器,当容器创建失败的时候，容器会被node agent自动的重启,这个node agent叫kubelet,但是，如果是Pod失败或者机器，它不会自动的转移并且启动，除非用户定义了 replication controller。 用户可以自己创建并管理Pod,Kubernetes将这些操作简化为两个操作：基于相同的Pod配置文件部署多个Pod复制品；创建可替代的Pod当一个Pod挂了或者机器挂了的时候。而Kubernetes API中负责来重新启动，迁移等行为的部分叫做“replication controller”，它根据一个模板生成了一个Pod,然后系统就根据用户的需求创建了许多冗余，这些冗余的Pod组成了一个整个应用，或者服务，或者服务中的一层。一旦一个Pod被创建，系统就会不停的监控Pod的健康情况以及Pod所在主机的健康情况，如果这个Pod因为软件原因挂掉了或者所在的机器挂掉了，replication controller 会自动在一个健康的机器上创建一个一摸一样的Pod,来维持原来的Pod冗余状态不变，一个应用的多个Pod可以共享一个机器。 我们经常需要选中一组Pod，例如，我们要限制一组Pod的某些操作，或者查询某组Pod的状态，作为Kubernetes的基本机制，用户可以给Kubernetes Api中的任何对象贴上一组 key:value的标签，然后，我们就可以通过标签来选择一组相关的Kubernetes Api 对象，然后去执行一些特定的操作，每个资源额外拥有一组（很多） keys 和 values,然后外部的工具可以使用这些keys和vlues值进行对象的检索，这些Map叫做annotations（注释）。 Kubernetes支持一种特殊的网络模型，Kubernetes创建了一个地址空间，并且不动态的分配端口，它可以允许用户选择任何想使用的端口，为了实现这个功能，它为每个Pod分配IP地址。 现代互联网应用一般都会包含多层服务构成，比如web前台空间与用来存储键值对的内存服务器以及对应的存储服务，为了更好的服务于这样的架构，Kubernetes提供了服务的抽象，并提供了固定的IP地址和DNS名称，而这些与一系列Pod进行动态关联，这些都通过之前提到的标签进行关联，所以我们可以关联任何我们想关联的Pod，当一个Pod中的容器访问这个地址的时候，这个请求会被转发到本地代理（kube proxy）,每台机器上均有一个本地代理，然后被转发到相应的后端容器。Kubernetes通过一种轮训机制选择相应的后端容器，这些动态的Pod被替换的时候,Kube proxy时刻追踪着，所以，服务的 IP地址（dns名称），从来不变。 所有Kubernetes中的资源，比如Pod,都通过一个叫URI的东西来区分，这个URI有一个UID,URI的重要组成部分是：对象的类型（比如pod），对象的名字，对象的命名空间，对于特殊的对象类型，在同一个命名空间内，所有的名字都是不同的，在对象只提供名称，不提供命名空间的情况下，这种情况是假定是默认的命名空间。UID是时间和空间上的唯一。 Hadoop图形化用户界面Hue Hue详细介绍 Hue 是运营和开发Hadoop应用的图形化用户界面。Hue程序被整合到一个类似桌面的环境，以web程序的形式发布，对于单独的用户来说不需要额外的安装。 大数据可视化工具Nanocubes Nanocubes 详细介绍 Nanocubes 是一个大数据可视化的工具，32Tb Twitter数据，在一台16GB内存的机器上流畅、交互式地可视化。 Hadoop集群监控工具HTools HTools详细介绍 HTools是一款专业的Hadoop管理工具，不管您是非专业IT人士，还是多年经验的技术人员，本工具都会为您提供优质的管理服务和轻松的操作过程， 释放无谓的工作压力，提高Hadoop的管理水平。我们以最权威的专家为您量身定做的Hadoop管理工具，本系统提供优秀的用户体验，让您能够轻松的管 理Hadoop集群环境。 友善的向导式操作流程图形报表、日志分析供您明了查看各节点使用情况智能诊断,修复故障并发出短信、邮件故障告警图形化UI、拖拖拽拽即可管理管理HDFS数据傻瓜式操作优化Hadoop,方便快捷 免客户端部署,无需安装HTools客户端版本控制灵活,不绑定Hadoop的JDK版本一键智能搜索当前网段可部署节点支持多个Hadoop集群同时监管 支持同时管理多个Hadoop集群和节点支持7 &times; 24小时多集群实时监控支持节点热插拔,服务不间断的情况下随时对节点进行扩展和调整支持系统配置文件的推送和同步 大数据查询引擎PrestoDB PrestoDB详细介绍 Presto是Facebook最新研发的数据查询引擎，可对250PB以上的数据进行快速地交互式分析。据称该引擎的性能是 Hive 的 10 倍以上。 PrestoDB 是 Facebook 推出的一个大数据的分布式 SQL 查询引擎。可对从数 G 到数 P 的大数据进行交互式的查询，查询的速度达到商业数据仓库的级别。 Presto 可以查询包括 Hive、Cassandra 甚至是一些商业的数据存储产品。单个 Presto 查询可合并来自多个数据源的数据进行统一分析。 Presto 的目标是在可期望的响应时间内返回查询结果。Facebook 在内部多个数据存储中使用 Presto 交互式查询，包括 300PB 的数据仓库，超过 1000 个 Facebook 员工每天在使用 Presto 运行超过 3 万个查询，每天扫描超过 1PB 的数据。此外包括 Airbnb 和 Dropbox 也在使用 Presto 产品。 Presto 是一个分布式系统，运行在集群环境中，完整的安装包括一个协调器 (coordinator) 和多个 workers。查询通过例如 Presto CLI 的客户端提交到协调器，协调器负责解析、分析和安排查询到不同的 worker 上执行。 此外，Presto 需要一个数据源来运行查询。当前 Presto 包含一个插件用来查询 Hive 上的数据，要求： Hadoop CDH4 远程 Hive metastore service Presto 不使用 MapReduce ，只需要 HDFS 大数据批处理和流处理标准Apache Beam Apache Beam详细介绍 Apache Beam 是 Apache 软件基金会越来越多的数据流项目中最新增添的成员，是 Google 在2016年2月份贡献给 Apache 基金会的孵化项目。 这个项目的名称表明了设计：结合了批处理（Batch）模式和数据流（Stream）处理模式。它基于一种统一模式，用于定义和执行数据并行处理管道（pipeline），这些管理随带一套针对特定语言的SDK用于构建管道，以及针对特定运行时环境的Runner用于执行管道。 Apache Beam 的主要目标是统一批处理和流处理的编程范式，为无限，乱序，web-scale的数据集处理提供简单灵活，功能丰富以及表达能力十分强大的SDK。Apache Beam项目重点在于数据处理的编程范式和接口定义，并不涉及具体执行引擎的实现，Apache Beam希望基于Beam开发的数据处理程序可以执行在任意的分布式计算引擎上。 安全大数据分析框架OpenSOC OpenSOC详细介绍 OpenSOC：安全大数据分析框架。OpenSOC已经加入Apache工程改名为Apache Metron。 思科在 BroCON 大会上亮相了其安全大数据分析架构 OpenSOC，引起了广泛关注。OpenSOC 是一个针对网络包和流的大数据分析框架，它是大数据分析与安全分析技术的结合, 能够实时的检测网络异常情况并且可以扩展很多节点，它的存储使用开源项目 Hadoop，实时索引使用开源项目 ElasticSearch，在线流分析使用著名的开源项目 Storm。OpenSOC 概念性体系架构如下图所示: OpenSOC 主要功能包括： 可扩展的接收器和分析器能够监视任何Telemetry数据源 是一个扩展性很强的框架，且支持各种Telemetry数据流 支持对Telemetry数据流的异常检测和基于规则实时告警 通过预设时间使用Hadoop存储Telemetry的数据流 支持使用ElasticSearch实现自动化实时索引Telemetry数据流 支持使用Hive利用SQL查询存储在Hadoop中的数据 能够兼容ODBC/JDBC和继承已有的分析工具 具有丰富的分析应用,且能够集成已有的分析工具 支持实时的Telemetry搜索和跨Telemetry的匹配 支持自动生成报告、和异常报警 支持原数据包的抓取、存储、重组 支持数据驱动的安全模型 OpenSOC 官方文档介绍了以下五大优点： 由思科全力支持，适用于内部多用户 免费、开源、基于Apache协议授权 基于高可扩展平台（Hadoop、Kafka、Storm）实现 基于可扩展的插件式设计 具有灵活的部署模式，可在企业内部部署或者云端部署 具有集中化的管理流程、人员和数据 实时网络安全监测框架Apache Metron Apache Metron详细介绍 Apache Metron 是一个网络安全的实时数据处理、分析、查询、可视化框架。 Metron 集成了各种开源大数据技术，为安全监控和分析提供了集中工具。 Metron 拥有支持大规模摄取、处理、检索与信息可视化的所有适当元素，一些关键的网络数据将推动数据保护、监控、分析与检测，并且有助于对恶意的非法行为予以回应。 亮点包括： 捕获、存储和规范化所有类型的安全机制; 高速远程检测; 实时处理和应用改进; 高效信息存储; 提供通过系统传递的数据和警报的集中视图的接口 使用统计摘要数据结构，即使在最大的数据集上也可执行安全分析 企业级流式计算引擎JStorm JStorm 详细介绍 JStorm 是参考 Apache Storm 实现的实时流式计算框架，在网络IO、线程模型、资源调度、可用性及稳定性上做了持续改进，已被越来越多企业使用。JStorm 可以看作是 storm 的java增强版本，除了内核用纯java实现外，还包括了thrift、python、facet ui。从架构上看，其本质是一个基于zk的分布式调度系统 JStorm 的性能是Apache Storm 的4倍， 可以自由切换行模式或 mini-batch 模式：]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>平台</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之常见面试题]]></title>
    <url>%2F2020%2F04%2F04%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[大数据–面试题一览[20180524] 阐述HDFS生成文件的过程 Hadoop有哪些优化，调优点 阐述对Hive分区的理解 Hive分桶 用Spark干过什么 你们公司生产的集群规模 懂不懂CDH [20180508] 七牛云面试题 快排 hive和hdfs之间的联系 inode和文件描述符 linux指令如何创建文件 http中header中放入key value 有什么变化 系统调用和库函数区别 http缓冲实现机智 session cookie 区别 进程间通信方式 jsp本质 http请求状状态 get post put remove 数据库join 数据库引擎 hibernate和mybiters区别 jvm垃圾回收 hive和关系型数据库区别 hive实现原理 spark与mr的区别 [20180502] 二三四五面试题 画图讲解Spark工作流程，以及在集群上和各个角色的对应关系 Spark Streaming程序代码更新后如何操作 在一个电商网站中，设计一个订单ID生成方案 spark-submit如何引入外部jar包 Spark对于OOM从什么角度下手调整 org.apache.spark.SparkExectption:Task not serializable，这个错误是什么意思？如何解决？哪些场景会出现这错误？ [20180427] 面试题 有10个文件，每个文件1G，每个文件的每一行存放的都是用户的query，每个文件的query都可能重复。要求你按照query的频度排序 有一个1G大小的一个文件，里面每一行是一个词，词的大小不超过16字节，内存限制大小是1M。返回频数最高的100个词 [20180426] 美图二面 ThriftServer的HA如何去实现，能说下实现的思路嘛 说下Zookeeper的watch机制是如何实现的嘛？ 场景题： 现在有1个client，2个server，当我动态加入一台机器，或者删除一台机器，或者某台机器宕机了，client该如何去感知到，说下实现思路(不使用Zookeeper) 如何通信，说说具体实现 [20180425] 蚂蚁金服编程题 编程题A：求一个整数的平方根，不保留小数。 编程题B： 1.年/月/日/xxx.jpg ， 文件夹以 这个形式组织。 2.新建文件夹，将所有jpg文件拷贝到该文件夹，更名为 年_月_日_xxx.jpg。 3.监控文件夹，如果有增加的jpg文件，自动同步到新文件夹。 4.jpg文件只增加不删除。 [20180424] 成都某公司面试题 谈谈你对HDFS的了解 Hadoop2.0做了哪些改动 Spark与MR的区别在哪里 知道除了Spark之外的大数据处理框架嘛 Spark shuffle，说说 StringBuilder与StringBuffer的区别 HashMap与Hashtable的区别 二叉树的数据结构是什么样的 数据库索引的实现原理 jvm垃圾收集器，挑一种讲讲 [20180423] 美图面试题 为什么选择美图，你知道美图地点在哪里嘛 介绍下你做的项目吧 数据统一管理平台，我挺感兴趣的，你说说吧 我大概知道是怎么回事了，java web这块你参与开发了吗 你刚刚项目提到了元数据，你能说说hive的元数据管理嘛，对它了解嘛 还是hive，你对hive有哪些原理性了解呢 知道AST、operator tree这些长什么样吗 那你的hive转mr过程是怎么了解的呢？ 除了谓词下推，还能说说其它的优化嘛？别说数据倾斜的调优 jvm了解不，说下垃圾收集算法 平常用java和scala语言哪个多点 如果我现在要使用map集合，你觉得哪种适合多线程情况下进行访问 如何去监控线程 Spark 出现OOM，你觉得该怎么进行调优呢？不去动jvm的参数 你觉得join该怎么优化 你对未来的规划是什么？(五年内) 你也就是走技术路线咯 [20180421] 北京3+家面试题hadoop面试:1、hadoop集群、namenode如何做到数据同步？2、hdfs副本存放策略3、HA如何在挂掉一台namenode节点的状态下，自动切换到另一台？4、mapreduce shuffle过程5、mapreduce优化 flume面试:1、你能二次源码修改支持parquent格式吗？ sqoop面试:1、抽取某个数据库下的某张表+条件 怎么抽取?2、sqoop增量导入 hbase面试:1、rowkey如何设计2、hbase热点问题3、协处理器4、hbase优化5、hbase的二级索引 hive面试:1、数据倾斜2、hive能加索引吗？ spark面试:1、rdd dataset dataframe 概念2、mapflat3、spark资源分配 kafka面试:1、怎么保证数据零丢失?和spark streaming结合说说看？2、怎么解决数据重复问题？3、某个kafka节点挂掉对生产和消费有影响吗？4、生产大于消费 lag产生大量的滞后怎么解决？ 数据库面试:1、btree2、索引3、拉链表 shell面试:1、如何查找在Linux目录下的某个文本里的包含相关内容的操作? [20180420] 蚂蚁金服面试题 小文件的合并 MR与Spark的区别 关注哪些名人的博客 对大数据领域有什么自己的见解 平常怎么学习大数据的 StringBuilder与StringBuffer的区别 HashMap与Hashtable的区别 谈谈你对树的理解 数据库索引的实现 jvm的内存模型 jvm的垃圾收集器 jvm的垃圾收集算法 HDFS架构 HDFS读写流程 Hadoop3.0做了哪些改进 谈谈YARN 为什么项目选择使用Spark，你觉得Spark的优点在哪里 了解Flink与Storm嘛，他们与Spark Streaming的区别在哪里 1TB文件，取重复的词，top5指定的资源的场景下，如何快速统计出来 [20180419] 网易大数据面试题 说说项目 Spark哪部分用得好，如何调优 Java哪部分了解比较好 聊聊并发，并发实现方法，volatile关键字说说 HashMap的底层原理 为什么要重写hashcode和equals 说说jvm 各个垃圾收集器运用在什么情形 jvm调优 说说io 为什么考虑转行呢？是因为原专业不好就业吗？ [20180418] 数据挖掘面试题 Java字符串拼接StringBuffer和+=区别 Scala map和foreach区别 Spark groupByKey和reduceByKey区别 Spark将数据写MySQL要注意什么 Spark repartition和coalesce函数的区别 梯度下降、随机梯度下降、mini batch 梯度下降的区别 SVM原理 SVM中为什么要转成对偶问题 SVM在分类时怎么选择合适的核函数 特征共线性问题 Hive外表和内表的区别 求解字符串的所有的回文子串 贝叶斯定理 人员画像 推荐系统 svd knn [20180417] 自我介绍 最近一个项目的架构，你所负责的模块 谈谈你对Spark的理解 在这个项目中，你觉得你做的模板中出彩的地方与哪些 Spark作业提交的流程 在工作中使用Spark遇到了哪些问题，如何解决的，请举3个例子 谈谈你对JVM的了解]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之博客推荐]]></title>
    <url>%2F2020%2F03%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%8D%9A%E5%AE%A2%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[大数据–博客一览大数据零基础： Linux And Shell:零基础大数据入门【free视频】 1.VMware Workstation9 下安装 CentOS6.5( 安装图文教程 )2.Linux最常用命令及快捷键整理3.配置多台机器SSH相互通信信任 4.Memory参数，你真的懂吗?5.yum安装xxx包时出错，提示No package xxx available.6.CentOS6.x使用163和epel yum源的选择7.Centos6.5 python2.6.6 升级到2.7.58.CentOS清理swap和buffer/cache 9.记录在shell脚本中使用sudo echo x &gt; 时,抛Permission denied错误 10.Linux系统重要参数调优，你知道吗 11.大数据之必会的Linux命令 DataBase And SQL:Hadoop:1.Hadoop2.8.1全网最详细编译2.Hadoop全网最详细的伪分布式部署(HDFS)3.Hadoop全网最详细的伪分布式部署(MapReduce+Yarn)4.Hadoop常用命令大全015.Hadoop-2.7.2+zookeeper-3.4.6完全分布式环境搭建(HDFS、YARN HA)6.Hadoop2.x 参数汇总7.YARN的Memory和CPU调优配置详解8.资源调度yarn之生产详解 9.fsimage？editlog？这些都是什么？？ 10.你真的了解jps命令吗 11.Hadoop HA 的配置，你了解吗？ 12.Hadoop之Yarn架构设计(command memory cpu) 13.HDFS之垃圾回收箱配置及使用 Zookeeper:Hive:Hive应用实战课程【buy视频】 1.Hive全网最详细的编译及部署2.Hive DDL，你真的了解吗？3.Hive自定义函数(UDF)的编程开发，你会吗？4.Hive自定义函数(UDF)的部署使用，你会吗？5. 2min快速了解，Hive内部表和外部表 6. 5min掌握，Hive的HiveServer2 和JDBC客户端&amp;代码的生产使用7.生产中Hive静态和动态分区表，该怎样抉择呢？ 8.Hive中自定义UDAF函数生产小案例 9.从Hive中的stored as file_foramt看hive调优 10.你真的了解 Hive 的元数据吗？ [11.hive实战 (https://blog.csdn.net/liweihope/article/details/88584985) SQOOP:Sqoop应用实战课程【buy视频】 大数据进阶：Compress And Storage Format:1.大数据压缩格式，你们真的了解吗？2.Hive压缩格式的生产应用 3.大数据存储格式，你们真的了解吗？4.Hive存储格式的生产应用 5.Hive生产上，压缩和存储结合使用案例 Flume:Kafka:批处理ETL已亡，Kafka才是数据处理的未来【buy视频】 Scala:Spark:Spark零基础实战【free视频】 1.Spark2.2.0 全网最详细的源码编译2.Spark-2.2.0-bin-2.6.0-cdh5.12.1.tgz 编译方法总结！ 3.生产改造Spark1.6源代码，create table语法支持Oracle列表分区4.Spark History Server Web UI配置5.Spark on YARN-Cluster和YARN-Client的区别 6.Spark RDD、DataFrame和DataSet的区别 7.Spark RDD、DataFrame和DataSet的区别 8.Spark不得不理解的重要概念——从源码角度看RDD 9.Spark 基本概念 10.Spark调优的关键之——RDD Cache缓存使用详解 11.Spark之序列化在生产中的应用 12.还不收藏？Spark动态内存管理源码解析！ 13.Spark SQL 外部数据源（External DataSource） 14.你大爷永远是你大爷，RDD血缘关系源码详解！ 15.Apache Spark 技术团队开源机器学习平台 MLflow 16.生产开发必用-Spark RDD转DataFrame的两种方法 17.最前沿！带你读Structured Streaming重量级论文！ 18.Apache Spark和DL/AI结合，谁与争锋? 期待Spark3.0的到来！ 19.又又又是源码！RDD 作业的DAG是如何切分的？ 20.Spark Streaming 状态管理函数，你了解吗 21.Spark序列化，你了解吗 Flink:1.数据Flink实战系列2.最全的Flink部署及开发案例(KafkaSource+SinkToMySQL) Phoenix:HBase:Kudu:Storm:Hue:Azkaban:全网唯一Azkaban3.x应用实战【buy视频】 Rundeck:Docker:Harbor:Kubernetes:Python:PyTorch 1.0宣布用于研究和生产AI项目 1.Python核心笔记（一） 2.Python核心笔记（二） 3.Pandas数据分析入门（一） 4.Pandas数据分析入门（二） 5.Kaggle入门经典：Titanic生还预测 7.Titanic生还预测（一）构建基本模型 7.Titanic生还预测（二） 8.Titanic生还预测（三） 9.Titanic生还预测（四） 10. Titanic生还预测（五） Spark MLlib:TensorFlow:实时同步中间件:大数据之实时数据源同步中间件–生产上Canal与Maxwell颠峰对决 Java:1.Java可扩展线程池之ThreadPoolExecutor 2.面试常考点-Java线程池之拒绝策略 3.再谈单例设计模式 4.Java类加载方式你知道几种？ Github:如何将我们谱写的代码供凡人瞻仰 生产项目： 线上项目: 承诺企业生产项目，而不是那种pv,uv网上搜搜的项目1.Spark实时分析预警平台(架构+提交流程+现场排错)【free视频】2.Spark实时分析预警平台项目(在进阶班课表)3.Strom互联网金融实时计算与分析项目(在进阶班课表)4.构建企业级PaaS平台项目(在进阶班课表) 线下项目: 承诺上课是直接VPN公司生产环境，直接生产环境讲解生产项目1.基于Spark的某互联网直播平台大数据分析项目实战第3季，正在报名！单击查看前2季的目录.2.线下班生产项目第10期，国庆节线下4天课程，正在报名！ 大数据平台运维： CDH 入门:CDH离线部署和暴力卸载、Kerberos【free视频】 1.CDH下载各种软件包2.CDH4/5集群正确启动和停止顺序3.CDH5 快速入门手册v1.0(体系架构+目录详解)4.CDH4/5配置文件之深度解析5.CDH5之Trash CDH 案例:1.记录一次帮网友解决CDH集群机器的时钟偏差2.CDH集群机器,安装多个CDH版,会出现命令找不到,如hadoop,hdfs等等3.CDH5.8.2安装之Hash verification failed4.记录CDH Spark2的spark2-submit的一个No such file or directory问题5.记录CDH5.10一个clients.NetworkClient: Bootstrap broker ip:9092 disconnected问题 6.记录自定义kafka的parcel库,CDH安装kafka服务,无法安装过去的排雷过程7.记录CDH安装的一个坑:could not contact scm server at localhost:7182, giving up8.CDH5之Found class jline.Terminal, but interface was expected9.CDH5之Exhausted available authentication methods10.CDH5之Unexpected error.Unable to verify database connection 11.生产CDH5配置lzo]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据完整实战视频教程]]></title>
    <url>%2F2020%2F03%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E5%AE%9E%E6%88%98%E8%A7%86%E9%A2%91%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[Big-Data-ProjectHadoop2.x、Zookeeper、Flume、Hive、Hbase、Kafka、Spark2.x、SparkStreaming、MySQL、Hue、J2EE、websoket、Echarts 项目名称：新闻日志大数据处理系统项目简介目标 1、完成大数据项目的架构设计，安装部署，架构继承与开发、用户可视化交互设计 2、完成实时在线数据分析 3、完成离线数据分析 具体功能 1）捕获用户浏览日志信息 2）实时分析前20名流量最高的新闻话题 3）实时统计当前线上已曝光的新闻话题 4）统计哪个时段用户浏览量最高 5）报表 项目技术点Hadoop2.x、Zookeeper、Flume、Hive、Hbase Kafka、Spark2.x、SparkStreaming MySQL、Hue、J2EE、websoket、Echarts 开发工具虚拟机： VMware、centos 虚拟机ssh: SecureCRT（在windows上链接多个虚拟机） 修改源码：idea 查看各种数据：notepad++（安装NppFTP插件，修改虚拟机中配置文件，好用的一批） 所有软件下载地址： 链接：https://pan.baidu.com/s/1aF_VmdXJVIjeB0WzAtfeEQ 提取码：cuao 项目架构图片来自于卡夫卡公司 集群资源规划利用VMware虚拟机+centos完成，基本要求笔记本电脑内存在8G以上。最低要去克隆出3台虚拟机，每台给2G内存。 项目实现步骤1、第一章：项目需求分析与设计 https://www.willxu.xyz/2018/12/19/project/1%E3%80%81%E9%A1%B9%E7%9B%AE%E9%9C%80%E6%B1%82/ 2、第二章：linux环境准备与设置 https://www.willxu.xyz/2018/12/19/project/2%E3%80%81linux%E9%85%8D%E7%BD%AE/ 3、第三章：Hadoop2.X分布式集群部署 https://www.willxu.xyz/2018/12/19/project/3%E3%80%81hadoop%E9%83%A8%E7%BD%B2/ 4、第四章：Zookeeper分布式集群部署 https://www.willxu.xyz/2018/12/29/project/4%E3%80%81zk%E9%83%A8%E7%BD%B2/ 5、第五章：hadoop的高可用配置（HA） https://www.willxu.xyz/2018/12/29/project/5%E3%80%81ha%E5%AE%9E%E7%8E%B0/ 6、第六章：hadoop的HA下的高可用HBase部署 https://www.willxu.xyz/2018/12/30/project/6%E3%80%81hbase%E9%83%A8%E7%BD%B2/ 7、第七章：Kafka简介和分布式部署 https://www.willxu.xyz/2019/01/01/project/7%E3%80%81kafka%E9%83%A8%E7%BD%B2/ 8、第八章：Flume简介和分布式部署 https://www.willxu.xyz/2019/01/01/project/8%E3%80%81flume%E9%83%A8%E7%BD%B2/ 9、第九章：Flume源码修改与HBase+Kafka集成 https://www.willxu.xyz/2019/01/20/project/9%E3%80%81flume-hbase-kfk%E9%85%8D%E7%BD%AE/ 10、第十章：Flume+HBase+Kafka集成全流程测试 https://www.willxu.xyz/2019/01/20/project/10%E3%80%81flume-hbase-kfk%E8%81%94%E8%B0%83/ 11、第十一章：mysql、Hive安装与集成 https://www.willxu.xyz/2019/01/22/project/11%E3%80%81mysql-hive/ 12、第十二章：Hive与Hbase集成 https://www.willxu.xyz/2019/01/23/project/12%E3%80%81hive-hbase/ 13、第十三章：Cloudera HUE大数据可视化分析 https://www.willxu.xyz/2019/01/26/project/13%E3%80%81hue/ 14、第十四章：Spark2.X集群安装与spark on yarn部署 https://www.willxu.xyz/2019/01/30/project/14%E3%80%81spark%20on%20yarn/ 15、第十五章：基于IDEA环境下的Spark2.X程序开发 https://www.willxu.xyz/2019/01/30/project/15%E3%80%81spark-idea/ 16、第十六章：Spark Streaming实时数据处理 https://www.willxu.xyz/2019/02/03/project/16%E3%80%81spark-streaming1/ 项目配套视频链接：https://pan.baidu.com/s/1-PQta6SCgps91oFNTkl6Qg 提取码：sh8x]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试之Zookeeper]]></title>
    <url>%2F2020%2F03%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E4%B9%8BZookeeper%2F</url>
    <content type="text"><![CDATA[zookeeper是什么,有什么功能Zookeeper 是 一个典型的分布式数据一致性的解决方案. Zookeeper的典型应用场景: 数据发布/订阅 负载均衡 命名服务 分布式协调/通知 集群管理 Master 分布式锁 分布式队列 zk 有几种部署模式zookeeper有两种运行模式: 集群模式和单机模式,还有一种伪集群模式,在单机模式下模拟集群的zookeeper服务 zk是怎样保证主从节点的状态同步zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 zab 协议。 zab 协议有两种模式，分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，zab 就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。 说一下zk的通知机制客户端端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些客户端会收到 zookeeper 的通知，然后客户端可以根据 znode 变化来做出业务上的改变 zk的分布式锁实现方式使用zookeeper实现分布式锁的算法流程，假设锁空间的根节点为/lock： 客户端连接zookeeper，并在/lock下创建临时的且有序的子节点，第一个客户端对应的子节点为/lock/lock-0000000000，第二个为/lock/lock-0000000001，以此类推。 客户端获取/lock下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁，否则监听刚好在自己之前一位的子节点删除消息，获得子节点变更通知后重复此步骤直至获得锁； 执行业务代码； 完成业务流程后，删除对应的子节点释放锁。 参考文章 zk 采用的哪种分布式一致性协议? 还有哪些分布式一致性协议常见的分布式一致性协议有: 两阶段提交协议，三阶段提交协议，向量时钟，RWN协议，paxos协议，Raft协议. zk采用的是paxos协议. 两阶段提交协议(2PC)两阶段提交协议，简称2PC，是比较常用的解决分布式事务问题的方式，要么所有参与进程都提交事务，要么都取消事务，即实现ACID中的原子性(A)的常用手段。 三阶段提交协议(3PC)3PC就是在2PC基础上将2PC的提交阶段细分位两个阶段：预提交阶段和提交阶段 向量时钟通过向量空间祖先继承的关系比较, 使数据保持最终一致性,这就是向量时钟的基本定义。 NWR协议NWR是一种在分布式存储系统中用于控制一致性级别的一种策略。在Amazon的Dynamo云存储系统中，就应用NWR来控制一致性。让我们先来看看这三个字母的含义：N：在分布式存储系统中，有多少份备份数据W：代表一次成功的更新操作要求至少有w份数据写入成功R： 代表一次成功的读数据操作要求至少有R份数据成功读取NWR值的不同组合会产生不同的一致性效果，当W+R&gt;N的时候，整个系统对于客户端来讲能保证强一致性。当W+R 以常见的N=3、W=2、R=2为例：N=3，表示，任何一个对象都必须有三个副本（Replica），W=2表示，对数据的修改操作（Write）只需要在3个Replica中的2个上面完成就返回，R=2表示，从三个对象中要读取到2个数据对象，才能返回。在分布式系统中，数据的单点是不允许存在的。即线上正常存在的Replica数量是1的情况是非常危险的，因为一旦这个Replica再次错误，就 可能发生数据的永久性错误。假如我们把N设置成为2，那么，只要有一个存储节点发生损坏，就会有单点的存在。所以N必须大于2。N约高，系统的维护和整体 成本就越高。工业界通常把N设置为3。当W是2、R是2的时候，W+R&gt;N，这种情况对于客户端就是强一致性的。 paxos协议架构师需要了解的Paxos原理，历程及实践 Raft协议Raft协议的动画 参考文章 讲一下leader 选举过程 这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。选举过程如下 (1) 每个Server发出一个投票。由于是初始情况，Server1和Server2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。 (2) 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。 (3) 处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下 · 优先检查ZXID。ZXID比较大的服务器优先作为Leader。 · 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。 对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。 (4) 统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。 (5) 改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。 Leader选举算法分析在3.4.0后的Zookeeper的版本只保留了TCP版本的FastLeaderElection选举算法。当一台机器进入Leader选举时，当前集群可能会处于以下两种状态 · 集群中已经存在Leader。 · 集群中不存在Leader。 对于集群中已经存在Leader而言，此种情况一般都是某台机器启动得较晚，在其启动之前，集群已经在正常工作，对这种情况，该机器试图去选举Leader时，会被告知当前服务器的Leader信息，对于该机器而言，仅仅需要和Leader机器建立起连接，并进行状态同步即可。而在集群中不存在Leader情况下则会相对复杂，其步骤如下 (1) 第一次投票。无论哪种导致进行Leader选举，集群的所有机器都处于试图选举出一个Leader的状态，即LOOKING状态，LOOKING机器会向所有其他机器发送消息，该消息称为投票。投票中包含了SID（服务器的唯一标识）和ZXID（事务ID），(SID, ZXID)形式来标识一次投票信息。假定Zookeeper由5台机器组成，SID分别为1、2、3、4、5，ZXID分别为9、9、9、8、8，并且此时SID为2的机器是Leader机器，某一时刻，1、2所在机器出现故障，因此集群开始进行Leader选举。在第一次投票时，每台机器都会将自己作为投票对象，于是SID为3、4、5的机器投票情况分别为(3, 9)，(4, 8)， (5, 8)。 (2) 变更投票。每台机器发出投票后，也会收到其他机器的投票，每台机器会根据一定规则来处理收到的其他机器的投票，并以此来决定是否需要变更自己的投票，这个规则也是整个Leader选举算法的核心所在，其中术语描述如下 · vote_sid：接收到的投票中所推举Leader服务器的SID。 · vote_zxid：接收到的投票中所推举Leader服务器的ZXID。 · self_sid：当前服务器自己的SID。 · self_zxid：当前服务器自己的ZXID。 每次对收到的投票的处理，都是对(vote_sid, vote_zxid)和(self_sid, self_zxid)对比的过程。 规则一：如果vote_zxid大于self_zxid，就认可当前收到的投票，并再次将该投票发送出去。 规则二：如果vote_zxid小于self_zxid，那么坚持自己的投票，不做任何变更。 规则三：如果vote_zxid等于self_zxid，那么就对比两者的SID，如果vote_sid大于self_sid，那么就认可当前收到的投票，并再次将该投票发送出去。 规则四：如果vote_zxid等于self_zxid，并且vote_sid小于self_sid，那么坚持自己的投票，不做任何变更。 结合上面规则，给出下面的集群变更过程。 ​ (3) 确定Leader。经过第二轮投票后，集群中的每台机器都会再次接收到其他机器的投票，然后开始统计投票，如果一台机器收到了超过半数的相同投票，那么这个投票对应的SID机器即为Leader。此时Server3将成为Leader。 由上面规则可知，通常那台服务器上的数据越新（ZXID会越大），其成为Leader的可能性越大，也就越能够保证数据的恢复。如果ZXID相同，则SID越大机会越大。 参考文章]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试之Kafka]]></title>
    <url>%2F2020%2F03%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E4%B9%8Bkafka%2F</url>
    <content type="text"><![CDATA[讲一下kafka 的架构 Producer：消息生产者 Producer可以发送消息到Topic Topic的消息存放在不同Partition中，不同Partition存放在不同Broker中 Producer只需要指定Topic的名字、要连接到的Broker，这样Kafka就可以自动地把消息数据路由到合适的Broker（不一定是指定连接的Broker） Producer发送消息后，可以选择是否要确认消息写入成功（ACK，Acknowledgment） ACK=0：Producer不会等待ACK（消息可能丢失） ACK=1：Producer会等待Leader Partition的ACK（Follower Partition消息可能丢失） ACK=all：Producer会等待Leader Partition和Follower Partition的ACK（消息不会丢失） 消息key：Producer可以给消息加上key，带相同key的消息会被分发到同一个Partition，这样就可以保证带相同key的消息的消费是有序的 Broker：每个Broker里包含了不同Topic的不同Partition，Partition中包含了有序的消息 一个Kafka集群由多个Broker（server）组成 每个Broker都有ID标识 每个Broker里保存一定数量的Partition 客户端只要连接上任意一个Broker，就可以连接上整个Kafka集群 大多数Kafka集群刚开始的时候建议使用至少3个Broker，集群大了可以有上百个Broker Consumer：消息消费者 Consumer可以从Topic读取消息进行消费 Topic的消息存放在不同Partition中，不同Partition存放在不同Broker中 Consumer只需要指定Topic的名字、要连接到的Broker，这样Kafka就可以自动地把Consumer路由到合适的Broker拉取消息进行消费（不一定是指定连接的Broker） 每一个Partition中的消息都会被有序消费 Consumer Group： Consumer Group由多个Consumer组成 Consumer Group里的每个Consumer都会从不同的Partition中读取消息 如果Consumer的数量大于Partition的数量，那么多出来的Consumer就会空闲下来（浪费资源） Consumer offset： Kafka会为Consumer Group要消费的每个Partion保存一个offset，这个offset标记了该Consumer Group最后消费消息的位置 这个offset保存在Kafka里一个名为“__consumer_offsets”的Topic中；当Consumer从Kafka拉取消息消费时，同时也要对这个offset提交修改更新操作。这样若一个Consumer消费消息时挂了，其他Consumer可以通过这个offset值重新找到上一个消息再进行处理 参考文章 kafka 与其他消息组件对比推荐阅读文章 特性 ActiveMQ RabbitMQ RocketMQ Kafka 单机吞吐量 万级，比 RocketMQ、Kafka 低一个数量级 同 ActiveMQ 10 万级，支撑高吞吐 10 万级，高吞吐，一般配合大数据类的系统来进行实时数据计算、日志采集等场景 topic 数量对吞吐量的影响 topic 可以达到几百/几千的级别，吞吐量会有较小幅度的下降，这是 RocketMQ 的一大优势，在同等机器下，可以支撑大量的 topic topic 从几十到几百个时候，吞吐量会大幅度下降，在同等机器下，Kafka 尽量保证 topic 数量不要过多，如果要支撑大规模的 topic，需要增加更多的机器资源 时效性 ms 级 微秒级，这是 RabbitMQ 的一大特点，延迟最低 ms 级 延迟在 ms 级以内 可用性 高，基于主从架构实现高可用 同 ActiveMQ 非常高，分布式架构 非常高，分布式，一个数据多个副本，少数机器宕机，不会丢失数据，不会导致不可用 消息可靠性 有较低的概率丢失数据 基本不丢 经过参数优化配置，可以做到 0 丢失 同 RocketMQ 功能支持 MQ 领域的功能极其完备 基于 erlang 开发，并发能力很强，性能极好，延时很低 MQ 功能较为完善，还是分布式的，扩展性好 功能较为简单，主要支持简单的 MQ 功能，在大数据领域的实时计算以及日志采集被大规模使用 kafka 实现高吞吐的原理 读写文件依赖OS文件系统的页缓存，而不是在JVM内部缓存数据，利用OS来缓存，内存利用率高 sendfile技术（零拷贝），避免了传统网络IO四步流程 支持End-to-End的压缩 顺序IO以及常量时间get、put消息 Partition 可以很好的横向扩展和提供高并发处理 参考文章1 参考文章2 kafka怎样保证不重复消费此问题其实等价于保证消息队列消费的幂等性 主要需要结合实际业务来操作: 比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update 一下好吧。 比如你是写 Redis，那没问题了，反正每次都是 set，天然幂等性。 比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的 id，类似订单 id 之类的东西，然后你这里消费到了之后，先根据这个 id 去比如 Redis 里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个 id 写 Redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。 比如基于数据库的唯一键来保证重复数据不会重复插入多条。因为有唯一键约束了，重复数据插入只会报错，不会导致数据库中出现脏数据。 参考文章 kafka怎样保证不丢失消息消费端弄丢了数据唯一可能导致消费者弄丢数据的情况，就是说，你消费到了这个消息，然后消费者那边自动提交了 offset，让 Kafka 以为你已经消费好了这个消息，但其实你才刚准备处理这个消息，你还没处理，你自己就挂了，此时这条消息就丢咯。 这不是跟 RabbitMQ 差不多吗，大家都知道 Kafka 会自动提交 offset，那么只要关闭自动提交 offset，在处理完之后自己手动提交 offset，就可以保证数据不会丢。但是此时确实还是可能会有重复消费，比如你刚处理完，还没提交 offset，结果自己挂了，此时肯定会重复消费一次，自己保证幂等性就好了。 生产环境碰到的一个问题，就是说我们的 Kafka 消费者消费到了数据之后是写到一个内存的 queue 里先缓冲一下，结果有的时候，你刚把消息写入内存 queue，然后消费者会自动提交 offset。然后此时我们重启了系统，就会导致内存 queue 里还没来得及处理的数据就丢失了。 Kafka 弄丢了数据这块比较常见的一个场景，就是 Kafka 某个 broker 宕机，然后重新选举 partition 的 leader。大家想想，要是此时其他的 follower 刚好还有些数据没有同步，结果此时 leader 挂了，然后选举某个 follower 成 leader 之后，不就少了一些数据？这就丢了一些数据啊。 生产环境也遇到过，我们也是，之前 Kafka 的 leader 机器宕机了，将 follower 切换为 leader 之后，就会发现说这个数据就丢了。 所以此时一般是要求起码设置如下 4 个参数： 给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。 在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。 在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。 在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。 我们生产环境就是按照上述要求配置的，这样配置之后，至少在 Kafka broker 端就可以保证在 leader 所在 broker 发生故障，进行 leader 切换时，数据不会丢失。 生产者会不会弄丢数据？如果按照上述的思路设置了 acks=all，一定不会丢，要求是，你的 leader 接收到消息，所有的 follower 都同步到了消息之后，才认为本次写成功了。如果没满足这个条件，生产者会自动不断的重试，重试无限次。 参考文章 kafka 与 spark streaming 集成,如何保证 exactly once 语义 Spark Streaming上游对接kafka时保证Exactly OnceSpark Streaming使用Direct模式对接上游kafka。无论kafka有多少个partition， 使用Direct模式总能保证SS中有相同数量的partition与之相对， 也就是说SS中的KafkaRDD的并发数量在Direct模式下是由上游kafka决定的。 在这个模式下，kafka的offset是作为KafkaRDD的一部分存在，会存储在checkpoints中， 由于checkpoints只存储offset内容，而不存储数据，这就使得checkpoints是相对轻的操作。 这就使得SS在遇到故障时，可以从checkpoint中恢复上游kafka的offset，从而保证exactly once Spark Streaming输出下游保证Exactly once 第一种“鸵鸟做法”，就是期望下游（数据）具有幂等特性。 多次尝试总是写入相同的数据，例如，saveAs***Files 总是将相同的数据写入生成的文件 使用事务更新 所有更新都是事务性的，以便更新完全按原子进行。这样做的一个方法如下： 使用批处理时间(在foreachRDD中可用)和RDD的partitionIndex（分区索引）来创建identifier（标识符)。 该标识符唯一地标识streaming application 中的blob数据。 使用该identifier，blob 事务地更新到外部系统中。也就是说，如果identifier尚未提交，则以 (atomicall)原子方式提交分区数据和identifier。否则，如果已经提交，请跳过更新。 参考文章1 参考文章2 参考文章3 Ack 有哪几种, 生产中怎样选择?ack=0/1/-1的不同情况： Ack = 0 producer不等待broker的ack，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据； Ack = 1 producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据； Ack = -1 producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack，数据一般不会丢失，延迟时间长但是可靠性高。 生产中主要以 Ack=-1为主,如果压力过大,可切换为Ack=1. Ack=0的情况只能在测试中使用. 如何通过offset寻找数据如果consumer要找offset是1008的消息，那么， 1，按照二分法找到小于1008的segment，也就是00000000000000001000.log和00000000000000001000.index 2，用目标offset减去文件名中的offset得到消息在这个segment中的偏移量。也就是1008-1000=8，偏移量是8。 3，再次用二分法在index文件中找到对应的索引，也就是第三行6,45。 4，到log文件中，从偏移量45的位置开始（实际上这里的消息offset是1006），顺序查找，直到找到offset为1008的消息。查找期间kafka是按照log的存储格式来判断一条消息是否结束的。 参考文章 如何清理过期数据 删除log.cleanup.policy=delete启用删除策略 直接删除，删除后的消息不可恢复。可配置以下两个策略：清理超过指定时间清理：log.retention.hours=16 超过指定大小后，删除旧的消息：log.retention.bytes=1073741824为了避免在删除时阻塞读操作，采用了copy-on-write形式的实现，删除操作进行时，读取操作的二分查找功能实际是在一个静态的快照副本上进行的，这类似于Java的CopyOnWriteArrayList。 压缩将数据压缩，只保留每个key最后一个版本的数据。首先在broker的配置中设置log.cleaner.enable=true启用cleaner，这个默认是关闭的。在topic的配置中设置log.cleanup.policy=compact启用压缩策略。 如上图，在整个数据流中，每个Key都有可能出现多次，压缩时将根据Key将消息聚合，只保留最后一次出现时的数据。这样，无论什么时候消费消息，都能拿到每个Key的最新版本的数据。压缩后的offset可能是不连续的，比如上图中没有5和7，因为这些offset的消息被merge了，当从这些offset消费消息时，将会拿到比这个offset大的offset对应的消息，比如，当试图获取offset为5的消息时，实际上会拿到offset为6的消息，并从这个位置开始消费。这种策略只适合特俗场景，比如消息的key是用户ID，消息体是用户的资料，通过这种压缩策略，整个消息集里就保存了所有用户最新的资料。压缩策略支持删除，当某个Key的最新版本的消息没有内容时，这个Key将被删除，这也符合以上逻辑。 参考文章 1条message中包含哪些信息 Field Description Attributes 该字节包含有关消息的元数据属性。 最低的2位包含用于消息的压缩编解码器。 其他位应设置为0。 Crc CRC是消息字节的其余部分的CRC32。 这用于检查代理和使用者上的消息的完整性。 key是用于分区分配的可选参数。 key可以为null。 MagicByte 这是用于允许向后兼容的消息二进制格式演变的版本ID。 当前值为0。 Offset 这是kafka中用作日志序列号的偏移量。 当producer发送消息时，它实际上并不知道偏移量，并且可以填写它喜欢的任何值。 Value 该值是实际的消息内容，作为不透明的字节数组。 Kafka支持递归消息，在这种情况下，它本身可能包含消息集。 消息可以为null。 讲一下zookeeper在kafka中的作用 zk的作用主要有如下几点: kafka的元数据都存放在zk上面,由zk来管理 0.8之前版本的kafka, consumer的消费状态，group的管理以及 offset的值都是由zk管理的,现在offset会保存在本地topic文件里 负责borker的lead选举和管理 kafka 可以脱离 zookeeper 单独使用吗kafka 不能脱离 zookeeper 单独使用，因为 kafka 使用 zookeeper 管理和协调 kafka 的节点服务器。 kafka 有几种数据保留策略kafka 有两种数据保存策略：按照过期时间保留和按照存储的消息大小保留。 kafka同时设置了7天和10G清除数据,到第5天的时候消息到达了10G,这个时候kafka如何处理?这个时候 kafka 会执行数据清除工作，时间和大小不论那个满足条件，都会清空数据。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试之HBase]]></title>
    <url>%2F2020%2F03%2F08%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E4%B9%8BHBase%2F</url>
    <content type="text"><![CDATA[讲一下 Hbase 架构 Hbase主要包含HMaster/HRegionServer/Zookeeper HRegionServer 负责实际数据的读写. 当访问数据时, 客户端直接与RegionServer通信. HBase的表根据Row Key的区域分成多个Region, 一个Region包含这这个区域内所有数据. 而Region server负责管理多个Region, 负责在这个Region server上的所有region的读写操作. HMaster 负责管理Region的位置, DDL(新增和删除表结构) 协调RegionServer 在集群处于数据恢复或者动态调整负载时,分配Region到某一个RegionServer中 管控集群,监控所有Region Server的状态 提供DDL相关的API, 新建(create),删除(delete)和更新(update)表结构. Zookeeper 负责维护和记录整个Hbase集群的状态 zookeeper探测和记录Hbase集群中服务器的状态信息.如果zookeeper发现服务器宕机,它会通知Hbase的master节点. hbase 如何设计rowkey RowKey长度原则 Rowkey是一个二进制码流，Rowkey的长度被很多开发者建议说设计在10~100个字节，不过建议是越短越好，不要超过16个字节。 原因如下： 数据的持久化文件HFile中是按照KeyValue存储的，如果Rowkey过长比如100个字节，1000万列数据光Rowkey就要占用100*1000万=10亿个字节，将近1G数据，这会极大影响HFile的存储效率； MemStore将缓存部分数据到内存，如果Rowkey字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此Rowkey的字节长度越短越好。 目前操作系统是都是64位系统，内存8字节对齐。控制在16个字节，8字节的整数倍利用操作系统的最佳特性。 RowKey散列原则 如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个RegionServer上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。 RowKey唯一原则 必须在设计上保证其唯一性。 参考文章1 参考文章2 讲一下hbase的存储结构,这样的存储结构有什么优缺点 Hbase的优点及应用场景: 半结构化或非结构化数据:对于数据结构字段不够确定或杂乱无章非常难按一个概念去进行抽取的数据适合用HBase，因为HBase支持动态添加列。 记录很稀疏：RDBMS的行有多少列是固定的。为null的列浪费了存储空间。HBase为null的Column不会被存储，这样既节省了空间又提高了读性能。 多版本号数据：依据Row key和Column key定位到的Value能够有随意数量的版本号值，因此对于须要存储变动历史记录的数据，用HBase是很方便的。比方某个用户的Address变更，用户的Address变更记录也许也是具有研究意义的。 仅要求最终一致性：对于数据存储事务的要求不像金融行业和财务系统这么高，只要保证最终一致性就行。（比如HBase+elasticsearch时，可能出现数据不一致） 高可用和海量数据以及很大的瞬间写入量：WAL解决高可用，支持PB级数据，put性能高适用于插入比查询操作更频繁的情况。比如，对于历史记录表和日志文件。（HBase的写操作更加高效） 业务场景简单：不需要太多的关系型数据库特性，列入交叉列，交叉表，事务，连接等。 Hbase的缺点： 单一RowKey固有的局限性决定了它不可能有效地支持多条件查询 不适合于大范围扫描查询 不直接支持 SQL 的语句查询 参考文章1 参考文章2 参考文章3 hbase的HA实现,zookeeper在其中的作用 HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。配置HBase高可用，只需要启动两个HMaster，让Zookeeper自己去选择一个Master Acitve即可 zk的在这里起到的作用就是用来管理master节点,以及帮助hbase做master选举 HMaster宕机的时候,哪些操作还能正常工作对表内数据的增删查改是可以正常进行的,因为hbase client 访问数据只需要通过 zookeeper 来找到 rowkey 的具体 region 位置即可. 但是对于创建表/删除表等的操作就无法进行了,因为这时候是需要HMaster介入, 并且region的拆分,合并,迁移等操作也都无法进行了 讲一下hbase的写数据的流程 Client先访问zookeeper，从.META.表获取相应region信息，然后从meta表获取相应region信息 根据namespace、表名和rowkey根据meta表的数据找到写入数据对应的region信息 找到对应的regionserver 把数据先写到WAL中，即HLog，然后写到MemStore上 MemStore达到设置的阈值后则把数据刷成一个磁盘上的StoreFile文件。 当多个StoreFile文件达到一定的大小后(这个可以称之为小合并，合并数据可以进行设置，必须大于等于2，小于10——hbase.hstore.compaction.max和hbase.hstore.compactionThreshold，默认为10和3)，会触发Compact合并操作，合并为一个StoreFile，（这里同时进行版本的合并和数据删除。） 当Storefile大小超过一定阈值后，会把当前的Region分割为两个（Split）【可称之为大合并，该阈值通过hbase.hregion.max.filesize设置，默认为10G】，并由Hmaster分配到相应的HRegionServer，实现负载均衡 讲一下hbase读数据的流程 首先，客户端需要获知其想要读取的信息的Region的位置，这个时候，Client访问hbase上数据时并不需要Hmaster参与（HMaster仅仅维护着table和Region的元数据信息，负载很低），只需要访问zookeeper，从meta表获取相应region信息(地址和端口等)。【Client请求ZK获取.META.所在的RegionServer的地址。】 客户端会将该保存着RegionServer的位置信息的元数据表.META.进行缓存。然后在表中确定待检索rowkey所在的RegionServer信息（得到持有对应行键的.META表的服务器名）。【获取访问数据所在的RegionServer地址】 根据数据所在RegionServer的访问信息，客户端会向该RegionServer发送真正的数据读取请求。服务器端接收到该请求之后需要进行复杂的处理。 先从MemStore找数据，如果没有，再到StoreFile上读(为了读取的效率)。 参考文章1 参考文章2]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试之Flink]]></title>
    <url>%2F2020%2F03%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E4%B9%8BFlink%2F</url>
    <content type="text"><![CDATA[讲一下flink的运行架构 当 Flink 集群启动后，首先会启动一个 JobManger 和一个或多个的 TaskManager。由 Client 提交任务给 JobManager，JobManager 再调度任务到各个 TaskManager 去执行，然后 TaskManager 将心跳和统计信息汇报给 JobManager。TaskManager 之间以流的形式进行数据的传输。上述三者均为独立的 JVM 进程。 Client 为提交 Job 的客户端，可以是运行在任何机器上（与 JobManager 环境连通即可）。提交 Job 后，Client 可以结束进程（Streaming的任务），也可以不结束并等待结果返回。 JobManager 主要负责调度 Job 并协调 Task 做 checkpoint，职责上很像 Storm 的 Nimbus。从 Client 处接收到 Job 和 JAR 包等资源后，会生成优化后的执行计划，并以 Task 的单元调度到各个 TaskManager 去执行。 TaskManager 在启动的时候就设置好了槽位数（Slot），每个 slot 能启动一个 Task，Task 为线程。从 JobManager 处接收需要部署的 Task，部署启动后，与自己的上游建立 Netty 连接，接收数据并处理。 参考文章1 参考文章2 讲一下flink的作业执行流程 以yarn模式Per-job方式为例概述作业提交执行流程 当执行executor() 之后,会首先在本地client 中将代码转化为可以提交的 JobGraph 如果提交为Per-Job模式,则首先需要启动AM, client会首先向资源系统申请资源, 在yarn下即为申请container开启AM, 如果是Session模式的话则不需要这个步骤 Yarn分配资源, 开启AM Client将Job提交给Dispatcher Dispatcher 会开启一个新的 JobManager线程 JM 向Flink 自己的 Resourcemanager申请slot资源来执行任务 RM 向 Yarn申请资源来启动 TaskManger (Session模式跳过此步) Yarn 分配 Container 来启动 taskManger (Session模式跳过此步) Flink 的 RM 向 TM 申请 slot资源来启动 task TM 将待分配的 slot 提供给 JM JM 提交 task, TM 会启动新的线程来执行任务,开始启动后就可以通过 shuffle模块进行 task之间的数据交换 参考视频 flink具体是如何实现exactly once 语义在谈到 flink 所实现的 exactly-once语义时,主要是2个层面上的,首先 flink在0.9版本以后已经实现了基于state的内部一致性语义, 在1.4版本以后也可以实现端到端 Exactly-Once语义 状态 Exactly-OnceFlink 提供 exactly-once 的状态（state）投递语义，这为有状态的（stateful）计算提供了准确性保证。也就是状态是不会重复使用的,有且仅有一次消费 ​ 这里需要注意的一点是如何理解state语义的exactly-once,并不是说在flink中的所有事件均只会处理一次,而是所有的事件所影响生成的state只有作用一次. ​ 在上图中, 假设每两条消息后出发一次checkPoint操作,持久化一次state. TaskManager 在 处理完 event c 之后被shutdown, 这时候当 JobManager重启task之后, TaskManager 会从 checkpoint 1 处恢复状态,重新执行流处理,也就是说 此时 event c 事件 的的确确是会被再一次处理的. 那么 这里所说的一致性语义是何意思呢? 本身,flink每处理完一条数据都会记录当前进度到 state中, 也就是说在 故障前, 处理完 event c 这件事情已经记录到了state中,但是,由于在checkPoint 2 之前, 就已经发生了宕机,那么 event c 对于state的影响并没有被记录下来,对于整个flink内部系统来说就好像没有发生过一样, 在 故障恢复后, 当触发 checkpoint 2 时, event c 的 state才最终被保存下来. 所以说,可以这样理解, 进入flink 系统中的 事件 永远只会被 一次state记录并checkpoint下来,而state是永远不会发生重复被消费的, 这也就是 flink内部的一致性语义,就叫做 状态 Exactly once. 端到端（end-to-end）Exactly-Once 2017年12月份发布的Apache Flink 1.4版本，引进了一个重要的特性：TwoPhaseCommitSinkFunction.，它抽取了两阶段提交协议的公共部分，使得构建端到端Excatly-Once的Flink程序变为了可能。这些外部系统包括Kafka0.11及以上的版本，以及一些其他的数据输入（data sources）和数据接收(data sink)。它提供了一个抽象层，需要用户自己手动去实现Exactly-Once语义. 为了提供端到端Exactly-Once语义，除了Flink应用程序本身的状态，Flink写入的外部存储也需要满足这个语义。也就是说，这些外部系统必须提供提交或者回滚的方法，然后通过Flink的checkpoint来协调 参考文章1 参考文章2 flink 的 window 实现机制Flink 中定义一个窗口主要需要以下三个组件。 Window Assigner：用来决定某个元素被分配到哪个/哪些窗口中去。 Trigger：触发器。决定了一个窗口何时能够被计算或清除，每个窗口都会拥有一个自己的Trigger。 Evictor：可以译为“驱逐者”。在Trigger触发之后，在窗口被处理之前，Evictor（如果有Evictor的话）会用来剔除窗口中不需要的元素，相当于一个filter。 Window 的实现 首先上图中的组件都位于一个算子（window operator）中，数据流源源不断地进入算子，每一个到达的元素都会被交给 WindowAssigner。WindowAssigner 会决定元素被放到哪个或哪些窗口（window），可能会创建新窗口。因为一个元素可以被放入多个窗口中，所以同时存在多个窗口是可能的。注意，Window本身只是一个ID标识符，其内部可能存储了一些元数据，如TimeWindow中有开始和结束时间，但是并不会存储窗口中的元素。窗口中的元素实际存储在 Key/Value State 中，key为Window，value为元素集合（或聚合值）。为了保证窗口的容错性，该实现依赖了 Flink 的 State 机制（参见 state 文档）。 每一个窗口都拥有一个属于自己的 Trigger，Trigger上会有定时器，用来决定一个窗口何时能够被计算或清除。每当有元素加入到该窗口，或者之前注册的定时器超时了，那么Trigger都会被调用。Trigger的返回结果可以是 continue（不做任何操作），fire（处理窗口数据），purge（移除窗口和窗口中的数据），或者 fire + purge。一个Trigger的调用结果只是fire的话，那么会计算窗口并保留窗口原样，也就是说窗口中的数据仍然保留不变，等待下次Trigger fire的时候再次执行计算。一个窗口可以被重复计算多次知道它被 purge 了。在purge之前，窗口会一直占用着内存。 当Trigger fire了，窗口中的元素集合就会交给Evictor（如果指定了的话）。Evictor 主要用来遍历窗口中的元素列表，并决定最先进入窗口的多少个元素需要被移除。剩余的元素会交给用户指定的函数进行窗口的计算。如果没有 Evictor 的话，窗口中的所有元素会一起交给函数进行计算。 计算函数收到了窗口的元素（可能经过了 Evictor 的过滤），并计算出窗口的结果值，并发送给下游。窗口的结果值可以是一个也可以是多个。DataStream API 上可以接收不同类型的计算函数，包括预定义的sum(),min(),max()，还有 ReduceFunction，FoldFunction，还有WindowFunction。WindowFunction 是最通用的计算函数，其他的预定义的函数基本都是基于该函数实现的。 Flink 对于一些聚合类的窗口计算（如sum,min）做了优化，因为聚合类的计算不需要将窗口中的所有数据都保存下来，只需要保存一个result值就可以了。每个进入窗口的元素都会执行一次聚合函数并修改result值。这样可以大大降低内存的消耗并提升性能。但是如果用户定义了 Evictor，则不会启用对聚合窗口的优化，因为 Evictor 需要遍历窗口中的所有元素，必须要将窗口中所有元素都存下来。 参考文章 flink 的 window 分类flink中的窗口主要分为3大类共5种窗口: Time Window 时间窗口 Tumbing Time Window 滚动时间窗口 实现统计每一分钟(或其他长度)窗口内 计算的效果 Sliding Time Window 滑动时间窗口 实现每过xxx时间 统计 xxx时间窗口的效果. 比如，我们可以每30秒计算一次最近一分钟用户购买的商品总数。 Count Window 计数窗口 Tumbing Count Window 滚动计数窗口 当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window） Sliding Count Window 滑动计数窗口 和Sliding Time Window含义是类似的，例如计算每10个元素计算一次最近100个元素的总和 Session Window 会话窗口 在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的商品数量，如果用户30秒没有活动则视为会话断开（假设raw data stream是单个用户的购买行为流） flink 的 state 是存储在哪里的Apache Flink内部有四种state的存储实现，具体如下： 基于内存的HeapStateBackend - 在debug模式使用，不 建议在生产模式下应用； 基于HDFS的FsStateBackend - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳； 基于RocksDB的RocksDBStateBackend - 本地文件+异步HDFS持久化； 基于Niagara(Alibaba内部实现)NiagaraStateBackend - 分布式持久化- 在Alibaba生产环境应用； 参考文章 flink是如何实现反压的flink的反压经历了两个发展阶段,分别是基于TCP的反压(&lt;1.5)和基于credit的反压(&gt;1.5) 基于 TCP 的反压flink中的消息发送通过RS(ResultPartition),消息接收通过IC(InputGate),两者的数据都是以 LocalBufferPool的形式来存储和提取,进一步的依托于Netty的NetworkBufferPool,之后更底层的便是依托于TCP的滑动窗口机制,当IC端的buffer池满了之后,两个task之间的滑动窗口大小便为0,此时RS端便无法再发送数据 基于TCP的反压最大的问题是会造成整个TaskManager端的反压,所有的task都会受到影响 基于 Credit 的反压RS与IC之间通过backlog和credit来确定双方可以发送和接受的数据量的大小以提前感知,而不是通过TCP滑动窗口的形式来确定buffer的大小之后再进行反压 参考视频 参考文章1 参考文章2 flink的部署模式都有哪些flink可以以多种方式部署,包括standlone模式/yarn/Mesos/Kubernetes/Docker/AWS/Google Compute Engine/MAPR等 一般公司中主要采用 on yarn模式 讲一下flink on yarn的部署Flink作业提交有两种类型: yarn session需要先启动集群，然后在提交作业，接着会向yarn申请一块空间后，资源永远保持不变。如果资源满了，下一个作业就无法提交，只能等到yarn中的其中一个作业执行完成后，释放了资源，那下一个作业才会正常提交. 客户端模式 对于客户端模式而言，你可以启动多个yarn session，一个yarn session模式对应一个JobManager,并按照需求提交作业，同一个Session中可以提交多个Flink作业。如果想要停止Flink Yarn Application，需要通过yarn application -kill命令来停止. 分离式模式 对于分离式模式，并不像客户端那样可以启动多个yarn session，如果启动多个，会出现下面的session一直处在等待状态。JobManager的个数只能是一个，同一个Session中可以提交多个Flink作业。如果想要停止Flink Yarn Application，需要通过yarn application -kill命令来停止 Flink run(Per-Job)直接在YARN上提交运行Flink作业(Run a Flink job on YARN)，这种方式的好处是一个任务会对应一个job,即没提交一个作业会根据自身的情况，向yarn申请资源，直到作业执行完成，并不会影响下一个作业的正常运行，除非是yarn上面没有任何资源的情况下 Session 共享Dispatcher和Resource Manager Dispatcher和Resource Manager 共享资源(即 TaskExecutor) 按需要申请资源 (即 TaskExecutor) 适合规模小,执行时间短的作业 flink中的时间概念 , eventTime 和 processTime的区别Flink中有三种时间概念,分别是 Processing Time、Event Time 和 Ingestion Time Processing TimeProcessing Time 是指事件被处理时机器的系统时间。 当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件 Event TimeEvent Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制 Ingestion TimeIngestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳 Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟） 与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印 参考文章 flink中的session Window怎样使用会话窗口主要是将某段时间内活跃度较高的数据聚合成一个窗口进行计算,窗口的触发条件是 Session Gap, 是指在规定的时间内如果没有数据活跃接入,则认为窗口结束,然后触发窗口结果 Session Windows窗口类型比较适合非连续性数据处理或周期性产生数据的场景,根据用户在线上某段时间内的活跃度对用户行为进行数据统计 12345val sessionWindowStream = inputStream.keyBy(_.id)//使用EventTimeSessionWindow 定义 Event Time 滚动窗口.window(EventTimeSessionWindow.withGap(Time.milliseconds(10))).process(......) Session Window 本质上没有固定的起止时间点,因此底层计算逻辑和Tumbling窗口及Sliding 窗口有一定的区别, Session Window 为每个进入的数据都创建了一个窗口,最后再将距离窗口Session Gap 最近的窗口进行合并,然后计算窗口结果]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试之Spark]]></title>
    <url>%2F2020%2F03%2F04%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E4%B9%8BSpark%2F</url>
    <content type="text"><![CDATA[讲一下spark 的运行架构 Cluster Manager(Master)：在standalone模式中即为Master主节点，控制整个集群，监控worker。在YARN模式中为资源管理器 Worker节点：从节点，负责控制计算节点，启动Executor或者Driver。 Driver： 运行Application 的main()函数 Executor：执行器，是为某个Application运行在worker node上的一个进程 参考文章 一个spark程序的执行流程 A -&gt; 当 Driver 进程被启动之后,首先它将发送请求到Master节点上,进行Spark应用程序的注册 B -&gt; Master在接受到Spark应用程序的注册申请之后,会发送给Worker,让其进行资源的调度和分配. C -&gt; Worker 在接受Master的请求之后,会为Spark应用程序启动Executor, 来分配资源 D -&gt; Executor启动分配资源好后,就会想Driver进行反注册,这是Driver已经知道哪些Executor为他服务了 E -&gt; 当Driver得到注册了Executor之后,就可以开始正式执行spark应用程序了. 首先第一步,就是创建初始RDD,读取数据源,再执行之后的一系列算子. HDFS文件内容被读取到多个worker节点上,形成内存中的分布式数据集,也就是初始RDD F -&gt; Driver就会根据 Job 任务任务中的算子形成对应的task,最后提交给 Executor, 来分配给task进行计算的线程 G -&gt; task就会去调用对应的任务数据来计算,并task会对调用过来的RDD的partition数据执行指定的算子操作,形成新的RDD的partition,这时一个大的循环就结束了 后续的RDD的partition数据又通过Driver形成新的一批task提交给Executor执行,循环这个操作,直到所有的算子结束 参考文章 spark的shuffle介绍spark中的shuffle主要有3种: Hash Shuffle 2.0以后移除 在map阶段(shuffle write)，每个map都会为下游stage的每个partition写一个临时文件，假如下游stage有1000个partition，那么每个map都会生成1000个临时文件，一般来说一个executor上会运行多个map task，这样下来，一个executor上会有非常多的临时文件，假如一个executor上运行M个map task，下游stage有N个partition，那么一个executor上会生成MN个文件。另一方面，如果一个executor上有K个core，那么executor同时可运行K个task，这样一来，就会同时申请KN个文件描述符，一旦partition数较多，势必会耗尽executor上的文件描述符，同时生成K*N个write handler也会带来大量内存的消耗。 在reduce阶段(shuffle read)，每个reduce task都会拉取所有map对应的那部分partition数据，那么executor会打开所有临时文件准备网络传输，这里又涉及到大量文件描述符，另外，如果reduce阶段有combiner操作，那么它会把网络中拉到的数据保存在一个HashMap中进行合并操作，如果数据量较大，很容易引发OOM操作。 Sort Shuffle 1.1开始(sort shuffle也经历过优化升级,详细见参考文章1) 在map阶段(shuffle write)，会按照partition id以及key对记录进行排序，将所有partition的数据写在同一个文件中，该文件中的记录首先是按照partition id排序一个一个分区的顺序排列，每个partition内部是按照key进行排序存放，map task运行期间会顺序写每个partition的数据，并通过一个索引文件记录每个partition的大小和偏移量。这样一来，每个map task一次只开两个文件描述符，一个写数据，一个写索引，大大减轻了Hash Shuffle大量文件描述符的问题，即使一个executor有K个core，那么最多一次性开K*2个文件描述符。 在reduce阶段(shuffle read)，reduce task拉取数据做combine时不再是采用HashMap，而是采用ExternalAppendOnlyMap，该数据结构在做combine时，如果内存不足，会刷写磁盘，很大程度的保证了鲁棒性，避免大数据情况下的OOM。 Unsafe Shuffle 1.5开始, 1.6与Sort shuffle合并 从spark 1.5.0开始，spark开始了钨丝计划(Tungsten)，目的是优化内存和CPU的使用，进一步提升spark的性能。为此，引入Unsafe Shuffle，它的做法是将数据记录用二进制的方式存储，直接在序列化的二进制数据上sort而不是在java 对象上，这样一方面可以减少memory的使用和GC的开销，另一方面避免shuffle过程中频繁的序列化以及反序列化。在排序过程中，它提供cache-efficient sorter，使用一个8 bytes的指针，把排序转化成了一个指针数组的排序，极大的优化了排序性能. 现在2.1 分为三种writer， 分为 BypassMergeSortShuffleWriter， SortShuffleWriter 和 UnsafeShuffleWriter 三种Writer的分类 上面是使用哪种 writer 的判断依据， 是否开启 mapSideCombine 这个判断，是因为有些算子会在 map 端先进行一次 combine， 减少传输数据。 因为 BypassMergeSortShuffleWriter 会临时输出Reducer个（分区数目）小文件，所以分区数必须要小于一个阀值，默认是小于200 UnsafeShuffleWriter需要Serializer支持relocation，Serializer支持relocation：原始数据首先被序列化处理，并且再也不需要反序列，在其对应的元数据被排序后，需要Serializer支持relocation，在指定位置读取对应数据 参考文章1 参考文章2 Spark的 partitioner 都有哪些?Partitioner主要有两个实现类：HashPartitioner和RangePartitioner,HashPartitioner是大部分transformation的默认实现，sortBy、sortByKey使用RangePartitioner实现，也可以自定义Partitioner. HashPartitioner numPartitions方法返回传入的分区数，getPartition方法使用key的hashCode值对分区数取模得到PartitionId，写入到对应的bucket中。 RangePartitioner RangePartitioner是先根据所有partition中数据的分布情况，尽可能均匀地构造出重分区的分隔符，再将数据的key值根据分隔符进行重新分区 使用reservoir Sample方法对每个Partition进行分别抽样 对数据量大(大于sampleSizePerPartition)的分区进行重新抽样 由权重信息计算出分区分隔符rangeBounds 由rangeBounds计算分区数和key的所属分区 参考文章 spark有哪几种joinSpark 中和 join 相关的算子有这几个：join、fullOuterJoin、leftOuterJoin、rightOuterJoin join join函数会输出两个RDD中key相同的所有项，并将它们的value联结起来，它联结的key要求在两个表中都存在，类似于SQL中的INNER JOIN。但它不满足交换律，a.join(b)与b.join(a)的结果不完全相同，值插入的顺序与调用关系有关。 leftOuterJoin leftOuterJoin会保留对象的所有key，而用None填充在参数RDD other中缺失的值，因此调用顺序会使结果完全不同。如下面展示的结果， rightOuterJoin rightOuterJoin与leftOuterJoin基本一致，区别在于它的结果保留的是参数other这个RDD中所有的key。 fullOuterJoin fullOuterJoin会保留两个RDD中所有的key，因此所有的值列都有可能出现缺失的情况，所有的值列都会转为Some对象。 参考文章 RDD有哪些特点 A list of partitionsRDD是一个由多个partition（某个节点里的某一片连续的数据）组成的的list；将数据加载为RDD时，一般会遵循数据的本地性（一般一个hdfs里的block会加载为一个partition）。 A function for computing each splitRDD的每个partition上面都会有function，也就是函数应用，其作用是实现RDD之间partition的转换。 A list of dependencies on other RDDsRDD会记录它的依赖 ，为了容错（重算，cache，checkpoint），也就是说在内存中的RDD操作时出错或丢失会进行重算。 Optionally,a Partitioner for Key-value RDDs 可选项，如果RDD里面存的数据是key-value形式，则可以传递一个自定义的Partitioner进行重新分区，例如这里自定义的Partitioner是基于key进行分区，那则会将不同RDD里面的相同key的数据放到同一个partition里面 Optionally, a list of preferred locations to compute each split on 最优的位置去计算，也就是数据的本地性。 讲一下宽依赖和窄依赖区别宽窄依赖的核心点是 子RDD的partition与父RDD的partition是否是1对多的关系,如果是这样的关系的话, 说明多个父rdd的partition需要经过shuffle过程汇总到一个子rdd的partition,这样就是一次宽依赖,在DAGScheduler中会产生stage的切分. Spark中的算子都有哪些总的来说,spark分为两大类算子: Transformation 变换/转换算子：这种变换并不触发提交作业，完成作业中间过程处理 Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算 Action 行动算子：这类算子会触发 SparkContext 提交 Job 作业 Action 算子会触发 Spark 提交作业（Job），并将数据输出 Spark系统 1. Value数据类型的Transformation算子 输入分区与输出分区一对一型 map算子 flatMap算子 mapPartitions算子 glom算子 输入分区与输出分区多对一型 union算子 cartesian算子 输入分区与输出分区多对多型 grouBy算子 输出分区为输入分区子集型 filter算子 distinct算子 subtract算子 sample算子 takeSample算子 Cache型 cache算子 persist算子 2. Key-Value数据类型的Transfromation算子 输入分区与输出分区一对一 mapValues算子 对单个RDD或两个RDD聚集 combineByKey算子 reduceByKey算子 partitionBy算子 Cogroup算子 连接 join算子 leftOutJoin 和 rightOutJoin算子 3. Action算子 无输出 foreach算子 HDFS算子 saveAsTextFile算子 saveAsObjectFile算子 Scala集合和数据类型 collect算子 collectAsMap算子 reduceByKeyLocally算子 lookup算子 count算子 top算子 reduce算子 fold算子 aggregate算子 countByValue countByKey 参考文章 RDD的缓存级别都有哪些NONE :什么类型都不是DISK_ONLY：磁盘DISK_ONLY_2：磁盘；双副本MEMORY_ONLY： 内存；反序列化；把RDD作为反序列化的方式存储，假如RDD的内容存不下，剩余的分区在以后需要时会重新计算，不会刷到磁盘上。MEMORY_ONLY_2：内存；反序列化；双副本MEMORY_ONLY_SER：内存；序列化；这种序列化方式，每一个partition以字节数据存储，好处是能带来更好的空间存储，但CPU耗费高MEMORY_ONLY_SER_2 : 内存；序列化；双副本MEMORY_AND_DISK：内存 + 磁盘；反序列化；双副本；RDD以反序列化的方式存内存，假如RDD的内容存不下，剩余的会存到磁盘MEMORY_AND_DISK_2 : 内存 + 磁盘；反序列化；双副本MEMORY_AND_DISK_SER：内存 + 磁盘；序列化MEMORY_AND_DISK_SER_2：内存 + 磁盘；序列化；双副本 RDD懒加载是什么意思Transformation 操作是延迟计算的，也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Acion 操作的时候才会真正触发运算,这也就是懒加载. 讲一下spark的几种部署方式目前,除了local模式为本地调试模式以为, Spark支持三种分布式部署方式，分别是standalone、spark on mesos和 spark on YARN Standalone模式 即独立模式，自带完整的服务，可单独部署到一个集群中，无需依赖任何其他资源管理系统。从一定程度上说，该模式是其他两种的基础。目前Spark在standalone模式下是没有任何单点故障问题的，这是借助zookeeper实现的，思想类似于Hbase master单点故障解决方案。将Spark standalone与MapReduce比较，会发现它们两个在架构上是完全一致的： 都是由master/slaves服务组成的，且起初master均存在单点故障，后来均通过zookeeper解决（Apache MRv1的JobTracker仍存在单点问题，但CDH版本得到了解决）； 各个节点上的资源被抽象成粗粒度的slot，有多少slot就能同时运行多少task。不同的是，MapReduce将slot分为map slot和reduce slot，它们分别只能供Map Task和Reduce Task使用，而不能共享，这是MapReduce资源利率低效的原因之一，而Spark则更优化一些，它不区分slot类型，只有一种slot，可以供各种类型的Task使用，这种方式可以提高资源利用率，但是不够灵活，不能为不同类型的Task定制slot资源。总之，这两种方式各有优缺点。 Spark On YARN模式 spark on yarn 的支持两种模式： yarn-cluster：适用于生产环境； yarn-client：适用于交互、调试，希望立即看到app的输出 yarn-cluster和yarn-client的区别在于yarn appMaster，每个yarn app实例有一个appMaster进程，是为app启动的第一个container；负责从ResourceManager请求资源，获取到资源后，告诉NodeManager为其启动container。yarn-cluster和yarn-client模式内部实现还是有很大的区别。如果你需要用于生产环境，那么请选择yarn-cluster；而如果你仅仅是Debug程序，可以选择yarn-client。 Spark On Mesos模式 Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。目前在Spark On Mesos环境中，用户可选择两种调度模式之一运行自己的应用程序 粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。 细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。与粗粒度模式一样，应用程序启动时，先会启动executor，但每个executor占用资源仅仅是自己运行所需的资源，不需要考虑将来要运行的任务，之后，mesos会为每个executor动态分配资源，每分配一些，便可以运行一个新任务，单个Task运行完之后可以马上释放对应的资源。 spark on yarn 模式下的 cluster模式和 client模式有什么区别 yarn-cluster 适用于生产环境。而 yarn-client 适用于交互和调试，也就是希望快速地看到 application 的输出. yarn-cluster 和 yarn-client 模式的区别其实就是 Application Master 进程的区别，yarn-cluster 模式下，driver 运行在 AM(Application Master)中，它负责向 YARN 申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN 上运行。然而 yarn-cluster 模式不适合运行交互类型的作业。而 yarn-client 模式下，Application Master 仅仅向 YARN 请求 executor，Client 会和请求的container 通信来调度他们工作，也就是说 Client 不能离开。 spark运行原理,从提交一个jar到最后返回结果,整个过程 spark-submit 提交代码，执行 new SparkContext()，在 SparkContext 里构造 DAGScheduler 和 TaskScheduler。 TaskScheduler 会通过后台的一个进程，连接 Master，向 Master 注册 Application。 Master 接收到 Application 请求后，会使用相应的资源调度算法，在 Worker 上为这个 Application 启动多个 Executer。 Executor 启动后，会自己反向注册到 TaskScheduler 中。 所有 Executor 都注册到 Driver 上之后，SparkContext 结束初始化，接下来往下执行我们自己的代码。 每执行到一个 Action，就会创建一个 Job。Job 会提交给 DAGScheduler。 DAGScheduler 会将 Job划分为多个 stage，然后每个 stage 创建一个 TaskSet。 TaskScheduler 会把每一个 TaskSet 里的 Task，提交到 Executor 上执行。 Executor 上有线程池，每接收到一个 Task，就用 TaskRunner 封装，然后从线程池里取出一个线程执行这个 task。(TaskRunner 将我们编写的代码，拷贝，反序列化，执行 Task，每个 Task 执行 RDD 里的一个 partition) spark的stage是如何划分的stage的划分依据就是看是否产生了shuflle(即宽依赖),遇到一个shuffle操作就划分为前后两个stage. spark2.0为什么放弃了akka 而用netty 很多Spark用户也使用Akka，但是由于Akka不同版本之间无法互相通信，这就要求用户必须使用跟Spark完全一样的Akka版本，导致用户无法升级Akka。 Spark的Akka配置是针对Spark自身来调优的，可能跟用户自己代码中的Akka配置冲突。 Spark用的Akka特性很少，这部分特性很容易自己实现。同时，这部分代码量相比Akka来说少很多，debug比较容易。如果遇到什么bug，也可以自己马上fix，不需要等Akka上游发布新版本。而且，Spark升级Akka本身又因为第一点会强制要求用户升级他们使用的Akka，对于某些用户来说是不现实的。 参考文章 spark的各种HA, master/worker/executor的ha Master异常spark可以在集群运行时启动一个或多个standby Master,当 Master 出现异常时,会根据规则启动某个standby master接管,在standlone模式下有如下几种配置 ZOOKEEPER 集群数据持久化到zk中,当master出现异常时,zk通过选举机制选出新的master,新的master接管是需要从zk获取持久化信息 FILESYSTEM 集群元数据信息持久化到本地文件系统, 当master出现异常时,只需要在该机器上重新启动master,启动后新的master获取持久化信息并根据这些信息恢复集群状态 CUSTOM 自定义恢复方式,对 standloneRecoveryModeFactory 抽象类 进行实现并把该类配置到系统中,当master出现异常时,会根据用户自定义行为恢复集群 None 不持久化集群的元数据, 当 master出现异常时, 新启动的Master 不进行恢复集群状态,而是直接接管集群 Worker异常Worker 以定时发送心跳给 Master, 让 Master 知道 Worker 的实时状态,当worker出现超时时,Master 调用 timeOutDeadWorker 方法进行处理,在处理时根据 Worker 运行的是 Executor 和 Driver 分别进行处理 如果是Executor, Master先把该 Worker 上运行的Executor 发送信息ExecutorUpdate给对应的Driver,告知Executor已经丢失,同时把这些Executor从其应用程序列表删除, 另外, 相关Executor的异常也需要处理 如果是Driver, 则判断是否设置重新启动,如果需要,则调用Master.shedule方法进行调度,分配合适节点重启Driver, 如果不需要重启, 则删除该应用程序 Executor异常 Executor发生异常时由ExecutorRunner捕获该异常并发送ExecutorStateChanged信息给Worker Worker接收到消息时, 在Worker的 handleExecutorStateChanged 方法中, 根据Executor状态进行信息更新,同时把Executor状态发送给Master Master在接受Executor状态变化消息之后,如果发现其是异常退出,会尝试可用的Worker节点去启动Executor spark的内存管理机制spark的内存结构分为3大块:storage/execution/系统自留 storage 内存：用于缓存 RDD、展开 partition、存放 Direct Task Result、存放广播变量。在 Spark Streaming receiver 模式中，也用来存放每个 batch 的 blocks execution 内存：用于 shuffle、join、sort、aggregation 中的缓存、buffer 系统自留: 在 spark 运行过程中使用：比如序列化及反序列化使用的内存，各个对象、元数据、临时变量使用的内存，函数调用使用的堆栈等 作为误差缓冲：由于 storage 和 execution 中有很多内存的使用是估算的，存在误差。当 storage 或 execution 内存使用超出其最大限制时，有这样一个安全的误差缓冲在可以大大减小 OOM 的概率 1.6版本以前的问题 旧方案最大的问题是 storage 和 execution 的内存大小都是固定的，不可改变，即使 execution 有大量的空闲内存且 storage 内存不足，storage 也无法使用 execution 的内存，只能进行 spill，反之亦然。所以，在很多情况下存在资源浪费 旧方案中，只有 execution 内存支持 off heap，storage 内存不支持 off heap 新方案的改进 新方案 storage 和 execution 内存可以互相借用，当一方内存不足可以向另一方借用内存，提高了整体的资源利用率 新方案中 execution 内存和 storage 内存均支持 off heap spark中的广播变量图片来源 /文字来源 顾名思义，broadcast 就是将数据从一个节点发送到其他各个节点上去。这样的场景很多，比如 driver 上有一张表，其他节点上运行的 task 需要 lookup 这张表，那么 driver 可以先把这张表 copy 到这些节点，这样 task 就可以在本地查表了。如何实现一个可靠高效的 broadcast 机制是一个有挑战性的问题。先看看 Spark 官网上的一段话： Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost. 问题：为什么只能 broadcast 只读的变量？这就涉及一致性的问题，如果变量可以被更新，那么一旦变量被某个节点更新，其他节点要不要一块更新？如果多个节点同时在更新，更新顺序是什么？怎么做同步？还会涉及 fault-tolerance 的问题。为了避免维护数据一致性问题，Spark 目前只支持 broadcast 只读变量。 问题：broadcast 到节点而不是 broadcast 到每个 task？因为每个 task 是一个线程，而且同在一个进程运行 tasks 都属于同一个 application。因此每个节点（executor）上放一份就可以被所有 task 共享。 问题： 具体怎么用 broadcast？driver program 例子： 12345val data = List(1, 2, 3, 4, 5, 6)val bdata = sc.broadcast(data)val rdd = sc.parallelize(1 to 6, 2)val observedSizes = rdd.map(_ =&gt; bdata.value.size) driver 使用 sc.broadcast() 声明要 broadcast 的 data，bdata 的类型是 Broadcast。 当 rdd.transformation(func) 需要用 bdata 时，直接在 func 中调用，比如上面的例子中的 map() 就使用了 bdata.value.size。 问题：怎么实现 broadcast？broadcast 的实现机制很有意思： 1. 分发 task 的时候先分发 bdata 的元信息Driver 先建一个本地文件夹用以存放需要 broadcast 的 data，并启动一个可以访问该文件夹的 HttpServer。当调用val bdata = sc.broadcast(data)时就把 data 写入文件夹，同时写入 driver 自己的 blockManger 中（StorageLevel 为内存＋磁盘），获得一个 blockId，类型为 BroadcastBlockId。当调用rdd.transformation(func)时，如果 func 用到了 bdata，那么 driver submitTask() 的时候会将 bdata 一同 func 进行序列化得到 serialized task，注意序列化的时候不会序列化 bdata 中包含的 data。上一章讲到 serialized task 从 driverActor 传递到 executor 时使用 Akka 的传消息机制，消息不能太大，而实际的 data 可能很大，所以这时候还不能 broadcast data。 driver 为什么会同时将 data 放到磁盘和 blockManager 里面？放到磁盘是为了让 HttpServer 访问到，放到 blockManager 是为了让 driver program 自身使用 bdata 时方便（其实我觉得不放到 blockManger 里面也行）。 那么什么时候传送真正的 data？在 executor 反序列化 task 的时候，会同时反序列化 task 中的 bdata 对象，这时候会调用 bdata 的 readObject() 方法。该方法先去本地 blockManager 那里询问 bdata 的 data 在不在 blockManager 里面，如果不在就使用下面的两种 fetch 方式之一去将 data fetch 过来。得到 data 后，将其存放到 blockManager 里面，这样后面运行的 task 如果需要 bdata 就不需要再去 fetch data 了。如果在，就直接拿来用了。 下面探讨 broadcast data 时候的两种实现方式： 2. HttpBroadcast顾名思义，HttpBroadcast 就是每个 executor 通过的 http 协议连接 driver 并从 driver 那里 fetch data。 Driver 先准备好要 broadcast 的 data，调用sc.broadcast(data)后会调用工厂方法建立一个 HttpBroadcast 对象。该对象做的第一件事就是将 data 存到 driver 的 blockManager 里面，StorageLevel 为内存＋磁盘，blockId 类型为 BroadcastBlockId。 同时 driver 也会将 broadcast 的 data 写到本地磁盘，例如写入后得到 /var/folders/87/grpn1_fn4xq5wdqmxk31v0l00000gp/T/spark-6233b09c-3c72-4a4d-832b-6c0791d0eb9c/broadcast_0， 这个文件夹作为 HttpServer 的文件目录。 Driver 和 executor 启动的时候，都会生成 broadcastManager 对象，调用 HttpBroadcast.initialize()，driver 会在本地建立一个临时目录用来存放 broadcast 的 data，并启动可以访问该目录的 httpServer。 Fetch data：在 executor 反序列化 task 的时候，会同时反序列化 task 中的 bdata 对象，这时候会调用 bdata 的 readObject() 方法。该方法先去本地 blockManager 那里询问 bdata 的 data 在不在 blockManager 里面，如果不在就使用 http 协议连接 driver 上的 httpServer，将 data fetch 过来。得到 data 后，将其存放到 blockManager 里面，这样后面运行的 task 如果需要 bdata 就不需要再去 fetch data 了。如果在，就直接拿来用了。 HttpBroadcast 最大的问题就是 driver 所在的节点可能会出现网络拥堵，因为 worker 上的 executor 都会去 driver 那里 fetch 数据。 3. TorrentBroadcast为了解决 HttpBroadast 中 driver 单点网络瓶颈的问题，Spark 又设计了一种 broadcast 的方法称为 TorrentBroadcast，这个类似于大家常用的 BitTorrent 技术。基本思想就是将 data 分块成 data blocks，然后假设有 executor fetch 到了一些 data blocks，那么这个 executor 就可以被当作 data server 了，随着 fetch 的 executor 越来越多，有更多的 data server 加入，data 就很快能传播到全部的 executor 那里去了。 HttpBroadcast 是通过传统的 http 协议和 httpServer 去传 data，在 TorrentBroadcast 里面使用在上一章介绍的 blockManager.getRemote() =&gt; NIO ConnectionManager 传数据的方法来传递，读取数据的过程与读取 cached rdd 的方式类似，可以参阅 CacheAndCheckpoint 中的最后一张图。 下面讨论 TorrentBroadcast 的一些细节： driver 端：Driver 先把 data 序列化到 byteArray，然后切割成 BLOCK_SIZE（由 spark.broadcast.blockSize = 4MB 设置）大小的 data block，每个 data block 被 TorrentBlock 对象持有。切割完 byteArray 后，会将其回收，因此内存消耗虽然可以达到 2 * Size(data)，但这是暂时的。 完成分块切割后，就将分块信息（称为 meta 信息）存放到 driver 自己的 blockManager 里面，StorageLevel 为内存＋磁盘，同时会通知 driver 自己的 blockManagerMaster 说 meta 信息已经存放好。通知 blockManagerMaster 这一步很重要，因为 blockManagerMaster 可以被 driver 和所有 executor 访问到，信息被存放到 blockManagerMaster 就变成了全局信息。 之后将每个分块 data block 存放到 driver 的 blockManager 里面，StorageLevel 为内存＋磁盘。存放后仍然通知 blockManagerMaster 说 blocks 已经存放好。到这一步，driver 的任务已经完成。 Executor 端：executor 收到 serialized task 后，先反序列化 task，这时候会反序列化 serialized task 中包含的 bdata 类型是 TorrentBroadcast，也就是去调用 TorrentBroadcast.readObject()。这个方法首先得到 bdata 对象，然后发现 bdata 里面没有包含实际的 data。怎么办？先询问所在的 executor 里的 blockManager 是会否包含 data（通过查询 data 的 broadcastId），包含就直接从本地 blockManager 读取 data。否则，就通过本地 blockManager 去连接 driver 的 blockManagerMaster 获取 data 分块的 meta 信息，获取信息后，就开始了 BT 过程。 BT 过程：task 先在本地开一个数组用于存放将要 fetch 过来的 data blocks arrayOfBlocks = new Array[TorrentBlock](totalBlocks)，TorrentBlock 是对 data block 的包装。然后打乱要 fetch 的 data blocks 的顺序，比如如果 data block 共有 5 个，那么打乱后的 fetch 顺序可能是 3-1-2-4-5。然后按照打乱后的顺序去 fetch 一个个 data block。fetch 的过程就是通过 “本地 blockManager －本地 connectionManager－driver/executor 的 connectionManager－driver/executor 的 blockManager－data” 得到 data，这个过程与 fetch cached rdd 类似。每 fetch 到一个 block 就将其存放到 executor 的 blockManager 里面，同时通知 driver 上的 blockManagerMaster 说该 data block 多了一个存储地址。这一步通知非常重要，意味着 blockManagerMaster 知道 data block 现在在 cluster 中有多份，下一个不同节点上的 task 再去 fetch 这个 data block 的时候，可以有两个选择了，而且会随机选择一个去 fetch。这个过程持续下去就是 BT 协议，随着下载的客户端越来越多，data block 服务器也越来越多，就变成 p2p下载了。关于 BT 协议，Wikipedia 上有一个动画)。 整个 fetch 过程结束后，task 会开一个大 Array[Byte]，大小为 data 的总大小，然后将 data block 都 copy 到这个 Array，然后对 Array 中 bytes 进行反序列化得到原始的 data，这个过程就是 driver 序列化 data 的反过程。 最后将 data 存放到 task 所在 executor 的 blockManager 里面，StorageLevel 为内存＋磁盘。显然，这时候 data 在 blockManager 里存了两份，不过等全部 executor 都 fetch 结束，存储 data blocks 那份可以删掉了。 问题：broadcast RDD 会怎样?@Andrew-Xia 回答道：不会怎样，就是这个rdd在每个executor中实例化一份。 Discussion公共数据的 broadcast 是很实用的功能，在 Hadoop 中使用 DistributedCache，比如常用的-libjars就是使用 DistributedCache 来将 task 依赖的 jars 分发到每个 task 的工作目录。不过分发前 DistributedCache 要先将文件上传到 HDFS。这种方式的主要问题是资源浪费，如果某个节点上要运行来自同一 job 的 4 个 mapper，那么公共数据会在该节点上存在 4 份（每个 task 的工作目录会有一份）。但是通过 HDFS 进行 broadcast 的好处在于单点瓶颈不明显，因为公共 data 首先被分成多个 block，然后不同的 block 存放在不同的节点。这样，只要所有的 task 不是同时去同一个节点 fetch 同一个 block，网络拥塞不会很严重。 对于 Spark 来讲，broadcast 时考虑的不仅是如何将公共 data 分发下去的问题，还要考虑如何让同一节点上的 task 共享 data。 对于第一个问题，Spark 设计了两种 broadcast 的方式，传统存在单点瓶颈问题的 HttpBroadcast，和类似 BT 方式的 TorrentBroadcast。HttpBroadcast 使用传统的 client-server 形式的 HttpServer 来传递真正的 data，而 TorrentBroadcast 使用 blockManager 自带的 NIO 通信方式来传递 data。TorrentBroadcast 存在的问题是慢启动和占内存，慢启动指的是刚开始 data 只在 driver 上有，要等 executors fetch 很多轮 data block 后，data server 才会变得可观，后面的 fetch 速度才会变快。executor 所占内存的在 fetch 完 data blocks 后进行反序列化时需要将近两倍 data size 的内存消耗。不管哪一种方式，driver 在分块时会有两倍 data size 的内存消耗。 对于第二个问题，每个 executor 都包含一个 blockManager 用来管理存放在 executor 里的数据，将公共数据存放在 blockManager 中（StorageLevel 为内存＋磁盘），可以保证在 executor 执行的 tasks 能够共享 data。 其实 Spark 之前还尝试了一种称为 TreeBroadcast 的机制，详情可以见技术报告 Performance and Scalability of Broadcast in Spark。 更深入点，broadcast 可以用多播协议来做，不过多播使用 UDP，不是可靠的，仍然需要应用层的设计一些可靠性保障机制。 什么是数据倾斜,怎样去处理数据倾斜数据倾斜是一种很常见的问题（依据二八定律），简单来说，比方WordCount中某个Key对应的数据量非常大的话，就会产生数据倾斜，导致两个后果： OOM（单或少数的节点）； 拖慢整个Job执行时间（其他已经完成的节点都在等这个还在做的节点） 数据倾斜主要分为两类: 聚合倾斜 和 join倾斜 聚合倾斜 双重聚合（局部聚合+全局聚合） 场景: 对RDD进行reduceByKey等聚合类shuffle算子，SparkSQL的groupBy做分组聚合这两种情况 思路：首先通过map给每个key打上n以内的随机数的前缀并进行局部聚合，即(hello, 1) (hello, 1) (hello, 1) (hello, 1)变为(1_hello, 1) (1_hello, 1) (2_hello, 1)，并进行reduceByKey的局部聚合，然后再次map将key的前缀随机数去掉再次进行全局聚合； 原理: 对原本相同的key进行随机数附加，变成不同key，让原本一个task处理的数据分摊到多个task做局部聚合，规避单task数据过量。之后再去随机前缀进行全局聚合； 优点：效果非常好（对聚合类Shuffle操作的倾斜问题）； 缺点：范围窄（仅适用于聚合类的Shuffle操作，join类的Shuffle还需其它方案） join倾斜 将reduce join转为map join 场景: 对RDD或Spark SQL使用join类操作或语句，且join操作的RDD或表比较小（百兆或1,2G）； 思路：使用broadcast和map类算子实现join的功能替代原本的join，彻底规避shuffle。对较小RDD直接collect到内存，并创建broadcast变量；并对另外一个RDD执行map类算子，在该算子的函数中，从broadcast变量（collect出的较小RDD）与当前RDD中的每条数据依次比对key，相同的key执行你需要方式的join； 原理: 若RDD较小，可采用广播小的RDD，并对大的RDD进行map，来实现与join同样的效果。简而言之，用broadcast-map代替join，规避join带来的shuffle（无Shuffle无倾斜）； 优点：效果很好（对join操作导致的倾斜），根治； 缺点：适用场景小（大表+小表），广播（driver和executor节点都会驻留小表数据）小表也耗内存 采样倾斜key并分拆join操作 场景: 两个较大的（无法采用方案五）RDD/Hive表进行join时，且一个RDD/Hive表中少数key数据量过大，另一个RDD/Hive表的key分布较均匀（RDD中两者之一有一个更倾斜）；思路: 对更倾斜rdd1进行采样（RDD.sample）并统计出数据量最大的几个key； 对这几个倾斜的key从原本rdd1中拆出形成一个单独的rdd1_1，并打上0~n的随机数前缀，被拆分的原rdd1的另一部分（不包含倾斜key）又形成一个新rdd1_2； 对rdd2过滤出rdd1倾斜的key，得到rdd2_1，并将其中每条数据扩n倍，对每条数据按顺序附加0~n的前缀，被拆分出key的rdd2也独立形成另一个rdd2_2； 【个人认为，这里扩了n倍，最后union完还需要将每个倾斜key对应的value减去(n-1)】 将加了随机前缀的rdd1_1和rdd2_1进行join（此时原本倾斜的key被打散n份并被分散到更多的task中进行join）； 【个人认为，这里应该做两次join，两次join中间有一个map去前缀】 另外两个普通的RDD（rdd1_2、rdd2_2）照常join； 最后将两次join的结果用union结合得到最终的join结果。 原理：对join导致的倾斜是因为某几个key，可将原本RDD中的倾斜key拆分出原RDD得到新RDD，并以加随机前缀的方式打散n份做join，将倾斜key对应的大量数据分摊到更多task上来规避倾斜； 优点: 前提是join导致的倾斜（某几个key倾斜），避免占用过多内存（只需对少数倾斜key扩容n倍）；缺点: 对过多倾斜key不适用。 用随机前缀和扩容RDD进行join 场景: RDD中有大量key导致倾斜； 思路：与方案六类似。 查看RDD/Hive表中数据分布并找到造成倾斜的RDD/表； 对倾斜RDD中的每条数据打上n以内的随机数前缀； 对另外一个正常RDD的每条数据扩容n倍，扩容出的每条数据依次打上0到n的前缀； 对处理后的两个RDD进行join。 原理: 与方案六只有唯一不同在于这里对不倾斜RDD中所有数据进行扩大n倍，而不是找出倾斜key进行扩容；优点: 对join类的数据倾斜都可处理，效果非常显著；缺点: 缓解，扩容需要大内存 参考文章1 参考文章2 分析一下一段spark代码中哪些部分在Driver端执行,哪些部分在Worker端执行Driver Program是用户编写的提交给Spark集群执行的application，它包含两部分 作为驱动： Driver与Master、Worker协作完成application进程的启动、DAG划分、计算任务封装、计算任务分发到各个计算节点(Worker)、计算资源的分配等。 计算逻辑本身，当计算任务在Worker执行时，执行计算逻辑完成application的计算任务 一般来说transformation算子均是在worker上执行的,其他类型的代码在driver端执行]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试之Hive]]></title>
    <url>%2F2020%2F03%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E4%B9%8BHive%2F</url>
    <content type="text"><![CDATA[hive 内部表和外部表的区别 建表时带有external关键字为外部表，否则为内部表 内部表和外部表建表时都可以自己指定location 删除表时，外部表不会删除对应的数据，只会删除元数据信息，内部表则会删除 其他用法是一样的 hive四种排序方式的区别 order by order by 是要对输出的结果进行全局排序，这就意味着**只有一个reducer**才能实现（多个reducer无法保证全局有序）但是当数据量过大的时候，效率就很低。如果在严格模式下（hive.mapred.mode=strict）,则必须配合limit使用 sort by sort by 不是全局排序，只是在进入到reducer之前完成排序，只保证了每个reducer中数据按照指定字段的有序性，是局部排序。配置mapred.reduce.tasks=[nums]可以对输出的数据执行归并排序。可以配合limit使用，提高性能 distribute by distribute by 指的是按照指定的字段划分到不同的输出reduce文件中，和sort by一起使用时需要注意， distribute by必须放在前面 cluster by cluster by 可以看做是一个特殊的distribute by+sort by，它具备二者的功能，但是只能实现倒序排序的方式,不能指定排序规则为asc 或者desc 参考文章 hive的metastore的三种模式 内嵌Derby方式 这个是Hive默认的启动模式，一般用于单元测试，这种存储方式有一个缺点：在同一时间只能有一个进程连接使用数据库。 Local方式 本地MySQL Remote方式 远程MySQL,一般常用此种方式 参考文章 hive中join都有哪些Hive中除了支持和传统数据库中一样的内关联（JOIN）、左关联（LEFT JOIN）、右关联（RIGHT JOIN）、全关联（FULL JOIN），还支持左半关联（LEFT SEMI JOIN） 内关联（JOIN） 只返回能关联上的结果。 左外关联（LEFT [OUTER] JOIN） 以LEFT [OUTER] JOIN关键字前面的表作为主表，和其他表进行关联，返回记录和主表的记录数一致，关联不上的字段置为NULL。 右外关联（RIGHT [OUTER] JOIN） 和左外关联相反，以RIGTH [OUTER] JOIN关键词后面的表作为主表，和前面的表做关联，返回记录数和主表一致，关联不上的字段为NULL。 全外关联（FULL [OUTER] JOIN） 以两个表的记录为基准，返回两个表的记录去重之和，关联不上的字段为NULL。 LEFT SEMI JOIN 以LEFT SEMI JOIN关键字前面的表为主表，返回主表的KEY也在副表中的记录 笛卡尔积关联（CROSS JOIN） 返回两个表的笛卡尔积结果，不需要指定关联键。 参考文章 Impala 和 hive 的查询有哪些区别Impala是基于Hive的大数据实时分析查询引擎，直接使用Hive的元数据库Metadata,意味着impala元数据都存储在Hive的metastore中。并且impala兼容Hive的sql解析，实现了Hive的SQL语义的子集，功能还在不断的完善中。 Impala相对于Hive所使用的优化技术 1、没有使用 MapReduce进行并行计算，虽然MapReduce是非常好的并行计算框架，但它更多的面向批处理模式，而不是面向交互式的SQL执行。与 MapReduce相比：Impala把整个查询分成一执行计划树，而不是一连串的MapReduce任务，在分发执行计划后，Impala使用拉式获取 数据的方式获取结果，把结果数据组成按执行树流式传递汇集，减少的了把中间结果写入磁盘的步骤，再从磁盘读取数据的开销。Impala使用服务的方式避免 每次执行查询都需要启动的开销，即相比Hive没了MapReduce启动时间。 2、使用LLVM产生运行代码，针对特定查询生成特定代码，同时使用Inline的方式减少函数调用的开销，加快执行效率。 3、充分利用可用的硬件指令（SSE4.2）。 4、更好的IO调度，Impala知道数据块所在的磁盘位置能够更好的利用多磁盘的优势，同时Impala支持直接数据块读取和本地代码计算checksum。 5、通过选择合适的数据存储格式可以得到最好的性能（Impala支持多种存储格式）。 6、最大使用内存，中间结果不写磁盘，及时通过网络以stream的方式传递。 参考文章 Hive中大表join小表的优化方法在小表和大表进行join时，将小表放在前边，效率会高，hive会将小表进行缓存 Hive Sql 是怎样解析成MR job的?主要分为6个阶段: Hive使用Antlr实现语法解析.根据Antlr制定的SQL语法解析规则,完成SQL语句的词法/语法解析,将SQL转为抽象语法树AST. 遍历AST,生成基本查询单元QueryBlock.QueryBlock是一条SQL最基本的组成单元，包括三个部分：输入源，计算过程，输出. 遍历QueryBlock,生成OperatorTree.Hive最终生成的MapReduce任务，Map阶段和Reduce阶段均由OperatorTree组成。Operator就是在Map阶段或者Reduce阶段完成单一特定的操作。QueryBlock生成Operator Tree就是遍历上一个过程中生成的QB和QBParseInfo对象的保存语法的属性. 优化OperatorTree.大部分逻辑层优化器通过变换OperatorTree，合并操作符，达到减少MapReduce Job，减少shuffle数据量的目的 OperatorTree生成MapReduce Job.遍历OperatorTree,翻译成MR任务. 对输出表生成MoveTask 从OperatorTree的其中一个根节点向下深度优先遍历 ReduceSinkOperator标示Map/Reduce的界限，多个Job间的界限 遍历其他根节点，遇过碰到JoinOperator合并MapReduceTask 生成StatTask更新元数据 剪断Map与Reduce间的Operator的关系 优化任务. 使用物理优化器对MR任务进行优化,生成最终执行任务 参考文章 Hive UDF简单介绍在Hive中，用户可以自定义一些函数，用于扩展HiveQL的功能，而这类函数叫做UDF（用户自定义函数）。UDF分为两大类：UDAF（用户自定义聚合函数）和UDTF（用户自定义表生成函数）。 Hive有两个不同的接口编写UDF程序。一个是基础的UDF接口，一个是复杂的GenericUDF接口。 org.apache.hadoop.hive.ql. exec.UDF 基础UDF的函数读取和返回基本类型，即Hadoop和Hive的基本类型。如，Text、IntWritable、LongWritable、DoubleWritable等。 org.apache.hadoop.hive.ql.udf.generic.GenericUDF 复杂的GenericUDF可以处理Map、List、Set类型。 参考文章 Hive SQL : 按照学生科目取每个科目的TopN123456789id,name,subject,score1,小明,语文,872,张三,语文,273,王五,语文,694,李四,语文,995,小明,数学,866,马六,数学,337,李四,数学,448,小红,数学,50 按照各个科目的成绩排名 取 Top3 123select a.* from(select id,name,subject,score,row_number() over(partition by subject order by score desc) rank from student) awhere a.rank &lt;= 3 参考文章 Hive SQL: 获取每个用户的前1/4次的数据12345678910111213141516cookieId createTime pv--------------------------cookie1 2015-04-10 1cookie1 2015-04-11 5cookie1 2015-04-12 7cookie1 2015-04-13 3cookie1 2015-04-14 2cookie1 2015-04-15 4cookie1 2015-04-16 4cookie2 2015-04-10 2cookie2 2015-04-11 3cookie2 2015-04-12 5cookie2 2015-04-13 6cookie2 2015-04-14 3cookie2 2015-04-15 9cookie2 2015-04-16 7 获取每个用户前1/4次的访问记录 12345SELECT a.* from (SELECT cookieid,createtime,pv,NTILE(4)OVER(PARTITION BY cookieId ORDER BY createtime) AS rnfrom table ) aWHERE a.rn = 1 NTILE(n)，用于将分组数据按照顺序切分成n片，返回当前切片值 参考文章]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试之Hadoop]]></title>
    <url>%2F2020%2F03%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E4%B9%8BHadoop%2F</url>
    <content type="text"><![CDATA[HDFS架构1. HDFS 1.0 架构HDFS 采用的是 Master/Slave 架构，一个 HDFS 集群包含一个单独的 NameNode 和多个 DataNode 节点 NameNodeNameNode 负责管理整个分布式系统的元数据，主要包括： 目录树结构； 文件到数据库 Block 的映射关系； Block 副本及其存储位置等管理数据； DataNode 的状态监控，两者通过段时间间隔的心跳来传递管理信息和数据信息，通过这种方式的信息传递，NameNode 可以获知每个 DataNode 保存的 Block 信息、DataNode 的健康状况、命令 DataNode 启动停止等（如果发现某个 DataNode 节点故障，NameNode 会将其负责的 block 在其他 DataNode 上进行备份）。 这些数据保存在内存中，同时在磁盘保存两个元数据管理文件：fsimage 和 editlog。 fsimage：是内存命名空间元数据在外存的镜像文件； editlog：则是各种元数据操作的 write-ahead-log 文件，在体现到内存数据变化前首先会将操作记入 editlog 中，以防止数据丢失。 这两个文件相结合可以构造完整的内存数据。 Secondary NameNodeSecondary NameNode 并不是 NameNode 的热备机，而是定期从 NameNode 拉取 fsimage 和 editlog 文件，并对两个文件进行合并，形成新的 fsimage 文件并传回 NameNode，这样做的目的是减轻 NameNod 的工作压力，本质上 SNN 是一个提供检查点功能服务的服务点。 DataNode负责数据块的实际存储和读写工作，Block 默认是64MB（HDFS2.0改成了128MB），当客户端上传一个大文件时，HDFS 会自动将其切割成固定大小的 Block，为了保证数据可用性，每个 Block 会以多备份的形式存储，默认是3份。 2. HDFS 2.0 的 HA 实现 Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务； ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）； Zookeeper 集群：为主备切换控制器提供主备选举支持； 共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。 DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 -&gt;参考文章链接 Yarn架构 1. ResourceManager（RM）RM 是一个全局的资源管理器，负责整个系统的资源管理和分配，它主要有两个组件构成： 调度器：Scheduler； 应用程序管理器：Applications Manager，ASM。 调度器调度器根据容量、队列等限制条件（如某个队列分配一定的资源，最多执行一定数量的作业等），将系统中的资源分配给各个正在运行的应用程序。要注意的是，该调度器是一个纯调度器，它不再从事任何与应用程序有关的工作，比如不负责重新启动（因应用程序失败或者硬件故障导致的失败），这些均交由应用程序相关的 ApplicationMaster 完成。调度器仅根据各个应用程序的资源需求进行资源分配，而资源分配单位用一个抽象概念 资源容器(Resource Container，也即 Container)，Container 是一个动态资源分配单位，它将内存、CPU、磁盘、网络等资源封装在一起，从而限定每个任务使用的资源量。此外，该调度器是一个可插拔的组件，用户可根据自己的需求设计新的调度器，YARN 提供了多种直接可用的调度器，比如 Fair Scheduler 和 Capacity Schedule 等。 应用程序管理器应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以 AM、监控 AM 运行状态并在失败时重新启动它等。 2. NodeManager（NM）NM 是每个节点上运行的资源和任务管理器，一方面，它会定时向 RM 汇报本节点上的资源使用情况和各个 Container 的运行状态；另一方面，它接收并处理来自 AM 的 Container 启动/停止等各种请求。 3. ApplicationMaster（AM）提交的每个作业都会包含一个 AM，主要功能包括： 与 RM 协商以获取资源（用 container 表示）； 将得到的任务进一步分配给内部的任务； 与 NM 通信以启动/停止任务； 监控所有任务的运行状态，当任务有失败时，重新为任务申请资源并重启任务。 MapReduce 就是原生支持 ON YARN 的一种框架，可以在 YARN 上运行 MapReduce 作业。有很多分布式应用都开发了对应的应用程序框架，用于在 YARN 上运行任务，例如 Spark，Storm、Flink 等。 4. ContainerContainer 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当 AM 向 RM 申请资源时，RM 为 AM 返回的资源便是用 Container 表示的。 YARN 会为每个任务分配一个 Container 且该任务只能使用该 Container 中描述的资源。 MapReduce过程MapReduce分为两个阶段: Map 和 Ruduce. Map阶段: input. 在进行map计算之前，mapreduce会根据输入文件计算输入分片（input split），每个输入分片（input split）针对一个map任务 map. 就是程序员编写好的map函数了，因此map函数效率相对好控制，而且一般map操作都是本地化操作也就是在数据存储节点上进行 Partition. 需要计算每一个map的结果需要发到哪个reduce端,partition数等于reducer数.默认采用HashPartition. spill.此阶段分为sort和combine.首先分区过得数据会经过排序之后写入环形内存缓冲区.在达到阈值之后守护线程将数据溢出分区文件. sort. 在写入环形缓冲区前,对数据排序.&lt;key,value,partition&gt;格式排序 combine(可选). 在溢出文件之前,提前开始combine,相当于本地化的reduce操作 merge. spill结果会有很多个文件,但最终输出只有一个,故有一个merge操作会合并所有的本地文件,并且该文件会有一个对应的索引文件. Reduce阶段: copy. 拉取数据,reduce启动数据copy线程(默认5个),通过Http请求对应节点的map task输出文件,copy的数据也会先放到内部缓冲区.之后再溢写,类似map端操作. merge. 合并多个copy的多个map端的数据.在一个reduce端先将多个map端的数据溢写到本地磁盘,之后再将多个文件合并成一个文件. 数据经过 内存-&gt;磁盘 , 磁盘-&gt;磁盘的过程. output.merge阶段最后会生成一个文件,将此文件转移到内存中,shuffle阶段结束 reduce. 开始执行reduce任务,最后结果保留在hdfs上. Yarn 调度MapReduce过程 Mr程序提交到客户端所在的节点（MapReduce） yarnrunner向Resourcemanager申请一个application。 rm将该应用程序的资源路径返回给yarnrunner 该程序将运行所需资源提交到HDFS上 程序资源提交完毕后，申请运行mrAppMaster RM将用户的请求初始化成一个task 其中一个NodeManager领取到task任务。 该NodeManager创建容器Container，并产生MRAppmaster Container从HDFS上拷贝资源到本地 MRAppmaster向RM申请运行maptask容器 RM将运行maptask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器. MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动maptask，maptask对数据分区排序。 MRAppmaster向RM申请2个容器，运行reduce task。 reduce task向maptask获取相应分区的数据。 程序运行完毕后，MR会向RM注销自己。 参考文章 hdfs写流程 Client 调用 DistributedFileSystem 对象的 create 方法，创建一个文件输出流（FSDataOutputStream）对象； 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，在 HDFS 的 Namespace 中创建一个文件条目（Entry），此时该条目没有任何的 Block，NameNode 会返回该数据每个块需要拷贝的 DataNode 地址信息； 通过 FSDataOutputStream 对象，开始向 DataNode 写入数据，数据首先被写入 FSDataOutputStream 对象内部的数据队列中，数据队列由 DataStreamer 使用，它通过选择合适的 DataNode 列表来存储副本，从而要求 NameNode 分配新的 block； DataStreamer 将数据包以流式传输的方式传输到分配的第一个 DataNode 中，该数据流将数据包存储到第一个 DataNode 中并将其转发到第二个 DataNode 中，接着第二个 DataNode 节点会将数据包转发到第三个 DataNode 节点； DataNode 确认数据传输完成，最后由第一个 DataNode 通知 client 数据写入成功； 完成向文件写入数据，Client 在文件输出流（FSDataOutputStream）对象上调用 close 方法，完成文件写入； 调用 DistributedFileSystem 对象的 complete 方法，通知 NameNode 文件写入成功，NameNode 会将相关结果记录到 editlog 中。 hdfs读流程 Client 通过 DistributedFileSystem 对象与集群的 NameNode 进行一次 RPC 远程调用，获取文件 block 位置信息； NameNode 返回存储的每个块的 DataNode 列表； Client 将连接到列表中最近的 DataNode； Client 开始从 DataNode 并行读取数据； 一旦 Client 获得了所有必须的 block，它就会将这些 block 组合起来形成一个文件。 hdfs创建一个文件的流程 客户端通过ClientProtocol协议向RpcServer发起创建文件的RPC请求。 FSNamesystem封装了各种HDFS操作的实现细节，RpcServer调用FSNamesystem中的相关方法以创建目录。 进一步的，FSDirectory封装了各种目录树操作的实现细节，FSNamesystem调用FSDirectory中的相关方法在目录树中创建目标文件，并通过日志系统备份文件系统的修改。 最后，RpcServer将RPC响应返回给客户端。 参考文章 hadoop1.x 和hadoop 2.x 的区别 资源调度方式的改变 在1.x, 使用Jobtracker负责任务调度和资源管理,单点负担过重,在2.x中,新增了yarn作为集群的调度工具.在yarn中,使用ResourceManager进行 资源管理, 单独开启一个Container作为ApplicationMaster来进行任务管理. HA模式 在1.x中没有HA模式,集群中只有一个NameNode,而在2.x中可以启用HA模式,存在一个Active NameNode 和Standby NameNode. HDFS Federation Hadoop 2.0中对HDFS进行了改进，使NameNode可以横向扩展成多个，每个NameNode分管一部分目录，进而产生了HDFS Federation，该机制的引入不仅增强了HDFS的扩展性，也使HDFS具备了隔离性 hadoop1.x的缺点 JobTracker存在单点故障的隐患 任务调度和资源管理全部是JobTracker来完成,单点负担过重 TaskTracker以Map/Reduce数量表示资源太过简单 TaskTracker 分Map Slot 和 Reduce Slot, 如果任务只需要map任务可能会造成资源浪费 hadoop HA介绍 Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务； ZKFailoverController（主备切换控制器，FC）：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换（当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换）； Zookeeper 集群：为主备切换控制器提供主备选举支持； 共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。 DataNode 节点：因为主 NameNode 和备 NameNode 需要共享 HDFS 的数据块和 DataNode 之间的映射关系，为了使故障切换能够快速进行，DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 hadoop的常用配置文件有哪些 hadoop-env.sh: 用于定义hadoop运行环境相关的配置信息，比如配置JAVA_HOME环境变量、为hadoop的JVM指定特定的选项、指定日志文件所在的目录路径以及master和slave文件的位置等； core-site.xml: 用于定义系统级别的参数，如HDFS URL、Hadoop的临时目录以及用于rack-aware集群中的配置文件的配置等，此中的参数定义会覆盖core-default.xml文件中的默认配置； hdfs-site.xml: HDFS的相关设定，如文件副本的个数、块大小及是否使用强制权限等，此中的参数定义会覆盖hdfs-default.xml文件中的默认配置； mapred-site.xml：HDFS的相关设定，如reduce任务的默认个数、任务所能够使用内存的默认上下限等，此中的参数定义会覆盖mapred-default.xml文件中的默认配置； 小文件过多会有什么危害,如何避免?Hadoop上大量HDFS元数据信息存储在NameNode内存中,因此过多的小文件必定会压垮NameNode的内存. 每个元数据对象约占150byte，所以如果有1千万个小文件，每个文件占用一个block，则NameNode大约需要2G空间。如果存储1亿个文件，则NameNode需要20G空间. 显而易见的解决这个问题的方法就是合并小文件,可以选择在客户端上传时执行一定的策略先合并,或者是使用Hadoop的CombineFileInputFormat&lt;K,V&gt;实现小文件的合并 参考文章 启动hadoop集群会分别启动哪些进程,各自的作用 NameNode： 维护文件系统树及整棵树内所有的文件和目录。这些信息永久保存在本地磁盘的两个文件中：命名空间镜像文件、编辑日志文件 记录每个文件中各个块所在的数据节点信息，这些信息在内存中保存，每次启动系统时重建这些信息 负责响应客户端的 数据块位置请求 。也就是客户端想存数据，应该往哪些节点的哪些块存；客户端想取数据，应该到哪些节点取 接受记录在数据存取过程中，datanode节点报告过来的故障、损坏信息 SecondaryNameNode(非HA模式)： 实现namenode容错的一种机制。定期合并编辑日志与命名空间镜像，当namenode挂掉时，可通过一定步骤进行上顶。(注意 并不是NameNode的备用节点) DataNode： 根据需要存取并检索数据块 定期向namenode发送其存储的数据块列表 ResourceManager： 负责Job的调度,将一个任务与一个NodeManager相匹配。也就是将一个MapReduce之类的任务分配给一个从节点的NodeManager来执行。 NodeManager： 运行ResourceManager分配的任务，同时将任务进度向application master报告 JournalNode(HA下启用): 高可用情况下存放namenode的editlog文件]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据面试题全套汇总+答案]]></title>
    <url>%2F2020%2F02%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E9%9D%A2%E8%AF%95%E9%A2%98%E5%85%A8%E5%A5%97%E6%B1%87%E6%80%BB%2B%E7%AD%94%E6%A1%88%2F</url>
    <content type="text"><![CDATA[大数据面试题全套汇总+答案 Hadoop Hive Spark Flink HBase Kafka Zookeeper 一、Hadoop HDFS架构 Yarn架构 MapReduce过程 Yarn 调度MapReduce hdfs写流程 hdfs读流程 hdfs创建一个文件的流程 hadoop1.x 和hadoop 2.x 的区别 hadoop1.x的缺点 hadoop HA介绍 hadoop的常用配置文件有哪些,自己实际改过哪些? 小文件过多会有什么危害,如何避免? 启动hadoop集群会分别启动哪些进程,各自的作用 二、Hive hive 内部表和外部表的区别 hive中 sort by / order by / cluster by / distribute by 的区别 hive的metastore的三种模式 hive 中 join都有哪些 Impala 和 hive 的查询有哪些区别 Hive中大表join小表的优化方法 Hive Sql 是怎样解析成MR job的? Hive UDF简单介绍 SQL题: 按照学生科目分组, 取每个科目的TopN SQL题: 获取每个用户的前1/4次的数据 三、Spark 讲一下spark 的运行架构 一个spark程序的执行流程 spark的shuffle介绍 Spark的 partitioner 都有哪些? spark 有哪几种join RDD有哪些特点 讲一下宽依赖和窄依赖 Spark中的算子都有哪些 RDD的缓存级别都有哪些 RDD 懒加载是什么意思 讲一下spark的几种部署方式 spark on yarn 模式下的 cluster模式和 client模式有什么区别 spark运行原理,从提交一个jar到最后返回结果,整个过程 spark的stage是如何划分的 spark的rpc: spark2.0为什么放弃了akka 而用netty? spark的各种HA, master/worker/executor/driver/task的ha spark的内存管理机制,spark 1.6前后分析对比, spark2.0 做出来哪些优化 讲一下spark 中的广播变量 什么是数据倾斜,怎样去处理数据倾斜 分析一下一段spark代码中哪些部分在Driver端执行,哪些部分在Worker端执行 四、Flink 讲一下flink的运行架构 讲一下flink的作业执行流程 flink具体是如何实现exactly once 语义 flink 的 window 实现机制 flink的window分类 flink 的 state 是存储在哪里的 flink是如何实现反压的 flink的部署模式都有哪些 讲一下flink on yarn的部署 flink中的时间概念 , eventTime 和 processTime的区别 flink中的session Window怎样使用 五、HBase 讲一下 Hbase 架构 hbase 如何设计 rowkey 讲一下hbase的存储结构,这样的存储结构有什么优缺点 hbase的HA实现,zookeeper在其中的作用 HMaster宕机的时候,哪些操作还能正常工作 讲一下hbase的写数据的流程 讲一下hbase读数据的流程 六、Kafka 讲一下 kafka 的架构 kafka 与其他消息组件对比? kafka 实现高吞吐的原理 kafka怎样保证不重复消费 kafka怎样保证不丢失消息 kafka 与 spark streaming 集成,如何保证 exactly once 语义 ack 有哪几种, 生产中怎样选择? 如何通过 offset 寻找数据 如何清理过期数据 1条message中包含哪些信息 讲一下zookeeper在kafka中的作用 kafka 可以脱离 zookeeper 单独使用吗 kafka有几种数据保留策略 kafka同时设置了7天和10G清除数据,到第5天的时候消息到达了10G,这个时候kafka如何处理? 七、Zookeeper zookeeper是什么,都有哪些功能 zk 有几种部署模式 zk 是怎样保证主从节点的状态同步 说一下 zk 的通知机制 zk 的分布式锁实现方式 zk 采用的哪种分布式一致性协议? 还有哪些分布式一致性协议 讲一下leader 选举过程]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据应用常用打包方式]]></title>
    <url>%2F2019%2F12%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8%E5%B8%B8%E7%94%A8%E6%89%93%E5%8C%85%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[大数据应用常用打包方式一、简介二、mvn package三、maven-assembly-plugin插件四、maven-shade-plugin插件五、其他打包需求1. 使用非Maven仓库中的Jar2. 排除集群中已经存在的Jar3. 打包Scala文件 一、简介在提交大数据作业到集群上运行时，通常需要先将项目打成 JAR 包。这里以 Maven 为例，常用打包方式如下： 不加任何插件，直接使用 mvn package 打包； 使用 maven-assembly-plugin 插件； 使用 maven-shade-plugin 插件； 使用 maven-jar-plugin 和 maven-dependency-plugin 插件； 以下分别进行详细的说明。 二、mvn package不在 POM 中配置任何插件，直接使用 mvn package 进行项目打包，这对于没有使用外部依赖包的项目是可行的。但如果项目中使用了第三方 JAR 包，就会出现问题，因为 mvn package 打的 JAR 包中是不含有依赖包，会导致作业运行时出现找不到第三方依赖的异常。这种方式局限性比较大，因为实际的项目往往很复杂，通常都会依赖第三方 JAR。 大数据框架的开发者也考虑到这个问题，所以基本所有的框架都支持在提交作业时使用 --jars 指定第三方依赖包，但是这种方式的问题同样很明显，就是你必须保持生产环境与开发环境中的所有 JAR 包版本一致，这是有维护成本的。 基于上面这些原因，最简单的是采用 All In One 的打包方式，把所有依赖都打包到一个 JAR 文件中，此时对环境的依赖性最小。要实现这个目的，可以使用 Maven 提供的 maven-assembly-plugin 或 maven-shade-plugin 插件。 三、maven-assembly-plugin插件Assembly 插件支持将项目的所有依赖、文件都打包到同一个输出文件中。目前支持输出以下文件类型： zip tar tar.gz (or tgz) tar.bz2 (or tbz2) tar.snappy tar.xz (or txz) jar dir war 3.1 基本使用在 POM.xml 中引入插件，指定打包格式的配置文件 assembly.xml(名称可自定义)，并指定作业的主入口类： 1234567891011121314151617&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptors&gt; &lt;descriptor&gt;src/main/resources/assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.heibaiying.wordcount.ClusterWordCountApp&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; assembly.xml 文件内容如下： 1234567891011121314151617181920212223242526&lt;assembly xmlns="http://maven.apache.org/ASSEMBLY/2.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/ASSEMBLY/2.0.0 http://maven.apache.org/xsd/assembly-2.0.0.xsd"&gt; &lt;id&gt;jar-with-dependencies&lt;/id&gt; &lt;!--指明打包方式--&gt; &lt;formats&gt; &lt;format&gt;jar&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;outputDirectory&gt;/&lt;/outputDirectory&gt; &lt;useProjectArtifact&gt;true&lt;/useProjectArtifact&gt; &lt;unpack&gt;true&lt;/unpack&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;!--这里以排除 storm 环境中已经提供的 storm-core 为例，演示排除 Jar 包--&gt; &lt;excludes&gt; &lt;exclude&gt;org.apache.storm:storm-core&lt;/exclude&gt; &lt;/excludes&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt;&lt;/assembly&gt; 3.2 打包命令采用 maven-assembly-plugin 进行打包时命令如下： 1# mvn assembly:assembly 打包后会同时生成两个 JAR 包，其中后缀为 jar-with-dependencies 是含有第三方依赖的 JAR 包，后缀是由 assembly.xml 中 &lt;id&gt; 标签指定的，可以自定义修改。 四、maven-shade-plugin插件maven-shade-plugin 比 maven-assembly-plugin 功能更为强大，比如你的工程依赖很多的 JAR 包，而被依赖的 JAR 又会依赖其他的 JAR 包，这样,当工程中依赖到不同的版本的 JAR 时，并且 JAR 中具有相同名称的资源文件时，shade 插件会尝试将所有资源文件打包在一起时，而不是和 assembly 一样执行覆盖操作。 通常使用 maven-shade-plugin 就能够完成大多数的打包需求，其配置简单且适用性最广，因此建议优先使用此方式。 4.1 基本配置采用 maven-shade-plugin 进行打包时候，配置示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;createDependencyReducedPom&gt;true&lt;/createDependencyReducedPom&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.sf&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.dsa&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.rsa&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.EC&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.ec&lt;/exclude&gt; &lt;exclude&gt;META-INF/MSFTSIG.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/MSFTSIG.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;artifactSet&gt; &lt;excludes&gt; &lt;exclude&gt;org.apache.storm:storm-core&lt;/exclude&gt; &lt;/excludes&gt; &lt;/artifactSet&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 以上配置来源于 Storm Github，在上面的配置中，排除了部分文件，这是因为有些 JAR 包生成时，会使用 jarsigner 生成文件签名 (完成性校验)，分为两个文件存放在 META-INF 目录下： a signature file, with a .SF extension； a signature block file, with a .DSA, .RSA, or .EC extension。 如果某些包的存在重复引用，这可能会导致在打包时候出现 Invalid signature file digest for Manifest main attributes 异常，所以在配置中排除这些文件。 4.2 打包命令使用 maven-shade-plugin 进行打包的时候，打包命令和普通打包一样： 1# mvn package 打包后会生成两个 JAR 包，提交到服务器集群时使用非 original 开头的 JAR。 五、其他打包需求1. 使用非Maven仓库中的Jar通常上面两种打包能够满足大多数的使用场景。但是如果你想把某些没有被 Maven 管理 JAR 包打入到最终的 JAR 中，比如你在 resources/lib 下引入的其他非 Maven 仓库中的 JAR，此时可以使用 maven-jar-plugin 和 maven-dependency-plugin 插件将其打入最终的 JAR 中。 123456789101112131415161718192021222324252627282930313233343536373839&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-jar-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;addClasspath&gt;true&lt;/addClasspath&gt; &lt;!--指定 resources/lib 目录--&gt; &lt;classpathPrefix&gt;lib/&lt;/classpathPrefix&gt; &lt;!--应用的主入口类--&gt; &lt;mainClass&gt;com.heibaiying.BigDataApp&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;copy&lt;/id&gt; &lt;phase&gt;compile&lt;/phase&gt; &lt;goals&gt; &lt;!--将 resources/lib 目录所有 Jar 包打进最终的依赖中--&gt; &lt;goal&gt;copy-dependencies&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;!--将 resources/lib 目录所有 Jar 包一并拷贝到输出目录的 lib 目录下--&gt; &lt;outputDirectory&gt; $&#123;project.build.directory&#125;/lib &lt;/outputDirectory&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 2. 排除集群中已经存在的Jar通常为了避免冲突，官方文档都会建议你排除集群中已经提供的 JAR 包，如下： Spark 官方文档 Submitting Applications 章节: When creating assembly jars, list Spark and Hadoop as provided dependencies; these need not be bundled since they are provided by the cluster manager at runtime. Strom 官方文档 Running Topologies on a Production Cluster 章节： Then run mvn assembly:assembly to get an appropriately packaged jar. Make sure you exclude the Storm jars since the cluster already has Storm on the classpath. 按照以上说明，排除 JAR 包的方式主要有两种： 对需要排除的依赖添加 &lt;scope&gt;provided&lt;/scope&gt; 标签，此时该 JAR 包会被排除，但是不建议使用这种方式，因为此时你在本地运行也无法使用该 JAR 包； 建议直接在 maven-assembly-plugin 或 maven-shade-plugin 的配置文件中使用 &lt;exclude&gt; 进行排除。 3. 打包Scala文件如果你使用到 Scala 语言进行编程，此时需要特别注意 ：默认情况下 Maven 是不会把 scala 文件打入最终的 JAR 中，需要额外添加 maven-scala-plugin 插件，常用配置如下： 123456789101112131415161718192021222324&lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*.scala&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 参考资料关于 Maven 各个插件的详细配置可以查看其官方文档： maven-assembly-plugin : http://maven.apache.org/plugins/maven-assembly-plugin/ maven-shade-plugin : http://maven.apache.org/plugins/maven-shade-plugin/ maven-jar-plugin : http://maven.apache.org/plugins/maven-jar-plugin/ maven-dependency-plugin : http://maven.apache.org/components/plugins/maven-dependency-plugin/ 关于 maven-shade-plugin 的更多配置也可以参考该博客： maven-shade-plugin 入门指南]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>打包方式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之资料与工具推荐]]></title>
    <url>%2F2019%2F12%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%B5%84%E6%96%99%E4%B8%8E%E5%B7%A5%E5%85%B7%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"><![CDATA[这里分享一些自己学习过程中觉得不错的资料和开发工具。 :book: 经典书籍 《hadoop 权威指南 (第四版)》 2017 年 《Kafka 权威指南》 2017 年 《从 Paxos 到 Zookeeper 分布式一致性原理与实践》 2015 年 《Spark 技术内幕 深入解析 Spark 内核架构设计与实现原理》 2015 年 《Spark.The.Definitive.Guide》 2018 年 《HBase 权威指南》 2012 年 《Hive 编程指南》 2013 年 《快学 Scala(第 2 版)》 2017 年 《Scala 编程》 2018 年 :computer: 官方文档上面的书籍我都列出了出版日期，可以看到大部分书籍的出版时间都比较久远了，虽然这些书籍比较经典，但是很多书籍在软件版本上已经滞后了很多。所以推荐优先选择各个框架的官方文档作为学习资料。大数据框架的官方文档都很全面，并且对知识点的讲解都做到了简明扼要。这里以 Spark RDD 官方文档 为例，你会发现不仅清晰的知识点导航，而且所有示例都给出了 Java，Scala，Python 三种语言的版本，除了官方文档，其他书籍很少能够做到这一点。 :orange_book: 优秀博客 有态度的 HBase/Spark/BigData：http://hbasefly.com/ 深入 Apache Spark 的设计和实现原理 ： https://github.com/JerryLead/SparkInternals Jark’s Blog - Flink 系列文章：http://wuchong.me/categories/Flink/ :triangular_ruler:开发工具1. VirtualBox一款开源、免费的虚拟机管理软件，虽然是轻量级软件，但功能很丰富，基本能够满足全部的使用需求。 官方网站：https://www.virtualbox.org/ 2. MobaXterm大数据的框架通常都部署在服务器上，这里推荐使用 MobaXterm 进行连接。同样是免费开源的，支持多种连接协议，支持拖拽上传文件，支持使用插件扩展。 官方网站：https://mobaxterm.mobatek.net/ 3. Translate ManTranslate Man 是一款浏览器上的翻译插件 (谷歌和火狐均支持)。它采用谷歌的翻译接口，准确性非常高，支持划词翻译，可以辅助进行官方文档的阅读。 4. ProcessOnProcessOn 式一个在线绘图平台，使用起来非常便捷，可以用于笔记或者博客配图的绘制。 官方网站：https://www.processon.com/]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>资料</tag>
        <tag>工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据常用软件安装指南]]></title>
    <url>%2F2019%2F11%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[大数据常用软件安装指南为方便大家查阅，本仓库所有软件的安装方式单独整理如下： 一、基础软件安装 Linux 环境下 JDK 安装 Linux 环境下 Python 安装 虚拟机静态 IP 及多 IP 配置 二、Hadoop Hadoop 单机环境搭建 Hadoop 集群环境搭建 基于 Zookeeper 搭建 Hadoop 高可用集群 三、Spark Spark 开发环境搭建 基于 Zookeeper 搭建 Spark 高可用集群 四、Flink Flink Standalone 集群部署 五、Storm Storm 单机环境搭建 Storm 集群环境搭建 六、HBase HBase 单机环境搭建 HBase 集群环境搭建 七、Flume Linux 环境下 Flume 的安装部署 八、Azkaban Azkaban3.x 编译及部署 九、Hive Linux 环境下 Hive 的安装部署 十、Zookeeper Zookeeper 单机环境和集群环境搭建 十一、Kafka 基于 Zookeeper 搭建 Kafka 高可用集群 版本说明由于 Apache Hadoop 原有安装包之间兼容性比较差，所以如无特殊需求，本仓库一律选择 CDH (Cloudera’s Distribution, including Apache Hadoop) 版本的安装包。它基于稳定版本的 Apache Hadoop 构建，并做了兼容性测试，是目前生产环境中使用最为广泛的版本。 最新的 CDH 5 的下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 。这个页面很大且加载速度比较慢，需要耐心等待页面加载完成。上半部分是文档链接，后半部分才是安装包。同一个 CDH 版本的不同框架间都做了集成测试，可以保证没有任何 JAR 包冲突。安装包包名通常如下所示，这里 CDH 版本都是 5.15.2 ，前面是各个软件自己的版本 ，未避免出现不必要的 JAR 包冲突，请务必保持 CDH 的版本一致。 123hadoop-2.6.0-cdh5.15.2.tar.gz hbase-1.2.0-cdh5.15.2hive-1.1.0-cdh5.15.2.tar.gz]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>软件安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据完整学习目录]]></title>
    <url>%2F2019%2F11%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%8C%E6%95%B4%E5%AD%A6%E4%B9%A0%E7%9B%AE%E5%BD%95%2F</url>
    <content type="text"><![CDATA[大数据成神之路目录大数据开发基础篇 Java基础 NIO 并发 JVM 分布式 Zookeeper RPC Netty Linux Java基础 NIO 并发容器 JVM 分布式 zookeeper RPC Netty Linux 大数据框架学习篇 Hadoop Hive Spark Flink HBase Kafka Zookeeper Flume Sqoop Azkaban 大数据开发实战进阶篇这里的文章主要是我平时发表在公众号，博客等的文章，精心挑选，以飨读者。 Flink实战进阶 Spark实战进阶 Kafka实战进阶 第一部分: 大数据开发基础篇一、Java基础 大数据成神之路-Java高级特性增强(多线程).md) 大数据成神之路-Java高级特性增强(Synchronized关键字).md) 大数据成神之路-Java高级特性增强(volatile关键字).md) 大数据成神之路-Java高级特性增强(锁).md) 大数据成神之路-Java高级特性增强(ArrayList/Vector).md) 大数据成神之路-Java高级特性增强(LinkedList) 大数据成神之路-Java高级特性增强(HashMap).md) 大数据成神之路-Java高级特性增强(HashSet).md) 大数据成神之路-Java高级特性增强(LinkedHashMap).md) 二、NIO基础 大数据成神之路-Java高级特性增强-NIO大纲 NIO概览 Java NIO之Buffer(缓冲区).md) Java NIO之Channel(通道).md) ava NIO之Selector(选择器).md) Java NIO之拥抱Path和Files 三、Java并发容器 大数据成神之路-Java高级特性增强(并发容器大纲).md) 大数据成神之路-Java高级特性增强(LinkedBlockingQueue).md) 大数据成神之路-Java高级特性增强(LinkedBlockingDeque).md) 大数据成神之路-Java高级特性增强(CopyOnWriteArraySet).md) 大数据成神之路-Java高级特性增强(CopyOnWriteArrayList).md) 大数据成神之路-Java高级特性增强(ConcurrentSkipListSet).md) 大数据成神之路-Java高级特性增强(ConcurrentSkipListMap).md) 大数据成神之路-Java高级特性增强(ConcurrentLinkedQueue).md) 大数据成神之路-Java高级特性增强(ConcurrentHashMap).md) 大数据成神之路-Java高级特性增强(ArrayBlockingQueue).md) 四、JVM深度解析和面试点先来10篇基础热身 JVM内存结构 HotSpot虚拟机对象探秘 垃圾收集策略与算法 HotSpot垃圾收集器 内存分配与回收策略 JVM性能调优 类文件结构 类加载的时机 类加载的过程 类加载器 再来5篇详细解说 java类的加载机制java%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6.md) JVM内存结构JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84.md) GC算法 垃圾收集器GC%E7%AE%97%E6%B3%95%20%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8.md) jvm调优-命令大全jvm%E8%B0%83%E4%BC%98-%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%EF%BC%88jps%20jstat%20jmap%20jhat%20jstack%20jinfo%EF%BC%89.md) Java GC 分析Java%20GC%20%E5%88%86%E6%9E%90.md) 五、分布式理论基础和原理 分布式系统的一些基本概念 分布式系统理论基础一： 一致性、2PC和3PC 分布式系统理论基础二-CAP 分布式系统理论基础三-时间、时钟和事件顺序 分布式系统理论进阶 - Paxos 分布式系统理论进阶 - Raft、Zab 分布式系统理论进阶：选举、多数派和租约 分布式锁的解决方案 分布式锁的解决方案(二).md) 分布式事务的解决方案 分布式ID生成器解决方案 六、大数据框架开发基础-Zookeeper 安装和运行 zookeeper服务 zookeeper应用程序 zookeeper开发实例 zookeeper集群构建 七、大数据框架开发基础-RPC RPC简单介绍 RPC的原理和框架 手把手教你实现一个简单的RPC 八、大数据框架基石之网路通信-Netty 关于Netty我们都需要知道什么 Netty源码解析-概述篇 Netty源码解析1-Buffer Netty源码解析2-Reactor Netty源码解析3-Pipeline Netty源码解析4-Handler综述 Netty源码解析5-ChannelHandler Netty源码解析6-ChannelHandler实例之LoggingHandler Netty源码解析7-ChannelHandler实例之TimeoutHandler Netty源码解析8-ChannelHandler实例之CodecHandler Netty源码解析9-ChannelHandler实例之MessageToByteEncoder 第二部分:大数据框架学习篇本部分引用了Bigdata-Notes的文章，作者是heibaiying，大佬写的文章非常好，欢迎大家关注他的博客。我个人会持续补充更有深度和实战性的文章~ 一、Hadoop 分布式文件存储系统 —— HDFS 分布式计算框架 —— MapReduce 集群资源管理器 —— YARN Hadoop 单机伪集群环境搭建 Hadoop 集群环境搭建 HDFS 常用 Shell 命令 HDFS Java API 的使用 基于 Zookeeper 搭建 Hadoop 高可用集群 Hadoop级简入门 MapReduce编程模型和计算框架架构原理 二、Hive Hive 简介及核心概念 Linux 环境下 Hive 的安装部署 Hive CLI 和 Beeline 命令行的基本使用 Hive 常用 DDL 操作 Hive 分区表和分桶表 Hive 视图和索引 Hive常用 DML 操作 Hive 数据查询详解 三、SparkSpark Core : Spark 简介 Spark 开发环境搭建 弹性式数据集 RDD RDD 常用算子详解 Spark 运行模式与作业提交 Spark 累加器与广播变量 基于 Zookeeper 搭建 Spark 高可用集群 Spark SQL : DateFrame 和 DataSet Structured API 的基本使用 Spark SQL 外部数据源 Spark SQL 常用聚合函数 Spark SQL JOIN 操作 Spark Streaming ： Spark Streaming 简介 Spark Streaming 基本操作 Spark Streaming 整合 Flume Spark Streaming 整合 Kafka 四、Flink Flink 核心概念综述 Flink 开发环境搭建 Flink Data Source Flink Data Transformation Flink Data Sink Flink 窗口模型 Flink 状态管理与检查点机制 Flink Standalone 集群部署 Flink当前最火的实时计算引擎-入门篇 Flink从入门到放弃(入门篇1)-Flink是什么-Flink%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F.md) Flink从入门到放弃(入门篇2)-本地环境搭建&amp;构建第一个Flink应用-%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%26%E6%9E%84%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AAFlink%E5%BA%94%E7%94%A8.md) Flink从入门到放弃(入门篇3)-DataSetAPI-DataSetAPI.md) Flink从入门到放弃(入门篇4)-DataStreamAPI-DataStreamAPI.md) Flink集群部署 Flink重启策略 Flink的分布式缓存 Flink中的窗口 Flink中的Time Flink集群搭建的HA.md) Flink中的时间戳和水印 Flink广播变量 Flink-Kafka-Connector Flink-Table-&amp;-SQL实战 15-Flink实战项目之实时热销排行 16-Flink-Redis-Sink 17-Flink消费Kafka写入Mysql Flink当前最火的实时计算引擎-放弃篇 Flink漫谈系列1-概述-%E6%A6%82%E8%BF%B0.md) Flink漫谈系列2-watermark-Watermark.md) Flink漫谈系列3-state-State.md) 五、HBase Hbase 简介 HBase 系统架构及数据结构 HBase 基本环境搭建 (Standalone /pseudo-distributed mode) HBase 集群环境搭建 HBase 常用 Shell 命令 HBase Java API Hbase 过滤器详解 HBase 协处理器详解 HBase 容灾与备份 HBase的 SQL 中间层 —— Phoenix Spring/Spring Boot 整合 Mybatis + Phoenix 六、KafkaKafka基本原理 ： Kafka 简介 基于 Zookeeper 搭建 Kafka 高可用集群 Kafka 生产者详解 Kafka 消费者详解 深入理解 Kafka 副本机制 分布式消息队列Kafka原理及与流式计算的集成 ： Apache-Kafka简介 Apache-Kafka核心概念 Apache-Kafka安装和使用 Apache-Kafka编程实战 Apache-Kafka核心组件和流程(副本管理器).md) Apache-Kafka核心组件和流程-协调器 Apache-Kafka核心组件和流程-控制器 Apache-Kafka核心组件和流程-日志管理器 七、Zookeeper Zookeeper 简介及核心概念 Zookeeper 单机环境和集群环境搭建 Zookeeper 常用 Shell 命令 Zookeeper Java 客户端 —— Apache Curator Zookeeper ACL 权限控制 八、Flume Flume 简介及基本使用 Linux 环境下 Flume 的安装部署 Flume 整合 Kafka 九、Sqoop Sqoop 简介与安装 Sqoop 的基本使用 十、Azkaban Azkaban 简介 Azkaban3.x 编译及部署 Azkaban Flow 1.0 的使用 Azkaban Flow 2.0 的使用 第三部分:大数据开发实战进阶篇Flink实战进阶文章合集 菜鸟供应链实时技术架构演进 趣头条实战-基于Flink+ClickHouse构建实时数据平台 ApacheFlink新场景-OLAP引擎 说说Flink DataStream的八种物理分区逻辑 State Processor API：如何读取，写入和修改 Flink 应用程序的状态 Flink滑动窗口原理与细粒度滑动窗口的性能问题 基于Flink快速开发实时TopN 使用 Apache Flink 开发实时 ETL Flink Source/Sink探究与实践：RocketMQ数据写入HBase Spark/Flink广播实现作业配置动态更新 Flink全链路延迟的测量方式 Flink原理-Flink中的数据抽象及数据交换过程 Flink SQL Window源码全解析 Flink DataStream维度表Join的简单方案 Apache Flink的内存管理 Flink1.9整合Kafka实战 Apache Flink在小米的发展和应用 基于Kafka+Flink+Redis的电商大屏实时计算案例 Flink实战-壳找房基于Flink的实时平台建设 用Flink取代Spark Streaming！知乎实时数仓架构演进 Flink实时数仓-美团点评实战 来将可留姓名？Flink最强学习资源合集! 数据不撒谎，Flink-Kafka性能压测全记录! 菜鸟在物流场景中基于Flink的流计算实践 基于Flink构建实时数据仓库 Flink/Spark 如何实现动态更新作业配置 Spark实战进阶文章合集 如果你在准备面试，好好看看这130道题 ORC文件存储格式的深入探究 基于SparkStreaming+Kafka+HBase实时点击流案例 HyperLogLog函数在Spark中的高级应用 我们常说的海量小文件的根源是什么？ Structured Streaming | Apache Spark中处理实时数据的声明式API Spark面对OOM问题的解决方法及优化总结 Spark 动态资源分配(Dynamic Resource Allocation) 解析 Apache Spark在海致大数据平台中的优化实践 Spark/Flink广播实现作业配置动态更新 Spark SQL读数据库时不支持某些数据类型的问题 这个面试问题很难么 | 如何处理大数据中的数据倾斜 Spark难点 | Join的实现原理 面试注意点 | Spark&amp;Flink的区别拾遗 Spark Checkpoint的运行原理和源码实现 阿里云Spark Shuffle的优化 使用Kafka+Spark+Cassandra构建实时处理引擎 基于HBase和Spark构建企业级数据处理平台 SparkSQL在字节跳动的应用实践和优化实战 SparkRDD转DataSet/DataFrame的一个深坑 Spark和Flink的状态管理State的区别和应用 Kafka+Spark Streaming管理offset的几种方法 从 PageRank Example谈Spark应用程序调优 Spark调优|SparkSQL参数调优 Flink/Spark 如何实现动态更新作业配置 Stream SQL的执行原理与Flink的实现 Spark将Dataframe数据写入Hive分区表的方案 Spark中几种ShuffleWriter的区别你都知道吗？ SparkSQL的3种Join实现 周期性清除Spark Streaming流状态的方法 Structured Streaming之状态存储解析 Spark SQL重点知识总结 SparkSQL极简入门 Spark Shuffle在网易的优化 广告点击数实时统计：Spark StructuredStreaming + Redis Streams Spark内存调优 Structured Streaming 实现思路与实现概述 Spark之数据倾斜调优 你不得不知道的知识-零拷贝 Spark Streaming消费Kafka数据的两种方案]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>学习目录</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Storm集成Redis详解]]></title>
    <url>%2F2019%2F11%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BStorm%E9%9B%86%E6%88%90Redis%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Storm 集成 Redis 详解一、简介二、集成案例三、storm-redis 实现原理四、自定义RedisBolt实现词频统计 一、简介Storm-Redis 提供了 Storm 与 Redis 的集成支持，你只需要引入对应的依赖即可使用： 123456&lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-redis&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;type&gt;jar&lt;/type&gt;&lt;/dependency&gt; Storm-Redis 使用 Jedis 为 Redis 客户端，并提供了如下三个基本的 Bolt 实现： RedisLookupBolt：从 Redis 中查询数据； RedisStoreBolt：存储数据到 Redis； RedisFilterBolt : 查询符合条件的数据； RedisLookupBolt、RedisStoreBolt、RedisFilterBolt 均继承自 AbstractRedisBolt 抽象类。我们可以通过继承该抽象类，实现自定义 RedisBolt，进行功能的拓展。 二、集成案例2.1 项目结构这里首先给出一个集成案例：进行词频统计并将最后的结果存储到 Redis。项目结构如下： 用例源码下载地址：storm-redis-integration 2.2 项目依赖项目主要依赖如下： 12345678910111213141516&lt;properties&gt; &lt;storm.version&gt;1.2.2&lt;/storm.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-redis&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.3 DataSourceSpout123456789101112131415161718192021222324252627282930313233343536373839/** * 产生词频样本的数据源 */public class DataSourceSpout extends BaseRichSpout &#123; private List&lt;String&gt; list = Arrays.asList("Spark", "Hadoop", "HBase", "Storm", "Flink", "Hive"); private SpoutOutputCollector spoutOutputCollector; @Override public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) &#123; this.spoutOutputCollector = spoutOutputCollector; &#125; @Override public void nextTuple() &#123; // 模拟产生数据 String lineData = productData(); spoutOutputCollector.emit(new Values(lineData)); Utils.sleep(1000); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields("line")); &#125; /** * 模拟数据 */ private String productData() &#123; Collections.shuffle(list); Random random = new Random(); int endIndex = random.nextInt(list.size()) % (list.size()) + 1; return StringUtils.join(list.toArray(), "\t", 0, endIndex); &#125;&#125; 产生的模拟数据格式如下： 123456789Spark HBaseHive Flink Storm Hadoop HBase SparkFlinkHBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase HiveHadoop Spark HBase Storm 2.4 SplitBolt1234567891011121314151617181920212223242526/** * 将每行数据按照指定分隔符进行拆分 */public class SplitBolt extends BaseRichBolt &#123; private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; @Override public void execute(Tuple input) &#123; String line = input.getStringByField("line"); String[] words = line.split("\t"); for (String word : words) &#123; collector.emit(new Values(word, String.valueOf(1))); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word", "count")); &#125;&#125; 2.5 CountBolt12345678910111213141516171819202122232425262728293031323334/** * 进行词频统计 */public class CountBolt extends BaseRichBolt &#123; private Map&lt;String, Integer&gt; counts = new HashMap&lt;&gt;(); private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector=collector; &#125; @Override public void execute(Tuple input) &#123; String word = input.getStringByField("word"); Integer count = counts.get(word); if (count == null) &#123; count = 0; &#125; count++; counts.put(word, count); // 输出 collector.emit(new Values(word, String.valueOf(count))); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word", "count")); &#125;&#125; 2.6 WordCountStoreMapper实现 RedisStoreMapper 接口，定义 tuple 与 Redis 中数据的映射关系：即需要指定 tuple 中的哪个字段为 key，哪个字段为 value，并且存储到 Redis 的何种数据结构中。 123456789101112131415161718192021222324252627/** * 定义 tuple 与 Redis 中数据的映射关系 */public class WordCountStoreMapper implements RedisStoreMapper &#123; private RedisDataTypeDescription description; private final String hashKey = "wordCount"; public WordCountStoreMapper() &#123; description = new RedisDataTypeDescription( RedisDataTypeDescription.RedisDataType.HASH, hashKey); &#125; @Override public RedisDataTypeDescription getDataTypeDescription() &#123; return description; &#125; @Override public String getKeyFromTuple(ITuple tuple) &#123; return tuple.getStringByField("word"); &#125; @Override public String getValueFromTuple(ITuple tuple) &#123; return tuple.getStringByField("count"); &#125;&#125; 2.7 WordCountToRedisApp123456789101112131415161718192021222324252627282930313233343536373839404142/** * 进行词频统计 并将统计结果存储到 Redis 中 */public class WordCountToRedisApp &#123; private static final String DATA_SOURCE_SPOUT = "dataSourceSpout"; private static final String SPLIT_BOLT = "splitBolt"; private static final String COUNT_BOLT = "countBolt"; private static final String STORE_BOLT = "storeBolt"; //在实际开发中这些参数可以将通过外部传入 使得程序更加灵活 private static final String REDIS_HOST = "192.168.200.226"; private static final int REDIS_PORT = 6379; public static void main(String[] args) &#123; TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(DATA_SOURCE_SPOUT, new DataSourceSpout()); // split builder.setBolt(SPLIT_BOLT, new SplitBolt()).shuffleGrouping(DATA_SOURCE_SPOUT); // count builder.setBolt(COUNT_BOLT, new CountBolt()).shuffleGrouping(SPLIT_BOLT); // save to redis JedisPoolConfig poolConfig = new JedisPoolConfig.Builder() .setHost(REDIS_HOST).setPort(REDIS_PORT).build(); RedisStoreMapper storeMapper = new WordCountStoreMapper(); RedisStoreBolt storeBolt = new RedisStoreBolt(poolConfig, storeMapper); builder.setBolt(STORE_BOLT, storeBolt).shuffleGrouping(COUNT_BOLT); // 如果外部传参 cluster 则代表线上环境启动否则代表本地启动 if (args.length &gt; 0 &amp;&amp; args[0].equals("cluster")) &#123; try &#123; StormSubmitter.submitTopology("ClusterWordCountToRedisApp", new Config(), builder.createTopology()); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; LocalCluster cluster = new LocalCluster(); cluster.submitTopology("LocalWordCountToRedisApp", new Config(), builder.createTopology()); &#125; &#125;&#125; 2.8 启动测试可以用直接使用本地模式运行，也可以打包后提交到服务器集群运行。本仓库提供的源码默认采用 maven-shade-plugin 进行打包，打包命令如下： 1# mvn clean package -D maven.test.skip=true 启动后，查看 Redis 中的数据： 三、storm-redis 实现原理3.1 AbstractRedisBoltRedisLookupBolt、RedisStoreBolt、RedisFilterBolt 均继承自 AbstractRedisBolt 抽象类，和我们自定义实现 Bolt 一样，AbstractRedisBolt 间接继承自 BaseRichBolt。 AbstractRedisBolt 中比较重要的是 prepare 方法，在该方法中通过外部传入的 jedis 连接池配置 ( jedisPoolConfig/jedisClusterConfig) 创建用于管理 Jedis 实例的容器 JedisCommandsInstanceContainer。 1234567891011121314151617181920212223242526public abstract class AbstractRedisBolt extends BaseTickTupleAwareRichBolt &#123; protected OutputCollector collector; private transient JedisCommandsInstanceContainer container; private JedisPoolConfig jedisPoolConfig; private JedisClusterConfig jedisClusterConfig; ...... @Override public void prepare(Map map, TopologyContext topologyContext, OutputCollector collector) &#123; // FIXME: stores map (stormConf), topologyContext and expose these to derived classes this.collector = collector; if (jedisPoolConfig != null) &#123; this.container = JedisCommandsContainerBuilder.build(jedisPoolConfig); &#125; else if (jedisClusterConfig != null) &#123; this.container = JedisCommandsContainerBuilder.build(jedisClusterConfig); &#125; else &#123; throw new IllegalArgumentException("Jedis configuration not found"); &#125; &#125; .......&#125; JedisCommandsInstanceContainer 的 build() 方法如下，实际上就是创建 JedisPool 或 JedisCluster 并传入容器中。 123456789public static JedisCommandsInstanceContainer build(JedisPoolConfig config) &#123; JedisPool jedisPool = new JedisPool(DEFAULT_POOL_CONFIG, config.getHost(), config.getPort(), config.getTimeout(), config.getPassword(), config.getDatabase()); return new JedisContainer(jedisPool); &#125; public static JedisCommandsInstanceContainer build(JedisClusterConfig config) &#123; JedisCluster jedisCluster = new JedisCluster(config.getNodes(), config.getTimeout(), config.getTimeout(), config.getMaxRedirections(), config.getPassword(), DEFAULT_POOL_CONFIG); return new JedisClusterContainer(jedisCluster); &#125; 3.2 RedisStoreBolt和RedisLookupBoltRedisStoreBolt 中比较重要的是 process 方法，该方法主要从 storeMapper 中获取传入 key/value 的值，并按照其存储类型 dataType 调用 jedisCommand 的对应方法进行存储。 RedisLookupBolt 的实现基本类似，从 lookupMapper 中获取传入的 key 值，并进行查询操作。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class RedisStoreBolt extends AbstractRedisBolt &#123; private final RedisStoreMapper storeMapper; private final RedisDataTypeDescription.RedisDataType dataType; private final String additionalKey; public RedisStoreBolt(JedisPoolConfig config, RedisStoreMapper storeMapper) &#123; super(config); this.storeMapper = storeMapper; RedisDataTypeDescription dataTypeDescription = storeMapper.getDataTypeDescription(); this.dataType = dataTypeDescription.getDataType(); this.additionalKey = dataTypeDescription.getAdditionalKey(); &#125; public RedisStoreBolt(JedisClusterConfig config, RedisStoreMapper storeMapper) &#123; super(config); this.storeMapper = storeMapper; RedisDataTypeDescription dataTypeDescription = storeMapper.getDataTypeDescription(); this.dataType = dataTypeDescription.getDataType(); this.additionalKey = dataTypeDescription.getAdditionalKey(); &#125; @Override public void process(Tuple input) &#123; String key = storeMapper.getKeyFromTuple(input); String value = storeMapper.getValueFromTuple(input); JedisCommands jedisCommand = null; try &#123; jedisCommand = getInstance(); switch (dataType) &#123; case STRING: jedisCommand.set(key, value); break; case LIST: jedisCommand.rpush(key, value); break; case HASH: jedisCommand.hset(additionalKey, key, value); break; case SET: jedisCommand.sadd(key, value); break; case SORTED_SET: jedisCommand.zadd(additionalKey, Double.valueOf(value), key); break; case HYPER_LOG_LOG: jedisCommand.pfadd(key, value); break; case GEO: String[] array = value.split(":"); if (array.length != 2) &#123; throw new IllegalArgumentException("value structure should be longitude:latitude"); &#125; double longitude = Double.valueOf(array[0]); double latitude = Double.valueOf(array[1]); jedisCommand.geoadd(additionalKey, longitude, latitude, key); break; default: throw new IllegalArgumentException("Cannot process such data type: " + dataType); &#125; collector.ack(input); &#125; catch (Exception e) &#123; this.collector.reportError(e); this.collector.fail(input); &#125; finally &#123; returnInstance(jedisCommand); &#125; &#125; .........&#125; 3.3 JedisCommandsJedisCommands 接口中定义了所有的 Redis 客户端命令，它有以下三个实现类，分别是 Jedis、JedisCluster、ShardedJedis。Strom 中主要使用前两种实现类，具体调用哪一个实现类来执行命令，由传入的是 jedisPoolConfig 还是 jedisClusterConfig 来决定。 3.4 RedisMapper 和 TupleMapperRedisMapper 和 TupleMapper 定义了 tuple 和 Redis 中的数据如何进行映射转换。 1. TupleMapperTupleMapper 主要定义了两个方法： getKeyFromTuple(ITuple tuple)： 从 tuple 中获取那个字段作为 Key； getValueFromTuple(ITuple tuple)：从 tuple 中获取那个字段作为 Value； 2. RedisMapper定义了获取数据类型的方法 getDataTypeDescription(),RedisDataTypeDescription 中 RedisDataType 枚举类定义了所有可用的 Redis 数据类型： 12345public class RedisDataTypeDescription implements Serializable &#123; public enum RedisDataType &#123; STRING, HASH, LIST, SET, SORTED_SET, HYPER_LOG_LOG, GEO &#125; ...... &#125; 3. RedisStoreMapperRedisStoreMapper 继承 TupleMapper 和 RedisMapper 接口，用于数据存储时，没有定义额外方法。 4. RedisLookupMapperRedisLookupMapper 继承 TupleMapper 和 RedisMapper 接口： 定义了 declareOutputFields 方法，声明输出的字段。 定义了 toTuple 方法，将查询结果组装为 Storm 的 Values 的集合，并用于发送。 下面的例子表示从输入 Tuple 的获取 word 字段作为 key，使用 RedisLookupBolt 进行查询后，将 key 和查询结果 value 组装为 values 并发送到下一个处理单元。 12345678910111213141516171819202122232425262728293031323334353637class WordCountRedisLookupMapper implements RedisLookupMapper &#123; private RedisDataTypeDescription description; private final String hashKey = "wordCount"; public WordCountRedisLookupMapper() &#123; description = new RedisDataTypeDescription( RedisDataTypeDescription.RedisDataType.HASH, hashKey); &#125; @Override public List&lt;Values&gt; toTuple(ITuple input, Object value) &#123; String member = getKeyFromTuple(input); List&lt;Values&gt; values = Lists.newArrayList(); values.add(new Values(member, value)); return values; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("wordName", "count")); &#125; @Override public RedisDataTypeDescription getDataTypeDescription() &#123; return description; &#125; @Override public String getKeyFromTuple(ITuple tuple) &#123; return tuple.getStringByField("word"); &#125; @Override public String getValueFromTuple(ITuple tuple) &#123; return null; &#125;&#125; 5. RedisFilterMapperRedisFilterMapper 继承 TupleMapper 和 RedisMapper 接口，用于查询数据时，定义了 declareOutputFields 方法，声明输出的字段。如下面的实现： 1234@Overridepublic void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("wordName", "count"));&#125; 四、自定义RedisBolt实现词频统计4.1 实现原理自定义 RedisBolt：主要利用 Redis 中哈希结构的 hincrby key field 命令进行词频统计。在 Redis 中 hincrby 的执行效果如下。hincrby 可以将字段按照指定的值进行递增，如果该字段不存在的话，还会新建该字段，并赋值为 0。通过这个命令可以非常轻松的实现词频统计功能。 123456789redis&gt; HSET myhash field 5(integer) 1redis&gt; HINCRBY myhash field 1(integer) 6redis&gt; HINCRBY myhash field -1(integer) 5redis&gt; HINCRBY myhash field -10(integer) -5redis&gt; 4.2 项目结构 4.3 自定义RedisBolt的代码实现123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 自定义 RedisBolt 利用 Redis 的哈希数据结构的 hincrby key field 命令进行词频统计 */public class RedisCountStoreBolt extends AbstractRedisBolt &#123; private final RedisStoreMapper storeMapper; private final RedisDataTypeDescription.RedisDataType dataType; private final String additionalKey; public RedisCountStoreBolt(JedisPoolConfig config, RedisStoreMapper storeMapper) &#123; super(config); this.storeMapper = storeMapper; RedisDataTypeDescription dataTypeDescription = storeMapper.getDataTypeDescription(); this.dataType = dataTypeDescription.getDataType(); this.additionalKey = dataTypeDescription.getAdditionalKey(); &#125; @Override protected void process(Tuple tuple) &#123; String key = storeMapper.getKeyFromTuple(tuple); String value = storeMapper.getValueFromTuple(tuple); JedisCommands jedisCommand = null; try &#123; jedisCommand = getInstance(); if (dataType == RedisDataTypeDescription.RedisDataType.HASH) &#123; jedisCommand.hincrBy(additionalKey, key, Long.valueOf(value)); &#125; else &#123; throw new IllegalArgumentException("Cannot process such data type for Count: " + dataType); &#125; collector.ack(tuple); &#125; catch (Exception e) &#123; this.collector.reportError(e); this.collector.fail(tuple); &#125; finally &#123; returnInstance(jedisCommand); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125;&#125; 4.4 CustomRedisCountApp1234567891011121314151617181920212223242526272829303132333435363738/** * 利用自定义的 RedisBolt 实现词频统计 */public class CustomRedisCountApp &#123; private static final String DATA_SOURCE_SPOUT = "dataSourceSpout"; private static final String SPLIT_BOLT = "splitBolt"; private static final String STORE_BOLT = "storeBolt"; private static final String REDIS_HOST = "192.168.200.226"; private static final int REDIS_PORT = 6379; public static void main(String[] args) &#123; TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(DATA_SOURCE_SPOUT, new DataSourceSpout()); // split builder.setBolt(SPLIT_BOLT, new SplitBolt()).shuffleGrouping(DATA_SOURCE_SPOUT); // save to redis and count JedisPoolConfig poolConfig = new JedisPoolConfig.Builder() .setHost(REDIS_HOST).setPort(REDIS_PORT).build(); RedisStoreMapper storeMapper = new WordCountStoreMapper(); RedisCountStoreBolt countStoreBolt = new RedisCountStoreBolt(poolConfig, storeMapper); builder.setBolt(STORE_BOLT, countStoreBolt).shuffleGrouping(SPLIT_BOLT); // 如果外部传参 cluster 则代表线上环境启动,否则代表本地启动 if (args.length &gt; 0 &amp;&amp; args[0].equals("cluster")) &#123; try &#123; StormSubmitter.submitTopology("ClusterCustomRedisCountApp", new Config(), builder.createTopology()); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; LocalCluster cluster = new LocalCluster(); cluster.submitTopology("LocalCustomRedisCountApp", new Config(), builder.createTopology()); &#125; &#125;&#125; 参考资料 Storm Redis Integration]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Storm</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Storm集成Kakfa]]></title>
    <url>%2F2019%2F11%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BStorm%E9%9B%86%E6%88%90Kakfa%2F</url>
    <content type="text"><![CDATA[Storm集成Kafka一、整合说明二、写入数据到Kafka三、从Kafka中读取数据 一、整合说明Storm 官方对 Kafka 的整合分为两个版本，官方说明文档分别如下： Storm Kafka Integration : 主要是针对 0.8.x 版本的 Kafka 提供整合支持； Storm Kafka Integration (0.10.x+) : 包含 Kafka 新版本的 consumer API，主要对 Kafka 0.10.x + 提供整合支持。 这里我服务端安装的 Kafka 版本为 2.2.0(Released Mar 22, 2019) ，按照官方 0.10.x+ 的整合文档进行整合，不适用于 0.8.x 版本的 Kafka。 二、写入数据到Kafka2.1 项目结构 2.2 项目主要依赖12345678910111213141516171819202122&lt;properties&gt; &lt;storm.version&gt;1.2.2&lt;/storm.version&gt; &lt;kafka.version&gt;2.2.0&lt;/kafka.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-kafka-client&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;$&#123;kafka.version&#125;&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.3 DataSourceSpout123456789101112131415161718192021222324252627282930313233343536373839/** * 产生词频样本的数据源 */public class DataSourceSpout extends BaseRichSpout &#123; private List&lt;String&gt; list = Arrays.asList("Spark", "Hadoop", "HBase", "Storm", "Flink", "Hive"); private SpoutOutputCollector spoutOutputCollector; @Override public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) &#123; this.spoutOutputCollector = spoutOutputCollector; &#125; @Override public void nextTuple() &#123; // 模拟产生数据 String lineData = productData(); spoutOutputCollector.emit(new Values(lineData)); Utils.sleep(1000); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields("line")); &#125; /** * 模拟数据 */ private String productData() &#123; Collections.shuffle(list); Random random = new Random(); int endIndex = random.nextInt(list.size()) % (list.size()) + 1; return StringUtils.join(list.toArray(), "\t", 0, endIndex); &#125;&#125; 产生的模拟数据格式如下： 123456789Spark HBaseHive Flink Storm Hadoop HBase SparkFlinkHBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase HiveHadoop Spark HBase Storm 2.4 WritingToKafkaApp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152/** * 写入数据到 Kafka 中 */public class WritingToKafkaApp &#123; private static final String BOOTSTRAP_SERVERS = "hadoop001:9092"; private static final String TOPIC_NAME = "storm-topic"; public static void main(String[] args) &#123; TopologyBuilder builder = new TopologyBuilder(); // 定义 Kafka 生产者属性 Properties props = new Properties(); /* * 指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找其他 broker 的信息。 * 不过建议至少要提供两个 broker 的信息作为容错。 */ props.put("bootstrap.servers", BOOTSTRAP_SERVERS); /* * acks 参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的。 * acks=0 : 生产者在成功写入消息之前不会等待任何来自服务器的响应。 * acks=1 : 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。 * acks=all : 只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。 */ props.put("acks", "1"); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); KafkaBolt bolt = new KafkaBolt&lt;String, String&gt;() .withProducerProperties(props) .withTopicSelector(new DefaultTopicSelector(TOPIC_NAME)) .withTupleToKafkaMapper(new FieldNameBasedTupleToKafkaMapper&lt;&gt;()); builder.setSpout("sourceSpout", new DataSourceSpout(), 1); builder.setBolt("kafkaBolt", bolt, 1).shuffleGrouping("sourceSpout"); if (args.length &gt; 0 &amp;&amp; args[0].equals("cluster")) &#123; try &#123; StormSubmitter.submitTopology("ClusterWritingToKafkaApp", new Config(), builder.createTopology()); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; LocalCluster cluster = new LocalCluster(); cluster.submitTopology("LocalWritingToKafkaApp", new Config(), builder.createTopology()); &#125; &#125;&#125; 2.5 测试准备工作进行测试前需要启动 Kakfa： 1. 启动KakfaKafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper,也可以启动自己安装的： 12345# zookeeper启动命令bin/zkServer.sh start# 内置zookeeper启动命令bin/zookeeper-server-start.sh config/zookeeper.properties 启动单节点 kafka 用于测试： 1# bin/kafka-server-start.sh config/server.properties 2. 创建topic12345# 创建用于测试主题bin/kafka-topics.sh --create --bootstrap-server hadoop001:9092 --replication-factor 1 --partitions 1 --topic storm-topic# 查看所有主题 bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092 3. 启动消费者 启动一个消费者用于观察写入情况，启动命令如下： 1# bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic storm-topic --from-beginning 2.6 测试可以用直接使用本地模式运行，也可以打包后提交到服务器集群运行。本仓库提供的源码默认采用 maven-shade-plugin 进行打包，打包命令如下： 1# mvn clean package -D maven.test.skip=true 启动后，消费者监听情况如下： 三、从Kafka中读取数据3.1 项目结构 3.2 ReadingFromKafkaApp123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * 从 Kafka 中读取数据 */public class ReadingFromKafkaApp &#123; private static final String BOOTSTRAP_SERVERS = "hadoop001:9092"; private static final String TOPIC_NAME = "storm-topic"; public static void main(String[] args) &#123; final TopologyBuilder builder = new TopologyBuilder(); builder.setSpout("kafka_spout", new KafkaSpout&lt;&gt;(getKafkaSpoutConfig(BOOTSTRAP_SERVERS, TOPIC_NAME)), 1); builder.setBolt("bolt", new LogConsoleBolt()).shuffleGrouping("kafka_spout"); // 如果外部传参 cluster 则代表线上环境启动,否则代表本地启动 if (args.length &gt; 0 &amp;&amp; args[0].equals("cluster")) &#123; try &#123; StormSubmitter.submitTopology("ClusterReadingFromKafkaApp", new Config(), builder.createTopology()); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; LocalCluster cluster = new LocalCluster(); cluster.submitTopology("LocalReadingFromKafkaApp", new Config(), builder.createTopology()); &#125; &#125; private static KafkaSpoutConfig&lt;String, String&gt; getKafkaSpoutConfig(String bootstrapServers, String topic) &#123; return KafkaSpoutConfig.builder(bootstrapServers, topic) // 除了分组 ID,以下配置都是可选的。分组 ID 必须指定,否则会抛出 InvalidGroupIdException 异常 .setProp(ConsumerConfig.GROUP_ID_CONFIG, "kafkaSpoutTestGroup") // 定义重试策略 .setRetry(getRetryService()) // 定时提交偏移量的时间间隔,默认是 15s .setOffsetCommitPeriodMs(10_000) .build(); &#125; // 定义重试策略 private static KafkaSpoutRetryService getRetryService() &#123; return new KafkaSpoutRetryExponentialBackoff(TimeInterval.microSeconds(500), TimeInterval.milliSeconds(2), Integer.MAX_VALUE, TimeInterval.seconds(10)); &#125;&#125; 3.3 LogConsoleBolt1234567891011121314151617181920212223242526272829/** * 打印从 Kafka 中获取的数据 */public class LogConsoleBolt extends BaseRichBolt &#123; private OutputCollector collector; public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector=collector; &#125; public void execute(Tuple input) &#123; try &#123; String value = input.getStringByField("value"); System.out.println("received from kafka : "+ value); // 必须 ack,否则会重复消费 kafka 中的消息 collector.ack(input); &#125;catch (Exception e)&#123; e.printStackTrace(); collector.fail(input); &#125; &#125; public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125;&#125; 这里从 value 字段中获取 kafka 输出的值数据。 在开发中，我们可以通过继承 RecordTranslator 接口定义了 Kafka 中 Record 与输出流之间的映射关系，可以在构建 KafkaSpoutConfig 的时候通过构造器或者 setRecordTranslator() 方法传入，并最后传递给具体的 KafkaSpout。 默认情况下使用内置的 DefaultRecordTranslator，其源码如下，FIELDS 中 定义了 tuple 中所有可用的字段：主题，分区，偏移量，消息键，值。 12345678910111213141516171819202122public class DefaultRecordTranslator&lt;K, V&gt; implements RecordTranslator&lt;K, V&gt; &#123; private static final long serialVersionUID = -5782462870112305750L; public static final Fields FIELDS = new Fields("topic", "partition", "offset", "key", "value"); @Override public List&lt;Object&gt; apply(ConsumerRecord&lt;K, V&gt; record) &#123; return new Values(record.topic(), record.partition(), record.offset(), record.key(), record.value()); &#125; @Override public Fields getFieldsFor(String stream) &#123; return FIELDS; &#125; @Override public List&lt;String&gt; streams() &#123; return DEFAULT_STREAM; &#125;&#125; 3.4 启动测试这里启动一个生产者用于发送测试数据，启动命令如下： 1# bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic storm-topic 本地运行的项目接收到从 Kafka 发送过来的数据： 用例源码下载地址：storm-kafka-integration 参考资料 Storm Kafka Integration (0.10.x+)]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Storm和流处理简介]]></title>
    <url>%2F2019%2F10%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BStorm%E5%92%8C%E6%B5%81%E5%A4%84%E7%90%86%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Storm和流处理简介一、Storm1.1 简介1.2 Storm 与 Hadoop对比1.3 Storm 与 Spark Streaming对比1.4 Storm 与 Flink对比二、流处理2.1 静态数据处理2.2 流处理 一、Storm1.1 简介Storm 是一个开源的分布式实时计算框架，可以以简单、可靠的方式进行大数据流的处理。通常用于实时分析，在线机器学习、持续计算、分布式 RPC、ETL 等场景。Storm 具有以下特点： 支持水平横向扩展； 具有高容错性，通过 ACK 机制每个消息都不丢失； 处理速度非常快，每个节点每秒能处理超过一百万个 tuples ； 易于设置和操作，并可以与任何编程语言一起使用； 支持本地模式运行，对于开发人员来说非常友好； 支持图形化管理界面。 1.2 Storm 与 Hadoop对比Hadoop 采用 MapReduce 处理数据，而 MapReduce 主要是对数据进行批处理，这使得 Hadoop 更适合于海量数据离线处理的场景。而 Strom 的设计目标是对数据进行实时计算，这使得其更适合实时数据分析的场景。 1.3 Storm 与 Spark Streaming对比Spark Streaming 并不是真正意义上的流处理框架。 Spark Streaming 接收实时输入的数据流，并将数据拆分为一系列批次，然后进行微批处理。只不过 Spark Streaming 能够将数据流进行极小粒度的拆分，使得其能够得到接近于流处理的效果，但其本质上还是批处理（或微批处理）。 1.4 Strom 与 Flink对比storm 和 Flink 都是真正意义上的实时计算框架。其对比如下： storm flink 状态管理 无状态 有状态 窗口支持 对事件窗口支持较弱，缓存整个窗口的所有数据，窗口结束时一起计算 窗口支持较为完善，自带一些窗口聚合方法，并且会自动管理窗口状态 消息投递 At Most OnceAt Least Once At Most OnceAt Least OnceExactly Once 容错方式 ACK 机制：对每个消息进行全链路跟踪，失败或者超时时候进行重发 检查点机制：通过分布式一致性快照机制，对数据流和算子状态进行保存。在发生错误时，使系统能够进行回滚。 注 : 对于消息投递，一般有以下三种方案： At Most Once : 保证每个消息会被投递 0 次或者 1 次，在这种机制下消息很有可能会丢失； At Least Once : 保证了每个消息会被默认投递多次，至少保证有一次被成功接收，信息可能有重复，但是不会丢失； Exactly Once : 每个消息对于接收者而言正好被接收一次，保证即不会丢失也不会重复。 二、流处理2.1 静态数据处理在流处理之前，数据通常存储在数据库或文件系统中，应用程序根据需要查询或计算数据，这就是传统的静态数据处理架构。Hadoop 采用 HDFS 进行数据存储，采用 MapReduce 进行数据查询或分析，这就是典型的静态数据处理架构。 2.2 流处理而流处理则是直接对运动中数据的处理，在接收数据的同时直接计算数据。实际上，在真实世界中的大多数数据都是连续的流，如传感器数据，网站用户活动数据，金融交易数据等等 ，所有这些数据都是随着时间的推移而源源不断地产生。 接收和发送数据流并执行应用程序或分析逻辑的系统称为流处理器。流处理器的基本职责是确保数据有效流动，同时具备可扩展性和容错能力，Storm 和 Flink 就是其代表性的实现。 流处理带来了很多优点： 可以立即对数据做出反应：降低了数据的滞后性，使得数据更具有时效性，更能反映对未来的预期； 可以处理更大的数据量：直接处理数据流，并且只保留数据中有意义的子集，然后将其传送到下一个处理单元，通过逐级过滤数据，从而降低实际需要处理的数据量； 更贴近现实的数据模型：在实际的环境中，一切数据都是持续变化的，想要通过历史数据推断未来的趋势，必须保证数据的不断输入和模型的持续修正，典型的就是金融市场、股票市场，流处理能更好地处理这些场景下对数据连续性和及时性的需求； 分散和分离基础设施：流式处理减少了对大型数据库的需求。每个流处理程序通过流处理框架维护了自己的数据和状态，这使其更适合于当下最流行的微服务架构。 参考资料 What is stream processing? 流计算框架 Flink 与 Storm 的性能对比]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Storm</tag>
        <tag>流处理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Storm集成HBase和HDFS]]></title>
    <url>%2F2019%2F10%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BStorm%E9%9B%86%E6%88%90HBase%E5%92%8CHDFS%2F</url>
    <content type="text"><![CDATA[Storm集成HDFS和HBase一、Storm集成HDFS二、Storm集成HBase 一、Storm集成HDFS1.1 项目结构 本用例源码下载地址：storm-hdfs-integration 1.2 项目主要依赖项目主要依赖如下，有两个地方需要注意： 这里由于我服务器上安装的是 CDH 版本的 Hadoop，在导入依赖时引入的也是 CDH 版本的依赖，需要使用 &lt;repository&gt; 标签指定 CDH 的仓库地址； hadoop-common、hadoop-client、hadoop-hdfs 均需要排除 slf4j-log4j12 依赖，原因是 storm-core 中已经有该依赖，不排除的话有 JAR 包冲突的风险； 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&lt;properties&gt; &lt;storm.version&gt;1.2.2&lt;/storm.version&gt;&lt;/properties&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--Storm 整合 HDFS 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-hdfs&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.15.2&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.15.2&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.6.0-cdh5.15.2&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 1.3 DataSourceSpout123456789101112131415161718192021222324252627282930313233343536373839/** * 产生词频样本的数据源 */public class DataSourceSpout extends BaseRichSpout &#123; private List&lt;String&gt; list = Arrays.asList("Spark", "Hadoop", "HBase", "Storm", "Flink", "Hive"); private SpoutOutputCollector spoutOutputCollector; @Override public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) &#123; this.spoutOutputCollector = spoutOutputCollector; &#125; @Override public void nextTuple() &#123; // 模拟产生数据 String lineData = productData(); spoutOutputCollector.emit(new Values(lineData)); Utils.sleep(1000); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields("line")); &#125; /** * 模拟数据 */ private String productData() &#123; Collections.shuffle(list); Random random = new Random(); int endIndex = random.nextInt(list.size()) % (list.size()) + 1; return StringUtils.join(list.toArray(), "\t", 0, endIndex); &#125;&#125; 产生的模拟数据格式如下： 123456789Spark HBaseHive Flink Storm Hadoop HBase SparkFlinkHBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase HiveHadoop Spark HBase Storm 1.4 将数据存储到HDFS这里 HDFS 的地址和数据存储路径均使用了硬编码，在实际开发中可以通过外部传参指定，这样程序更为灵活。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class DataToHdfsApp &#123; private static final String DATA_SOURCE_SPOUT = "dataSourceSpout"; private static final String HDFS_BOLT = "hdfsBolt"; public static void main(String[] args) &#123; // 指定 Hadoop 的用户名 如果不指定,则在 HDFS 创建目录时候有可能抛出无权限的异常 (RemoteException: Permission denied) System.setProperty("HADOOP_USER_NAME", "root"); // 定义输出字段 (Field) 之间的分隔符 RecordFormat format = new DelimitedRecordFormat() .withFieldDelimiter("|"); // 同步策略: 每 100 个 tuples 之后就会把数据从缓存刷新到 HDFS 中 SyncPolicy syncPolicy = new CountSyncPolicy(100); // 文件策略: 每个文件大小上限 1M,超过限定时,创建新文件并继续写入 FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(1.0f, Units.MB); // 定义存储路径 FileNameFormat fileNameFormat = new DefaultFileNameFormat() .withPath("/storm-hdfs/"); // 定义 HdfsBolt HdfsBolt hdfsBolt = new HdfsBolt() .withFsUrl("hdfs://hadoop001:8020") .withFileNameFormat(fileNameFormat) .withRecordFormat(format) .withRotationPolicy(rotationPolicy) .withSyncPolicy(syncPolicy); // 构建 Topology TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(DATA_SOURCE_SPOUT, new DataSourceSpout()); // save to HDFS builder.setBolt(HDFS_BOLT, hdfsBolt, 1).shuffleGrouping(DATA_SOURCE_SPOUT); // 如果外部传参 cluster 则代表线上环境启动,否则代表本地启动 if (args.length &gt; 0 &amp;&amp; args[0].equals("cluster")) &#123; try &#123; StormSubmitter.submitTopology("ClusterDataToHdfsApp", new Config(), builder.createTopology()); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; LocalCluster cluster = new LocalCluster(); cluster.submitTopology("LocalDataToHdfsApp", new Config(), builder.createTopology()); &#125; &#125;&#125; 1.5 启动测试可以用直接使用本地模式运行，也可以打包后提交到服务器集群运行。本仓库提供的源码默认采用 maven-shade-plugin 进行打包，打包命令如下： 1# mvn clean package -D maven.test.skip=true 运行后，数据会存储到 HDFS 的 /storm-hdfs 目录下。使用以下命令可以查看目录内容： 1234# 查看目录内容hadoop fs -ls /storm-hdfs# 监听文内容变化hadoop fs -tail -f /strom-hdfs/文件名 二、Storm集成HBase2.1 项目结构集成用例： 进行词频统计并将最后的结果存储到 HBase，项目主要结构如下： 本用例源码下载地址：storm-hbase-integration 2.2 项目主要依赖123456789101112131415161718&lt;properties&gt; &lt;storm.version&gt;1.2.2&lt;/storm.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!--Storm 整合 HBase 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-hbase&lt;/artifactId&gt; &lt;version&gt;$&#123;storm.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2.3 DataSourceSpout123456789101112131415161718192021222324252627282930313233343536373839/** * 产生词频样本的数据源 */public class DataSourceSpout extends BaseRichSpout &#123; private List&lt;String&gt; list = Arrays.asList("Spark", "Hadoop", "HBase", "Storm", "Flink", "Hive"); private SpoutOutputCollector spoutOutputCollector; @Override public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) &#123; this.spoutOutputCollector = spoutOutputCollector; &#125; @Override public void nextTuple() &#123; // 模拟产生数据 String lineData = productData(); spoutOutputCollector.emit(new Values(lineData)); Utils.sleep(1000); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields("line")); &#125; /** * 模拟数据 */ private String productData() &#123; Collections.shuffle(list); Random random = new Random(); int endIndex = random.nextInt(list.size()) % (list.size()) + 1; return StringUtils.join(list.toArray(), "\t", 0, endIndex); &#125;&#125; 产生的模拟数据格式如下： 123456789Spark HBaseHive Flink Storm Hadoop HBase SparkFlinkHBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase HiveHadoop Spark HBase Storm 2.4 SplitBolt1234567891011121314151617181920212223242526/** * 将每行数据按照指定分隔符进行拆分 */public class SplitBolt extends BaseRichBolt &#123; private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector = collector; &#125; @Override public void execute(Tuple input) &#123; String line = input.getStringByField("line"); String[] words = line.split("\t"); for (String word : words) &#123; collector.emit(tuple(word, 1)); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word", "count")); &#125;&#125; 2.5 CountBolt12345678910111213141516171819202122232425262728293031323334/** * 进行词频统计 */public class CountBolt extends BaseRichBolt &#123; private Map&lt;String, Integer&gt; counts = new HashMap&lt;&gt;(); private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector=collector; &#125; @Override public void execute(Tuple input) &#123; String word = input.getStringByField("word"); Integer count = counts.get(word); if (count == null) &#123; count = 0; &#125; count++; counts.put(word, count); // 输出 collector.emit(new Values(word, String.valueOf(count))); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word", "count")); &#125;&#125; 2.6 WordCountToHBaseApp12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * 进行词频统计 并将统计结果存储到 HBase 中 */public class WordCountToHBaseApp &#123; private static final String DATA_SOURCE_SPOUT = "dataSourceSpout"; private static final String SPLIT_BOLT = "splitBolt"; private static final String COUNT_BOLT = "countBolt"; private static final String HBASE_BOLT = "hbaseBolt"; public static void main(String[] args) &#123; // storm 的配置 Config config = new Config(); // HBase 的配置 Map&lt;String, Object&gt; hbConf = new HashMap&lt;&gt;(); hbConf.put("hbase.rootdir", "hdfs://hadoop001:8020/hbase"); hbConf.put("hbase.zookeeper.quorum", "hadoop001:2181"); // 将 HBase 的配置传入 Storm 的配置中 config.put("hbase.conf", hbConf); // 定义流数据与 HBase 中数据的映射 SimpleHBaseMapper mapper = new SimpleHBaseMapper() .withRowKeyField("word") .withColumnFields(new Fields("word","count")) .withColumnFamily("info"); /* * 给 HBaseBolt 传入表名、数据映射关系、和 HBase 的配置信息 * 表需要预先创建: create 'WordCount','info' */ HBaseBolt hbase = new HBaseBolt("WordCount", mapper) .withConfigKey("hbase.conf"); // 构建 Topology TopologyBuilder builder = new TopologyBuilder(); builder.setSpout(DATA_SOURCE_SPOUT, new DataSourceSpout(),1); // split builder.setBolt(SPLIT_BOLT, new SplitBolt(), 1).shuffleGrouping(DATA_SOURCE_SPOUT); // count builder.setBolt(COUNT_BOLT, new CountBolt(),1).shuffleGrouping(SPLIT_BOLT); // save to HBase builder.setBolt(HBASE_BOLT, hbase, 1).shuffleGrouping(COUNT_BOLT); // 如果外部传参 cluster 则代表线上环境启动,否则代表本地启动 if (args.length &gt; 0 &amp;&amp; args[0].equals("cluster")) &#123; try &#123; StormSubmitter.submitTopology("ClusterWordCountToRedisApp", config, builder.createTopology()); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125; else &#123; LocalCluster cluster = new LocalCluster(); cluster.submitTopology("LocalWordCountToRedisApp", config, builder.createTopology()); &#125; &#125;&#125; 2.7 启动测试可以用直接使用本地模式运行，也可以打包后提交到服务器集群运行。本仓库提供的源码默认采用 maven-shade-plugin 进行打包，打包命令如下： 1# mvn clean package -D maven.test.skip=true 运行后，数据会存储到 HBase 的 WordCount 表中。使用以下命令查看表的内容： 1hbase &gt; scan 'WordCount' 2.8 withCounterFields在上面的用例中我们是手动编码来实现词频统计，并将最后的结果存储到 HBase 中。其实也可以在构建 SimpleHBaseMapper 的时候通过 withCounterFields 指定 count 字段，被指定的字段会自动进行累加操作，这样也可以实现词频统计。需要注意的是 withCounterFields 指定的字段必须是 Long 类型，不能是 String 类型。 12345SimpleHBaseMapper mapper = new SimpleHBaseMapper() .withRowKeyField("word") .withColumnFields(new Fields("word")) .withCounterFields(new Fields("count")) .withColumnFamily("cf"); 参考资料 Apache HDFS Integration Apache HBase Integration]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>HDFS</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Storm集群环境搭建]]></title>
    <url>%2F2019%2F10%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BStorm%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Storm集群环境搭建一、集群规划二、前置条件三、集群搭建1. 下载并解压2. 配置环境变量3. 集群配置4. 安装包分发四. 启动集群4.1 启动ZooKeeper集群4.2 启动Storm集群4.3 查看集群五、高可用验证 一、集群规划这里搭建一个 3 节点的 Storm 集群：三台主机上均部署 Supervisor 和 LogViewer 服务。同时为了保证高可用，除了在 hadoop001 上部署主 Nimbus 服务外，还在 hadoop002 上部署备用的 Nimbus 服务。Nimbus 服务由 Zookeeper 集群进行协调管理，如果主 Nimbus 不可用，则备用 Nimbus 会成为新的主 Nimbus。 二、前置条件Storm 运行依赖于 Java 7+ 和 Python 2.6.6 +，所以需要预先安装这两个软件。同时为了保证高可用，这里我们不采用 Storm 内置的 Zookeeper，而采用外置的 Zookeeper 集群。由于这三个软件在多个框架中都有依赖，其安装步骤单独整理至 ： Linux 环境下 JDK 安装 Linux 环境下 Python 安装 Zookeeper 单机环境和集群环境搭建 三、集群搭建1. 下载并解压下载安装包，之后进行解压。官方下载地址：http://storm.apache.org/downloads.html 12# 解压tar -zxvf apache-storm-1.2.2.tar.gz 2. 配置环境变量1# vim /etc/profile 添加环境变量： 12export STORM_HOME=/usr/app/apache-storm-1.2.2export PATH=$STORM_HOME/bin:$PATH 使得配置的环境变量生效： 1# source /etc/profile 3. 集群配置修改 ${STORM_HOME}/conf/storm.yaml 文件，配置如下： 123456789101112131415161718# Zookeeper集群的主机列表storm.zookeeper.servers: - "hadoop001" - "hadoop002" - "hadoop003"# Nimbus的节点列表nimbus.seeds: ["hadoop001","hadoop002"]# Nimbus和Supervisor需要使用本地磁盘上来存储少量状态（如jar包，配置文件等）storm.local.dir: "/home/storm"# workers进程的端口，每个worker进程会使用一个端口来接收消息supervisor.slots.ports: - 6700 - 6701 - 6702 - 6703 supervisor.slots.ports 参数用来配置 workers 进程接收消息的端口，默认每个 supervisor 节点上会启动 4 个 worker，当然你也可以按照自己的需要和服务器性能进行设置，假设只想启动 2 个 worker 的话，此处配置 2 个端口即可。 4. 安装包分发将 Storm 的安装包分发到其他服务器，分发后建议在这两台服务器上也配置一下 Storm 的环境变量。 12scp -r /usr/app/apache-storm-1.2.2/ root@hadoop002:/usr/app/scp -r /usr/app/apache-storm-1.2.2/ root@hadoop003:/usr/app/ 四. 启动集群4.1 启动ZooKeeper集群分别到三台服务器上启动 ZooKeeper 服务： 1zkServer.sh start 4.2 启动Storm集群因为要启动多个进程，所以统一采用后台进程的方式启动。进入到 ${STORM_HOME}/bin 目录下，执行下面的命令： hadoop001 &amp; hadoop002 ： 12345678# 启动主节点 nimbusnohup sh storm nimbus &amp;# 启动从节点 supervisor nohup sh storm supervisor &amp;# 启动UI界面 ui nohup sh storm ui &amp;# 启动日志查看服务 logviewer nohup sh storm logviewer &amp; hadoop003 ： hadoop003 上只需要启动 supervisor 服务和 logviewer 服务： 1234# 启动从节点 supervisor nohup sh storm supervisor &amp;# 启动日志查看服务 logviewer nohup sh storm logviewer &amp; 4.3 查看集群使用 jps 查看进程，三台服务器的进程应该分别如下： 访问 hadoop001 或 hadoop002 的 8080 端口，界面如下。可以看到有一主一备 2 个 Nimbus 和 3 个 Supervisor，并且每个 Supervisor 有四个 slots，即四个可用的 worker 进程，此时代表集群已经搭建成功。 五、高可用验证这里手动模拟主 Nimbus 异常的情况，在 hadoop001 上使用 kill 命令杀死 Nimbus 的线程，此时可以看到 hadoop001 上的 Nimbus 已经处于 offline 状态，而 hadoop002 上的 Nimbus 则成为新的 Leader。]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>集群搭建</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Storm单机环境搭建]]></title>
    <url>%2F2019%2F10%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BStorm%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Storm单机版本环境搭建1. 安装环境要求 you need to install Storm’s dependencies on Nimbus and the worker machines. These are: Java 7+ (Apache Storm 1.x is tested through travis ci against both java 7 and java 8 JDKs) Python 2.6.6 (Python 3.x should work too, but is not tested as part of our CI enviornment) 按照官方文档 的说明：storm 运行依赖于 Java 7+ 和 Python 2.6.6 +，所以需要预先安装这两个软件。由于这两个软件在多个框架中都有依赖，其安装步骤单独整理至 ： Linux 环境下 JDK 安装 Linux 环境下 Python 安装 2. 下载并解压下载并解压，官方下载地址：http://storm.apache.org/downloads.html 1# tar -zxvf apache-storm-1.2.2.tar.gz 3. 配置环境变量1# vim /etc/profile 添加环境变量： 12export STORM_HOME=/usr/app/apache-storm-1.2.2export PATH=$STORM_HOME/bin:$PATH 使得配置的环境变量生效： 1# source /etc/profile 4. 启动相关进程因为要启动多个进程，所以统一采用后台进程的方式启动。进入到 ${STORM_HOME}/bin 目录下，依次执行下面的命令： 12345678910# 启动zookeepernohup sh storm dev-zookeeper &amp;# 启动主节点 nimbusnohup sh storm nimbus &amp;# 启动从节点 supervisor nohup sh storm supervisor &amp;# 启动UI界面 ui nohup sh storm ui &amp;# 启动日志查看服务 logviewer nohup sh storm logviewer &amp; 5. 验证是否启动成功验证方式一：jps 查看进程： 123456[root@hadoop001 app]# jps1074 nimbus1283 Supervisor620 dev_zookeeper1485 core9630 logviewer 验证方式二： 访问 8080 端口，查看 Web-UI 界面：]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>环境搭建</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Storm编程模型详解]]></title>
    <url>%2F2019%2F10%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BStorm%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Storm 编程模型一、简介二、IComponent接口三、Spout3.1 ISpout接口3.2 BaseRichSpout抽象类四、Bolt4.1 IBolt 接口4.2 BaseRichBolt抽象类五、词频统计案例六、提交到服务器集群运行七、关于项目打包的扩展说明 一、简介下图为 Strom 的运行流程图，在开发 Storm 流处理程序时，我们需要采用内置或自定义实现 spout(数据源) 和 bolt(处理单元)，并通过 TopologyBuilder 将它们之间进行关联，形成 Topology。 二、IComponent接口IComponent 接口定义了 Topology 中所有组件 (spout/bolt) 的公共方法，自定义的 spout 或 bolt 必须直接或间接实现这个接口。 123456789101112131415public interface IComponent extends Serializable &#123; /** * 声明此拓扑的所有流的输出模式。 * @param declarer 这用于声明输出流 id，输出字段以及每个输出流是否是直接流（direct stream） */ void declareOutputFields(OutputFieldsDeclarer declarer); /** * 声明此组件的配置。 * */ Map&lt;String, Object&gt; getComponentConfiguration();&#125; 三、Spout3.1 ISpout接口自定义的 spout 需要实现 ISpout 接口，它定义了 spout 的所有可用方法： 123456789101112131415161718192021222324252627282930313233343536373839404142public interface ISpout extends Serializable &#123; /** * 组件初始化时候被调用 * * @param conf ISpout 的配置 * @param context 应用上下文，可以通过其获取任务 ID 和组件 ID，输入和输出信息等。 * @param collector 用来发送 spout 中的 tuples，它是线程安全的，建议保存为此 spout 对象的实例变量 */ void open(Map conf, TopologyContext context, SpoutOutputCollector collector); /** * ISpout 将要被关闭的时候调用。但是其不一定会被执行，如果在集群环境中通过 kill -9 杀死进程时其就无法被执行。 */ void close(); /** * 当 ISpout 从停用状态激活时被调用 */ void activate(); /** * 当 ISpout 停用时候被调用 */ void deactivate(); /** * 这是一个核心方法，主要通过在此方法中调用 collector 将 tuples 发送给下一个接收器，这个方法必须是非阻塞的。 * nextTuple/ack/fail/是在同一个线程中执行的，所以不用考虑线程安全方面。当没有 tuples 发出时应该让 * nextTuple 休眠 (sleep) 一下，以免浪费 CPU。 */ void nextTuple(); /** * 通过 msgId 进行 tuples 处理成功的确认，被确认后的 tuples 不会再次被发送 */ void ack(Object msgId); /** * 通过 msgId 进行 tuples 处理失败的确认，被确认后的 tuples 会再次被发送进行处理 */ void fail(Object msgId);&#125; 3.2 BaseRichSpout抽象类通常情况下，我们实现自定义的 Spout 时不会直接去实现 ISpout 接口，而是继承 BaseRichSpout。BaseRichSpout 继承自 BaseCompont，同时实现了 IRichSpout 接口。 IRichSpout 接口继承自 ISpout 和 IComponent,自身并没有定义任何方法： 123public interface IRichSpout extends ISpout, IComponent &#123;&#125; BaseComponent 抽象类空实现了 IComponent 中 getComponentConfiguration 方法： 123456public abstract class BaseComponent implements IComponent &#123; @Override public Map&lt;String, Object&gt; getComponentConfiguration() &#123; return null; &#125; &#125; BaseRichSpout 继承自 BaseCompont 类并实现了 IRichSpout 接口，并且空实现了其中部分方法： 12345678910111213141516public abstract class BaseRichSpout extends BaseComponent implements IRichSpout &#123; @Override public void close() &#123;&#125; @Override public void activate() &#123;&#125; @Override public void deactivate() &#123;&#125; @Override public void ack(Object msgId) &#123;&#125; @Override public void fail(Object msgId) &#123;&#125;&#125; 通过这样的设计，我们在继承 BaseRichSpout 实现自定义 spout 时，就只有三个方法必须实现： open ： 来源于 ISpout，可以通过此方法获取用来发送 tuples 的 SpoutOutputCollector； nextTuple ：来源于 ISpout，必须在此方法内部发送 tuples； declareOutputFields ：来源于 IComponent，声明发送的 tuples 的名称，这样下一个组件才能知道如何接受。 四、Boltbolt 接口的设计与 spout 的类似： 4.1 IBolt 接口1234567891011121314151617181920212223242526 /** * 在客户端计算机上创建的 IBolt 对象。会被被序列化到 topology 中（使用 Java 序列化）,并提交给集群的主机（Nimbus）。 * Nimbus 启动 workers 反序列化对象，调用 prepare，然后开始处理 tuples。 */public interface IBolt extends Serializable &#123; /** * 组件初始化时候被调用 * * @param conf storm 中定义的此 bolt 的配置 * @param context 应用上下文，可以通过其获取任务 ID 和组件 ID，输入和输出信息等。 * @param collector 用来发送 spout 中的 tuples，它是线程安全的，建议保存为此 spout 对象的实例变量 */ void prepare(Map stormConf, TopologyContext context, OutputCollector collector); /** * 处理单个 tuple 输入。 * * @param Tuple 对象包含关于它的元数据（如来自哪个组件/流/任务） */ void execute(Tuple input); /** * IBolt 将要被关闭的时候调用。但是其不一定会被执行，如果在集群环境中通过 kill -9 杀死进程时其就无法被执行。 */ void cleanup(); 4.2 BaseRichBolt抽象类同样的，在实现自定义 bolt 时，通常是继承 BaseRichBolt 抽象类来实现。BaseRichBolt 继承自 BaseComponent 抽象类并实现了 IRichBolt 接口。 IRichBolt 接口继承自 IBolt 和 IComponent,自身并没有定义任何方法： 123public interface IRichBolt extends IBolt, IComponent &#123;&#125; 通过这样的设计，在继承 BaseRichBolt 实现自定义 bolt 时，就只需要实现三个必须的方法： prepare： 来源于 IBolt，可以通过此方法获取用来发送 tuples 的 OutputCollector； execute：来源于 IBolt，处理 tuples 和发送处理完成的 tuples； declareOutputFields ：来源于 IComponent，声明发送的 tuples 的名称，这样下一个组件才能知道如何接收。 五、词频统计案例5.1 案例简介这里我们使用自定义的 DataSourceSpout 产生词频数据，然后使用自定义的 SplitBolt 和 CountBolt 来进行词频统计。 案例源码下载地址：storm-word-count 5.2 代码实现1. 项目依赖12345&lt;dependency&gt; &lt;groupId&gt;org.apache.storm&lt;/groupId&gt; &lt;artifactId&gt;storm-core&lt;/artifactId&gt; &lt;version&gt;1.2.2&lt;/version&gt;&lt;/dependency&gt; 2. DataSourceSpout123456789101112131415161718192021222324252627282930313233343536public class DataSourceSpout extends BaseRichSpout &#123; private List&lt;String&gt; list = Arrays.asList("Spark", "Hadoop", "HBase", "Storm", "Flink", "Hive"); private SpoutOutputCollector spoutOutputCollector; @Override public void open(Map map, TopologyContext topologyContext, SpoutOutputCollector spoutOutputCollector) &#123; this.spoutOutputCollector = spoutOutputCollector; &#125; @Override public void nextTuple() &#123; // 模拟产生数据 String lineData = productData(); spoutOutputCollector.emit(new Values(lineData)); Utils.sleep(1000); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) &#123; outputFieldsDeclarer.declare(new Fields("line")); &#125; /** * 模拟数据 */ private String productData() &#123; Collections.shuffle(list); Random random = new Random(); int endIndex = random.nextInt(list.size()) % (list.size()) + 1; return StringUtils.join(list.toArray(), "\t", 0, endIndex); &#125;&#125; 上面类使用 productData 方法来产生模拟数据，产生数据的格式如下： 123456789Spark HBaseHive Flink Storm Hadoop HBase SparkFlinkHBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase HiveHadoop Spark HBase Storm 3. SplitBolt1234567891011121314151617181920212223public class SplitBolt extends BaseRichBolt &#123; private OutputCollector collector; @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; this.collector=collector; &#125; @Override public void execute(Tuple input) &#123; String line = input.getStringByField("line"); String[] words = line.split("\t"); for (String word : words) &#123; collector.emit(new Values(word)); &#125; &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; declarer.declare(new Fields("word")); &#125;&#125; 4. CountBolt1234567891011121314151617181920212223242526272829public class CountBolt extends BaseRichBolt &#123; private Map&lt;String, Integer&gt; counts = new HashMap&lt;&gt;(); @Override public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) &#123; &#125; @Override public void execute(Tuple input) &#123; String word = input.getStringByField("word"); Integer count = counts.get(word); if (count == null) &#123; count = 0; &#125; count++; counts.put(word, count); // 输出 System.out.print("当前实时统计结果:"); counts.forEach((key, value) -&gt; System.out.print(key + ":" + value + "; ")); System.out.println(); &#125; @Override public void declareOutputFields(OutputFieldsDeclarer declarer) &#123; &#125;&#125; 5. LocalWordCountApp通过 TopologyBuilder 将上面定义好的组件进行串联形成 Topology，并提交到本地集群（LocalCluster）运行。通常在开发中，可先用本地模式进行测试，测试完成后再提交到服务器集群运行。 1234567891011121314151617181920public class LocalWordCountApp &#123; public static void main(String[] args) &#123; TopologyBuilder builder = new TopologyBuilder(); builder.setSpout("DataSourceSpout", new DataSourceSpout()); // 指明将 DataSourceSpout 的数据发送到 SplitBolt 中处理 builder.setBolt("SplitBolt", new SplitBolt()).shuffleGrouping("DataSourceSpout"); // 指明将 SplitBolt 的数据发送到 CountBolt 中 处理 builder.setBolt("CountBolt", new CountBolt()).shuffleGrouping("SplitBolt"); // 创建本地集群用于测试 这种模式不需要本机安装 storm,直接运行该 Main 方法即可 LocalCluster cluster = new LocalCluster(); cluster.submitTopology("LocalWordCountApp", new Config(), builder.createTopology()); &#125;&#125; 6. 运行结果启动 WordCountApp 的 main 方法即可运行，采用本地模式 Storm 会自动在本地搭建一个集群，所以启动的过程会稍慢一点，启动成功后即可看到输出日志。 六、提交到服务器集群运行6.1 代码更改提交到服务器的代码和本地代码略有不同，提交到服务器集群时需要使用 StormSubmitter 进行提交。主要代码如下： 为了结构清晰，这里新建 ClusterWordCountApp 类来演示集群模式的提交。实际开发中可以将两种模式的代码写在同一个类中，通过外部传参来决定启动何种模式。 12345678910111213141516171819202122public class ClusterWordCountApp &#123; public static void main(String[] args) &#123; TopologyBuilder builder = new TopologyBuilder(); builder.setSpout("DataSourceSpout", new DataSourceSpout()); // 指明将 DataSourceSpout 的数据发送到 SplitBolt 中处理 builder.setBolt("SplitBolt", new SplitBolt()).shuffleGrouping("DataSourceSpout"); // 指明将 SplitBolt 的数据发送到 CountBolt 中 处理 builder.setBolt("CountBolt", new CountBolt()).shuffleGrouping("SplitBolt"); // 使用 StormSubmitter 提交 Topology 到服务器集群 try &#123; StormSubmitter.submitTopology("ClusterWordCountApp", new Config(), builder.createTopology()); &#125; catch (AlreadyAliveException | InvalidTopologyException | AuthorizationException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 6.2 打包上传打包后上传到服务器任意位置，这里我打包后的名称为 storm-word-count-1.0.jar 1# mvn clean package -Dmaven.test.skip=true 6.3 提交Topology使用以下命令提交 Topology 到集群： 12# 命令格式: storm jar jar包位置 主类的全路径 ...可选传参storm jar /usr/appjar/storm-word-count-1.0.jar com.heibaiying.wordcount.ClusterWordCountApp 出现 successfully 则代表提交成功： 6.4 查看Topology与停止Topology（命令行方式）12345# 查看所有Topologystorm list# 停止 storm kill topology-name [-w wait-time-secs]storm kill ClusterWordCountApp -w 3 6.5 查看Topology与停止Topology（界面方式）使用 UI 界面同样也可进行停止操作，进入 WEB UI 界面（8080 端口），在 Topology Summary 中点击对应 Topology 即可进入详情页面进行操作。 七、关于项目打包的扩展说明mvn package的局限性在上面的步骤中，我们没有在 POM 中配置任何插件，就直接使用 mvn package 进行项目打包，这对于没有使用外部依赖包的项目是可行的。但如果项目中使用了第三方 JAR 包，就会出现问题，因为 package 打包后的 JAR 中是不含有依赖包的，如果此时你提交到服务器上运行，就会出现找不到第三方依赖的异常。 这时候可能大家会有疑惑，在我们的项目中不是使用了 storm-core 这个依赖吗？其实上面之所以我们能运行成功，是因为在 Storm 的集群环境中提供了这个 JAR 包，在安装目录的 lib 目录下： 为了说明这个问题我在 Maven 中引入了一个第三方的 JAR 包，并修改产生数据的方法：12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.8.1&lt;/version&gt;&lt;/dependency&gt;StringUtils.join() 这个方法在 commons.lang3 和 storm-core 中都有，原来的代码无需任何更改，只需要在 import 时指明使用 commons.lang3。12345678import org.apache.commons.lang3.StringUtils;private String productData() &#123; Collections.shuffle(list); Random random = new Random(); int endIndex = random.nextInt(list.size()) % (list.size()) + 1; return StringUtils.join(list.toArray(), "\t", 0, endIndex);&#125;此时直接使用 mvn clean package 打包运行，就会抛出下图的异常。因此这种直接打包的方式并不适用于实际的开发，因为实际开发中通常都是需要第三方的 JAR 包。 想把依赖包一并打入最后的 JAR 中，maven 提供了两个插件来实现，分别是 maven-assembly-plugin 和 maven-shade-plugin。鉴于本篇文章篇幅已经比较长，且关于 Storm 打包还有很多需要说明的地方，所以关于 Storm 的打包方式单独整理至下一篇文章： Storm 三种打包方式对比分析 参考资料 Running Topologies on a Production Cluster Pre-defined Descriptor Files]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>编程模型</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Storm核心概念详解]]></title>
    <url>%2F2019%2F10%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BStorm%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Storm 核心概念详解一、Storm核心概念1.1 Topologies（拓扑）1.2 Streams（流）1.3 Spouts 1.4 Bolts1.5 Stream groupings（分组策略）二、Storm架构详解2.1 nimbus进程2.2 supervisor进程2.3 zookeeper的作用2.4 worker进程2.5 executor线程2.6 并行度 一、Storm核心概念 1.1 Topologies（拓扑）一个完整的 Storm 流处理程序被称为 Storm topology(拓扑)。它是一个是由 Spouts 和 Bolts 通过 Stream 连接起来的有向无环图，Storm 会保持每个提交到集群的 topology 持续地运行，从而处理源源不断的数据流，直到你将主动其杀死 (kill) 为止。 1.2 Streams（流）Stream 是 Storm 中的核心概念。一个 Stream 是一个无界的、以分布式方式并行创建和处理的 Tuple 序列。Tuple 可以包含大多数基本类型以及自定义类型的数据。简单来说，Tuple 就是流数据的实际载体，而 Stream 就是一系列 Tuple。 1.3 SpoutsSpouts 是流数据的源头，一个 Spout 可以向不止一个 Streams 中发送数据。Spout 通常分为可靠和不可靠两种：可靠的 Spout 能够在失败时重新发送 Tuple, 不可靠的 Spout 一旦把 Tuple 发送出去就置之不理了。 1.4 BoltsBolts 是流数据的处理单元，它可以从一个或者多个 Streams 中接收数据，处理完成后再发射到新的 Streams 中。Bolts 可以执行过滤 (filtering)，聚合 (aggregations)，连接 (joins) 等操作，并能与文件系统或数据库进行交互。 1.5 Stream groupings（分组策略） spouts 和 bolts 在集群上执行任务时，是由多个 Task 并行执行 (如上图，每一个圆圈代表一个 Task)。当一个 Tuple 需要从 Bolt A 发送给 Bolt B 执行的时候，程序如何知道应该发送给 Bolt B 的哪一个 Task 执行呢？ 这是由 Stream groupings 分组策略来决定的，Storm 中一共有如下 8 个内置的 Stream Grouping。当然你也可以通过实现 CustomStreamGrouping 接口来实现自定义 Stream 分组策略。 Shuffle grouping Tuples 随机的分发到每个 Bolt 的每个 Task 上，每个 Bolt 获取到等量的 Tuples。 Fields grouping Streams 通过 grouping 指定的字段 (field) 来分组。假设通过 user-id 字段进行分区，那么具有相同 user-id 的 Tuples 就会发送到同一个 Task。 Partial Key grouping Streams 通过 grouping 中指定的字段 (field) 来分组，与 Fields Grouping 相似。但是对于两个下游的 Bolt 来说是负载均衡的，可以在输入数据不平均的情况下提供更好的优化。 All grouping Streams 会被所有的 Bolt 的 Tasks 进行复制。由于存在数据重复处理，所以需要谨慎使用。 Global grouping 整个 Streams 会进入 Bolt 的其中一个 Task，通常会进入 id 最小的 Task。 None grouping 当前 None grouping 和 Shuffle grouping 等价，都是进行随机分发。 Direct grouping Direct grouping 只能被用于 direct streams 。使用这种方式需要由 Tuple 的生产者直接指定由哪个 Task 进行处理。 Local or shuffle grouping 如果目标 Bolt 有 Tasks 和当前 Bolt 的 Tasks 处在同一个 Worker 进程中，那么则优先将 Tuple Shuffled 到处于同一个进程的目标 Bolt 的 Tasks 上，这样可以最大限度地减少网络传输。否则，就和普通的 Shuffle Grouping 行为一致。 二、Storm架构详解 2.1 Nimbus进程 也叫做 Master Node，是 Storm 集群工作的全局指挥官。主要功能如下： 通过 Thrift 接口，监听并接收 Client 提交的 Topology； 根据集群 Workers 的资源情况，将 Client 提交的 Topology 进行任务分配，分配结果写入 Zookeeper; 通过 Thrift 接口，监听 Supervisor 的下载 Topology 代码的请求，并提供下载 ; 通过 Thrift 接口，监听 UI 对统计信息的读取，从 Zookeeper 上读取统计信息，返回给 UI; 若进程退出后，立即在本机重启，则不影响集群运行。 2.2 Supervisor进程也叫做 Worker Node , 是 Storm 集群的资源管理者，按需启动 Worker 进程。主要功能如下： 定时从 Zookeeper 检查是否有新 Topology 代码未下载到本地 ，并定时删除旧 Topology 代码 ; 根据 Nimbus 的任务分配计划，在本机按需启动 1 个或多个 Worker 进程，并监控所有的 Worker 进程的情况； 若进程退出，立即在本机重启，则不影响集群运行。 2.3 zookeeper的作用Nimbus 和 Supervisor 进程都被设计为快速失败（遇到任何意外情况时进程自毁）和无状态（所有状态保存在 Zookeeper 或磁盘上）。 这样设计的好处就是如果它们的进程被意外销毁，那么在重新启动后，就只需要从 Zookeeper 上获取之前的状态数据即可，并不会造成任何数据丢失。 2.4 Worker进程Storm 集群的任务构造者 ，构造 Spoult 或 Bolt 的 Task 实例，启动 Executor 线程。主要功能如下： 根据 Zookeeper 上分配的 Task，在本进程中启动 1 个或多个 Executor 线程，将构造好的 Task 实例交给 Executor 去运行； 向 Zookeeper 写入心跳 ； 维持传输队列，发送 Tuple 到其他的 Worker ； 若进程退出，立即在本机重启，则不影响集群运行。 2.5 Executor线程Storm 集群的任务执行者 ，循环执行 Task 代码。主要功能如下： 执行 1 个或多个 Task； 执行 Acker 机制，负责发送 Task 处理状态给对应 Spout 所在的 worker。 2.6 并行度 1 个 Worker 进程执行的是 1 个 Topology 的子集，不会出现 1 个 Worker 为多个 Topology 服务的情况，因此 1 个运行中的 Topology 就是由集群中多台物理机上的多个 Worker 进程组成的。1 个 Worker 进程会启动 1 个或多个 Executor 线程来执行 1 个 Topology 的 Component(组件，即 Spout 或 Bolt)。 Executor 是 1 个被 Worker 进程启动的单独线程。每个 Executor 会运行 1 个 Component 中的一个或者多个 Task。 Task 是组成 Component 的代码单元。Topology 启动后，1 个 Component 的 Task 数目是固定不变的，但该 Component 使用的 Executor 线程数可以动态调整（例如：1 个 Executor 线程可以执行该 Component 的 1 个或多个 Task 实例）。这意味着，对于 1 个 Component 来说，#threads&lt;=#tasks（线程数小于等于 Task 数目）这样的情况是存在的。默认情况下 Task 的数目等于 Executor 线程数，即 1 个 Executor 线程只运行 1 个 Task。 总结如下： 一个运行中的 Topology 由集群中的多个 Worker 进程组成的； 在默认情况下，每个 Worker 进程默认启动一个 Executor 线程； 在默认情况下，每个 Executor 默认启动一个 Task 线程； Task 是组成 Component 的代码单元。 参考资料 storm documentation -&gt; Concepts Internal Working of Apache Storm Understanding the Parallelism of a Storm Topology Storm nimbus 单节点宕机的处理]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>核心概念</tag>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala模式匹配]]></title>
    <url>%2F2019%2F10%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E6%A8%A1%E5%BC%8F%E5%8C%B9%E9%85%8D%2F</url>
    <content type="text"><![CDATA[Scala模式匹配一、模式匹配1.1 更好的swith1.2 用作类型检查1.3 匹配数据结构1.4 提取器二、样例类2.1 样例类2.3 用于模式匹配 一、模式匹配Scala 支持模式匹配机制，可以代替 swith 语句、执行类型检查、以及支持析构表达式等。 1.1 更好的swithScala 不支持 swith，可以使用模式匹配 match...case 语法代替。但是 match 语句与 Java 中的 switch 有以下三点不同： Scala 中的 case 语句支持任何类型；而 Java 中 case 语句仅支持整型、枚举和字符串常量； Scala 中每个分支语句后面不需要写 break，因为在 case 语句中 break 是隐含的，默认就有； 在 Scala 中 match 语句是有返回值的，而 Java 中 switch 语句是没有返回值的。如下： 12345678910111213object ScalaApp extends App &#123; def matchTest(x: Int) = x match &#123; case 1 =&gt; "one" case 2 =&gt; "two" case _ if x &gt; 9 &amp;&amp; x &lt; 100 =&gt; "两位数" //支持条件表达式 这被称为模式守卫 case _ =&gt; "other" &#125; println(matchTest(1)) //输出 one println(matchTest(10)) //输出 两位数 println(matchTest(200)) //输出 other&#125; 1.2 用作类型检查1234567891011121314object ScalaApp extends App &#123; def matchTest[T](x: T) = x match &#123; case x: Int =&gt; "数值型" case x: String =&gt; "字符型" case x: Float =&gt; "浮点型" case _ =&gt; "other" &#125; println(matchTest(1)) //输出 数值型 println(matchTest(10.3f)) //输出 浮点型 println(matchTest("str")) //输出 字符型 println(matchTest(2.1)) //输出 other&#125; 1.3 匹配数据结构匹配元组示例： 123456789101112object ScalaApp extends App &#123; def matchTest(x: Any) = x match &#123; case (0, _, _) =&gt; "匹配第一个元素为 0 的元组" case (a, b, c) =&gt; println(a + "~" + b + "~" + c) case _ =&gt; "other" &#125; println(matchTest((0, 1, 2))) // 输出: 匹配第一个元素为 0 的元组 matchTest((1, 2, 3)) // 输出: 1~2~3 println(matchTest(Array(10, 11, 12, 14))) // 输出: other&#125; 匹配数组示例： 1234567891011121314object ScalaApp extends App &#123; def matchTest[T](x: Array[T]) = x match &#123; case Array(0) =&gt; "匹配只有一个元素 0 的数组" case Array(a, b) =&gt; println(a + "~" + b) case Array(10, _*) =&gt; "第一个元素为 10 的数组" case _ =&gt; "other" &#125; println(matchTest(Array(0))) // 输出: 匹配只有一个元素 0 的数组 matchTest(Array(1, 2)) // 输出: 1~2 println(matchTest(Array(10, 11, 12))) // 输出: 第一个元素为 10 的数组 println(matchTest(Array(3, 2, 1))) // 输出: other&#125; 1.4 提取器数组、列表和元组能使用模式匹配，都是依靠提取器 (extractor) 机制，它们伴生对象中定义了 unapply 或 unapplySeq 方法： unapply：用于提取固定数量的对象； unapplySeq：用于提取一个序列； 这里以数组为例，Array.scala 定义了 unapplySeq 方法： 1def unapplySeq[T](x : scala.Array[T]) : scala.Option[scala.IndexedSeq[T]] = &#123; /* compiled code */ &#125; unapplySeq 返回一个序列，包含数组中的所有值，这样在模式匹配时，才能知道对应位置上的值。 二、样例类2.1 样例类样例类是一种的特殊的类，它们被经过优化以用于模式匹配，样例类的声明比较简单，只需要在 class 前面加上关键字 case。下面给出一个样例类及其用于模式匹配的示例： 12//声明一个抽象类abstract class Person&#123;&#125; 12// 样例类 Employeecase class Employee(name: String, age: Int, salary: Double) extends Person &#123;&#125; 12// 样例类 Studentcase class Student(name: String, age: Int) extends Person &#123;&#125; 当你声明样例类后，编译器自动进行以下配置： 构造器中每个参数都默认为 val； 自动地生成 equals, hashCode, toString, copy 等方法； 伴生对象中自动生成 apply 方法，使得可以不用 new 关键字就能构造出相应的对象； 伴生对象中自动生成 unapply 方法，以支持模式匹配。 除了上面的特征外，样例类和其他类相同，可以任意添加方法和字段，扩展它们。 2.3 用于模式匹配样例的伴生对象中自动生成 unapply 方法，所以样例类可以支持模式匹配，使用如下： 1234567891011object ScalaApp extends App &#123; def matchTest(person: Person) = person match &#123; case Student(name, _) =&gt; "student:" + name case Employee(_, _, salary) =&gt; "employee salary:" + salary case _ =&gt; "other" &#125; println(matchTest(Student("heibai", 12))) //输出: student:heibai println(matchTest(Employee("ying", 22, 999999))) //输出: employee salary:999999.0&#125; 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>模式匹配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala继承和特质]]></title>
    <url>%2F2019%2F10%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E7%BB%A7%E6%89%BF%E5%92%8C%E7%89%B9%E8%B4%A8%2F</url>
    <content type="text"><![CDATA[继承和特质一、继承1.1 Scala中的继承结构1.2 extends &amp; override1.3 调用超类构造器1.4 类型检查和转换1.5 构造顺序和提前定义二、抽象类三、特质3.1 trait &amp; with3.2 特质中的字段3.3 带有特质的对象3.4 特质构造顺序 一、继承1.1 Scala中的继承结构Scala 中继承关系如下图： Any 是整个继承关系的根节点； AnyRef 包含 Scala Classes 和 Java Classes，等价于 Java 中的 java.lang.Object； AnyVal 是所有值类型的一个标记； Null 是所有引用类型的子类型，唯一实例是 null，可以将 null 赋值给除了值类型外的所有类型的变量; Nothing 是所有类型的子类型。 1.2 extends &amp; overrideScala 的集成机制和 Java 有很多相似之处，比如都使用 extends 关键字表示继承，都使用 override 关键字表示重写父类的方法或成员变量。示例如下： 123456789101112131415161718//父类class Person &#123; var name = "" // 1.不加任何修饰词,默认为 public,能被子类和外部访问 var age = 0 // 2.使用 protected 修饰的变量能子类访问，但是不能被外部访问 protected var birthday = "" // 3.使用 private 修饰的变量不能被子类和外部访问 private var sex = "" def setSex(sex: String): Unit = &#123; this.sex = sex &#125; // 4.重写父类的方法建议使用 override 关键字修饰 override def toString: String = name + ":" + age + ":" + birthday + ":" + sex&#125; 使用 extends 关键字实现继承： 1234567891011// 1.使用 extends 关键字实现继承class Employee extends Person &#123; override def toString: String = "Employee~" + super.toString // 2.使用 public 或 protected 关键字修饰的变量能被子类访问 def setBirthday(date: String): Unit = &#123; birthday = date &#125;&#125; 测试继承： 1234567891011121314object ScalaApp extends App &#123; val employee = new Employee employee.name = "heibaiying" employee.age = 20 employee.setBirthday("2019-03-05") employee.setSex("男") println(employee)&#125;// 输出： Employee~heibaiying:20:2019-03-05:男 1.3 调用超类构造器在 Scala 的类中，每个辅助构造器都必须首先调用其他构造器或主构造器，这样就导致了子类的辅助构造器永远无法直接调用超类的构造器，只有主构造器才能调用超类的构造器。所以想要调用超类的构造器，代码示例如下： 123class Employee(name:String,age:Int,salary:Double) extends Person(name:String,age:Int) &#123; .....&#125; 1.4 类型检查和转换想要实现类检查可以使用 isInstanceOf，判断一个实例是否来源于某个类或者其子类，如果是，则可以使用 asInstanceOf 进行强制类型转换。 12345678910111213141516object ScalaApp extends App &#123; val employee = new Employee val person = new Person // 1. 判断一个实例是否来源于某个类或者其子类 输出 true println(employee.isInstanceOf[Person]) println(person.isInstanceOf[Person]) // 2. 强制类型转换 var p: Person = employee.asInstanceOf[Person] // 3. 判断一个实例是否来源于某个类 (而不是其子类) println(employee.getClass == classOf[Employee])&#125; 1.5 构造顺序和提前定义1. 构造顺序在 Scala 中还有一个需要注意的问题，如果你在子类中重写父类的 val 变量，并且超类的构造器中使用了该变量，那么可能会产生不可预期的错误。下面给出一个示例： 12345678910111213141516171819// 父类class Person &#123; println("父类的默认构造器") val range: Int = 10 val array: Array[Int] = new Array[Int](range)&#125;//子类class Employee extends Person &#123; println("子类的默认构造器") override val range = 2&#125;//测试object ScalaApp extends App &#123; val employee = new Employee println(employee.array.mkString("(", ",", ")"))&#125; 这里初始化 array 用到了变量 range，这里你会发现实际上 array 既不会被初始化 Array(10)，也不会被初始化为 Array(2)，实际的输出应该如下： 123父类的默认构造器子类的默认构造器() 可以看到 array 被初始化为 Array(0)，主要原因在于父类构造器的执行顺序先于子类构造器，这里给出实际的执行步骤： 父类的构造器被调用，执行 new Array[Int](range) 语句; 这里想要得到 range 的值，会去调用子类 range() 方法，因为 override val 重写变量的同时也重写了其 get 方法； 调用子类的 range() 方法，自然也是返回子类的 range 值，但是由于子类的构造器还没有执行，这也就意味着对 range 赋值的 range = 2 语句还没有被执行，所以自然返回 range 的默认值，也就是 0。 这里可能比较疑惑的是为什么 val range = 2 没有被执行，却能使用 range 变量，这里因为在虚拟机层面，是先对成员变量先分配存储空间并赋给默认值，之后才赋予给定的值。想要证明这一点其实也比较简单，代码如下: 123456789101112class Person &#123; // val range: Int = 10 正常代码 array 为 Array(10) val array: Array[Int] = new Array[Int](range) val range: Int = 10 //如果把变量的声明放在使用之后，此时数据 array 为 array(0)&#125;object Person &#123; def main(args: Array[String]): Unit = &#123; val person = new Person println(person.array.mkString("(", ",", ")")) &#125;&#125; 2. 提前定义想要解决上面的问题，有以下几种方法： (1) . 将变量用 final 修饰，代表不允许被子类重写，即 final val range: Int = 10； (2) . 将变量使用 lazy 修饰，代表懒加载，即只有当你实际使用到 array 时候，才去进行初始化； 1lazy val array: Array[Int] = new Array[Int](range) (3) . 采用提前定义，代码如下，代表 range 的定义优先于超类构造器。 1234567class Employee extends &#123; //这里不能定义其他方法 override val range = 2&#125; with Person &#123; // 定义其他变量或者方法 def pr(): Unit = &#123;println("Employee")&#125;&#125; 但是这种语法也有其限制：你只能在上面代码块中重写已有的变量，而不能定义新的变量和方法，定义新的变量和方法只能写在下面代码块中。 注意事项：类的继承和下文特质 (trait) 的继承都存在这个问题，也同样可以通过提前定义来解决。虽然如此，但还是建议合理设计以规避该类问题。 二、抽象类Scala 中允许使用 abstract 定义抽象类，并且通过 extends 关键字继承它。 定义抽象类： 12345678910111213abstract class Person &#123; // 1.定义字段 var name: String val age: Int // 2.定义抽象方法 def geDetail: String // 3. scala 的抽象类允许定义具体方法 def print(): Unit = &#123; println("抽象类中的默认方法") &#125;&#125; 继承抽象类： 12345678class Employee extends Person &#123; // 覆盖抽象类中变量 override var name: String = "employee" override val age: Int = 12 // 覆盖抽象方法 def geDetail: String = name + ":" + age&#125; 三、特质3.1 trait &amp; withScala 中没有 interface 这个关键字，想要实现类似的功能，可以使用特质 (trait)。trait 等价于 Java 8 中的接口，因为 trait 中既能定义抽象方法，也能定义具体方法，这和 Java 8 中的接口是类似的。 1234567891011// 1.特质使用 trait 关键字修饰trait Logger &#123; // 2.定义抽象方法 def log(msg: String) // 3.定义具体方法 def logInfo(msg: String): Unit = &#123; println("INFO:" + msg) &#125;&#125; 想要使用特质，需要使用 extends 关键字，而不是 implements 关键字，如果想要添加多个特质，可以使用 with 关键字。 12345678// 1.使用 extends 关键字,而不是 implements,如果想要添加多个特质，可以使用 with 关键字class ConsoleLogger extends Logger with Serializable with Cloneable &#123; // 2. 实现特质中的抽象方法 def log(msg: String): Unit = &#123; println("CONSOLE:" + msg) &#125;&#125; 3.2 特质中的字段和方法一样，特质中的字段可以是抽象的，也可以是具体的： 如果是抽象字段，则混入特质的类需要重写覆盖该字段； 如果是具体字段，则混入特质的类获得该字段，但是并非是通过继承关系得到，而是在编译时候，简单将该字段加入到子类。 123456trait Logger &#123; // 抽象字段 var LogLevel:String // 具体字段 var LogType = "FILE"&#125; 覆盖抽象字段： 1234class InfoLogger extends Logger &#123; // 覆盖抽象字段 override var LogLevel: String = "INFO"&#125; 3.3 带有特质的对象Scala 支持在类定义的时混入 父类 trait，而在类实例化为具体对象的时候指明其实际使用的 子类 trait。示例如下： trait Logger： 12345// 父类trait Logger &#123; // 定义空方法 日志打印 def log(msg: String) &#123;&#125;&#125; trait ErrorLogger： 1234567// 错误日志打印，继承自 Loggertrait ErrorLogger extends Logger &#123; // 覆盖空方法 override def log(msg: String): Unit = &#123; println("Error:" + msg) &#125;&#125; trait InfoLogger： 12345678// 通知日志打印，继承自 Loggertrait InfoLogger extends Logger &#123; // 覆盖空方法 override def log(msg: String): Unit = &#123; println("INFO:" + msg) &#125;&#125; 具体的使用类： 1234567// 混入 trait Loggerclass Person extends Logger &#123; // 调用定义的抽象方法 def printDetail(detail: String): Unit = &#123; log(detail) &#125;&#125; 这里通过 main 方法来测试： 1234567891011object ScalaApp extends App &#123; // 使用 with 指明需要具体使用的 trait val person01 = new Person with InfoLogger val person02 = new Person with ErrorLogger val person03 = new Person with InfoLogger with ErrorLogger person01.log("scala") //输出 INFO:scala person02.log("scala") //输出 Error:scala person03.log("scala") //输出 Error:scala&#125; 这里前面两个输出比较明显，因为只指明了一个具体的 trait，这里需要说明的是第三个输出，因为 trait 的调用是由右到左开始生效的，所以这里打印出 Error:scala。 3.4 特质构造顺序trait 有默认的无参构造器，但是不支持有参构造器。一个类混入多个特质后初始化顺序应该如下： 12// 示例class Employee extends Person with InfoLogger with ErrorLogger &#123;...&#125; 超类首先被构造，即 Person 的构造器首先被执行； 特质的构造器在超类构造器之前，在类构造器之后；特质由左到右被构造；每个特质中，父特质首先被构造； Logger 构造器执行（Logger 是 InfoLogger 的父类）； InfoLogger 构造器执行； ErrorLogger 构造器执行; 所有超类和特质构造完毕，子类才会被构造。 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>集成与特质</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala列表和集]]></title>
    <url>%2F2019%2F10%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E5%88%97%E8%A1%A8%E5%92%8C%E9%9B%86%2F</url>
    <content type="text"><![CDATA[List &amp; Set一、List字面量二、List类型三、构建List四、模式匹配五、列表的基本操作六、列表的高级操作七、List对象的方法八、处理多个List九、缓冲列表ListBuffer十、集(Set) 一、List字面量List 是 Scala 中非常重要的一个数据结构，其与 Array(数组) 非常类似，但是 List 是不可变的，和 Java 中的 List 一样，其底层实现是链表。 123456scala&gt; val list = List("hadoop", "spark", "storm")list: List[String] = List(hadoop, spark, storm)// List 是不可变scala&gt; list(1) = "hive"&lt;console&gt;:9: error: value update is not a member of List[String] 二、List类型Scala 中 List 具有以下两个特性： 同构 (homogeneous)：同一个 List 中的所有元素都必须是相同的类型； 协变 (covariant)：如果 S 是 T 的子类型，那么 List[S] 就是 List[T] 的子类型，例如 List[String] 是 List[Object] 的子类型。 需要特别说明的是空列表的类型为 List[Nothing]： 12scala&gt; List()res1: List[Nothing] = List() 三、构建List所有 List 都由两个基本单元构成：Nil 和 ::(读作”cons”)。即列表要么是空列表 (Nil)，要么是由一个 head 加上一个 tail 组成，而 tail 又是一个 List。我们在上面使用的 List(&quot;hadoop&quot;, &quot;spark&quot;, &quot;storm&quot;) 最终也是被解释为 &quot;hadoop&quot;::&quot;spark&quot;:: &quot;storm&quot;::Nil。 123456scala&gt; val list01 = "hadoop"::"spark":: "storm"::Nillist01: List[String] = List(hadoop, spark, storm)// :: 操作符号是右结合的，所以上面的表达式和下面的等同scala&gt; val list02 = "hadoop"::("spark":: ("storm"::Nil))list02: List[String] = List(hadoop, spark, storm) 四、模式匹配Scala 支持展开列表以实现模式匹配。 1234567scala&gt; val list = List("hadoop", "spark", "storm")list: List[String] = List(hadoop, spark, storm)scala&gt; val List(a,b,c)=lista: String = hadoopb: String = sparkc: String = storm 如果只需要匹配部分内容，可以如下： 123scala&gt; val a::rest=lista: String = hadooprest: List[String] = List(spark, storm) 五、列表的基本操作5.1 常用方法1234567891011121314151617181920212223242526272829303132object ScalaApp extends App &#123; val list = List("hadoop", "spark", "storm") // 1.列表是否为空 list.isEmpty // 2.返回列表中的第一个元素 list.head // 3.返回列表中除第一个元素外的所有元素 这里输出 List(spark, storm) list.tail // 4.tail 和 head 可以结合使用 list.tail.head // 5.返回列表中的最后一个元素 与 head 相反 list.init // 6.返回列表中除了最后一个元素之外的其他元素 与 tail 相反 这里输出 List(hadoop, spark) list.last // 7.使用下标访问元素 list(2) // 8.获取列表长度 list.length // 9. 反转列表 list.reverse&#125; 5.2 indicesindices 方法返回所有下标。 12scala&gt; list.indicesres2: scala.collection.immutable.Range = Range(0, 1, 2) 5.3 take &amp; drop &amp; splitAt take：获取前 n 个元素； drop：删除前 n 个元素； splitAt：从第几个位置开始拆分。 12345678scala&gt; list take 2res3: List[String] = List(hadoop, spark)scala&gt; list drop 2res4: List[String] = List(storm)scala&gt; list splitAt 2res5: (List[String], List[String]) = (List(hadoop, spark),List(storm)) 5.4 flattenflatten 接收一个由列表组成的列表，并将其进行扁平化操作，返回单个列表。 12scala&gt; List(List(1, 2), List(3), List(), List(4, 5)).flattenres6: List[Int] = List(1, 2, 3, 4, 5) 5.5 zip &amp; unzip对两个 List 执行 zip 操作结果如下，返回对应位置元素组成的元组的列表，unzip 则执行反向操作。 12345678scala&gt; val list = List("hadoop", "spark", "storm")scala&gt; val score = List(10,20,30)scala&gt; val zipped=list zip scorezipped: List[(String, Int)] = List((hadoop,10), (spark,20), (storm,30))scala&gt; zipped.unzipres7: (List[String], List[Int]) = (List(hadoop, spark, storm),List(10, 20, 30)) 5.6 toString &amp; mkStringtoString 返回 List 的字符串表现形式。 12scala&gt; list.toStringres8: String = List(hadoop, spark, storm) 如果想改变 List 的字符串表现形式，可以使用 mkString。mkString 有三个重载方法，方法定义如下： 123456789// start：前缀 sep：分隔符 end:后缀def mkString(start: String, sep: String, end: String): String = addString(new StringBuilder(), start, sep, end).toString// seq 分隔符def mkString(sep: String): String = mkString("", sep, "")// 如果不指定分隔符 默认使用""分隔def mkString: String = mkString("") 使用示例如下： 12345678scala&gt; list.mkStringres9: String = hadoopsparkstormscala&gt; list.mkString(",")res10: String = hadoop,spark,stormscala&gt; list.mkString("&#123;",",","&#125;")res11: String = &#123;hadoop,spark,storm&#125; 5.7 iterator &amp; toArray &amp; copyToArrayiterator 方法返回的是迭代器，这和其他语言的使用是一样的。 1234567891011object ScalaApp extends App &#123; val list = List("hadoop", "spark", "storm") val iterator: Iterator[String] = list.iterator while (iterator.hasNext) &#123; println(iterator.next) &#125; &#125; toArray 和 toList 用于 List 和数组之间的互相转换。 12345scala&gt; val array = list.toArrayarray: Array[String] = Array(hadoop, spark, storm)scala&gt; array.toListres13: List[String] = List(hadoop, spark, storm) copyToArray 将 List 中的元素拷贝到数组中指定位置。 1234567891011object ScalaApp extends App &#123; val list = List("hadoop", "spark", "storm") val array = Array("10", "20", "30") list.copyToArray(array,1) println(array.toBuffer)&#125;// 输出 ：ArrayBuffer(10, hadoop, spark) 六、列表的高级操作6.1 列表转换：map &amp; flatMap &amp; foreachmap 与 Java 8 函数式编程中的 map 类似，都是对 List 中每一个元素执行指定操作。 12scala&gt; List(1,2,3).map(_+10)res15: List[Int] = List(11, 12, 13) flatMap 与 map 类似，但如果 List 中的元素还是 List，则会对其进行 flatten 操作。 12345scala&gt; list.map(_.toList)res16: List[List[Char]] = List(List(h, a, d, o, o, p), List(s, p, a, r, k), List(s, t, o, r, m))scala&gt; list.flatMap(_.toList)res17: List[Char] = List(h, a, d, o, o, p, s, p, a, r, k, s, t, o, r, m) foreach 要求右侧的操作是一个返回值为 Unit 的函数，你也可以简单理解为执行一段没有返回值代码。 1234567scala&gt; var sum = 0sum: Int = 0scala&gt; List(1, 2, 3, 4, 5) foreach (sum += _)scala&gt; sumres19: Int = 15 6.2 列表过滤：filter &amp; partition &amp; find &amp; takeWhile &amp; dropWhile &amp; spanfilter 用于筛选满足条件元素，返回新的 List。 12scala&gt; List(1, 2, 3, 4, 5) filter (_ % 2 == 0)res20: List[Int] = List(2, 4) partition 会按照筛选条件对元素进行分组，返回类型是 tuple(元组)。 12scala&gt; List(1, 2, 3, 4, 5) partition (_ % 2 == 0)res21: (List[Int], List[Int]) = (List(2, 4),List(1, 3, 5)) find 查找第一个满足条件的值，由于可能并不存在这样的值，所以返回类型是 Option，可以通过 getOrElse 在不存在满足条件值的情况下返回默认值。 12345scala&gt; List(1, 2, 3, 4, 5) find (_ % 2 == 0)res22: Option[Int] = Some(2)val result: Option[Int] = List(1, 2, 3, 4, 5) find (_ % 2 == 0)result.getOrElse(10) takeWhile 遍历元素，直到遇到第一个不符合条件的值则结束遍历，返回所有遍历到的值。 12scala&gt; List(1, 2, 3, -4, 5) takeWhile (_ &gt; 0)res23: List[Int] = List(1, 2, 3) dropWhile 遍历元素，直到遇到第一个不符合条件的值则结束遍历，返回所有未遍历到的值。 1234567// 第一个值就不满足条件,所以返回列表中所有的值scala&gt; List(1, 2, 3, -4, 5) dropWhile (_ &lt; 0)res24: List[Int] = List(1, 2, 3, -4, 5)scala&gt; List(1, 2, 3, -4, 5) dropWhile (_ &lt; 3)res26: List[Int] = List(3, -4, 5) span 遍历元素，直到遇到第一个不符合条件的值则结束遍历，将遍历到的值和未遍历到的值分别放入两个 List 中返回，返回类型是 tuple(元组)。 12scala&gt; List(1, 2, 3, -4, 5) span (_ &gt; 0)res27: (List[Int], List[Int]) = (List(1, 2, 3),List(-4, 5)) 6.3 列表检查：forall &amp; existsforall 检查 List 中所有元素，如果所有元素都满足条件，则返回 true。 12scala&gt; List(1, 2, 3, -4, 5) forall ( _ &gt; 0 )res28: Boolean = false exists 检查 List 中的元素，如果某个元素已经满足条件，则返回 true。 12scala&gt; List(1, 2, 3, -4, 5) exists (_ &gt; 0 )res29: Boolean = true 6.4 列表排序：sortWithsortWith 对 List 中所有元素按照指定规则进行排序，由于 List 是不可变的，所以排序返回一个新的 List。 12345678scala&gt; List(1, -3, 4, 2, 6) sortWith (_ &lt; _)res30: List[Int] = List(-3, 1, 2, 4, 6)scala&gt; val list = List( "hive","spark","azkaban","hadoop")list: List[String] = List(hive, spark, azkaban, hadoop)scala&gt; list.sortWith(_.length&gt;_.length)res33: List[String] = List(azkaban, hadoop, spark, hive) 七、List对象的方法上面介绍的所有方法都是 List 类上的方法，下面介绍的是 List 伴生对象中的方法。 7.1 List.rangeList.range 可以产生指定的前闭后开区间内的值组成的 List，它有三个可选参数: start(开始值)，end(结束值，不包含)，step(步长)。 12345678scala&gt; List.range(1, 5)res34: List[Int] = List(1, 2, 3, 4)scala&gt; List.range(1, 9, 2)res35: List[Int] = List(1, 3, 5, 7)scala&gt; List.range(9, 1, -3)res36: List[Int] = List(9, 6, 3) 7.2 List.fillList.fill 使用指定值填充 List。 12345scala&gt; List.fill(3)("hello")res37: List[String] = List(hello, hello, hello)scala&gt; List.fill(2,3)("world")res38: List[List[String]] = List(List(world, world, world), List(world, world, world)) 7.3 List.concatList.concat 用于拼接多个 List。 12345678scala&gt; List.concat(List('a', 'b'), List('c'))res39: List[Char] = List(a, b, c)scala&gt; List.concat(List(), List('b'), List('c'))res40: List[Char] = List(b, c)scala&gt; List.concat()res41: List[Nothing] = List() 八、处理多个List当多个 List 被放入同一个 tuple 中时候，可以通过 zipped 对多个 List 进行关联处理。 1234567891011// 两个 List 对应位置的元素相乘scala&gt; (List(10, 20), List(3, 4, 5)).zipped.map(_ * _)res42: List[Int] = List(30, 80)// 三个 List 的操作也是一样的scala&gt; (List(10, 20), List(3, 4, 5), List(100, 200)).zipped.map(_ * _ + _)res43: List[Int] = List(130, 280)// 判断第一个 List 中元素的长度与第二个 List 中元素的值是否相等scala&gt; (List("abc", "de"), List(3, 2)).zipped.forall(_.length == _)res44: Boolean = true 九、缓冲列表ListBuffer上面介绍的 List，由于其底层实现是链表，这意味着能快速访问 List 头部元素，但对尾部元素的访问则比较低效，这时候可以采用 ListBuffer，ListBuffer 提供了在常量时间内往头部和尾部追加元素。 12345678910111213141516import scala.collection.mutable.ListBufferobject ScalaApp extends App &#123; val buffer = new ListBuffer[Int] // 1.在尾部追加元素 buffer += 1 buffer += 2 // 2.在头部追加元素 3 +=: buffer // 3. ListBuffer 转 List val list: List[Int] = buffer.toList println(list)&#125;//输出：List(3, 1, 2) 十、集(Set)Set 是不重复元素的集合。分为可变 Set 和不可变 Set。 10.1 可变Set12345678910111213141516171819202122232425object ScalaApp extends App &#123; // 可变 Set val mutableSet = new collection.mutable.HashSet[Int] // 1.添加元素 mutableSet.add(1) mutableSet.add(2) mutableSet.add(3) mutableSet.add(3) mutableSet.add(4) // 2.移除元素 mutableSet.remove(2) // 3.调用 mkString 方法 输出 1,3,4 println(mutableSet.mkString(",")) // 4. 获取 Set 中最小元素 println(mutableSet.min) // 5. 获取 Set 中最大元素 println(mutableSet.max)&#125; 10.2 不可变Set不可变 Set 没有 add 方法，可以使用 + 添加元素，但是此时会返回一个新的不可变 Set，原来的 Set 不变。 123456789101112object ScalaApp extends App &#123; // 不可变 Set val immutableSet = new collection.immutable.HashSet[Int] val ints: HashSet[Int] = immutableSet+1 println(ints)&#125;// 输出 Set(1) 10.3 Set间操作多个 Set 之间可以进行求交集或者合集等操作。 12345678910111213object ScalaApp extends App &#123; // 声明有序 Set val mutableSet = collection.mutable.SortedSet(1, 2, 3, 4, 5) val immutableSet = collection.immutable.SortedSet(3, 4, 5, 6, 7) // 两个 Set 的合集 输出：TreeSet(1, 2, 3, 4, 5, 6, 7) println(mutableSet ++ immutableSet) // 两个 Set 的交集 输出：TreeSet(3, 4, 5) println(mutableSet intersect immutableSet)&#125; 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>列表与集合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala数组]]></title>
    <url>%2F2019%2F10%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E6%95%B0%E7%BB%84%2F</url>
    <content type="text"><![CDATA[Scala 数组相关操作一、定长数组二、变长数组三、数组遍历四、数组转换五、多维数组六、与Java互操作 一、定长数组在 Scala 中，如果你需要一个长度不变的数组，可以使用 Array。但需要注意以下两点： 在 Scala 中使用 (index) 而不是 [index] 来访问数组中的元素，因为访问元素，对于 Scala 来说是方法调用，(index) 相当于执行了 .apply(index) 方法。 Scala 中的数组与 Java 中的是等价的，Array[Int]() 在虚拟机层面就等价于 Java 的 int[]。 123456789101112131415// 10 个整数的数组，所有元素初始化为 0scala&gt; val nums=new Array[Int](10)nums: Array[Int] = Array(0, 0, 0, 0, 0, 0, 0, 0, 0, 0)// 10 个元素的字符串数组，所有元素初始化为 nullscala&gt; val strings=new Array[String](10)strings: Array[String] = Array(null, null, null, null, null, null, null, null, null, null)// 使用指定值初始化，此时不需要 new 关键字scala&gt; val a=Array("hello","scala")a: Array[String] = Array(hello, scala)// 使用 () 来访问元素scala&gt; a(0)res3: String = hello 二、变长数组在 scala 中通过 ArrayBuffer 实现变长数组 (又称缓冲数组)。在构建 ArrayBuffer 时必须给出类型参数，但不必指定长度，因为 ArrayBuffer 会在需要的时候自动扩容和缩容。变长数组的构建方式及常用操作如下： 123456789101112131415161718192021222324252627282930import scala.collection.mutable.ArrayBufferobject ScalaApp &#123; // 相当于 Java 中的 main 方法 def main(args: Array[String]): Unit = &#123; // 1.声明变长数组 (缓冲数组) val ab = new ArrayBuffer[Int]() // 2.在末端增加元素 ab += 1 // 3.在末端添加多个元素 ab += (2, 3, 4) // 4.可以使用 ++=追加任何集合 ab ++= Array(5, 6, 7) // 5.缓冲数组可以直接打印查看 println(ab) // 6.移除最后三个元素 ab.trimEnd(3) // 7.在第 1 个元素之后插入多个新元素 ab.insert(1, 8, 9) // 8.从第 2 个元素开始,移除 3 个元素,不指定第二个参数的话,默认值为 1 ab.remove(2, 3) // 9.缓冲数组转定长数组 val abToA = ab.toArray // 10. 定长数组打印为其 hashcode 值 println(abToA) // 11. 定长数组转缓冲数组 val aToAb = abToA.toBuffer &#125;&#125; 需要注意的是：使用 += 在末尾插入元素是一个高效的操作，其时间复杂度是 O(1)。而使用 insert 随机插入元素的时间复杂度是 O(n)，因为在其插入位置之后的所有元素都要进行对应的后移，所以在 ArrayBuffer 中随机插入元素是一个低效的操作。 三、数组遍历12345678910111213141516171819202122232425object ScalaApp extends App &#123; val a = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) // 1.方式一 相当于 Java 中的增强 for 循环 for (elem &lt;- a) &#123; print(elem) &#125; // 2.方式二 for (index &lt;- 0 until a.length) &#123; print(a(index)) &#125; // 3.方式三, 是第二种方式的简写 for (index &lt;- a.indices) &#123; print(a(index)) &#125; // 4.反向遍历 for (index &lt;- a.indices.reverse) &#123; print(a(index)) &#125;&#125; 这里我们没有将代码写在 main 方法中，而是继承自 App.scala，这是 Scala 提供的一种简写方式，此时将代码写在类中，等价于写在 main 方法中，直接运行该类即可。 四、数组转换数组转换是指由现有数组产生新的数组。假设当前拥有 a 数组，想把 a 中的偶数元素乘以 10 后产生一个新的数组，可以采用下面两种方式来实现： 12345678910111213141516object ScalaApp extends App &#123; val a = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10) // 1.方式一 yield 关键字 val ints1 = for (elem &lt;- a if elem % 2 == 0) yield 10 * elem for (elem &lt;- ints1) &#123; println(elem) &#125; // 2.方式二 采用函数式编程的方式,这和 Java 8 中的函数式编程是类似的，这里采用下划线标表示其中的每个元素 val ints2 = a.filter(_ % 2 == 0).map(_ * 10) for (elem &lt;- ints1) &#123; println(elem) &#125;&#125; 五、多维数组和 Java 中一样，多维数组由单维数组组成。 123456789101112131415161718192021object ScalaApp extends App &#123; val matrix = Array(Array(11, 12, 13, 14, 15, 16, 17, 18, 19, 20), Array(21, 22, 23, 24, 25, 26, 27, 28, 29, 30), Array(31, 32, 33, 34, 35, 36, 37, 38, 39, 40)) for (elem &lt;- matrix) &#123; for (elem &lt;- elem) &#123; print(elem + "-") &#125; println() &#125;&#125;打印输出如下：11-12-13-14-15-16-17-18-19-20-21-22-23-24-25-26-27-28-29-30-31-32-33-34-35-36-37-38-39-40- 六、与Java互操作由于 Scala 的数组是使用 Java 的数组来实现的，所以两者之间可以相互转换。 12345678910111213141516import java.utilimport scala.collection.mutable.ArrayBufferimport scala.collection.&#123;JavaConverters, mutable&#125;object ScalaApp extends App &#123; val element = ArrayBuffer("hadoop", "spark", "storm") // Scala 转 Java val javaList: util.List[String] = JavaConverters.bufferAsJavaList(element) // Java 转 Scala val scalaBuffer: mutable.Buffer[String] = JavaConverters.asScalaBuffer(javaList) for (elem &lt;- scalaBuffer) &#123; println(elem) &#125;&#125; 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>数组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala函数和闭包]]></title>
    <url>%2F2019%2F09%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E5%87%BD%E6%95%B0%E5%92%8C%E9%97%AD%E5%8C%85%2F</url>
    <content type="text"><![CDATA[函数和闭包一、函数1.1 函数与方法1.2 函数类型1.3 一等公民&amp;匿名函数1.4 特殊的函数表达式二、闭包2.1 闭包的定义2.2 修改自由变量2.3 自由变量多副本三、高阶函数3.1 使用函数作为参数3.2 函数柯里化 一、函数1.1 函数与方法Scala 中函数与方法的区别非常小，如果函数作为某个对象的成员，这样的函数被称为方法，否则就是一个正常的函数。 1234567// 定义方法def multi1(x:Int) = &#123;x * x&#125;// 定义函数val multi2 = (x: Int) =&gt; &#123;x * x&#125;println(multi1(3)) //输出 9println(multi2(3)) //输出 9 也可以使用 def 定义函数： 12def multi3 = (x: Int) =&gt; &#123;x * x&#125;println(multi3(3)) //输出 9 multi2 和 multi3 本质上没有区别，这是因为函数是一等公民，val multi2 = (x: Int) =&gt; {x * x} 这个语句相当于是使用 def 预先定义了函数，之后赋值给变量 multi2。 1.2 函数类型上面我们说过 multi2 和 multi3 本质上是一样的，那么作为函数它们是什么类型的？两者的类型实际上都是 Int =&gt; Int，前面一个 Int 代表输入参数类型，后面一个 Int 代表返回值类型。 123456789scala&gt; val multi2 = (x: Int) =&gt; &#123;x * x&#125;multi2: Int =&gt; Int = $$Lambda$1092/594363215@1dd1a777scala&gt; def multi3 = (x: Int) =&gt; &#123;x * x&#125;multi3: Int =&gt; Int// 如果有多个参数，则类型为：（参数类型，参数类型 ...）=&gt;返回值类型scala&gt; val multi4 = (x: Int,name: String) =&gt; &#123;name + x * x &#125;multi4: (Int, String) =&gt; String = $$Lambda$1093/1039732747@2eb4fe7 1.3 一等公民&amp;匿名函数在 Scala 中函数是一等公民，这意味着不仅可以定义函数并调用它们，还可以将它们作为值进行传递： 1234567import scala.math.ceilobject ScalaApp extends App &#123; // 将函数 ceil 赋值给变量 fun,使用下划线 (_) 指明是 ceil 函数但不传递参数 val fun = ceil _ println(fun(2.3456)) //输出 3.0&#125; 在 Scala 中你不必给每一个函数都命名，如 (x: Int) =&gt; 3 * x 就是一个匿名函数： 12345678910111213object ScalaApp extends App &#123; // 1.匿名函数 (x: Int) =&gt; 3 * x // 2.具名函数 val fun = (x: Int) =&gt; 3 * x // 3.直接使用匿名函数 val array01 = Array(1, 2, 3).map((x: Int) =&gt; 3 * x) // 4.使用占位符简写匿名函数 val array02 = Array(1, 2, 3).map(_ * 3) // 5.使用具名函数 val array03 = Array(1, 2, 3).map(fun) &#125; 1.4 特殊的函数表达式1. 可变长度参数列表在 Java 中如果你想要传递可变长度的参数，需要使用 String ...args 这种形式，Scala 中等效的表达为 args: String*。 12345678910object ScalaApp extends App &#123; def echo(args: String*): Unit = &#123; for (arg &lt;- args) println(arg) &#125; echo("spark","hadoop","flink")&#125;// 输出sparkhadoopflink 2. 传递具名参数向函数传递参数时候可以指定具体的参数名。 12345678910object ScalaApp extends App &#123; def detail(name: String, age: Int): Unit = println(name + ":" + age) // 1.按照参数定义的顺序传入 detail("heibaiying", 12) // 2.传递参数的时候指定具体的名称,则不必遵循定义的顺序 detail(age = 12, name = "heibaiying")&#125; 3. 默认值参数在定义函数时，可以为参数指定默认值。 123456789object ScalaApp extends App &#123; def detail(name: String, age: Int = 88): Unit = println(name + ":" + age) // 如果没有传递 age 值,则使用默认值 detail("heibaiying") detail("heibaiying", 12)&#125; 二、闭包2.1 闭包的定义123var more = 10// addMore 一个闭包函数:因为其捕获了自由变量 more 从而闭合了该函数字面量val addMore = (x: Int) =&gt; x + more 如上函数 addMore 中有两个变量 x 和 more: x : 是一个绑定变量 (bound variable)，因为其是该函数的入参，在函数的上下文中有明确的定义； more : 是一个自由变量 (free variable)，因为函数字面量本生并没有给 more 赋予任何含义。 按照定义：在创建函数时，如果需要捕获自由变量，那么包含指向被捕获变量的引用的函数就被称为闭包函数。 2.2 修改自由变量这里需要注意的是，闭包捕获的是变量本身，即是对变量本身的引用，这意味着： 闭包外部对自由变量的修改，在闭包内部是可见的； 闭包内部对自由变量的修改，在闭包外部也是可见的。 1234567891011121314151617// 声明 more 变量scala&gt; var more = 10more: Int = 10// more 变量必须已经被声明，否则下面的语句会报错scala&gt; val addMore = (x: Int) =&gt; &#123;x + more&#125;addMore: Int =&gt; Int = $$Lambda$1076/1844473121@876c4f0scala&gt; addMore(10)res7: Int = 20// 注意这里是给 more 变量赋值，而不是重新声明 more 变量scala&gt; more=1000more: Int = 1000scala&gt; addMore(10)res8: Int = 1010 2.3 自由变量多副本自由变量可能随着程序的改变而改变，从而产生多个副本，但是闭包永远指向创建时候有效的那个变量副本。 12345678910111213141516171819202122232425262728293031// 第一次声明 more 变量scala&gt; var more = 10more: Int = 10// 创建闭包函数scala&gt; val addMore10 = (x: Int) =&gt; &#123;x + more&#125;addMore10: Int =&gt; Int = $$Lambda$1077/1144251618@1bdaa13c// 调用闭包函数scala&gt; addMore10(9)res9: Int = 19// 重新声明 more 变量scala&gt; var more = 100more: Int = 100// 创建新的闭包函数scala&gt; val addMore100 = (x: Int) =&gt; &#123;x + more&#125;addMore100: Int =&gt; Int = $$Lambda$1078/626955849@4d0be2ac// 引用的是重新声明 more 变量scala&gt; addMore100(9)res10: Int = 109// 引用的还是第一次声明的 more 变量scala&gt; addMore10(9)res11: Int = 19// 对于全局而言 more 还是 100scala&gt; moreres12: Int = 100 从上面的示例可以看出重新声明 more 后，全局的 more 的值是 100，但是对于闭包函数 addMore10 还是引用的是值为 10 的 more，这是由虚拟机来实现的，虚拟机会保证 more 变量在重新声明后，原来的被捕获的变量副本继续在堆上保持存活。 三、高阶函数3.1 使用函数作为参数定义函数时候支持传入函数作为参数，此时新定义的函数被称为高阶函数。 12345678910111213141516171819object ScalaApp extends App &#123; // 1.定义函数 def square = (x: Int) =&gt; &#123; x * x &#125; // 2.定义高阶函数: 第一个参数是类型为 Int =&gt; Int 的函数 def multi(fun: Int =&gt; Int, x: Int) = &#123; fun(x) * 100 &#125; // 3.传入具名函数 println(multi(square, 5)) // 输出 2500 // 4.传入匿名函数 println(multi(_ * 100, 5)) // 输出 50000&#125; 3.2 函数柯里化我们上面定义的函数都只支持一个参数列表，而柯里化函数则支持多个参数列表。柯里化指的是将原来接受两个参数的函数变成接受一个参数的函数的过程。新的函数以原有第二个参数作为参数。 12345object ScalaApp extends App &#123; // 定义柯里化函数 def curriedSum(x: Int)(y: Int) = x + y println(curriedSum(2)(3)) //输出 5&#125; 这里当你调用 curriedSum 时候，实际上是连着做了两次传统的函数调用，实际执行的柯里化过程如下： 第一次调用接收一个名为 x 的 Int 型参数，返回一个用于第二次调用的函数，假设 x 为 2，则返回函数 2+y； 返回的函数接收参数 y，并计算并返回值 2+3 的值。 想要获得柯里化的中间返回的函数其实也比较简单： 123456789object ScalaApp extends App &#123; // 定义柯里化函数 def curriedSum(x: Int)(y: Int) = x + y println(curriedSum(2)(3)) //输出 5 // 获取传入值为 10 返回的中间函数 10 + y val plus: Int =&gt; Int = curriedSum(10)_ println(plus(3)) //输出值 13&#125; 柯里化支持多个参数列表，多个参数按照从左到右的顺序依次执行柯里化操作： 123456object ScalaApp extends App &#123; // 定义柯里化函数 def curriedSum(x: Int)(y: Int)(z: String) = x + y + z println(curriedSum(2)(3)("name")) // 输出 5name &#125; 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>函数</tag>
        <tag>闭包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala映射和元组]]></title>
    <url>%2F2019%2F09%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E6%98%A0%E5%B0%84%E5%92%8C%E5%85%83%E7%BB%84%2F</url>
    <content type="text"><![CDATA[Map &amp; Tuple一、映射(Map)1.1 构造Map1.2 获取值1.3 新增/修改/删除值1.4 遍历Map1.5 yield关键字1.6 其他Map结构1.7 可选方法1.8 与Java互操作二、元组(Tuple)2.1 模式匹配2.2 zip方法 一、映射(Map)1.1 构造Map12345678// 初始化一个空 mapval scores01 = new HashMap[String, Int]// 从指定的值初始化 Map（方式一）val scores02 = Map("hadoop" -&gt; 10, "spark" -&gt; 20, "storm" -&gt; 30)// 从指定的值初始化 Map（方式二）val scores03 = Map(("hadoop", 10), ("spark", 20), ("storm", 30)) 采用上面方式得到的都是不可变 Map(immutable map)，想要得到可变 Map(mutable map)，则需要使用： 1val scores04 = scala.collection.mutable.Map("hadoop" -&gt; 10, "spark" -&gt; 20, "storm" -&gt; 30) 1.2 获取值12345678910object ScalaApp extends App &#123; val scores = Map("hadoop" -&gt; 10, "spark" -&gt; 20, "storm" -&gt; 30) // 1.获取指定 key 对应的值 println(scores("hadoop")) // 2. 如果对应的值不存在则使用默认值 println(scores.getOrElse("hadoop01", 100))&#125; 1.3 新增/修改/删除值可变 Map 允许进行新增、修改、删除等操作。 123456789101112131415161718192021222324object ScalaApp extends App &#123; val scores = scala.collection.mutable.Map("hadoop" -&gt; 10, "spark" -&gt; 20, "storm" -&gt; 30) // 1.如果 key 存在则更新 scores("hadoop") = 100 // 2.如果 key 不存在则新增 scores("flink") = 40 // 3.可以通过 += 来进行多个更新或新增操作 scores += ("spark" -&gt; 200, "hive" -&gt; 50) // 4.可以通过 -= 来移除某个键和值 scores -= "storm" for (elem &lt;- scores) &#123;println(elem)&#125;&#125;// 输出内容如下(spark,200)(hadoop,100)(flink,40)(hive,50) 不可变 Map 不允许进行新增、修改、删除等操作，但是允许由不可变 Map 产生新的 Map。 123456789101112131415object ScalaApp extends App &#123; val scores = Map("hadoop" -&gt; 10, "spark" -&gt; 20, "storm" -&gt; 30) val newScores = scores + ("spark" -&gt; 200, "hive" -&gt; 50) for (elem &lt;- scores) &#123;println(elem)&#125;&#125;// 输出内容如下(hadoop,10)(spark,200)(storm,30)(hive,50) 1.4 遍历Map1234567891011121314object ScalaApp extends App &#123; val scores = Map("hadoop" -&gt; 10, "spark" -&gt; 20, "storm" -&gt; 30) // 1. 遍历键 for (key &lt;- scores.keys) &#123; println(key) &#125; // 2. 遍历值 for (value &lt;- scores.values) &#123; println(value) &#125; // 3. 遍历键值对 for ((key, value) &lt;- scores) &#123; println(key + ":" + value) &#125;&#125; 1.5 yield关键字可以使用 yield 关键字从现有 Map 产生新的 Map。 1234567891011121314151617181920212223object ScalaApp extends App &#123; val scores = Map("hadoop" -&gt; 10, "spark" -&gt; 20, "storm" -&gt; 30) // 1.将 scores 中所有的值扩大 10 倍 val newScore = for ((key, value) &lt;- scores) yield (key, value * 10) for (elem &lt;- newScore) &#123; println(elem) &#125; // 2.将键和值互相调换 val reversalScore: Map[Int, String] = for ((key, value) &lt;- scores) yield (value, key) for (elem &lt;- reversalScore) &#123; println(elem) &#125;&#125;// 输出(hadoop,100)(spark,200)(storm,300)(10,hadoop)(20,spark)(30,storm) 1.6 其他Map结构在使用 Map 时候，如果不指定，默认使用的是 HashMap，如果想要使用 TreeMap 或者 LinkedHashMap，则需要显式的指定。 12345678910111213141516171819object ScalaApp extends App &#123; // 1.使用 TreeMap,按照键的字典序进行排序 val scores01 = scala.collection.mutable.TreeMap("B" -&gt; 20, "A" -&gt; 10, "C" -&gt; 30) for (elem &lt;- scores01) &#123;println(elem)&#125; // 2.使用 LinkedHashMap,按照键值对的插入顺序进行排序 val scores02 = scala.collection.mutable.LinkedHashMap("B" -&gt; 20, "A" -&gt; 10, "C" -&gt; 30) for (elem &lt;- scores02) &#123;println(elem)&#125;&#125;// 输出(A,10)(B,20)(C,30)(B,20)(A,10)(C,30) 1.7 可选方法1234567891011121314object ScalaApp extends App &#123; val scores = scala.collection.mutable.TreeMap("B" -&gt; 20, "A" -&gt; 10, "C" -&gt; 30) // 1. 获取长度 println(scores.size) // 2. 判断是否为空 println(scores.isEmpty) // 3. 判断是否包含特定的 key println(scores.contains("A"))&#125; 1.8 与Java互操作123456789101112131415import java.utilimport scala.collection.&#123;JavaConverters, mutable&#125;object ScalaApp extends App &#123; val scores = Map("hadoop" -&gt; 10, "spark" -&gt; 20, "storm" -&gt; 30) // scala map 转 java map val javaMap: util.Map[String, Int] = JavaConverters.mapAsJavaMap(scores) // java map 转 scala map val scalaMap: mutable.Map[String, Int] = JavaConverters.mapAsScalaMap(javaMap) for (elem &lt;- scalaMap) &#123;println(elem)&#125;&#125; 二、元组(Tuple)元组与数组类似，但是数组中所有的元素必须是同一种类型，而元组则可以包含不同类型的元素。 12scala&gt; val tuple=(1,3.24f,"scala")tuple: (Int, Float, String) = (1,3.24,scala) 2.1 模式匹配可以通过模式匹配来获取元组中的值并赋予对应的变量： 1234scala&gt; val (a,b,c)=tuplea: Int = 1b: Float = 3.24c: String = scala 如果某些位置不需要赋值，则可以使用下划线代替： 12scala&gt; val (a,_,_)=tuplea: Int = 1 2.2 zip方法12345678910111213141516171819202122object ScalaApp extends App &#123; val array01 = Array("hadoop", "spark", "storm") val array02 = Array(10, 20, 30) // 1.zip 方法得到的是多个 tuple 组成的数组 val tuples: Array[(String, Int)] = array01.zip(array02) // 2.也可以在 zip 后调用 toMap 方法转换为 Map val map: Map[String, Int] = array01.zip(array02).toMap for (elem &lt;- tuples) &#123; println(elem) &#125; for (elem &lt;- map) &#123;println(elem)&#125;&#125;// 输出(hadoop,10)(spark,20)(storm,30)(hadoop,10)(spark,20)(storm,30) 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>映射</tag>
        <tag>元组</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala隐式转换和隐式参数]]></title>
    <url>%2F2019%2F09%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E9%9A%90%E5%BC%8F%E8%BD%AC%E6%8D%A2%E5%92%8C%E9%9A%90%E5%BC%8F%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[隐式转换和隐式参数一、隐式转换1.1 使用隐式转换1.2 隐式转换规则1.3 引入隐式转换二、隐式参数2.1 使用隐式参数2.2 引入隐式参数2.3 利用隐式参数进行隐式转换 一、隐式转换1.1 使用隐式转换隐式转换指的是以 implicit 关键字声明带有单个参数的转换函数，它将值从一种类型转换为另一种类型，以便使用之前类型所没有的功能。示例如下： 12345678910111213141516171819// 普通人class Person(val name: String)// 雷神class Thor(val name: String) &#123; // 正常情况下只有雷神才能举起雷神之锤 def hammer(): Unit = &#123; println(name + "举起雷神之锤") &#125;&#125;object Thor extends App &#123; // 定义隐式转换方法 将普通人转换为雷神 通常建议方法名使用 source2Target,即：被转换对象 To 转换对象 implicit def person2Thor(p: Person): Thor = new Thor(p.name) // 这样普通人也能举起雷神之锤 new Person("普通人").hammer()&#125;输出： 普通人举起雷神之锤 1.2 隐式转换规则并不是你使用 implicit 转换后，隐式转换就一定会发生，比如上面如果不调用 hammer() 方法的时候，普通人就还是普通人。通常程序会在以下情况下尝试执行隐式转换： 当对象访问一个不存在的成员时，即调用的方法不存在或者访问的成员变量不存在； 当对象调用某个方法，该方法存在，但是方法的声明参数与传入参数不匹配时。 而在以下三种情况下编译器不会尝试执行隐式转换： 如果代码能够在不使用隐式转换的前提下通过编译，则不会使用隐式转换； 编译器不会尝试同时执行多个转换，比如 convert1(convert2(a))*b； 转换存在二义性，也不会发生转换。 这里首先解释一下二义性，上面的代码进行如下修改，由于两个隐式转换都是生效的，所以就存在了二义性： 12345//两个隐式转换都是有效的implicit def person2Thor(p: Person): Thor = new Thor(p.name)implicit def person2Thor2(p: Person): Thor = new Thor(p.name)// 此时下面这段语句无法通过编译new Person("普通人").hammer() 其次再解释一下多个转换的问题： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class ClassA &#123; override def toString = "This is Class A"&#125;class ClassB &#123; override def toString = "This is Class B" def printB(b: ClassB): Unit = println(b)&#125;class ClassCclass ClassDobject ImplicitTest extends App &#123; implicit def A2B(a: ClassA): ClassB = &#123; println("A2B") new ClassB &#125; implicit def C2B(c: ClassC): ClassB = &#123; println("C2B") new ClassB &#125; implicit def D2C(d: ClassD): ClassC = &#123; println("D2C") new ClassC &#125; // 这行代码无法通过编译，因为要调用到 printB 方法，需要执行两次转换 C2B(D2C(ClassD)) new ClassD().printB(new ClassA) /* * 下面的这一行代码虽然也进行了两次隐式转换，但是两次的转换对象并不是一个对象,所以它是生效的: * 转换流程如下: * 1. ClassC 中并没有 printB 方法,因此隐式转换为 ClassB,然后调用 printB 方法; * 2. 但是 printB 参数类型为 ClassB,然而传入的参数类型是 ClassA,所以需要将参数 ClassA 转换为 ClassB,这是第二次; * 即: C2B(ClassC) -&gt; ClassB.printB(ClassA) -&gt; ClassB.printB(A2B(ClassA)) -&gt; ClassB.printB(ClassB) * 转换过程 1 的对象是 ClassC,而转换过程 2 的转换对象是 ClassA,所以虽然是一行代码两次转换，但是仍然是有效转换 */ new ClassC().printB(new ClassA)&#125;// 输出：C2BA2BThis is Class B 1.3 引入隐式转换隐式转换的可以定义在以下三个地方： 定义在原类型的伴生对象中； 直接定义在执行代码的上下文作用域中； 统一定义在一个文件中，在使用时候导入。 上面我们使用的方法相当于直接定义在执行代码的作用域中，下面分别给出其他两种定义的代码示例： 定义在原类型的伴生对象中： 12345class Person(val name: String)// 在伴生对象中定义隐式转换函数object Person&#123; implicit def person2Thor(p: Person): Thor = new Thor(p.name)&#125; 12345class Thor(val name: String) &#123; def hammer(): Unit = &#123; println(name + "举起雷神之锤") &#125;&#125; 1234// 使用示例object ScalaApp extends App &#123; new Person("普通人").hammer()&#125; 定义在一个公共的对象中： 123object Convert &#123; implicit def person2Thor(p: Person): Thor = new Thor(p.name)&#125; 123456// 导入 Convert 下所有的隐式转换函数import com.heibaiying.Convert._object ScalaApp extends App &#123; new Person("普通人").hammer()&#125; 注：Scala 自身的隐式转换函数大部分定义在 Predef.scala 中，你可以打开源文件查看，也可以在 Scala 交互式命令行中采用 :implicit -v 查看全部隐式转换函数。 二、隐式参数2.1 使用隐式参数在定义函数或方法时可以使用标记为 implicit 的参数，这种情况下，编译器将会查找默认值，提供给函数调用。 1234567891011121314// 定义分隔符类class Delimiters(val left: String, val right: String)object ScalaApp extends App &#123; // 进行格式化输出 def formatted(context: String)(implicit deli: Delimiters): Unit = &#123; println(deli.left + context + deli.right) &#125; // 定义一个隐式默认值 使用左右中括号作为分隔符 implicit val bracket = new Delimiters("(", ")") formatted("this is context") // 输出: (this is context)&#125; 关于隐式参数，有两点需要注意： 1.我们上面定义 formatted 函数的时候使用了柯里化，如果你不使用柯里化表达式，按照通常习惯只有下面两种写法： 12345678// 这种写法没有语法错误，但是无法通过编译def formatted(implicit context: String, deli: Delimiters): Unit = &#123; println(deli.left + context + deli.right)&#125; // 不存在这种写法，IDEA 直接会直接提示语法错误def formatted( context: String, implicit deli: Delimiters): Unit = &#123; println(deli.left + context + deli.right)&#125; 上面第一种写法编译的时候会出现下面所示 error 信息,从中也可以看出 implicit 是作用于参数列表中每个参数的，这显然不是我们想要到达的效果，所以上面的写法采用了柯里化。 12not enough arguments for method formatted: (implicit context: String, implicit deli: com.heibaiying.Delimiters) 2.第二个问题和隐式函数一样，隐式默认值不能存在二义性，否则无法通过编译，示例如下： 123implicit val bracket = new Delimiters("(", ")")implicit val brace = new Delimiters("&#123;", "&#125;")formatted("this is context") 上面代码无法通过编译，出现错误提示 ambiguous implicit values，即隐式值存在冲突。 2.2 引入隐式参数引入隐式参数和引入隐式转换函数方法是一样的，有以下三种方式： 定义在隐式参数对应类的伴生对象中； 直接定义在执行代码的上下文作用域中； 统一定义在一个文件中，在使用时候导入。 我们上面示例程序相当于直接定义执行代码的上下文作用域中，下面给出其他两种方式的示例： 定义在隐式参数对应类的伴生对象中； 12345class Delimiters(val left: String, val right: String)object Delimiters &#123; implicit val bracket = new Delimiters("(", ")")&#125; 12345678// 此时执行代码的上下文中不用定义object ScalaApp extends App &#123; def formatted(context: String)(implicit deli: Delimiters): Unit = &#123; println(deli.left + context + deli.right) &#125; formatted("this is context") &#125; 统一定义在一个文件中，在使用时候导入： 123object Convert &#123; implicit val bracket = new Delimiters("(", ")")&#125; 123456789// 在使用的时候导入import com.heibaiying.Convert.bracketobject ScalaApp extends App &#123; def formatted(context: String)(implicit deli: Delimiters): Unit = &#123; println(deli.left + context + deli.right) &#125; formatted("this is context") // 输出: (this is context)&#125; 2.3 利用隐式参数进行隐式转换1def smaller[T] (a: T, b: T) = if (a &lt; b) a else b 在 Scala 中如果定义了一个如上所示的比较对象大小的泛型方法，你会发现无法通过编译。对于对象之间进行大小比较，Scala 和 Java 一样，都要求被比较的对象需要实现 java.lang.Comparable 接口。在 Scala 中，直接继承 Java 中 Comparable 接口的是特质 Ordered，它在继承 compareTo 方法的基础上，额外定义了关系符方法，源码如下: 12345678trait Ordered[A] extends Any with java.lang.Comparable[A] &#123; def compare(that: A): Int def &lt; (that: A): Boolean = (this compare that) &lt; 0 def &gt; (that: A): Boolean = (this compare that) &gt; 0 def &lt;= (that: A): Boolean = (this compare that) &lt;= 0 def &gt;= (that: A): Boolean = (this compare that) &gt;= 0 def compareTo(that: A): Int = compare(that)&#125; 所以要想在泛型中解决这个问题，有两种方法： 1. 使用视图界定1234567object Pair extends App &#123; // 视图界定 def smaller[T&lt;% Ordered[T]](a: T, b: T) = if (a &lt; b) a else b println(smaller(1,2)) //输出 1&#125; 视图限定限制了 T 可以通过隐式转换 Ordered[T]，即对象一定可以进行大小比较。在上面的代码中 smaller(1,2) 中参数 1 和 2 实际上是通过定义在 Predef 中的隐式转换方法 intWrapper 转换为 RichInt。 12// Predef.scala@inline implicit def intWrapper(x: Int) = new runtime.RichInt(x) 为什么要这么麻烦执行隐式转换，原因是 Scala 中的 Int 类型并不能直接进行比较，因为其没有实现 Ordered 特质，真正实现 Ordered 特质的是 RichInt。 2. 利用隐式参数进行隐式转换Scala2.11+ 后，视图界定被标识为废弃，官方推荐使用类型限定来解决上面的问题，本质上就是使用隐式参数进行隐式转换。 1234567object Pair extends App &#123; // order 既是一个隐式参数也是一个隐式转换，即如果 a 不存在 &lt; 方法，则转换为 order(a)&lt;b def smaller[T](a: T, b: T)(implicit order: T =&gt; Ordered[T]) = if (a &lt; b) a else b println(smaller(1,2)) //输出 1&#125; 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>隐式转换</tag>
        <tag>隐式参数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala类型参数]]></title>
    <url>%2F2019%2F09%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E7%B1%BB%E5%9E%8B%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[类型参数一、泛型1.1 泛型类1.2 泛型方法二、类型限定2.1 类型上界限定2.2 视图界定 2.3 类型约束2.4 上下文界定2.5 ClassTag上下文界定2.6 类型下界限定2.7 多重界定三、Ordering &amp; Ordered3.1 Comparable3.2 Comparator3.3 上下文界定的优点四、通配符 一、泛型Scala 支持类型参数化，使得我们能够编写泛型程序。 1.1 泛型类Java 中使用 &lt;&gt; 符号来包含定义的类型参数，Scala 则使用 []。 123class Pair[T, S](val first: T, val second: S) &#123; override def toString: String = first + ":" + second&#125; 123456789object ScalaApp extends App &#123; // 使用时候你直接指定参数类型，也可以不指定，由程序自动推断 val pair01 = new Pair("heibai01", 22) val pair02 = new Pair[String,Int]("heibai02", 33) println(pair01) println(pair02)&#125; 1.2 泛型方法函数和方法也支持类型参数。 123object Utils &#123; def getHalf[T](a: Array[T]): Int = a.length / 2&#125; 二、类型限定2.1 类型上界限定Scala 和 Java 一样，对于对象之间进行大小比较，要求被比较的对象实现 java.lang.Comparable 接口。所以如果想对泛型进行比较，需要限定类型上界为 java.lang.Comparable，语法为 S &lt;: T，代表类型 S 是类型 T 的子类或其本身。示例如下： 12345// 使用 &lt;: 符号，限定 T 必须是 Comparable[T]的子类型class Pair[T &lt;: Comparable[T]](val first: T, val second: T) &#123; // 返回较小的值 def smaller: T = if (first.compareTo(second) &lt; 0) first else second&#125; 123// 测试代码val pair = new Pair("abc", "abcd")println(pair.smaller) // 输出 abc 扩展：如果你想要在 Java 中实现类型变量限定，需要使用关键字 extends 来实现，等价的 Java 代码如下： 123456789101112&gt;public class Pair&lt;T extends Comparable&lt;T&gt;&gt; &#123;&gt; private T first;&gt; private T second;&gt; Pair(T first, T second) &#123;&gt; this.first = first;&gt; this.second = second;&gt; &#125;&gt; public T smaller() &#123;&gt; return first.compareTo(second) &lt; 0 ? first : second;&gt; &#125;&gt;&#125;&gt; 2.2 视图界定在上面的例子中，如果你使用 Int 类型或者 Double 等类型进行测试，点击运行后，你会发现程序根本无法通过编译： 12val pair1 = new Pair(10, 12)val pair2 = new Pair(10.0, 12.0) 之所以出现这样的问题，是因为 Scala 中的 Int 类并没有实现 Comparable 接口。在 Scala 中直接继承 Comparable 接口的是特质 Ordered，它在继承 compareTo 方法的基础上，额外定义了关系符方法，源码如下: 123456789// 除了 compareTo 方法外，还提供了额外的关系符方法trait Ordered[A] extends Any with java.lang.Comparable[A] &#123; def compare(that: A): Int def &lt; (that: A): Boolean = (this compare that) &lt; 0 def &gt; (that: A): Boolean = (this compare that) &gt; 0 def &lt;= (that: A): Boolean = (this compare that) &lt;= 0 def &gt;= (that: A): Boolean = (this compare that) &gt;= 0 def compareTo(that: A): Int = compare(that)&#125; 之所以在日常的编程中之所以你能够执行 3&gt;2 这样的判断操作，是因为程序执行了定义在 Predef 中的隐式转换方法 intWrapper(x: Int)，将 Int 类型转换为 RichInt 类型，而 RichInt 间接混入了 Ordered 特质，所以能够进行比较。 12// Predef.scala@inline implicit def intWrapper(x: Int) = new runtime.RichInt(x) 要想解决传入数值无法进行比较的问题，可以使用视图界定。语法为 T &lt;% U，代表 T 能够通过隐式转换转为 U，即允许 Int 型参数在无法进行比较的时候转换为 RichInt 类型。示例如下： 12345// 视图界定符号 &lt;%class Pair[T &lt;% Comparable[T]](val first: T, val second: T) &#123; // 返回较小的值 def smaller: T = if (first.compareTo(second) &lt; 0) first else second&#125; 注：由于直接继承 Java 中 Comparable 接口的是特质 Ordered，所以如下的视图界定和上面是等效的： 12345&gt; // 隐式转换为 Ordered[T]&gt; class Pair[T &lt;% Ordered[T]](val first: T, val second: T) &#123;&gt; def smaller: T = if (first.compareTo(second) &lt; 0) first else second&gt; &#125;&gt; 2.3 类型约束如果你用的 Scala 是 2.11+，会发现视图界定已被标识为废弃。官方推荐使用类型约束 (type constraint) 来实现同样的功能，其本质是使用隐式参数进行隐式转换，示例如下： 123456789 // 1.使用隐式参数隐式转换为 Comparable[T]class Pair[T](val first: T, val second: T)(implicit ev: T =&gt; Comparable[T]) def smaller: T = if (first.compareTo(second) &lt; 0) first else second&#125;// 2.由于直接继承 Java 中 Comparable 接口的是特质 Ordered，所以也可以隐式转换为 Ordered[T]class Pair[T](val first: T, val second: T)(implicit ev: T =&gt; Ordered[T]) &#123; def smaller: T = if (first.compareTo(second) &lt; 0) first else second&#125; 当然，隐式参数转换也可以运用在具体的方法上： 123object PairUtils&#123; def smaller[T](a: T, b: T)(implicit order: T =&gt; Ordered[T]) = if (a &lt; b) a else b&#125; 2.4 上下文界定上下文界定的形式为 T:M，其中 M 是一个泛型，它要求必须存在一个类型为 M[T]的隐式值，当你声明一个带隐式参数的方法时，需要定义一个隐式默认值。所以上面的程序也可以使用上下文界定进行改写： 12345678class Pair[T](val first: T, val second: T) &#123; // 请注意 这个地方用的是 Ordering[T]，而上面视图界定和类型约束，用的是 Ordered[T]，两者的区别会在后文给出解释 def smaller(implicit ord: Ordering[T]): T = if (ord.compare(first, second) &lt; 0) first else second &#125;// 测试val pair= new Pair(88, 66)println(pair.smaller) //输出：66 在上面的示例中，我们无需手动添加隐式默认值就可以完成转换，这是因为 Scala 自动引入了 Ordering[Int]这个隐式值。为了更好的说明上下文界定，下面给出一个自定义类型的比较示例： 12345678910111213141516171819202122// 1.定义一个人员类class Person(val name: String, val age: Int) &#123; override def toString: String = name + ":" + age&#125;// 2.继承 Ordering[T],实现自定义比较器,按照自己的规则重写比较方法class PersonOrdering extends Ordering[Person] &#123; override def compare(x: Person, y: Person): Int = if (x.age &gt; y.age) 1 else -1&#125;class Pair[T](val first: T, val second: T) &#123; def smaller(implicit ord: Ordering[T]): T = if (ord.compare(first, second) &lt; 0) first else second&#125;object ScalaApp extends App &#123; val pair = new Pair(new Person("hei", 88), new Person("bai", 66)) // 3.定义隐式默认值,如果不定义,则下一行代码无法通过编译 implicit val ImpPersonOrdering = new PersonOrdering println(pair.smaller) //输出： bai:66&#125; 2.5 ClassTag上下文界定这里先看一个例子：下面这段代码，没有任何语法错误，但是在运行时会抛出异常：Error: cannot find class tag for element type T, 这是由于 Scala 和 Java 一样，都存在类型擦除，即泛型信息只存在于代码编译阶段，在进入 JVM 之前，与泛型相关的信息会被擦除掉。对于下面的代码，在运行阶段创建 Array 时，你必须明确指明其类型，但是此时泛型信息已经被擦除，导致出现找不到类型的异常。 123456object ScalaApp extends App &#123; def makePair[T](first: T, second: T) = &#123; // 创建以一个数组 并赋值 val r = new Array[T](2); r(0) = first; r(1) = second; r &#125;&#125; Scala 针对这个问题，提供了 ClassTag 上下文界定，即把泛型的信息存储在 ClassTag 中，这样在运行阶段需要时，只需要从 ClassTag 中进行获取即可。其语法为 T : ClassTag，示例如下： 123456import scala.reflect._object ScalaApp extends App &#123; def makePair[T : ClassTag](first: T, second: T) = &#123; val r = new Array[T](2); r(0) = first; r(1) = second; r &#125;&#125; 2.6 类型下界限定2.1 小节介绍了类型上界的限定，Scala 同时也支持下界的限定，语法为：U &gt;: T，即 U 必须是类型 T 的超类或本身。 1234567891011121314151617181920212223242526272829303132333435363738// 首席执行官class CEO// 部门经理class Manager extends CEO// 本公司普通员工class Employee extends Manager// 其他公司人员class OtherCompanyobject ScalaApp extends App &#123; // 限定：只有本公司部门经理以上人员才能获取权限 def Check[T &gt;: Manager](t: T): T = &#123; println("获得审核权限") t &#125; // 错误写法: 省略泛型参数后,以下所有人都能获得权限,显然这是不正确的 Check(new CEO) Check(new Manager) Check(new Employee) Check(new OtherCompany) // 正确写法,传入泛型参数 Check[CEO](new CEO) Check[Manager](new Manager) /* * 以下两条语句无法通过编译,异常信息为: * do not conform to method Check's type parameter bounds(不符合方法 Check 的类型参数边界) * 这种情况就完成了下界限制，即只有本公司经理及以上的人员才能获得审核权限 */ Check[Employee](new Employee) Check[OtherCompany](new OtherCompany)&#125; 2.7 多重界定 类型变量可以同时有上界和下界。 写法为 ：T &gt; : Lower &lt;: Upper； 不能同时有多个上界或多个下界 。但可以要求一个类型实现多个特质，写法为 : T &lt; : Comparable[T] with Serializable with Cloneable； 你可以有多个上下文界定，写法为 T : Ordering : ClassTag 。 三、Ordering &amp; Ordered上文中使用到 Ordering 和 Ordered 特质，它们最主要的区别在于分别继承自不同的 Java 接口：Comparable 和 Comparator： Comparable：可以理解为内置的比较器，实现此接口的对象可以与自身进行比较； Comparator：可以理解为外置的比较器；当对象自身并没有定义比较规则的时候，可以传入外部比较器进行比较。 为什么 Java 中要同时给出这两个比较接口，这是因为你要比较的对象不一定实现了 Comparable 接口，而你又想对其进行比较，这时候当然你可以修改代码实现 Comparable，但是如果这个类你无法修改 (如源码中的类)，这时候就可以使用外置的比较器。同样的问题在 Scala 中当然也会出现，所以 Scala 分别使用了 Ordering 和 Ordered 来继承它们。 下面分别给出 Java 中 Comparable 和 Comparator 接口的使用示例： 3.1 Comparable12345678910111213141516171819202122232425262728import java.util.Arrays;// 实现 Comparable 接口public class Person implements Comparable&lt;Person&gt; &#123; private String name; private int age; Person(String name,int age) &#123;this.name=name;this.age=age;&#125; @Override public String toString() &#123; return name+":"+age; &#125; // 核心的方法是重写比较规则，按照年龄进行排序 @Override public int compareTo(Person person) &#123; return this.age - person.age; &#125; public static void main(String[] args) &#123; Person[] peoples= &#123;new Person("hei", 66), new Person("bai", 55), new Person("ying", 77)&#125;; Arrays.sort(peoples); Arrays.stream(peoples).forEach(System.out::println); &#125;&#125;输出：bai:55hei:66ying:77 3.2 Comparator12345678910111213141516171819202122232425import java.util.Arrays;import java.util.Comparator;public class Person &#123; private String name; private int age; Person(String name,int age) &#123;this.name=name;this.age=age;&#125; @Override public String toString() &#123; return name+":"+age; &#125; public static void main(String[] args) &#123; Person[] peoples= &#123;new Person("hei", 66), new Person("bai", 55), new Person("ying", 77)&#125;; // 这里为了直观直接使用匿名内部类,实现 Comparator 接口 //如果是 Java8 你也可以写成 Arrays.sort(peoples, Comparator.comparingInt(o -&gt; o.age)); Arrays.sort(peoples, new Comparator&lt;Person&gt;() &#123; @Override public int compare(Person o1, Person o2) &#123; return o1.age-o2.age; &#125; &#125;); Arrays.stream(peoples).forEach(System.out::println); &#125;&#125; 使用外置比较器还有一个好处，就是你可以随时定义其排序规则： 123456// 按照年龄大小排序Arrays.sort(peoples, Comparator.comparingInt(o -&gt; o.age));Arrays.stream(peoples).forEach(System.out::println);// 按照名字长度倒序排列Arrays.sort(peoples, Comparator.comparingInt(o -&gt; -o.name.length()));Arrays.stream(peoples).forEach(System.out::println); 3.3 上下文界定的优点这里再次给出上下文界定中的示例代码作为回顾： 12345678910111213141516171819202122// 1.定义一个人员类class Person(val name: String, val age: Int) &#123; override def toString: String = name + ":" + age&#125;// 2.继承 Ordering[T],实现自定义比较器,这个比较器就是一个外置比较器class PersonOrdering extends Ordering[Person] &#123; override def compare(x: Person, y: Person): Int = if (x.age &gt; y.age) 1 else -1&#125;class Pair[T](val first: T, val second: T) &#123; def smaller(implicit ord: Ordering[T]): T = if (ord.compare(first, second) &lt; 0) first else second&#125;object ScalaApp extends App &#123; val pair = new Pair(new Person("hei", 88), new Person("bai", 66)) // 3.在当前上下文定义隐式默认值,这就相当于传入了外置比较器 implicit val ImpPersonOrdering = new PersonOrdering println(pair.smaller) //输出： bai:66&#125; 使用上下文界定和 Ordering 带来的好处是：传入 Pair 中的参数不一定需要可比较，只要在比较时传入外置比较器即可。 需要注意的是由于隐式默认值二义性的限制，你不能像上面 Java 代码一样，在同一个上下文作用域中传入两个外置比较器，即下面的代码是无法通过编译的。但是你可以在不同的上下文作用域中引入不同的隐式默认值，即使用不同的外置比较器。 1234implicit val ImpPersonOrdering = new PersonOrderingprintln(pair.smaller) implicit val ImpPersonOrdering2 = new PersonOrderingprintln(pair.smaller) 四、通配符在实际编码中，通常需要把泛型限定在某个范围内，比如限定为某个类及其子类。因此 Scala 和 Java 一样引入了通配符这个概念，用于限定泛型的范围。不同的是 Java 使用 ? 表示通配符，Scala 使用 _ 表示通配符。 1234567891011121314151617class Ceo(val name: String) &#123; override def toString: String = name&#125;class Manager(name: String) extends Ceo(name)class Employee(name: String) extends Manager(name)class Pair[T](val first: T, val second: T) &#123; override def toString: String = "first:" + first + ", second: " + second&#125;object ScalaApp extends App &#123; // 限定部门经理及以下的人才可以组队 def makePair(p: Pair[_ &lt;: Manager]): Unit = &#123;println(p)&#125; makePair(new Pair(new Employee("heibai"), new Manager("ying")))&#125; 目前 Scala 中的通配符在某些复杂情况下还不完善，如下面的语句在 Scala 2.12 中并不能通过编译： 1def min[T &lt;: Comparable[_ &gt;: T]](p: Pair[T]) =&#123;&#125; 可以使用以下语法代替： 12type SuperComparable[T] = Comparable[_ &gt;: T]def min[T &lt;: SuperComparable[T]](p: Pair[T]) = &#123;&#125; 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>参数类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala类和对象]]></title>
    <url>%2F2019%2F09%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E7%B1%BB%E5%92%8C%E5%AF%B9%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[类和对象一、初识类和对象二、类2.1 成员变量可见性2.2 getter和setter属性2.3 @BeanProperty2.4 主构造器2.5 辅助构造器2.6 方法传参不可变三、对象3.1 工具类&amp;单例&amp;全局静态常量&amp;拓展特质3.2 伴生对象3.3 实现枚举类 一、初识类和对象Scala 的类与 Java 的类具有非常多的相似性，示例如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 1. 在 scala 中，类不需要用 public 声明,所有的类都具有公共的可见性class Person &#123; // 2. 声明私有变量,用 var 修饰的变量默认拥有 getter/setter 属性 private var age = 0 // 3.如果声明的变量不需要进行初始赋值，此时 Scala 就无法进行类型推断，所以需要显式指明类型 private var name: String = _ // 4. 定义方法,应指明传参类型。返回值类型不是必须的，Scala 可以自动推断出来，但是为了方便调用者，建议指明 def growUp(step: Int): Unit = &#123; age += step &#125; // 5.对于改值器方法 (即改变对象状态的方法),即使不需要传入参数,也建议在声明中包含 () def growUpFix(): Unit = &#123; age += 10 &#125; // 6.对于取值器方法 (即不会改变对象状态的方法),不必在声明中包含 () def currentAge: Int = &#123; age &#125; /** * 7.不建议使用 return 关键字,默认方法中最后一行代码的计算结果为返回值 * 如果方法很简短，甚至可以写在同一行中 */ def getName: String = name&#125;// 伴生对象object Person &#123; def main(args: Array[String]): Unit = &#123; // 8.创建类的实例 val counter = new Person() // 9.用 var 修饰的变量默认拥有 getter/setter 属性，可以直接对其进行赋值 counter.age = 12 counter.growUp(8) counter.growUpFix() // 10.用 var 修饰的变量默认拥有 getter/setter 属性，可以直接对其进行取值，输出: 30 println(counter.age) // 输出: 30 println(counter.currentAge) // 输出: null println(counter.getName) &#125;&#125; 二、类2.1 成员变量可见性Scala 中成员变量的可见性默认都是 public，如果想要保证其不被外部干扰，可以声明为 private，并通过 getter 和 setter 方法进行访问。 2.2 getter和setter属性getter 和 setter 属性与声明变量时使用的关键字有关： 使用 var 关键字：变量同时拥有 getter 和 setter 属性； 使用 val 关键字：变量只拥有 getter 属性； 使用 private[this]：变量既没有 getter 属性、也没有 setter 属性，只能通过内部的方法访问； 需要特别说明的是：假设变量名为 age,则其对应的 get 和 set 的方法名分别叫做 age 和 age_=。 123456789101112131415161718class Person &#123; private val name = "heibaiying" private var age = 12 private[this] var birthday = "2019-08-08" // birthday 只能被内部方法所访问 def getBirthday: String = birthday&#125;object Person &#123; def main(args: Array[String]): Unit = &#123; val person = new Person person.age = 30 println(person.name) println(person.age) println(person.getBirthday) &#125;&#125; 解释说明： 示例代码中 person.age=30 在执行时内部实际是调用了方法 person.age_=(30)，而 person.age 内部执行时实际是调用了 person.age() 方法。想要证明这一点，可以对代码进行反编译。同时为了说明成员变量可见性的问题，我们对下面这段代码进行反编译： 12345&gt; class Person &#123;&gt; var name = ""&gt; private var age = ""&gt; &#125;&gt; 依次执行下面编译命令： 123&gt; &gt; scalac Person.scala&gt; &gt; javap -private Person&gt; 编译结果如下，从编译结果可以看到实际的 get 和 set 的方法名 (因为 JVM 不允许在方法名中出现＝，所以它被翻译成$eq)，同时也验证了成员变量默认的可见性为 public。 1234567891011121314&gt; Compiled from "Person.scala"&gt; public class Person &#123;&gt; private java.lang.String name;&gt; private java.lang.String age;&gt; &gt; public java.lang.String name();&gt; public void name_$eq(java.lang.String);&gt; &gt; private java.lang.String age();&gt; private void age_$eq(java.lang.String);&gt; &gt; public Person();&gt; &#125;&gt; 2.3 @BeanProperty在上面的例子中可以看到我们是使用 . 来对成员变量进行访问的，如果想要额外生成和 Java 中一样的 getXXX 和 setXXX 方法，则需要使用@BeanProperty 进行注解。 1234567891011class Person &#123; @BeanProperty var name = ""&#125;object Person &#123; def main(args: Array[String]): Unit = &#123; val person = new Person person.setName("heibaiying") println(person.getName) &#125;&#125; 2.4 主构造器和 Java 不同的是，Scala 类的主构造器直接写在类名后面，但注意以下两点： 主构造器传入的参数默认就是 val 类型的，即不可变，你没有办法在内部改变传参； 写在主构造器中的代码块会在类初始化的时候被执行，功能类似于 Java 的静态代码块 static{} 1234567891011121314151617181920class Person(val name: String, val age: Int) &#123; println("功能类似于 Java 的静态代码块 static&#123;&#125;") def getDetail: String = &#123; //name="heibai" 无法通过编译 name + ":" + age &#125;&#125;object Person &#123; def main(args: Array[String]): Unit = &#123; val person = new Person("heibaiying", 20) println(person.getDetail) &#125;&#125;输出：功能类似于 Java 的静态代码块 static&#123;&#125;heibaiying:20 2.5 辅助构造器辅助构造器有两点硬性要求： 辅助构造器的名称必须为 this； 每个辅助构造器必须以主构造器或其他的辅助构造器的调用开始。 1234567891011121314151617181920class Person(val name: String, val age: Int) &#123; private var birthday = "" // 1.辅助构造器的名称必须为 this def this(name: String, age: Int, birthday: String) &#123; // 2.每个辅助构造器必须以主构造器或其他的辅助构造器的调用开始 this(name, age) this.birthday = birthday &#125; // 3.重写 toString 方法 override def toString: String = name + ":" + age + ":" + birthday&#125;object Person &#123; def main(args: Array[String]): Unit = &#123; println(new Person("heibaiying", 20, "2019-02-21")) &#125;&#125; 2.6 方法传参不可变在 Scala 中，方法传参默认是 val 类型，即不可变，这意味着你在方法体内部不能改变传入的参数。这和 Scala 的设计理念有关，Scala 遵循函数式编程理念，强调方法不应该有副作用。 1234567class Person() &#123; def low(word: String): String = &#123; word="word" // 编译无法通过 word.toLowerCase &#125;&#125; 三、对象Scala 中的 object(对象) 主要有以下几个作用： 因为 object 中的变量和方法都是静态的，所以可以用于存放工具类； 可以作为单例对象的容器； 可以作为类的伴生对象； 可以拓展类或特质； 可以拓展 Enumeration 来实现枚举。 3.1 工具类&amp;单例&amp;全局静态常量&amp;拓展特质这里我们创建一个对象 Utils,代码如下： 1234567891011121314151617object Utils &#123; /* *1. 相当于 Java 中的静态代码块 static,会在对象初始化时候被执行 * 这种方式实现的单例模式是饿汉式单例,即无论你的单例对象是否被用到， * 都在一开始被初始化完成 */ val person = new Person // 2. 全局固定常量 等价于 Java 的 public static final val CONSTANT = "固定常量" // 3. 全局静态方法 def low(word: String): String = &#123; word.toLowerCase &#125;&#125; 其中 Person 类代码如下： 123class Person() &#123; println("Person 默认构造器被调用")&#125; 新建测试类： 12345678910111213141516171819// 1.ScalaApp 对象扩展自 trait Appobject ScalaApp extends App &#123; // 2.验证单例 println(Utils.person == Utils.person) // 3.获取全局常量 println(Utils.CONSTANT) // 4.调用工具类 println(Utils.low("ABCDEFG")) &#125;// 输出如下：Person 默认构造器被调用true固定常量abcdefg 3.2 伴生对象在 Java 中，你通常会用到既有实例方法又有静态方法的类，在 Scala 中，可以通过类和与类同名的伴生对象来实现。类和伴生对象必须存在与同一个文件中。 1234567891011121314151617181920212223242526class Person() &#123; private val name = "HEIBAIYING" def getName: String = &#123; // 调用伴生对象的方法和属性 Person.toLow(Person.PREFIX + name) &#125;&#125;// 伴生对象object Person &#123; val PREFIX = "prefix-" def toLow(word: String): String = &#123; word.toLowerCase &#125; def main(args: Array[String]): Unit = &#123; val person = new Person // 输出 prefix-heibaiying println(person.getName) &#125;&#125; 3.3 实现枚举类Scala 中没有直接提供枚举类，需要通过扩展 Enumeration，并调用其中的 Value 方法对所有枚举值进行初始化来实现。 1234567891011121314151617object Color extends Enumeration &#123; // 1.类型别名,建议声明,在 import 时有用 type Color = Value // 2.调用 Value 方法 val GREEN = Value // 3.只传入 id val RED = Value(3) // 4.只传入值 val BULE = Value("blue") // 5.传入 id 和值 val YELLOW = Value(5, "yellow") // 6. 不传入 id 时,id 为上一个声明变量的 id+1,值默认和变量名相同 val PINK = Value &#125; 使用枚举类： 1234567891011121314151617181920212223// 1.使用类型别名导入枚举类import com.heibaiying.Color.Colorobject ScalaApp extends App &#123; // 2.使用枚举类型,这种情况下需要导入枚举类 def printColor(color: Color): Unit = &#123; println(color.toString) &#125; // 3.判断传入值和枚举值是否相等 println(Color.YELLOW.toString == "yellow") // 4.遍历枚举类和值 for (c &lt;- Color.values) println(c.id + ":" + c.toString)&#125;//输出true0:GREEN3:RED4:blue5:yellow6:PINK 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>类与对象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala流程控制语句]]></title>
    <url>%2F2019%2F09%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E6%B5%81%E7%A8%8B%E6%8E%A7%E5%88%B6%E8%AF%AD%E5%8F%A5%2F</url>
    <content type="text"><![CDATA[流程控制语句一、条件表达式if二、块表达式三、循环表达式while四、循环表达式for五、异常处理try六、条件选择表达式match七、没有break和continue八、输入与输出 一、条件表达式ifScala 中的 if/else 语法结构与 Java 中的一样，唯一不同的是，Scala 中的 if 表达式是有返回值的。 1234567object ScalaApp extends App &#123; val x = "scala" val result = if (x.length == 5) "true" else "false" print(result) &#125; 在 Java 中，每行语句都需要使用 ; 表示结束，但是在 Scala 中并不需要。除非你在单行语句中写了多行代码。 二、块表达式在 Scala 中，可以使用 {} 块包含一系列表达式，块中最后一个表达式的值就是块的值。 123456789object ScalaApp extends App &#123; val result = &#123; val a = 1 + 1; val b = 2 + 2; a + b &#125; print(result)&#125;// 输出： 6 如果块中的最后一个表达式没有返回值，则块的返回值是 Unit 类型。 12scala&gt; val result =&#123; val a = 1 + 1; val b = 2 + 2 &#125;result: Unit = () 三、循环表达式whileScala 和大多数语言一样，支持 while 和 do ... while 表达式。 1234567891011121314object ScalaApp extends App &#123; var n = 0 while (n &lt; 10) &#123; n += 1 println(n) &#125; // 循环至少要执行一次 do &#123; println(n) &#125; while (n &gt; 10)&#125; 四、循环表达式forfor 循环的基本使用如下： 123456789101112object ScalaApp extends App &#123; // 1.基本使用 输出[1,9) for (n &lt;- 1 until 10) &#123;print(n)&#125; // 2.使用多个表达式生成器 输出: 11 12 13 21 22 23 31 32 33 for (i &lt;- 1 to 3; j &lt;- 1 to 3) print(f"$&#123;10 * i + j&#125;%3d") // 3.使用带条件的表达式生成器 输出: 12 13 21 23 31 32 for (i &lt;- 1 to 3; j &lt;- 1 to 3 if i != j) print(f"$&#123;10 * i + j&#125;%3d")&#125; 除了基本使用外，还可以使用 yield 关键字从 for 循环中产生 Vector，这称为 for 推导式。 12scala&gt; for (i &lt;- 1 to 10) yield i * 6res1: scala.collection.immutable.IndexedSeq[Int] = Vector(6, 12, 18, 24, 30, 36, 42, 48, 54, 60) 五、异常处理try和 Java 中一样，支持 try...catch...finally 语句。 1234567891011121314import java.io.&#123;FileNotFoundException, FileReader&#125;object ScalaApp extends App &#123; try &#123; val reader = new FileReader("wordCount.txt") &#125; catch &#123; case ex: FileNotFoundException =&gt; ex.printStackTrace() println("没有找到对应的文件!") &#125; finally &#123; println("finally 语句一定会被执行！") &#125;&#125; 这里需要注意的是因为 finally 语句一定会被执行，所以不要在该语句中返回值，否则返回值会被作为整个 try 语句的返回值，如下： 123456scala&gt; def g():Int = try return 1 finally return 2g: ()Int// 方法 g() 总会返回 2scala&gt; g()res3: Int = 2 六、条件选择表达式matchmatch 类似于 java 中的 switch 语句。 12345678910111213object ScalaApp extends App &#123; val elements = Array("A", "B", "C", "D", "E") for (elem &lt;- elements) &#123; elem match &#123; case "A" =&gt; println(10) case "B" =&gt; println(20) case "C" =&gt; println(30) case _ =&gt; println(50) &#125; &#125;&#125; 但是与 Java 中的 switch 有以下三点不同： Scala 中的 case 语句支持任何类型；而 Java 中 case 语句仅支持整型、枚举和字符串常量； Scala 中每个分支语句后面不需要写 break，因为在 case 语句中 break 是隐含的，默认就有； 在 Scala 中 match 语句是有返回值的，而 Java 中 switch 语句是没有返回值的。如下： 123456789101112131415object ScalaApp extends App &#123; val elements = Array("A", "B", "C", "D", "E") for (elem &lt;- elements) &#123; val score = elem match &#123; case "A" =&gt; 10 case "B" =&gt; 20 case "C" =&gt; 30 case _ =&gt; 50 &#125; print(elem + ":" + score + ";") &#125;&#125;// 输出： A:10;B:20;C:30;D:50;E:50; 七、没有break和continue额外注意一下：Scala 中并不支持 Java 中的 break 和 continue 关键字。 八、输入与输出在 Scala 中可以使用 print、println、printf 打印输出，这与 Java 中是一样的。如果需要从控制台中获取输入，则可以使用 StdIn 中定义的各种方法。 1234val name = StdIn.readLine("Your name: ")print("Your age: ")val age = StdIn.readInt()println(s"Hello, $&#123;name&#125;! Next year, you will be $&#123;age + 1&#125;.") 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 凯.S.霍斯特曼 . 快学 Scala(第 2 版)[M] . 电子工业出版社 . 2017-7]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>流程控制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala集合类型]]></title>
    <url>%2F2019%2F09%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E9%9B%86%E5%90%88%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[集合一、集合简介二、集合结构3.1 scala.collection3.2 scala.collection.mutable3.2 scala.collection.immutable三、Trait Traversable四、Trait Iterable五、修改集合 一、集合简介Scala 中拥有多种集合类型，主要分为可变的和不可变的集合两大类： 可变集合： 可以被修改。即可以更改，添加，删除集合中的元素； 不可变集合类：不能被修改。对集合执行更改，添加或删除操作都会返回一个新的集合，而不是修改原来的集合。 二、集合结构Scala 中的大部分集合类都存在三类变体，分别位于 scala.collection, scala.collection.immutable, scala.collection.mutable 包中。还有部分集合类位于 scala.collection.generic 包下。 scala.collection.immutable ：包是中的集合是不可变的； scala.collection.mutable ：包中的集合是可变的； scala.collection ：包中的集合，既可以是可变的，也可以是不可变的。 123val sortSet = scala.collection.SortedSet(1, 2, 3, 4, 5)val mutableSet = collection.mutable.SortedSet(1, 2, 3, 4, 5)val immutableSet = collection.immutable.SortedSet(1, 2, 3, 4, 5) 如果你仅写了 Set 而没有加任何前缀也没有进行任何 import，则 Scala 默认采用不可变集合类。 12scala&gt; Set(1,2,3,4,5)res0: scala.collection.immutable.Set[Int] = Set(5, 1, 2, 3, 4) 3.1 scala.collectionscala.collection 包中所有集合如下图： 3.2 scala.collection.mutablescala.collection.mutable 包中所有集合如下图： 3.2 scala.collection.immutablescala.collection.immutable 包中所有集合如下图： 三、Trait TraversableScala 中所有集合的顶层实现是 Traversable 。它唯一的抽象方法是 foreach： 1def foreach[U](f: Elem =&gt; U) 实现 Traversable 的集合类只需要实现这个抽象方法，其他方法可以从 Traversable 继承。Traversable 中的所有可用方法如下： 方法 作用 Abstract Method: xs foreach f 为 xs 的每个元素执行函数 f Addition: xs ++ ys 一个包含 xs 和 ys 中所有元素的新的集合。 ys 是一个 Traversable 或 Iterator。 Maps: xs map f 对 xs 中每一个元素应用函数 f，并返回一个新的集合 xs flatMap f 对 xs 中每一个元素应用函数 f，最后将结果合并成一个新的集合 xs collect f 对 xs 中每一个元素调用偏函数 f，并返回一个新的集合 Conversions: xs.toArray 将集合转化为一个 Array xs.toList 将集合转化为一个 List xs.toIterable 将集合转化为一个 Iterable xs.toSeq 将集合转化为一个 Seq xs.toIndexedSeq 将集合转化为一个 IndexedSeq xs.toStream 将集合转化为一个延迟计算的流 xs.toSet 将集合转化为一个 Set xs.toMap 将一个（key, value）对的集合转化为一个 Map。 如果当前集合的元素类型不是（key, value）对形式， 则报静态类型错误。 Copying: xs copyToBuffer buf 拷贝集合中所有元素到缓存 buf xs copyToArray(arr,s,n) 从索引 s 开始，将集合中最多 n 个元素复制到数组 arr。 最后两个参数是可选的。 Size info: xs.isEmpty 判断集合是否为空 xs.nonEmpty 判断集合是否包含元素 xs.size 返回集合中元素的个数 xs.hasDefiniteSize 如果 xs 具有有限大小，则为真。 Element Retrieval: xs.head 返回集合中的第一个元素（如果无序，则随机返回） xs.headOption 以 Option 的方式返回集合中的第一个元素， 如果集合为空则返回 None xs.last 返回集合中的最后一个元素（如果无序，则随机返回） xs.lastOption 以 Option 的方式返回集合中的最后一个元素， 如果集合为空则返回 None xs find p 以 Option 的方式返回满足条件 p 的第一个元素， 如果都不满足则返回 None Subcollection: xs.tail 除了第一个元素之外的其他元素组成的集合 xs.init 除了最后一个元素之外的其他元素组成的集合 xs slice (from, to) 返回给定索引范围之内的元素组成的集合 （包含 from 位置的元素但不包含 to 位置的元素） xs take n 返回 xs 的前 n 个元素组成的集合（如果无序，则返回任意 n 个元素） xs drop n 返回 xs 的后 n 个元素组成的集合（如果无序，则返回任意 n 个元素） xs takeWhile p 从第一个元素开始查找满足条件 p 的元素， 直到遇到一个不满足条件的元素，返回所有遍历到的值。 xs dropWhile p 从第一个元素开始查找满足条件 p 的元素， 直到遇到一个不满足条件的元素，返回所有未遍历到的值。 xs filter p 返回满足条件 p 的所有元素的集合 xs withFilter p 集合的非严格的过滤器。后续对 xs 调用方法 map、flatMap 以及 withFilter 都只用作于满足条件 p 的元素，而忽略其他元素 xs filterNot p 返回不满足条件 p 的所有元素组成的集合 Subdivisions: xs splitAt n 在给定位置拆分集合，返回一个集合对 (xs take n, xs drop n) xs span p 根据给定条件拆分集合，返回一个集合对 (xs takeWhile p, xs dropWhile p)。即遍历元素，直到遇到第一个不符合条件的值则结束遍历，将遍历到的值和未遍历到的值分别放入两个集合返回。 xs partition p 按照筛选条件对元素进行分组 xs groupBy f 根据鉴别器函数 f 将 xs 划分为集合映射 Element Conditions: xs forall p 判断集合中所有的元素是否都满足条件 p xs exists p 判断集合中是否存在一个元素满足条件 p xs count p xs 中满足条件 p 的元素的个数 Folds: (z /: xs) (op) 以 z 为初始值，从左到右对 xs 中的元素执行操作为 op 的归约操作 (xs :\ z) (op) 以 z 为初始值，从右到左对 xs 中的元素执行操作为 op 的归约操作 xs.foldLeft(z) (op) 同 (z /: xs) (op) xs.foldRight(z) (op) 同 (xs :\ z) (op) xs reduceLeft op 从左到右对 xs 中的元素执行操作为 op 的归约操作 xs reduceRight op 从右到左对 xs 中的元素执行操作为 op 的归约操作 Specific Folds: xs.sum 累计求和 xs.product 累计求积 xs.min xs 中的最小值 xs.max xs 中的最大值 String: xs addString (b, start, sep, end) 向 StringBuilder b 中添加一个字符串， 该字符串包含 xs 的所有元素。start、seq 和 end 都是可选的，seq 为分隔符，start 为开始符号，end 为结束符号。 xs mkString (start, seq, end) 将集合转化为一个字符串。start、seq 和 end 都是可选的，seq 为分隔符，start 为开始符号，end 为结束符号。 xs.stringPrefix 返回 xs.toString 字符串开头的集合名称 Views: xs.view 生成 xs 的视图 xs view (from, to) 生成 xs 上指定索引范围内元素的视图 下面为部分方法的使用示例： 1234567891011121314151617scala&gt; List(1, 2, 3, 4, 5, 6).collect &#123; case i if i % 2 == 0 =&gt; i * 10 &#125;res0: List[Int] = List(20, 40, 60)scala&gt; List(1, 2, 3, 4, 5, 6).withFilter(_ % 2 == 0).map(_ * 10)res1: List[Int] = List(20, 40, 60)scala&gt; (10 /: List(1, 2, 3)) (_ + _)res2: Int = 16scala&gt; List(1, 2, 3, -4, 5) takeWhile (_ &gt; 0)res3: List[Int] = List(1, 2, 3)scala&gt; List(1, 2, 3, -4, 5) span (_ &gt; 0)res4: (List[Int], List[Int]) = (List(1, 2, 3),List(-4, 5))scala&gt; List(1, 2, 3).mkString("[","-","]")res5: String = [1-2-3] 四、Trait IterableScala 中所有的集合都直接或者间接实现了 Iterable 特质，Iterable 拓展自 Traversable，并额外定义了部分方法： 方法 作用 Abstract Method: xs.iterator 返回一个迭代器，用于遍历 xs 中的元素， 与 foreach 遍历元素的顺序相同。 Other Iterators: xs grouped size 返回一个固定大小的迭代器 xs sliding size 返回一个固定大小的滑动窗口的迭代器 Subcollections: xs takeRigtht n 返回 xs 中最后 n 个元素组成的集合（如果无序，则返回任意 n 个元素组成的集合） xs dropRight n 返回 xs 中除了最后 n 个元素外的部分 Zippers: xs zip ys 返回 xs 和 ys 的对应位置上的元素对组成的集合 xs zipAll (ys, x, y) 返回 xs 和 ys 的对应位置上的元素对组成的集合。其中较短的序列通过附加元素 x 或 y 来扩展以匹配较长的序列。 xs.zipWithIndex 返回一个由 xs 中元素及其索引所组成的元素对的集合 Comparison: xs sameElements ys 测试 xs 和 ys 是否包含相同顺序的相同元素 所有方法示例如下： 12345678910111213141516171819202122232425262728293031323334353637scala&gt; List(1, 2, 3).iterator.reduce(_ * _ * 10)res0: Int = 600scala&gt; List("a","b","c","d","e") grouped 2 foreach printlnList(a, b)List(c, d)List(e)scala&gt; List("a","b","c","d","e") sliding 2 foreach printlnList(a, b)List(b, c)List(c, d)List(d, e)scala&gt; List("a","b","c","d","e").takeRight(3)res1: List[String] = List(c, d, e)scala&gt; List("a","b","c","d","e").dropRight(3)res2: List[String] = List(a, b)scala&gt; List("a","b","c").zip(List(1,2,3))res3: List[(String, Int)] = List((a,1), (b,2), (c,3))scala&gt; List("a","b","c","d").zipAll(List(1,2,3),"",4)res4: List[(String, Int)] = List((a,1), (b,2), (c,3), (d,4))scala&gt; List("a","b","c").zipAll(List(1,2,3,4),"d","")res5: List[(String, Any)] = List((a,1), (b,2), (c,3), (d,4))scala&gt; List("a", "b", "c").zipWithIndexres6: List[(String, Int)] = List((a,0), (b,1), (c,2))scala&gt; List("a", "b") sameElements List("a", "b")res7: Boolean = truescala&gt; List("a", "b") sameElements List("b", "a")res8: Boolean = false 五、修改集合当你想对集合添加或者删除元素，需要根据不同的集合类型选择不同的操作符号： 操作符 描述 集合类型 coll(k)即 coll.apply(k) 获取指定位置的元素 Seq, Map coll :+ elemelem +: coll 向集合末尾或者集合头增加元素 Seq coll + elemcoll + (e1, e2, …) 追加元素 Seq, Map coll - elemcoll - (e1, e2, …) 删除元素 Set, Map, ArrayBuffer coll ++ coll2coll2 ++: coll 合并集合 Iterable coll – coll2 移除 coll 中包含的 coll2 中的元素 Set, Map, ArrayBuffer elem :: lstlst2 :: lst 把指定列表 (lst2) 或者元素 (elem) 添加到列表 (lst) 头部 List list ::: list2 合并 List List set \ set2set &amp; set2set &amp;~ set2 并集、交集、差集 Set coll += elemcoll += (e1, e2, …)coll ++= coll2coll -= elemcoll -= (e1, e2, …)coll –= coll2 添加或者删除元素，并将修改后的结果赋值给集合本身 可变集合 elem +=: collcoll2 ++=: coll 在集合头部追加元素或集合 ArrayBuffer 参考资料 https://docs.scala-lang.org/overviews/collections/overview.html https://docs.scala-lang.org/overviews/collections/trait-traversable.html https://docs.scala-lang.org/overviews/collections/trait-iterable.html]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Scala</tag>
        <tag>集合类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala基本数据类型和运算符]]></title>
    <url>%2F2019%2F09%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%92%8C%E8%BF%90%E7%AE%97%E7%AC%A6%2F</url>
    <content type="text"><![CDATA[Scala基本数据类型和运算符一、数据类型二、字面量三、运算符 一、数据类型1.1 类型支持Scala 拥有下表所示的数据类型，其中 Byte、Short、Int、Long 和 Char 类型统称为整数类型，整数类型加上 Float 和 Double 统称为数值类型。Scala 数值类型的取值范围和 Java 对应类型的取值范围相同。 数据类型 描述 Byte 8 位有符号补码整数。数值区间为 -128 到 127 Short 16 位有符号补码整数。数值区间为 -32768 到 32767 Int 32 位有符号补码整数。数值区间为 -2147483648 到 2147483647 Long 64 位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807 Float 32 位, IEEE 754 标准的单精度浮点数 Double 64 位 IEEE 754 标准的双精度浮点数 Char 16 位无符号 Unicode 字符, 区间值为 U+0000 到 U+FFFF String 字符序列 Boolean true 或 false Unit 表示无值，等同于 Java 中的 void。用作不返回任何结果的方法的结果类型。Unit 只有一个实例值，写成 ()。 Null null 或空引用 Nothing Nothing 类型在 Scala 的类层级的最低端；它是任何其他类型的子类型。 Any Any 是所有其他类的超类 AnyRef AnyRef 类是 Scala 里所有引用类 (reference class) 的基类 1.2 定义变量Scala 的变量分为两种，val 和 var，其区别如下： val ： 类似于 Java 中的 final 变量，一旦初始化就不能被重新赋值； var ：类似于 Java 中的非 final 变量，在整个声明周期内 var 可以被重新赋值； 1234567891011scala&gt; val a=1a: Int = 1scala&gt; a=2&lt;console&gt;:8: error: reassignment to val // 不允许重新赋值scala&gt; var b=1b: Int = 1scala&gt; b=2b: Int = 2 1.3 类型推断在上面的演示中，并没有声明 a 是 Int 类型，但是程序还是把 a 当做 Int 类型，这就是 Scala 的类型推断。在大多数情况下，你都无需指明变量的类型，程序会自动进行推断。如果你想显式的声明类型，可以在变量后面指定，如下： 12scala&gt; val c:String="hello scala"c: String = hello scala 1.4 Scala解释器在 scala 命令行中，如果没有对输入的值指定赋值的变量，则输入的值默认会赋值给 resX(其中 X 是一个从 0 开始递增的整数)，res 是 result 的缩写，这个变量可以在后面的语句中进行引用。 12345678scala&gt; 5res0: Int = 5scala&gt; res0*6res1: Int = 30scala&gt; println(res1)30 二、字面量Scala 和 Java 字面量在使用上很多相似，比如都使用 F 或 f 表示浮点型，都使用 L 或 l 表示 Long 类型。下文主要介绍两者差异部分。 1234567891011121314151617scala&gt; 1.2res0: Double = 1.2scala&gt; 1.2fres1: Float = 1.2scala&gt; 1.4Fres2: Float = 1.4scala&gt; 1res3: Int = 1scala&gt; 1lres4: Long = 1scala&gt; 1Lres5: Long = 1 2.1 整数字面量Scala 支持 10 进制和 16 进制，但不支持八进制字面量和以 0 开头的整数字面量。 12scala&gt; 012&lt;console&gt;:1: error: Decimal integer literals may not have a leading zero. (Octal syntax is obsolete.) 2.2 字符串字面量1. 字符字面量字符字面量由一对单引号和中间的任意 Unicode 字符组成。你可以显式的给出原字符、也可以使用字符的 Unicode 码来表示，还可以包含特殊的转义字符。 12345678scala&gt; '\u0041'res0: Char = Ascala&gt; 'a'res1: Char = ascala&gt; '\n'res2: Char = 2. 字符串字面量字符串字面量由双引号包起来的字符组成。 12scala&gt; "hello world"res3: String = hello world 3.原生字符串Scala 提供了 &quot;&quot;&quot; ... &quot;&quot;&quot; 语法，通过三个双引号来表示原生字符串和多行字符串，使用该种方式，原生字符串中的特殊字符不会被转义。 1234567891011scala&gt; "hello \tool"res4: String = hello oolscala&gt; """hello \tool"""res5: String = hello \toolscala&gt; """hello | world"""res6: String =helloworld 2.3 符号字面量符号字面量写法为： &#39;标识符 ，这里 标识符可以是任何字母或数字的组合。符号字面量会被映射成 scala.Symbol 的实例，如:符号字面量 &#39;x 会被编译器翻译为 scala.Symbol(&quot;x&quot;)。符号字面量可选方法很少，只能通过 .name 获取其名称。 注意：具有相同 name 的符号字面量一定指向同一个 Symbol 对象，不同 name 的符号字面量一定指向不同的 Symbol 对象。 12345scala&gt; val sym = 'ID008sym: Symbol = 'ID008scala&gt; sym.nameres12: String = ID008 2.4 插值表达式Scala 支持插值表达式。 12345scala&gt; val name="xiaoming"name: String = xiaomingscala&gt; println(s"My name is $name,I'm $&#123;2*9&#125;.")My name is xiaoming,I'm 18. 三、运算符Scala 和其他语言一样，支持大多数的操作运算符： 算术运算符（+，-，*，/，%） 关系运算符（==，!=，&gt;，&lt;，&gt;=，&lt;=） 逻辑运算符 (&amp;&amp;，||，!，&amp;，|) 位运算符 (~，&amp;，|，^，&lt;&lt;，&gt;&gt;，&gt;&gt;&gt;) 赋值运算符 (=，+=，-=，*=，/=，%=，&lt;&lt;=，&gt;&gt;=，&amp;=，^=，|=) 以上操作符的基本使用与 Java 类似，下文主要介绍差异部分和注意事项。 3.1 运算符即方法Scala 的面向对象比 Java 更加纯粹，在 Scala 中一切都是对象。所以对于 1+2,实际上是调用了 Int 类中名为 + 的方法，所以 1+2,也可以写成 1.+(2)。 12345scala&gt; 1+2res14: Int = 3scala&gt; 1.+(2)res15: Int = 3 Int 类中包含了多个重载的 + 方法，用于分别接收不同类型的参数。 3.2 逻辑运算符和其他语言一样，在 Scala 中 &amp;&amp;，|| 的执行是短路的，即如果左边的表达式能确定整个结果，右边的表达式就不会被执行，这满足大多数使用场景。但是如果你需要在无论什么情况下，都执行右边的表达式，则可以使用 &amp; 或 | 代替。 3.3 赋值运算符在 Scala 中没有 Java 中的 ++ 和 -- 运算符，如果你想要实现类似的操作，只能使用 +=1，或者 -=1。 123456789101112scala&gt; var a=1a: Int = 1scala&gt; a+=1scala&gt; ares8: Int = 2scala&gt; a-=1scala&gt; ares10: Int = 1 3.4 运算符优先级操作符的优先级如下：优先级由上至下，逐级递减。 在表格中某个字符的优先级越高，那么以这个字符打头的方法就拥有更高的优先级。如 + 的优先级大于 &lt;，也就意味则 + 的优先级大于以 &lt; 开头的 &lt;&lt;，所以 2&lt;&lt;2+2 , 实际上等价于 2&lt;&lt;(2+2) : 12345scala&gt; 2&lt;&lt;2+2res0: Int = 32scala&gt; 2&lt;&lt;(2+2)res1: Int = 32 3.5 对象相等性如果想要判断两个对象是否相等，可以使用 == 和 !=,这两个操作符可以用于所有的对象，包括 null。 1234567891011121314scala&gt; 1==2res2: Boolean = falsescala&gt; List(1,2,3)==List(1,2,3)res3: Boolean = truescala&gt; 1==1.0res4: Boolean = truescala&gt; List(1,2,3)==nullres5: Boolean = falsescala&gt; null==nullres6: Boolean = true 参考资料 Martin Odersky . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>数据类型</tag>
        <tag>Scala</tag>
        <tag>基本数据类型</tag>
        <tag>运算符</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Scala简介及开发环境配置]]></title>
    <url>%2F2019%2F09%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BScala%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[Scala简介及开发环境配置一、Scala简介二、配置IDEA开发环境 一、Scala简介1.1 概念Scala 全称为 Scalable Language，即“可伸缩的语言”，之所以这样命名，是因为它的设计目标是希望伴随着用户的需求一起成长。Scala 是一门综合了面向对象和函数式编程概念的静态类型的编程语言，它运行在标准的 Java 平台上，可以与所有的 Java 类库无缝协作。 1.2 特点1. Scala是面向对象的Scala 是一种面向对象的语言，每个值都是对象，每个方法都是调用。举例来说，如果你执行 1+2，则对于 Scala 而言，实际是在调用 Int 类里定义的名为 + 的方法。 2. Scala是函数式的Scala 不只是一门纯的面对对象的语言，它也是功能完整的函数式编程语言。函数式编程以两大核心理念为指导： 函数是一等公民； 程序中的操作应该将输入值映射成输出值，而不是当场修改数据。即方法不应该有副作用。 1.3 Scala的优点1. 与Java的兼容Scala 可以与 Java 无缝对接，其在执行时会被编译成 JVM 字节码，这使得其性能与 Java 相当。Scala 可以直接调用 Java 中的方法、访问 Java 中的字段、继承 Java 类、实现 Java 接口。Scala 重度复用并包装了原生的 Java 类型，并支持隐式转换。 2. 精简的语法Scala 的程序通常比较简洁，相比 Java 而言，代码行数会大大减少，这使得程序员对代码的阅读和理解更快，缺陷也更少。 3. 高级语言的特性Scala 具有高级语言的特定，对代码进行了高级别的抽象，能够让你更好地控制程序的复杂度，保证开发的效率。 4. 静态类型Scala 拥有非常先进的静态类型系统，Scala 不仅拥有与 Java 类似的允许嵌套类的类型系统，还支持使用泛型对类型进行参数化，用交集（intersection）来组合类型，以及使用抽象类型来进行隐藏类型的细节。通过这些特性，可以更快地设计出安全易用的程序和接口。 二、配置IDEA开发环境2.1 前置条件Scala 的运行依赖于 JDK，Scala 2.12.x 需要 JDK 1.8+。 2.2 安装Scala插件IDEA 默认不支持 Scala 语言的开发，需要通过插件进行扩展。打开 IDEA，依次点击 File =&gt; settings=&gt; plugins 选项卡，搜索 Scala 插件 (如下图)。找到插件后进行安装，并重启 IDEA 使得安装生效。 2.3 创建Scala项目在 IDEA 中依次点击 File =&gt; New =&gt; Project 选项卡，然后选择创建 Scala—IDEA 工程： 2.4 下载Scala SDK1. 方式一此时看到 Scala SDK 为空，依次点击 Create =&gt; Download ，选择所需的版本后，点击 OK 按钮进行下载，下载完成点击 Finish 进入工程。 2. 方式二方式一是 Scala 官方安装指南里使用的方式，但下载速度通常比较慢，且这种安装下并没有直接提供 Scala 命令行工具。所以个人推荐到官网下载安装包进行安装，下载地址：https://www.scala-lang.org/download/ 这里我的系统是 Windows，下载 msi 版本的安装包后，一直点击下一步进行安装，安装完成后会自动配置好环境变量。 由于安装时已经自动配置好环境变量，所以 IDEA 会自动选择对应版本的 SDK。 2.5 创建Hello World在工程 src 目录上右击 New =&gt; Scala class 创建 Hello.scala。输入代码如下，完成后点击运行按钮，成功运行则代表搭建成功。 2.6 切换Scala版本在日常的开发中，由于对应软件（如 Spark）的版本切换，可能导致需要切换 Scala 的版本，则可以在 Project Structures 中的 Global Libraries 选项卡中进行切换。 2.7 使用scala命令行采用 msi 方式安装，程序会自动配置好环境变量。此时可以直接使用命令行工具： 参考资料 Martin Odersky(著)，高宇翔 (译) . Scala 编程 (第 3 版)[M] . 电子工业出版社 . 2018-1-1 https://www.scala-lang.org/download/]]></content>
      <categories>
        <category>Scala</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>环境搭建</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Azkaban_Flow_2.0_的使用]]></title>
    <url>%2F2019%2F09%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BAzkaban_Flow_2.0_%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Azkaban Flow 2.0的使用一、Flow 2.0 简介二、YAML语法三、简单任务调度四、多任务调度五、内嵌流 一、Flow 2.0 简介1.1 Flow 2.0 的产生Azkaban 目前同时支持 Flow 1.0 和 Flow2.0 ，但是官方文档上更推荐使用 Flow 2.0，因为 Flow 1.0 会在将来的版本被移除。Flow 2.0 的主要设计思想是提供 1.0 所没有的流级定义。用户可以将属于给定流的所有 job / properties 文件合并到单个流定义文件中，其内容采用 YAML 语法进行定义，同时还支持在流中再定义流，称为为嵌入流或子流。 1.2 基本结构项目 zip 将包含多个流 YAML 文件，一个项目 YAML 文件以及可选库和源代码。Flow YAML 文件的基本结构如下： 每个 Flow 都在单个 YAML 文件中定义； 流文件以流名称命名，如：my-flow-name.flow； 包含 DAG 中的所有节点； 每个节点可以是作业或流程； 每个节点 可以拥有 name, type, config, dependsOn 和 nodes sections 等属性； 通过列出 dependsOn 列表中的父节点来指定节点依赖性； 包含与流相关的其他配置； 当前 properties 文件中流的所有常见属性都将迁移到每个流 YAML 文件中的 config 部分。 官方提供了一个比较完善的配置样例，如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677config: user.to.proxy: azktest param.hadoopOutData: /tmp/wordcounthadoopout param.inData: /tmp/wordcountpigin param.outData: /tmp/wordcountpigout# This section defines the list of jobs# A node can be a job or a flow# In this example, all nodes are jobsnodes: # Job definition # The job definition is like a YAMLified version of properties file # with one major difference. All custom properties are now clubbed together # in a config section in the definition. # The first line describes the name of the job - name: AZTest type: noop # The dependsOn section contains the list of parent nodes the current # node depends on dependsOn: - hadoopWC1 - NoOpTest1 - hive2 - java1 - jobCommand2 - name: pigWordCount1 type: pig # The config section contains custom arguments or parameters which are # required by the job config: pig.script: src/main/pig/wordCountText.pig - name: hadoopWC1 type: hadoopJava dependsOn: - pigWordCount1 config: classpath: ./* force.output.overwrite: true input.path: $&#123;param.inData&#125; job.class: com.linkedin.wordcount.WordCount main.args: $&#123;param.inData&#125; $&#123;param.hadoopOutData&#125; output.path: $&#123;param.hadoopOutData&#125; - name: hive1 type: hive config: hive.script: src/main/hive/showdb.q - name: NoOpTest1 type: noop - name: hive2 type: hive dependsOn: - hive1 config: hive.script: src/main/hive/showTables.sql - name: java1 type: javaprocess config: Xms: 96M java.class: com.linkedin.foo.HelloJavaProcessJob - name: jobCommand1 type: command config: command: echo "hello world from job_command_1" - name: jobCommand2 type: command dependsOn: - jobCommand1 config: command: echo "hello world from job_command_2" 二、YAML语法想要使用 Flow 2.0 进行工作流的配置，首先需要了解 YAML 。YAML 是一种简洁的非标记语言，有着严格的格式要求的，如果你的格式配置失败，上传到 Azkaban 的时候就会抛出解析异常。 2.1 基本规则 大小写敏感 ； 使用缩进表示层级关系 ； 缩进长度没有限制，只要元素对齐就表示这些元素属于一个层级； 使用#表示注释 ； 字符串默认不用加单双引号，但单引号和双引号都可以使用，双引号表示不需要对特殊字符进行转义； YAML 中提供了多种常量结构，包括：整数，浮点数，字符串，NULL，日期，布尔，时间。 2.2 对象的写法12# value 与 ： 符号之间必须要有一个空格key: value 2.3 map的写法1234567# 写法一 同一缩进的所有键值对属于一个mapkey: key1: value1 key2: value2# 写法二&#123;key1: value1, key2: value2&#125; 2.3 数组的写法1234567# 写法一 使用一个短横线加一个空格代表一个数组项- a- b- c# 写法二[a,b,c] 2.5 单双引号支持单引号和双引号，但双引号不会对特殊字符进行转义： 12345s1: '内容\n 字符串's2: "内容\n 字符串"转换后：&#123; s1: '内容\\n 字符串', s2: '内容\n 字符串' &#125; 2.6 特殊符号一个 YAML 文件中可以包括多个文档，使用 --- 进行分割。 2.7 配置引用Flow 2.0 建议将公共参数定义在 config 下，并通过 ${} 进行引用。 三、简单任务调度3.1 任务配置新建 flow 配置文件： 12345nodes: - name: jobA type: command config: command: echo "Hello Azkaban Flow 2.0." 在当前的版本中，Azkaban 同时支持 Flow 1.0 和 Flow 2.0，如果你希望以 2.0 的方式运行，则需要新建一个 project 文件，指明是使用的是 Flow 2.0： 1azkaban-flow-version: 2.0 3.2 打包上传 3.3 执行结果由于在 1.0 版本中已经介绍过 Web UI 的使用，这里就不再赘述。对于 1.0 和 2.0 版本，只有配置方式有所不同，其他上传执行的方式都是相同的。执行结果如下： 四、多任务调度和 1.0 给出的案例一样，这里假设我们有五个任务（jobA——jobE）, D 任务需要在 A，B，C 任务执行完成后才能执行，而 E 任务则需要在 D 任务执行完成后才能执行，相关配置文件应如下。可以看到在 1.0 中我们需要分别定义五个配置文件，而在 2.0 中我们只需要一个配置文件即可完成配置。 123456789101112131415161718192021222324252627282930313233nodes: - name: jobE type: command config: command: echo "This is job E" # jobE depends on jobD dependsOn: - jobD - name: jobD type: command config: command: echo "This is job D" # jobD depends on jobA、jobB、jobC dependsOn: - jobA - jobB - jobC - name: jobA type: command config: command: echo "This is job A" - name: jobB type: command config: command: echo "This is job B" - name: jobC type: command config: command: echo "This is job C" 五、内嵌流Flow2.0 支持在一个 Flow 中定义另一个 Flow，称为内嵌流或者子流。这里给出一个内嵌流的示例，其 Flow 配置如下： 123456789101112131415161718192021222324nodes: - name: jobC type: command config: command: echo "This is job C" dependsOn: - embedded_flow - name: embedded_flow type: flow config: prop: value nodes: - name: jobB type: command config: command: echo "This is job B" dependsOn: - jobA - name: jobA type: command config: command: echo "This is job A" 内嵌流的 DAG 图如下： 执行情况如下： 参考资料 Azkaban Flow 2.0 Design Getting started with Azkaban Flow 2.0]]></content>
      <categories>
        <category>Azkaban</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Azkaban</tag>
        <tag>Flow2.0使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Azkaban_Flow_1.0_的使用]]></title>
    <url>%2F2019%2F09%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BAzkaban_Flow_1.0_%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Azkaban Flow 1.0 的使用一、简介二、基本任务调度三、多任务调度四、调度HDFS作业五、调度MR作业六、调度Hive作业七、在线修改作业配置 一、简介Azkaban 主要通过界面上传配置文件来进行任务的调度。它有两个重要的概念： Job： 你需要执行的调度任务； Flow：一个获取多个 Job 及它们之间的依赖关系所组成的图表叫做 Flow。 目前 Azkaban 3.x 同时支持 Flow 1.0 和 Flow 2.0，本文主要讲解 Flow 1.0 的使用，下一篇文章会讲解 Flow 2.0 的使用。 二、基本任务调度2.1 新建项目在 Azkaban 主界面可以创建对应的项目： 2.2 任务配置新建任务配置文件 Hello-Azkaban.job，内容如下。这里的任务很简单，就是输出一句 &#39;Hello Azkaban!&#39; ： 123#command.jobtype=commandcommand=echo 'Hello Azkaban!' 2.3 打包上传将 Hello-Azkaban.job 打包为 zip 压缩文件： 通过 Web UI 界面上传： 上传成功后可以看到对应的 Flows： 2.4 执行任务点击页面上的 Execute Flow 执行任务： 2.5 执行结果点击 detail 可以查看到任务的执行日志： 三、多任务调度3.1 依赖配置这里假设我们有五个任务（TaskA——TaskE）,D 任务需要在 A，B，C 任务执行完成后才能执行，而 E 任务则需要在 D 任务执行完成后才能执行，这种情况下需要使用 dependencies 属性定义其依赖关系。各任务配置如下： Task-A.job : 12type=commandcommand=echo 'Task A' Task-B.job : 12type=commandcommand=echo 'Task B' Task-C.job : 12type=commandcommand=echo 'Task C' Task-D.job : 123type=commandcommand=echo 'Task D'dependencies=Task-A,Task-B,Task-C Task-E.job : 123type=commandcommand=echo 'Task E'dependencies=Task-D 3.2 压缩上传压缩后进行上传，这里需要注意的是一个 Project 只能接收一个压缩包，这里我还沿用上面的 Project，默认后面的压缩包会覆盖前面的压缩包： 3.3 依赖关系多个任务存在依赖时，默认采用最后一个任务的文件名作为 Flow 的名称，其依赖关系如图： 3.4 执行结果 从这个案例可以看出，Flow1.0 无法通过一个 job 文件来完成多个任务的配置，但是 Flow 2.0 就很好的解决了这个问题。 四、调度HDFS作业步骤与上面的步骤一致，这里以查看 HDFS 上的文件列表为例。命令建议采用完整路径，配置文件如下： 12type=commandcommand=/usr/app/hadoop-2.6.0-cdh5.15.2/bin/hadoop fs -ls / 执行结果： 五、调度MR作业MR 作业配置： 12type=commandcommand=/usr/app/hadoop-2.6.0-cdh5.15.2/bin/hadoop jar /usr/app/hadoop-2.6.0-cdh5.15.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3 执行结果： 六、调度Hive作业作业配置： 12type=commandcommand=/usr/app/hive-1.1.0-cdh5.15.2/bin/hive -f 'test.sql' 其中 test.sql 内容如下，创建一张雇员表，然后查看其结构： 123456789101112131415CREATE DATABASE IF NOT EXISTS hive;use hive;drop table if exists emp;CREATE TABLE emp(empno int,ename string,job string,mgr int,hiredate string,sal double,comm double,deptno int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';-- 查看 emp 表的信息desc emp; 打包的时候将 job 文件与 sql 文件一并进行打包： 执行结果如下： 七、在线修改作业配置在测试时，我们可能需要频繁修改配置，如果每次修改都要重新打包上传，这会比较麻烦。所以 Azkaban 支持配置的在线修改，点击需要修改的 Flow，就可以进入详情页面： 在详情页面点击 Eidt 按钮可以进入编辑页面： 在编辑页面可以新增配置或者修改配置： 附：可能出现的问题如果出现以下异常，多半是因为执行主机内存不足，Azkaban 要求执行主机的可用内存必须大于 3G 才能执行任务： 1Cannot request memory (Xms 0 kb, Xmx 0 kb) from system for job 如果你的执行主机没办法增大内存，那么可以通过修改 plugins/jobtypes/ 目录下的 commonprivate.properties 文件来关闭内存检查，配置如下： 1memCheck.enabled=false]]></content>
      <categories>
        <category>Azkaban</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Azkaban</tag>
        <tag>Flow1.0使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Azkaban_3.x_编译及部署]]></title>
    <url>%2F2019%2F09%2F19%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BAzkaban_3.x_%E7%BC%96%E8%AF%91%E5%8F%8A%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Azkaban 3.x 编译及部署一、Azkaban 源码编译二、Azkaban 部署模式三、Solo Server 模式部署 一、Azkaban 源码编译1.1 下载并解压Azkaban 在 3.0 版本之后就不提供对应的安装包，需要自己下载源码进行编译。 下载所需版本的源码，Azkaban 的源码托管在 GitHub 上，地址为 https://github.com/azkaban/azkaban 。可以使用 git clone 的方式获取源码，也可以使用 wget 直接下载对应 release 版本的 tar.gz 文件，这里我采用第二种方式： 1234# 下载wget https://github.com/azkaban/azkaban/archive/3.70.0.tar.gz# 解压tar -zxvf azkaban-3.70.0.tar.gz 1.2 准备编译环境1. JDKAzkaban 编译依赖 JDK 1.8+ ，JDK 安装方式见本仓库： Linux 环境下 JDK 安装 2. GradleAzkaban 3.70.0 编译需要依赖 gradle-4.6-all.zip。Gradle 是一个项目自动化构建开源工具，类似于 Maven，但由于采用 Groovy 语言进行项目配置，所以比 Maven 更为灵活，目前广泛用于 Android 开发、Spring 项目的构建。 需要注意的是不同版本的 Azkaban 依赖 Gradle 版本不同，可以在解压后的 /gradle/wrapper/gradle-wrapper.properties 文件查看 在编译时程序会自动去图中所示的地址进行下载，但是下载速度很慢。为避免影响编译过程，建议先手动下载至 /gradle/wrapper/ 目录下： 1# wget https://services.gradle.org/distributions/gradle-4.6-all.zip 然后修改配置文件 gradle-wrapper.properties 中的 distributionUrl 属性，指明使用本地的 gradle。 3. GitAzkaban 的编译过程需要用 Git 下载部分 JAR 包，所以需要预先安装 Git： 1# yum install git 1.3 项目编译在根目录下执行编译命令，编译成功后会有 BUILD SUCCESSFUL 的提示： 1# ./gradlew build installDist -x test 编译过程中需要注意以下问题： 因为编译的过程需要下载大量的 Jar 包，下载速度根据网络情况而定，通常都不会很快，如果网络不好，耗费半个小时，一个小时都是很正常的； 编译过程中如果出现网络问题而导致 JAR 无法下载，编译可能会被强行终止，这时候重复执行编译命令即可，gradle 会把已经下载的 JAR 缓存到本地，所以不用担心会重复下载 JAR 包。 二、Azkaban 部署模式 After version 3.0, we provide two modes: the stand alone “solo-server” mode and distributed multiple-executor mode. The following describes thedifferences between the two modes. 按照官方文档的说明，Azkaban 3.x 之后版本提供 2 种运行模式： solo server model(单服务模式) ：元数据默认存放在内置的 H2 数据库（可以修改为 MySQL），该模式中 webServer(管理服务器) 和 executorServer(执行服务器) 运行在同一个进程中，进程名是 AzkabanSingleServer。该模式适用于小规模工作流的调度。 multiple-executor(分布式多服务模式) ：存放元数据的数据库为 MySQL，MySQL 应采用主从模式进行备份和容错。这种模式下 webServer 和 executorServer 在不同进程中运行，彼此之间互不影响，适合用于生产环境。 下面主要介绍 Solo Server 模式。 三 、Solo Server 模式部署2.1 解压Solo Server 模式安装包在编译后的 /azkaban-solo-server/build/distributions 目录下，找到后进行解压即可： 12# 解压tar -zxvf azkaban-solo-server-3.70.0.tar.gz 2.2 修改时区这一步不是必须的。但是因为 Azkaban 默认采用的时区是 America/Los_Angeles，如果你的调度任务中有定时任务的话，就需要进行相应的更改，这里我更改为常用的 Asia/Shanghai 2.3 启动执行启动命令，需要注意的是一定要在根目录下执行，不能进入 bin 目录下执行，不然会抛出 Cannot find &#39;database.properties&#39; 异常。 1# bin/start-solo.sh 2.4 验证验证方式一：使用 jps 命令查看是否有 AzkabanSingleServer 进程： 验证方式二：访问 8081 端口，查看 Web UI 界面，默认的登录名密码都是 azkaban，如果需要修改或新增用户，可以在 conf/azkaban-users.xml 文件中进行配置：]]></content>
      <categories>
        <category>Azkaban</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Azkaban</tag>
        <tag>编译部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Azkaban简介]]></title>
    <url>%2F2019%2F09%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BAzkaban%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Azkaban简介一、Azkaban 介绍1.1 背景一个完整的大数据分析系统，必然由很多任务单元 (如数据收集、数据清洗、数据存储、数据分析等) 组成，所有的任务单元及其之间的依赖关系组成了复杂的工作流。复杂的工作流管理涉及到很多问题： 如何定时调度某个任务？ 如何在某个任务执行完成后再去执行另一个任务？ 如何在任务失败时候发出预警？ …… 面对这些问题，工作流调度系统应运而生。Azkaban 就是其中之一。 1.2 功能Azkaban 产生于 LinkedIn，并经过多年生产环境的检验，它具备以下功能： 兼容任何版本的 Hadoop 易于使用的 Web UI 可以使用简单的 Web 页面进行工作流上传 支持按项目进行独立管理 定时任务调度 模块化和可插入 身份验证和授权 跟踪用户操作 支持失败和成功的电子邮件提醒 SLA 警报和自动查杀失败任务 重试失败的任务 Azkaban 的设计理念是在保证功能实现的基础上兼顾易用性，其页面风格清晰明朗，下面是其 WEB UI 界面： 二、Azkaban 和 OozieAzkaban 和 Oozie 都是目前使用最为广泛的工作流调度程序，其主要区别如下： 功能对比 两者均可以调度 Linux 命令、MapReduce、Spark、Pig、Java、Hive 等工作流任务； 两者均可以定时执行工作流任务。 工作流定义 Azkaban 使用 Properties(Flow 1.0) 和 YAML(Flow 2.0) 文件定义工作流； Oozie 使用 Hadoop 流程定义语言（hadoop process defination language，HPDL）来描述工作流，HPDL 是一种 XML 流程定义语言。 资源管理 Azkaban 有较严格的权限控制，如用户对工作流进行读/写/执行等操作； Oozie 暂无严格的权限控制。 运行模式 Azkaban 3.x 提供了两种运行模式： solo server model(单服务模式) ：元数据默认存放在内置的 H2 数据库（可以修改为 MySQL），该模式中 webServer(管理服务器) 和 executorServer(执行服务器) 运行在同一个进程中，进程名是 AzkabanSingleServer。该模式适用于小规模工作流的调度。 multiple-executor(分布式多服务模式) ：存放元数据的数据库为 MySQL，MySQL 应采用主从模式进行备份和容错。这种模式下 webServer 和 executorServer 在不同进程中运行，彼此之间互不影响，适合用于生产环境。 Oozie 使用 Tomcat 等 Web 容器来展示 Web 页面，默认使用 derby 存储工作流的元数据，由于 derby 过于轻量，实际使用中通常用 MySQL 代替。 三、总结如果你的工作流不是特别复杂，推荐使用轻量级的 Azkaban，主要有以下原因： 安装方面：Azkaban 3.0 之前都是提供安装包的，直接解压部署即可。Azkaban 3.0 之后的版本需要编译，这个编译是基于 gradle 的，自动化程度比较高； 页面设计：所有任务的依赖关系、执行结果、执行日志都可以从界面上直观查看到； 配置方面：Azkaban Flow 1.0 基于 Properties 文件来定义工作流，这个时候的限制可能会多一点。但是在 Flow 2.0 就支持了 YARM。YARM 语法更加灵活简单，著名的微服务框架 Spring Boot 就采用的 YAML 代替了繁重的 XML。]]></content>
      <categories>
        <category>Azkaban</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Azkaban</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Storm三种打包方式对比分析]]></title>
    <url>%2F2019%2F09%2F13%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BStorm%E4%B8%89%E7%A7%8D%E6%89%93%E5%8C%85%E6%96%B9%E5%BC%8F%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Storm三种打包方式对比分析一、简介二、mvn package三、maven-assembly-plugin插件四、maven-shade-plugin插件五、结论六、打包注意事项 一、简介在将 Storm Topology 提交到服务器集群运行时，需要先将项目进行打包。本文主要对比分析各种打包方式，并将打包过程中需要注意的事项进行说明。主要打包方式有以下三种： 第一种：不加任何插件，直接使用 mvn package 打包； 第二种：使用 maven-assembly-plugin 插件进行打包； 第三种：使用 maven-shade-plugin 进行打包。 以下分别进行详细的说明。 二、mvn package2.1 mvn package的局限不在 POM 中配置任何插件，直接使用 mvn package 进行项目打包，这对于没有使用外部依赖包的项目是可行的。 但如果项目中使用了第三方 JAR 包，就会出现问题，因为 mvn package 打包后的 JAR 中是不含有依赖包的，如果此时你提交到服务器上运行，就会出现找不到第三方依赖的异常。 如果你想采用这种方式进行打包，但是又使用了第三方 JAR，有没有解决办法？答案是有的，这一点在官方文档的Command Line Client 章节有所讲解，主要解决办法如下。 2.2 解决办法在使用 storm jar 提交 Topology 时，可以使用如下方式指定第三方依赖： 如果第三方 JAR 包在本地，可以使用 --jars 指定； 如果第三方 JAR 包在远程中央仓库，可以使用 --artifacts 指定，此时如果想要排除某些依赖，可以使用 ^ 符号。指定后 Storm 会自动到中央仓库进行下载，然后缓存到本地； 如果第三方 JAR 包在其他仓库，还需要使用 --artifactRepositories 指明仓库地址，库名和地址使用 ^ 符号分隔。 以下是一个包含上面三种情况的命令示例： 123456./bin/storm jar example/storm-starter/storm-starter-topologies-*.jar \org.apache.storm.starter.RollingTopWords blobstore-remote2 remote \--jars "./external/storm-redis/storm-redis-1.1.0.jar,./external/storm-kafka/storm-kafka-1.1.0.jar" \--artifacts "redis.clients:jedis:2.9.0,org.apache.kafka:kafka_2.10:0.8.2.2^org.slf4j:slf4j-log4j12" \--artifactRepositories "jboss-repository^http://repository.jboss.com/maven2, \HDPRepo^http://repo.hortonworks.com/content/groups/public/" 这种方式是建立在你能够连接到外网的情况下，如果你的服务器不能连接外网，或者你希望能把项目直接打包成一个 ALL IN ONE 的 JAR，即包含所有相关依赖，此时可以采用下面介绍的两个插件。 三、maven-assembly-plugin插件maven-assembly-plugin 是官方文档中介绍的打包方法，来源于官方文档：Running Topologies on a Production Cluster If you’re using Maven, the Maven Assembly Plugin can do the packaging for you. Just add this to your pom.xml: 1234567891011121314&gt; &lt;plugin&gt;&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;&gt; &lt;configuration&gt;&gt; &lt;descriptorRefs&gt; &gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt;&gt; &lt;/descriptorRefs&gt;&gt; &lt;archive&gt;&gt; &lt;manifest&gt;&gt; &lt;mainClass&gt;com.path.to.main.Class&lt;/mainClass&gt;&gt; &lt;/manifest&gt;&gt; &lt;/archive&gt;&gt; &lt;/configuration&gt;&gt; &lt;/plugin&gt;&gt; Then run mvn assembly:assembly to get an appropriately packaged jar. Make sure you exclude the Storm jars since the cluster already has Storm on the classpath. 官方文档主要说明了以下几点： 使用 maven-assembly-plugin 可以把所有的依赖一并打入到最后的 JAR 中； 需要排除掉 Storm 集群环境中已经提供的 Storm jars； 通过 &lt;mainClass&gt; 标签指定主入口类； 通过 &lt;descriptorRef&gt; 标签指定打包相关配置。 jar-with-dependencies 是 Maven预定义 的一种最基本的打包配置，其 XML 文件如下： 123456789101112131415161718&lt;assembly xmlns="http://maven.apache.org/ASSEMBLY/2.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/ASSEMBLY/2.0.0 http://maven.apache.org/xsd/assembly-2.0.0.xsd"&gt; &lt;id&gt;jar-with-dependencies&lt;/id&gt; &lt;formats&gt; &lt;format&gt;jar&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;outputDirectory&gt;/&lt;/outputDirectory&gt; &lt;useProjectArtifact&gt;true&lt;/useProjectArtifact&gt; &lt;unpack&gt;true&lt;/unpack&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt;&lt;/assembly&gt; 我们可以通过对该配置文件进行拓展，从而实现更多的功能，比如排除指定的 JAR 等。使用示例如下： 1. 引入插件在 POM.xml 中引入插件，并指定打包格式的配置文件为 assembly.xml(名称可自定义)： 1234567891011121314151617&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptors&gt; &lt;descriptor&gt;src/main/resources/assembly.xml&lt;/descriptor&gt; &lt;/descriptors&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.heibaiying.wordcount.ClusterWordCountApp&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; assembly.xml 拓展自 jar-with-dependencies.xml，使用了 &lt;excludes&gt; 标签排除 Storm jars，具体内容如下： 1234567891011121314151617181920212223242526&lt;assembly xmlns="http://maven.apache.org/ASSEMBLY/2.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/ASSEMBLY/2.0.0 http://maven.apache.org/xsd/assembly-2.0.0.xsd"&gt; &lt;id&gt;jar-with-dependencies&lt;/id&gt; &lt;!--指明打包方式--&gt; &lt;formats&gt; &lt;format&gt;jar&lt;/format&gt; &lt;/formats&gt; &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt; &lt;dependencySets&gt; &lt;dependencySet&gt; &lt;outputDirectory&gt;/&lt;/outputDirectory&gt; &lt;useProjectArtifact&gt;true&lt;/useProjectArtifact&gt; &lt;unpack&gt;true&lt;/unpack&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;!--排除 storm 环境中已经提供的 storm-core--&gt; &lt;excludes&gt; &lt;exclude&gt;org.apache.storm:storm-core&lt;/exclude&gt; &lt;/excludes&gt; &lt;/dependencySet&gt; &lt;/dependencySets&gt;&lt;/assembly&gt; 在配置文件中不仅可以排除依赖，还可以排除指定的文件，更多的配置规则可以参考官方文档：Descriptor Format 2. 打包命令采用 maven-assembly-plugin 进行打包时命令如下： 1# mvn assembly:assembly 打包后会同时生成两个 JAR 包，其中后缀为 jar-with-dependencies 是含有第三方依赖的 JAR 包，后缀是由 assembly.xml 中 &lt;id&gt; 标签指定的，可以自定义修改。提交该 JAR 到集群环境即可直接使用。 四、maven-shade-plugin插件4.1 官方文档说明第三种方式是使用 maven-shade-plugin，既然已经有了 maven-assembly-plugin，为什么还需要 maven-shade-plugin，这一点在官方文档中也是有所说明的，来自于官方对 HDFS 整合讲解的章节Storm HDFS Integration，原文如下： When packaging your topology, it’s important that you use the maven-shade-plugin as opposed to the maven-assembly-plugin. The shade plugin provides facilities for merging JAR manifest entries, which the hadoop client leverages for URL scheme resolution. If you experience errors such as the following: 12&gt;java.lang.RuntimeException: Error preparing HdfsBolt: No FileSystem for scheme: hdfs&gt; it’s an indication that your topology jar file isn’t packaged properly. If you are using maven to create your topology jar, you should use the following maven-shade-plugin configuration to create your topology jar。 这里第一句就说的比较清晰，在集成 HDFS 时候，你必须使用 maven-shade-plugin 来代替 maven-assembly-plugin，否则会抛出 RuntimeException 异常。 采用 maven-shade-plugin 打包有很多好处，比如你的工程依赖很多的 JAR 包，而被依赖的 JAR 又会依赖其他的 JAR 包，这样,当工程中依赖到不同的版本的 JAR 时，并且 JAR 中具有相同名称的资源文件时，shade 插件会尝试将所有资源文件打包在一起时，而不是和 assembly 一样执行覆盖操作。 4.2 配置采用 maven-shade-plugin 进行打包时候，配置示例如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546&lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;createDependencyReducedPom&gt;true&lt;/createDependencyReducedPom&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.sf&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.dsa&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.rsa&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.EC&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.ec&lt;/exclude&gt; &lt;exclude&gt;META-INF/MSFTSIG.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/MSFTSIG.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;artifactSet&gt; &lt;excludes&gt; &lt;exclude&gt;org.apache.storm:storm-core&lt;/exclude&gt; &lt;/excludes&gt; &lt;/artifactSet&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt;&lt;/plugin&gt; 以上配置示例来源于 Storm Github，这里做一下说明： 在上面的配置中，排除了部分文件，这是因为有些 JAR 包生成时，会使用 jarsigner 生成文件签名（完成性校验），分为两个文件存放在 META-INF 目录下： a signature file, with a .SF extension； a signature block file, with a .DSA, .RSA, or .EC extension； 如果某些包的存在重复引用，这可能会导致在打包时候出现 Invalid signature file digest for Manifest main attributes 异常，所以在配置中排除这些文件。 4.3 打包命令使用 maven-shade-plugin 进行打包的时候，打包命令和普通的一样： 1# mvn package 打包后会生成两个 JAR 包，提交到服务器集群时使用 非 original 开头的 JAR。 五、结论通过以上三种打包方式的详细介绍，这里给出最后的结论：建议使用 maven-shade-plugin 插件进行打包，因为其通用性最强，操作最简单，并且 Storm Github 中所有examples 都是采用该方式进行打包。 六、打包注意事项无论采用任何打包方式，都必须排除集群环境中已经提供的 storm jars。这里比较典型的是 storm-core，其在安装目录的 lib 目录下已经存在。 如果你不排除 storm-core，通常会抛出下面的异常： 123456789Caused by: java.lang.RuntimeException: java.io.IOException: Found multiple defaults.yaml resources. You&apos;re probably bundling the Storm jars with your topology jar. [jar:file:/usr/app/apache-storm-1.2.2/lib/storm-core-1.2.2.jar!/defaults.yaml, jar:file:/usr/appjar/storm-hdfs-integration-1.0.jar!/defaults.yaml] at org.apache.storm.utils.Utils.findAndReadConfigFile(Utils.java:384) at org.apache.storm.utils.Utils.readDefaultConfig(Utils.java:428) at org.apache.storm.utils.Utils.readStormConfig(Utils.java:464) at org.apache.storm.utils.Utils.&lt;clinit&gt;(Utils.java:178) ... 39 more 参考资料关于 maven-shade-plugin 的更多配置可以参考： maven-shade-plugin 入门指南]]></content>
      <categories>
        <category>Storm</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Storm</tag>
        <tag>打包方式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Sqoop基本使用]]></title>
    <url>%2F2019%2F09%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSqoop%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Sqoop基本使用一、Sqoop 基本命令二、Sqoop 与 MySQL三、Sqoop 与 HDFS3.1 MySQL数据导入到HDFS3.2 HDFS数据导出到MySQL四、Sqoop 与 Hive4.1 MySQL数据导入到Hive4.2 Hive 导出数据到MySQL五、Sqoop 与 HBase5.1 MySQL导入数据到HBase六、全库导出七、Sqoop 数据过滤八、类型支持 一、Sqoop 基本命令1. 查看所有命令1# sqoop help 2. 查看某条命令的具体使用方法1# sqoop help 命令名 二、Sqoop 与 MySQL1. 查询MySQL所有数据库通常用于 Sqoop 与 MySQL 连通测试： 1234sqoop list-databases \--connect jdbc:mysql://hadoop001:3306/ \--username root \--password root 2. 查询指定数据库中所有数据表1234sqoop list-tables \--connect jdbc:mysql://hadoop001:3306/mysql \--username root \--password root 三、Sqoop 与 HDFS3.1 MySQL数据导入到HDFS1. 导入命令示例：导出 MySQL 数据库中的 help_keyword 表到 HDFS 的 /sqoop 目录下，如果导入目录存在则先删除再导入，使用 3 个 map tasks 并行导入。 注：help_keyword 是 MySQL 内置的一张字典表，之后的示例均使用这张表。 123456789sqoop import \--connect jdbc:mysql://hadoop001:3306/mysql \ --username root \--password root \--table help_keyword \ # 待导入的表--delete-target-dir \ # 目标目录存在则先删除--target-dir /sqoop \ # 导入的目标目录--fields-terminated-by '\t' \ # 指定导出数据的分隔符-m 3 # 指定并行执行的 map tasks 数量 日志输出如下，可以看到输入数据被平均 split 为三份，分别由三个 map task 进行处理。数据默认以表的主键列作为拆分依据，如果你的表没有主键，有以下两种方案： 添加 -- autoreset-to-one-mapper 参数，代表只启动一个 map task，即不并行执行； 若仍希望并行执行，则可以使用 --split-by &lt;column-name&gt; 指明拆分数据的参考列。 2. 导入验证1234# 查看导入后的目录hadoop fs -ls -R /sqoop# 查看导入内容hadoop fs -text /sqoop/part-m-00000 查看 HDFS 导入目录,可以看到表中数据被分为 3 部分进行存储，这是由指定的并行度决定的。 3.2 HDFS数据导出到MySQL12345678sqoop export \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword_from_hdfs \ # 导出数据存储在 MySQL 的 help_keyword_from_hdf 的表中 --export-dir /sqoop \ --input-fields-terminated-by '\t'\ --m 3 表必须预先创建，建表语句如下： 1CREATE TABLE help_keyword_from_hdfs LIKE help_keyword ; 四、Sqoop 与 Hive4.1 MySQL数据导入到HiveSqoop 导入数据到 Hive 是通过先将数据导入到 HDFS 上的临时目录，然后再将数据从 HDFS 上 Load 到 Hive 中，最后将临时目录删除。可以使用 target-dir 来指定临时目录。 1. 导入命令1234567891011sqoop import \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword \ # 待导入的表 --delete-target-dir \ # 如果临时目录存在删除 --target-dir /sqoop_hive \ # 临时目录位置 --hive-database sqoop_test \ # 导入到 Hive 的 sqoop_test 数据库，数据库需要预先创建。不指定则默认为 default 库 --hive-import \ # 导入到 Hive --hive-overwrite \ # 如果 Hive 表中有数据则覆盖，这会清除表中原有的数据，然后再写入 -m 3 # 并行度 导入到 Hive 中的 sqoop_test 数据库需要预先创建，不指定则默认使用 Hive 中的 default 库。 1234# 查看 hive 中的所有数据库hive&gt; SHOW DATABASES;# 创建 sqoop_test 数据库hive&gt; CREATE DATABASE sqoop_test; 2. 导入验证1234# 查看 sqoop_test 数据库的所有表 hive&gt; SHOW TABLES IN sqoop_test;# 查看表中数据 hive&gt; SELECT * FROM sqoop_test.help_keyword; 3. 可能出现的问题 如果执行报错 java.io.IOException: java.lang.ClassNotFoundException: org.apache.hadoop.hive.conf.HiveConf，则需将 Hive 安装目录下 lib 下的 hive-exec-**.jar 放到 sqoop 的 lib 。 123[root@hadoop001 lib]# ll hive-exec-*-rw-r--r--. 1 1106 4001 19632031 11 月 13 21:45 hive-exec-1.1.0-cdh5.15.2.jar[root@hadoop001 lib]# cp hive-exec-1.1.0-cdh5.15.2.jar $&#123;SQOOP_HOME&#125;/lib 4.2 Hive 导出数据到MySQL由于 Hive 的数据是存储在 HDFS 上的，所以 Hive 导入数据到 MySQL，实际上就是 HDFS 导入数据到 MySQL。 1. 查看Hive表在HDFS的存储位置1234# 进入对应的数据库hive&gt; use sqoop_test;# 查看表信息hive&gt; desc formatted help_keyword; Location 属性为其存储位置： 这里可以查看一下这个目录，文件结构如下： 3.2 执行导出命令12345678sqoop export \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword_from_hive \ --export-dir /user/hive/warehouse/sqoop_test.db/help_keyword \ -input-fields-terminated-by '\001' \ # 需要注意的是 hive 中默认的分隔符为 \001 --m 3 MySQL 中的表需要预先创建： 1CREATE TABLE help_keyword_from_hive LIKE help_keyword ; 五、Sqoop 与 HBase 本小节只讲解从 RDBMS 导入数据到 HBase，因为暂时没有命令能够从 HBase 直接导出数据到 RDBMS。 5.1 MySQL导入数据到HBase1. 导入数据将 help_keyword 表中数据导入到 HBase 上的 help_keyword_hbase 表中，使用原表的主键 help_keyword_id 作为 RowKey，原表的所有列都会在 keywordInfo 列族下，目前只支持全部导入到一个列族下，不支持分别指定列族。 12345678sqoop import \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword \ # 待导入的表 --hbase-table help_keyword_hbase \ # hbase 表名称，表需要预先创建 --column-family keywordInfo \ # 所有列导入到 keywordInfo 列族下 --hbase-row-key help_keyword_id # 使用原表的 help_keyword_id 作为 RowKey 导入的 HBase 表需要预先创建： 123456# 查看所有表hbase&gt; list# 创建表hbase&gt; create 'help_keyword_hbase', 'keywordInfo'# 查看表信息hbase&gt; desc 'help_keyword_hbase' 2. 导入验证使用 scan 查看表数据： 六、全库导出Sqoop 支持通过 import-all-tables 命令进行全库导出到 HDFS/Hive，但需要注意有以下两个限制： 所有表必须有主键；或者使用 --autoreset-to-one-mapper，代表只启动一个 map task; 你不能使用非默认的分割列，也不能通过 WHERE 子句添加任何限制。 第二点解释得比较拗口，这里列出官方原本的说明： You must not intend to use non-default splitting column, nor impose any conditions via a WHERE clause. 全库导出到 HDFS： 1234567sqoop import-all-tables \ --connect jdbc:mysql://hadoop001:3306/数据库名 \ --username root \ --password root \ --warehouse-dir /sqoop_all \ # 每个表会单独导出到一个目录，需要用此参数指明所有目录的父目录 --fields-terminated-by '\t' \ -m 3 全库导出到 Hive： 12345678sqoop import-all-tables -Dorg.apache.sqoop.splitter.allow_text_splitter=true \ --connect jdbc:mysql://hadoop001:3306/数据库名 \ --username root \ --password root \ --hive-database sqoop_test \ # 导出到 Hive 对应的库 --hive-import \ --hive-overwrite \ -m 3 七、Sqoop 数据过滤7.1 query参数Sqoop 支持使用 query 参数定义查询 SQL，从而可以导出任何想要的结果集。使用示例如下： 12345678910111213sqoop import \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --query 'select * from help_keyword where $CONDITIONS and help_keyword_id &lt; 50' \ --delete-target-dir \ --target-dir /sqoop_hive \ --hive-database sqoop_test \ # 指定导入目标数据库 不指定则默认使用 Hive 中的 default 库 --hive-table filter_help_keyword \ # 指定导入目标表 --split-by help_keyword_id \ # 指定用于 split 的列 --hive-import \ # 导入到 Hive --hive-overwrite \ 、 -m 3 在使用 query 进行数据过滤时，需要注意以下三点： 必须用 --hive-table 指明目标表； 如果并行度 -m 不为 1 或者没有指定 --autoreset-to-one-mapper，则需要用 --split-by 指明参考列； SQL 的 where 字句必须包含 $CONDITIONS，这是固定写法，作用是动态替换。 7.2 增量导入123456789101112sqoop import \ --connect jdbc:mysql://hadoop001:3306/mysql \ --username root \ --password root \ --table help_keyword \ --target-dir /sqoop_hive \ --hive-database sqoop_test \ --incremental append \ # 指明模式 --check-column help_keyword_id \ # 指明用于增量导入的参考列 --last-value 300 \ # 指定参考列上次导入的最大值 --hive-import \ -m 3 incremental 参数有以下两个可选的选项： append：要求参考列的值必须是递增的，所有大于 last-value 的值都会被导入； lastmodified：要求参考列的值必须是 timestamp 类型，且插入数据时候要在参考列插入当前时间戳，更新数据时也要更新参考列的时间戳，所有时间晚于 last-value 的数据都会被导入。 通过上面的解释我们可以看出来，其实 Sqoop 的增量导入并没有太多神器的地方，就是依靠维护的参考列来判断哪些是增量数据。当然我们也可以使用上面介绍的 query 参数来进行手动的增量导出，这样反而更加灵活。 八、类型支持Sqoop 默认支持数据库的大多数字段类型，但是某些特殊类型是不支持的。遇到不支持的类型，程序会抛出异常 Hive does not support the SQL type for column xxx 异常，此时可以通过下面两个参数进行强制类型转换： –map-column-java\ ：重写 SQL 到 Java 类型的映射； –map-column-hive \ ： 重写 Hive 到 Java 类型的映射。 示例如下，将原先 id 字段强制转为 String 类型，value 字段强制转为 Integer 类型： 1$ sqoop import ... --map-column-java id=String,value=Integer 参考资料Sqoop User Guide (v1.4.7)]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Sqoop</tag>
        <tag>基本使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Sqoop简介与安装]]></title>
    <url>%2F2019%2F09%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSqoop%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Sqoop 简介与安装一、Sqoop 简介二、安装2.1 下载并解压2.2 配置环境变量2.3 修改配置2.4 拷贝数据库驱动2.5 验证 一、Sqoop 简介Sqoop 是一个常用的数据迁移工具，主要用于在不同存储系统之间实现数据的导入与导出： 导入数据：从 MySQL，Oracle 等关系型数据库中导入数据到 HDFS、Hive、HBase 等分布式文件存储系统中； 导出数据：从 分布式文件系统中导出数据到关系数据库中。 其原理是将执行命令转化成 MapReduce 作业来实现数据的迁移，如下图： 二、安装版本选择：目前 Sqoop 有 Sqoop 1 和 Sqoop 2 两个版本，但是截至到目前，官方并不推荐使用 Sqoop 2，因为其与 Sqoop 1 并不兼容，且功能还没有完善，所以这里优先推荐使用 Sqoop 1。 2.1 下载并解压下载所需版本的 Sqoop ，这里我下载的是 CDH 版本的 Sqoop 。下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 12# 下载后进行解压tar -zxvf sqoop-1.4.6-cdh5.15.2.tar.gz 2.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export SQOOP_HOME=/usr/app/sqoop-1.4.6-cdh5.15.2export PATH=$SQOOP_HOME/bin:$PATH 使得配置的环境变量立即生效： 1# source /etc/profile 2.3 修改配置进入安装目录下的 conf/ 目录，拷贝 Sqoop 的环境配置模板 sqoop-env.sh.template 1# cp sqoop-env-template.sh sqoop-env.sh 修改 sqoop-env.sh，内容如下 (以下配置中 HADOOP_COMMON_HOME 和 HADOOP_MAPRED_HOME 是必选的，其他的是可选的)： 123456789101112131415# Set Hadoop-specific environment variables here.#Set path to where bin/hadoop is availableexport HADOOP_COMMON_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2#Set path to where hadoop-*-core.jar is availableexport HADOOP_MAPRED_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2#set the path to where bin/hbase is availableexport HBASE_HOME=/usr/app/hbase-1.2.0-cdh5.15.2#Set the path to where bin/hive is availableexport HIVE_HOME=/usr/app/hive-1.1.0-cdh5.15.2#Set the path for where zookeper config dir isexport ZOOCFGDIR=/usr/app/zookeeper-3.4.13/conf 2.4 拷贝数据库驱动将 MySQL 驱动包拷贝到 Sqoop 安装目录的 lib 目录下, 驱动包的下载地址为 https://dev.mysql.com/downloads/connector/j/ 。在本仓库的resources 目录下我也上传了一份，有需要的话可以自行下载。 2.5 验证由于已经将 sqoop 的 bin 目录配置到环境变量，直接使用以下命令验证是否配置成功： 1# sqoop version 出现对应的版本信息则代表配置成功： 这里出现的两个 Warning 警告是因为我们本身就没有用到 HCatalog 和 Accumulo，忽略即可。Sqoop 在启动时会去检查环境变量中是否有配置这些软件，如果想去除这些警告，可以修改 bin/configure-sqoop，注释掉不必要的检查。 1234567891011121314151617181920212223242526272829303132# Check: If we can't find our dependencies, give up here.if [ ! -d "$&#123;HADOOP_COMMON_HOME&#125;" ]; then echo "Error: $HADOOP_COMMON_HOME does not exist!" echo 'Please set $HADOOP_COMMON_HOME to the root of your Hadoop installation.' exit 1fiif [ ! -d "$&#123;HADOOP_MAPRED_HOME&#125;" ]; then echo "Error: $HADOOP_MAPRED_HOME does not exist!" echo 'Please set $HADOOP_MAPRED_HOME to the root of your Hadoop MapReduce installation.' exit 1fi## Moved to be a runtime check in sqoop.if [ ! -d "$&#123;HBASE_HOME&#125;" ]; then echo "Warning: $HBASE_HOME does not exist! HBase imports will fail." echo 'Please set $HBASE_HOME to the root of your HBase installation.'fi## Moved to be a runtime check in sqoop.if [ ! -d "$&#123;HCAT_HOME&#125;" ]; then echo "Warning: $HCAT_HOME does not exist! HCatalog jobs will fail." echo 'Please set $HCAT_HOME to the root of your HCatalog installation.'fiif [ ! -d "$&#123;ACCUMULO_HOME&#125;" ]; then echo "Warning: $ACCUMULO_HOME does not exist! Accumulo imports will fail." echo 'Please set $ACCUMULO_HOME to the root of your Accumulo installation.'fiif [ ! -d "$&#123;ZOOKEEPER_HOME&#125;" ]; then echo "Warning: $ZOOKEEPER_HOME does not exist! Accumulo imports will fail." echo 'Please set $ZOOKEEPER_HOME to the root of your Zookeeper installation.'fi]]></content>
      <categories>
        <category>Sqoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Sqoop</tag>
        <tag>简介安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flume整合Kafka]]></title>
    <url>%2F2019%2F09%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlume%E6%95%B4%E5%90%88Kafka%2F</url>
    <content type="text"><![CDATA[Flume 整合 Kafka一、背景二、整合流程1. 启动Zookeeper和Kafka2. 创建主题3. 启动kafka消费者4. 配置Flume5. 启动Flume6. 测试 一、背景先说一下，为什么要使用 Flume + Kafka？ 以实时流处理项目为例，由于采集的数据量可能存在峰值和峰谷，假设是一个电商项目，那么峰值通常出现在秒杀时，这时如果直接将 Flume 聚合后的数据输入到 Storm 等分布式计算框架中，可能就会超过集群的处理能力，这时采用 Kafka 就可以起到削峰的作用。Kafka 天生为大数据场景而设计，具有高吞吐的特性，能很好地抗住峰值数据的冲击。 二、整合流程Flume 发送数据到 Kafka 上主要是通过 KafkaSink 来实现的，主要步骤如下： 1. 启动Zookeeper和Kafka这里启动一个单节点的 Kafka 作为测试： 12345# 启动ZookeeperzkServer.sh start# 启动kafkabin/kafka-server-start.sh config/server.properties 2. 创建主题创建一个主题 flume-kafka，之后 Flume 收集到的数据都会发到这个主题上： 12345678# 创建主题bin/kafka-topics.sh --create \--zookeeper hadoop001:2181 \--replication-factor 1 \--partitions 1 --topic flume-kafka# 查看创建的主题bin/kafka-topics.sh --zookeeper hadoop001:2181 --list 3. 启动kafka消费者启动一个消费者，监听我们刚才创建的 flume-kafka 主题： 1# bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic flume-kafka 4. 配置Flume新建配置文件 exec-memory-kafka.properties，文件内容如下。这里我们监听一个名为 kafka.log 的文件，当文件内容有变化时，将新增加的内容发送到 Kafka 的 flume-kafka 主题上。 123456789101112131415161718192021a1.sources = s1a1.channels = c1a1.sinks = k1 a1.sources.s1.type=execa1.sources.s1.command=tail -F /tmp/kafka.loga1.sources.s1.channels=c1 #设置Kafka接收器a1.sinks.k1.type= org.apache.flume.sink.kafka.KafkaSink#设置Kafka地址a1.sinks.k1.brokerList=hadoop001:9092#设置发送到Kafka上的主题a1.sinks.k1.topic=flume-kafka#设置序列化方式a1.sinks.k1.serializer.class=kafka.serializer.StringEncodera1.sinks.k1.channel=c1 a1.channels.c1.type=memorya1.channels.c1.capacity=10000a1.channels.c1.transactionCapacity=100 5. 启动Flume1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/exec-memory-kafka.properties \--name a1 -Dflume.root.logger=INFO,console 6. 测试向监听的 /tmp/kafka.log 文件中追加内容，查看 Kafka 消费者的输出： 可以看到 flume-kafka 主题的消费端已经收到了对应的消息：]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flume</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Linux下Flume的安装]]></title>
    <url>%2F2019%2F09%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BLinux%E4%B8%8BFlume%E7%9A%84%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Linux下Flume的安装一、前置条件Flume 需要依赖 JDK 1.8+，JDK 安装方式见本仓库： Linux 环境下 JDK 安装 二 、安装步骤2.1 下载并解压下载所需版本的 Flume，这里我下载的是 CDH 版本的 Flume。下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 12# 下载后进行解压tar -zxvf flume-ng-1.6.0-cdh5.15.2.tar.gz 2.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export FLUME_HOME=/usr/app/apache-flume-1.6.0-cdh5.15.2-binexport PATH=$FLUME_HOME/bin:$PATH 使得配置的环境变量立即生效： 1# source /etc/profile 2.3 修改配置进入安装目录下的 conf/ 目录，拷贝 Flume 的环境配置模板 flume-env.sh.template： 1# cp flume-env.sh.template flume-env.sh 修改 flume-env.sh,指定 JDK 的安装路径： 12# Enviroment variables can be set here.export JAVA_HOME=/usr/java/jdk1.8.0_201 2.4 验证由于已经将 Flume 的 bin 目录配置到环境变量，直接使用以下命令验证是否配置成功： 1# flume-ng version 出现对应的版本信息则代表配置成功。]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flume</tag>
        <tag>安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flume简介及基本使用]]></title>
    <url>%2F2019%2F09%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlume%E7%AE%80%E4%BB%8B%E5%8F%8A%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Flume 简介及基本使用一、Flume简介二、Flume架构和基本概念2.1 基本架构2.2 基本概念2.3 组件种类三、Flume架构模式四、Flume配置格式五、Flume安装部署六、Flume使用案例 一、Flume简介Apache Flume 是一个分布式，高可用的数据收集系统。它可以从不同的数据源收集数据，经过聚合后发送到存储系统中，通常用于日志数据的收集。Flume 分为 NG 和 OG (1.0 之前) 两个版本，NG 在 OG 的基础上进行了完全的重构，是目前使用最为广泛的版本。下面的介绍均以 NG 为基础。 二、Flume架构和基本概念下图为 Flume 的基本架构图： 2.1 基本架构外部数据源以特定格式向 Flume 发送 events (事件)，当 source 接收到 events 时，它将其存储到一个或多个 channel，channe 会一直保存 events 直到它被 sink 所消费。sink 的主要功能从 channel 中读取 events，并将其存入外部存储系统或转发到下一个 source，成功后再从 channel 中移除 events。 2.2 基本概念1. Event Event 是 Flume NG 数据传输的基本单元。类似于 JMS 和消息系统中的消息。一个 Event 由标题和正文组成：前者是键/值映射，后者是任意字节数组。 2. Source 数据收集组件，从外部数据源收集数据，并存储到 Channel 中。 3. Channel Channel 是源和接收器之间的管道，用于临时存储数据。可以是内存或持久化的文件系统： Memory Channel : 使用内存，优点是速度快，但数据可能会丢失 (如突然宕机)； File Channel : 使用持久化的文件系统，优点是能保证数据不丢失，但是速度慢。 4. Sink Sink 的主要功能从 Channel 中读取 Event，并将其存入外部存储系统或将其转发到下一个 Source，成功后再从 Channel 中移除 Event。 5. Agent 是一个独立的 (JVM) 进程，包含 Source、 Channel、 Sink 等组件。 2.3 组件种类Flume 中的每一个组件都提供了丰富的类型，适用于不同场景： Source 类型 ：内置了几十种类型，如 Avro Source，Thrift Source，Kafka Source，JMS Source； Sink 类型 ：HDFS Sink，Hive Sink，HBaseSinks，Avro Sink 等； Channel 类型 ：Memory Channel，JDBC Channel，Kafka Channel，File Channel 等。 对于 Flume 的使用，除非有特别的需求，否则通过组合内置的各种类型的 Source，Sink 和 Channel 就能满足大多数的需求。在 Flume 官网 上对所有类型组件的配置参数均以表格的方式做了详尽的介绍，并附有配置样例；同时不同版本的参数可能略有所不同，所以使用时建议选取官网对应版本的 User Guide 作为主要参考资料。 三、Flume架构模式Flume 支持多种架构模式，分别介绍如下 3.1 multi-agent flow Flume 支持跨越多个 Agent 的数据传递，这要求前一个 Agent 的 Sink 和下一个 Agent 的 Source 都必须是 Avro 类型，Sink 指向 Source 所在主机名 (或 IP 地址) 和端口（详细配置见下文案例三）。 3.2 Consolidation 日志收集中常常存在大量的客户端（比如分布式 web 服务），Flume 支持使用多个 Agent 分别收集日志，然后通过一个或者多个 Agent 聚合后再存储到文件系统中。 3.3 Multiplexing the flow Flume 支持从一个 Source 向多个 Channel，也就是向多个 Sink 传递事件，这个操作称之为 Fan Out(扇出)。默认情况下 Fan Out 是向所有的 Channel 复制 Event，即所有 Channel 收到的数据都是相同的。同时 Flume 也支持在 Source 上自定义一个复用选择器 (multiplexing selector) 来实现自定义的路由规则。 四、Flume配置格式Flume 配置通常需要以下两个步骤： 分别定义好 Agent 的 Sources，Sinks，Channels，然后将 Sources 和 Sinks 与通道进行绑定。需要注意的是一个 Source 可以配置多个 Channel，但一个 Sink 只能配置一个 Channel。基本格式如下： 123456789&lt;Agent&gt;.sources = &lt;Source&gt;&lt;Agent&gt;.sinks = &lt;Sink&gt;&lt;Agent&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt;# set channel for source&lt;Agent&gt;.sources.&lt;Source&gt;.channels = &lt;Channel1&gt; &lt;Channel2&gt; ...# set channel for sink&lt;Agent&gt;.sinks.&lt;Sink&gt;.channel = &lt;Channel1&gt; 分别定义 Source，Sink，Channel 的具体属性。基本格式如下： 12345678&lt;Agent&gt;.sources.&lt;Source&gt;.&lt;someProperty&gt; = &lt;someValue&gt;# properties for channels&lt;Agent&gt;.channel.&lt;Channel&gt;.&lt;someProperty&gt; = &lt;someValue&gt;# properties for sinks&lt;Agent&gt;.sources.&lt;Sink&gt;.&lt;someProperty&gt; = &lt;someValue&gt; 五、Flume的安装部署为方便大家后期查阅，本仓库中所有软件的安装均单独成篇，Flume 的安装见： Linux 环境下 Flume 的安装部署 六、Flume使用案例介绍几个 Flume 的使用案例： 案例一：使用 Flume 监听文件内容变动，将新增加的内容输出到控制台。 案例二：使用 Flume 监听指定目录，将目录下新增加的文件存储到 HDFS。 案例三：使用 Avro 将本服务器收集到的日志数据发送到另外一台服务器。 6.1 案例一需求： 监听文件内容变动，将新增加的内容输出到控制台。 实现： 主要使用 Exec Source 配合 tail 命令实现。 1. 配置新建配置文件 exec-memory-logger.properties,其内容如下： 123456789101112131415161718192021#指定agent的sources,sinks,channelsa1.sources = s1 a1.sinks = k1 a1.channels = c1 #配置sources属性a1.sources.s1.type = execa1.sources.s1.command = tail -F /tmp/log.txta1.sources.s1.shell = /bin/bash -c#将sources与channels进行绑定a1.sources.s1.channels = c1 #配置sink a1.sinks.k1.type = logger#将sinks与channels进行绑定 a1.sinks.k1.channel = c1 #配置channel类型a1.channels.c1.type = memory 2. 启动 12345flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/exec-memory-logger.properties \--name a1 \-Dflume.root.logger=INFO,console 3. 测试向文件中追加数据： 控制台的显示： 6.2 案例二需求： 监听指定目录，将目录下新增加的文件存储到 HDFS。 实现：使用 Spooling Directory Source 和 HDFS Sink。 1. 配置1234567891011121314151617181920212223242526#指定agent的sources,sinks,channelsa1.sources = s1 a1.sinks = k1 a1.channels = c1 #配置sources属性a1.sources.s1.type =spooldir a1.sources.s1.spoolDir =/tmp/logsa1.sources.s1.basenameHeader = truea1.sources.s1.basenameHeaderKey = fileName #将sources与channels进行绑定 a1.sources.s1.channels =c1 #配置sink a1.sinks.k1.type = hdfsa1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H/a1.sinks.k1.hdfs.filePrefix = %&#123;fileName&#125;#生成的文件类型，默认是Sequencefile，可用DataStream，则为普通文本a1.sinks.k1.hdfs.fileType = DataStream a1.sinks.k1.hdfs.useLocalTimeStamp = true#将sinks与channels进行绑定 a1.sinks.k1.channel = c1 #配置channel类型a1.channels.c1.type = memory 2. 启动1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/spooling-memory-hdfs.properties \--name a1 -Dflume.root.logger=INFO,console 3. 测试拷贝任意文件到监听目录下，可以从日志看到文件上传到 HDFS 的路径： 1# cp log.txt logs/ 查看上传到 HDFS 上的文件内容与本地是否一致： 1# hdfs dfs -cat /flume/events/19-04-09/13/log.txt.1554788567801 6.3 案例三需求： 将本服务器收集到的数据发送到另外一台服务器。 实现：使用 avro sources 和 avro Sink 实现。 1. 配置日志收集Flume新建配置 netcat-memory-avro.properties，监听文件内容变化，然后将新的文件内容通过 avro sink 发送到 hadoop001 这台服务器的 8888 端口： 12345678910111213141516171819202122#指定agent的sources,sinks,channelsa1.sources = s1a1.sinks = k1a1.channels = c1#配置sources属性a1.sources.s1.type = execa1.sources.s1.command = tail -F /tmp/log.txta1.sources.s1.shell = /bin/bash -ca1.sources.s1.channels = c1#配置sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop001a1.sinks.k1.port = 8888a1.sinks.k1.batch-size = 1a1.sinks.k1.channel = c1#配置channel类型a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 2. 配置日志聚合Flume使用 avro source 监听 hadoop001 服务器的 8888 端口，将获取到内容输出到控制台： 1234567891011121314151617181920212223#指定agent的sources,sinks,channelsa2.sources = s2a2.sinks = k2a2.channels = c2#配置sources属性a2.sources.s2.type = avroa2.sources.s2.bind = hadoop001a2.sources.s2.port = 8888#将sources与channels进行绑定a2.sources.s2.channels = c2#配置sinka2.sinks.k2.type = logger#将sinks与channels进行绑定a2.sinks.k2.channel = c2#配置channel类型a2.channels.c2.type = memorya2.channels.c2.capacity = 1000a2.channels.c2.transactionCapacity = 100 3. 启动启动日志聚集 Flume： 1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/avro-memory-logger.properties \--name a2 -Dflume.root.logger=INFO,console 在启动日志收集 Flume: 1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/netcat-memory-avro.properties \--name a1 -Dflume.root.logger=INFO,console 这里建议按以上顺序启动，原因是 avro.source 会先与端口进行绑定，这样 avro sink 连接时才不会报无法连接的异常。但是即使不按顺序启动也是没关系的，sink 会一直重试，直至建立好连接。 4.测试向文件 tmp/log.txt 中追加内容： 可以看到已经从 8888 端口监听到内容，并成功输出到控制台：]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flume</tag>
        <tag>简介和使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Zookeeper_ACL权限控制]]></title>
    <url>%2F2019%2F08%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BZookeeper_ACL%E6%9D%83%E9%99%90%E6%8E%A7%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Zookeeper ACL一、前言二、使用Shell进行权限管理2.1 设置与查看权限2.2 权限组成2.3 添加认证信息2.4 权限设置示例1. world模式2. auth模式3. digest模式4. ip模式5. super模式三、使用Java客户端进行权限管理3.1 主要依赖3.2 权限管理API 一、前言为了避免存储在 Zookeeper 上的数据被其他程序或者人为误修改，Zookeeper 提供了 ACL(Access Control Lists) 进行权限控制。只有拥有对应权限的用户才可以对节点进行增删改查等操作。下文分别介绍使用原生的 Shell 命令和 Apache Curator 客户端进行权限设置。 二、使用Shell进行权限管理2.1 设置与查看权限想要给某个节点设置权限 (ACL)，有以下两个可选的命令： 12345# 1.给已有节点赋予权限setAcl path acl# 2.在创建节点时候指定权限create [-s] [-e] path data acl 查看指定节点的权限命令如下： 1getAcl path 2.2 权限组成Zookeeper 的权限由[scheme : id :permissions]三部分组成，其中 Schemes 和 Permissions 内置的可选项分别如下： Permissions 可选项： CREATE：允许创建子节点； READ：允许从节点获取数据并列出其子节点； WRITE：允许为节点设置数据； DELETE：允许删除子节点； ADMIN：允许为节点设置权限。 Schemes 可选项： world：默认模式，所有客户端都拥有指定的权限。world 下只有一个 id 选项，就是 anyone，通常组合写法为 world:anyone:[permissons]； auth：只有经过认证的用户才拥有指定的权限。通常组合写法为 auth:user:password:[permissons]，使用这种模式时，你需要先进行登录，之后采用 auth 模式设置权限时，user 和 password 都将使用登录的用户名和密码； digest：只有经过认证的用户才拥有指定的权限。通常组合写法为 auth:user:BASE64(SHA1(password)):[permissons]，这种形式下的密码必须通过 SHA1 和 BASE64 进行双重加密； ip：限制只有特定 IP 的客户端才拥有指定的权限。通常组成写法为 ip:182.168.0.168:[permissions]； super：代表超级管理员，拥有所有的权限，需要修改 Zookeeper 启动脚本进行配置。 2.3 添加认证信息可以使用如下所示的命令为当前 Session 添加用户认证信息，等价于登录操作。 12345# 格式addauth scheme auth #示例：添加用户名为heibai,密码为root的用户认证信息addauth digest heibai:root 2.4 权限设置示例1. world模式world 是一种默认的模式，即创建时如果不指定权限，则默认的权限就是 world。 123456789[zk: localhost:2181(CONNECTED) 32] create /hadoop 123Created /hadoop[zk: localhost:2181(CONNECTED) 33] getAcl /hadoop'world,'anyone #默认的权限: cdrwa[zk: localhost:2181(CONNECTED) 34] setAcl /hadoop world:anyone:cwda # 修改节点，不允许所有客户端读....[zk: localhost:2181(CONNECTED) 35] get /hadoopAuthentication is not valid : /hadoop # 权限不足 2. auth模式1234567891011[zk: localhost:2181(CONNECTED) 36] addauth digest heibai:heibai # 登录[zk: localhost:2181(CONNECTED) 37] setAcl /hadoop auth::cdrwa # 设置权限[zk: localhost:2181(CONNECTED) 38] getAcl /hadoop # 获取权限'digest,'heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s= #用户名和密码 (密码经过加密处理)，注意返回的权限类型是 digest: cdrwa#用户名和密码都是使用登录的用户名和密码，即使你在创建权限时候进行指定也是无效的[zk: localhost:2181(CONNECTED) 39] setAcl /hadoop auth:root:root:cdrwa #指定用户名和密码为 root[zk: localhost:2181(CONNECTED) 40] getAcl /hadoop'digest,'heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s= #无效，使用的用户名和密码依然还是 heibai: cdrwa 3. digest模式1234[zk:44] create /spark "spark" digest:heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s=:cdrwa #指定用户名和加密后的密码[zk:45] getAcl /spark #获取权限'digest,'heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s= # 返回的权限类型是 digest: cdrwa 到这里你可以发现使用 auth 模式设置的权限和使用 digest 模式设置的权限，在最终结果上，得到的权限模式都是 digest。某种程度上，你可以把 auth 模式理解成是 digest 模式的一种简便实现。因为在 digest 模式下，每次设置都需要书写用户名和加密后的密码，这是比较繁琐的，采用 auth 模式就可以避免这种麻烦。 4. ip模式限定只有特定的 ip 才能访问。 123[zk: localhost:2181(CONNECTED) 46] create /hive "hive" ip:192.168.0.108:cdrwa [zk: localhost:2181(CONNECTED) 47] get /hiveAuthentication is not valid : /hive # 当前主机已经不能访问 这里可以看到当前主机已经不能访问，想要能够再次访问，可以使用对应 IP 的客户端，或使用下面介绍的 super 模式。 5. super模式需要修改启动脚本 zkServer.sh，并在指定位置添加超级管理员账户和密码信息： 1"-Dzookeeper.DigestAuthenticationProvider.superDigest=heibai:sCxtVJ1gPG8UW/jzFHR0A1ZKY5s=" 修改完成后需要使用 zkServer.sh restart 重启服务，此时再次访问限制 IP 的节点： 12345678910111213141516[zk: localhost:2181(CONNECTED) 0] get /hive #访问受限Authentication is not valid : /hive[zk: localhost:2181(CONNECTED) 1] addauth digest heibai:heibai # 登录 (添加认证信息)[zk: localhost:2181(CONNECTED) 2] get /hive #成功访问hivecZxid = 0x158ctime = Sat May 25 09:11:29 CST 2019mZxid = 0x158mtime = Sat May 25 09:11:29 CST 2019pZxid = 0x158cversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 4numChildren = 0 三、使用Java客户端进行权限管理3.1 主要依赖这里以 Apache Curator 为例，使用前需要导入相关依赖，完整依赖如下： 123456789101112131415161718192021222324&lt;dependencies&gt; &lt;!--Apache Curator 相关依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;!--单元测试相关依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 3.2 权限管理API Apache Curator 权限设置的示例如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172public class AclOperation &#123; private CuratorFramework client = null; private static final String zkServerPath = "192.168.0.226:2181"; private static final String nodePath = "/hadoop/hdfs"; @Before public void prepare() &#123; RetryPolicy retryPolicy = new RetryNTimes(3, 5000); client = CuratorFrameworkFactory.builder() .authorization("digest", "heibai:123456".getBytes()) //等价于 addauth 命令 .connectString(zkServerPath) .sessionTimeoutMs(10000).retryPolicy(retryPolicy) .namespace("workspace").build(); client.start(); &#125; /** * 新建节点并赋予权限 */ @Test public void createNodesWithAcl() throws Exception &#123; List&lt;ACL&gt; aclList = new ArrayList&lt;&gt;(); // 对密码进行加密 String digest1 = DigestAuthenticationProvider.generateDigest("heibai:123456"); String digest2 = DigestAuthenticationProvider.generateDigest("ying:123456"); Id user01 = new Id("digest", digest1); Id user02 = new Id("digest", digest2); // 指定所有权限 aclList.add(new ACL(Perms.ALL, user01)); // 如果想要指定权限的组合，中间需要使用 | ,这里的|代表的是位运算中的 按位或 aclList.add(new ACL(Perms.DELETE | Perms.CREATE, user02)); // 创建节点 byte[] data = "abc".getBytes(); client.create().creatingParentsIfNeeded() .withMode(CreateMode.PERSISTENT) .withACL(aclList, true) .forPath(nodePath, data); &#125; /** * 给已有节点设置权限,注意这会删除所有原来节点上已有的权限设置 */ @Test public void SetAcl() throws Exception &#123; String digest = DigestAuthenticationProvider.generateDigest("admin:admin"); Id user = new Id("digest", digest); client.setACL() .withACL(Collections.singletonList(new ACL(Perms.READ | Perms.DELETE, user))) .forPath(nodePath); &#125; /** * 获取权限 */ @Test public void getAcl() throws Exception &#123; List&lt;ACL&gt; aclList = client.getACL().forPath(nodePath); ACL acl = aclList.get(0); System.out.println(acl.getId().getId() + "是否有删读权限:" + (acl.getPerms() == (Perms.READ | Perms.DELETE))); &#125; @After public void destroy() &#123; if (client != null) &#123; client.close(); &#125; &#125;&#125; 完整源码见本仓库： https://github.com/heibaiying/BigData-Notes/tree/master/code/Zookeeper/curator]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
        <tag>权限控制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Zookeeper_Java客户端Curator]]></title>
    <url>%2F2019%2F08%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BZookeeper_Java%E5%AE%A2%E6%88%B7%E7%AB%AFCurator%2F</url>
    <content type="text"><![CDATA[Zookeeper Java 客户端 ——Apache Curator一、基本依赖二、客户端相关操作2.1 创建客户端实例2.2 重试策略2.3 判断服务状态三、节点增删改查3.1 创建节点2.2 获取节点信息2.3 获取子节点列表2.4 更新节点2.5 删除节点2.6 判断节点是否存在三、监听事件3.1 创建一次性监听3.2 创建永久监听3.3 监听子节点 一、基本依赖Curator 是 Netflix 公司开源的一个 Zookeeper 客户端，目前由 Apache 进行维护。与 Zookeeper 原生客户端相比，Curator 的抽象层次更高，功能也更加丰富，是目前 Zookeeper 使用范围最广的 Java 客户端。本篇文章主要讲解其基本使用，项目采用 Maven 构建，以单元测试的方法进行讲解，相关依赖如下： 123456789101112131415161718192021222324&lt;dependencies&gt; &lt;!--Curator 相关依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-framework&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.curator&lt;/groupId&gt; &lt;artifactId&gt;curator-recipes&lt;/artifactId&gt; &lt;version&gt;4.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.zookeeper&lt;/groupId&gt; &lt;artifactId&gt;zookeeper&lt;/artifactId&gt; &lt;version&gt;3.4.13&lt;/version&gt; &lt;/dependency&gt; &lt;!--单元测试相关依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 完整源码见本仓库： https://github.com/heibaiying/BigData-Notes/tree/master/code/Zookeeper/curator 二、客户端相关操作2.1 创建客户端实例这里使用 @Before 在单元测试执行前创建客户端实例，并使用 @After 在单元测试后关闭客户端连接。 123456789101112131415161718192021222324public class BasicOperation &#123; private CuratorFramework client = null; private static final String zkServerPath = "192.168.0.226:2181"; private static final String nodePath = "/hadoop/yarn"; @Before public void prepare() &#123; // 重试策略 RetryPolicy retryPolicy = new RetryNTimes(3, 5000); client = CuratorFrameworkFactory.builder() .connectString(zkServerPath) .sessionTimeoutMs(10000).retryPolicy(retryPolicy) .namespace("workspace").build(); //指定命名空间后，client 的所有路径操作都会以/workspace 开头 client.start(); &#125; @After public void destroy() &#123; if (client != null) &#123; client.close(); &#125; &#125;&#125; 2.2 重试策略在连接 Zookeeper 时，Curator 提供了多种重试策略以满足各种需求，所有重试策略均继承自 RetryPolicy 接口，如下图： 这些重试策略类主要分为以下两类： RetryForever ：代表一直重试，直到连接成功； SleepingRetry ： 基于一定间隔时间的重试。这里以其子类 ExponentialBackoffRetry 为例说明，其构造器如下： 123456/** * @param baseSleepTimeMs 重试之间等待的初始时间 * @param maxRetries 最大重试次数 * @param maxSleepMs 每次重试间隔的最长睡眠时间（毫秒） */ExponentialBackoffRetry(int baseSleepTimeMs, int maxRetries, int maxSleepMs) 2.3 判断服务状态12345@Testpublic void getStatus() &#123; CuratorFrameworkState state = client.getState(); System.out.println("服务是否已经启动:" + (state == CuratorFrameworkState.STARTED));&#125; 三、节点增删改查3.1 创建节点12345678@Testpublic void createNodes() throws Exception &#123; byte[] data = "abc".getBytes(); client.create().creatingParentsIfNeeded() .withMode(CreateMode.PERSISTENT) //节点类型 .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE) .forPath(nodePath, data);&#125; 创建时可以指定节点类型，这里的节点类型和 Zookeeper 原生的一致，全部类型定义在枚举类 CreateMode 中： 1234567891011public enum CreateMode &#123; // 永久节点 PERSISTENT (0, false, false), //永久有序节点 PERSISTENT_SEQUENTIAL (2, false, true), // 临时节点 EPHEMERAL (1, true, false), // 临时有序节点 EPHEMERAL_SEQUENTIAL (3, true, true); ....&#125; 2.2 获取节点信息1234567@Testpublic void getNode() throws Exception &#123; Stat stat = new Stat(); byte[] data = client.getData().storingStatIn(stat).forPath(nodePath); System.out.println("节点数据:" + new String(data)); System.out.println("节点信息:" + stat.toString());&#125; 如上所示，节点信息被封装在 Stat 类中，其主要属性如下： 1234567891011121314public class Stat implements Record &#123; private long czxid; private long mzxid; private long ctime; private long mtime; private int version; private int cversion; private int aversion; private long ephemeralOwner; private int dataLength; private int numChildren; private long pzxid; ...&#125; 每个属性的含义如下： 状态属性 说明 czxid 数据节点创建时的事务 ID ctime 数据节点创建时的时间 mzxid 数据节点最后一次更新时的事务 ID mtime 数据节点最后一次更新时的时间 pzxid 数据节点的子节点最后一次被修改时的事务 ID cversion 子节点的更改次数 version 节点数据的更改次数 aversion 节点的 ACL 的更改次数 ephemeralOwner 如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0 dataLength 数据内容的长度 numChildren 数据节点当前的子节点个数 2.3 获取子节点列表1234567@Testpublic void getChildrenNodes() throws Exception &#123; List&lt;String&gt; childNodes = client.getChildren().forPath("/hadoop"); for (String s : childNodes) &#123; System.out.println(s); &#125;&#125; 2.4 更新节点更新时可以传入版本号也可以不传入，如果传入则类似于乐观锁机制，只有在版本号正确的时候才会被更新。 123456@Testpublic void updateNode() throws Exception &#123; byte[] newData = "defg".getBytes(); client.setData().withVersion(0) // 传入版本号，如果版本号错误则拒绝更新操作,并抛出 BadVersion 异常 .forPath(nodePath, newData);&#125; 2.5 删除节点12345678@Testpublic void deleteNodes() throws Exception &#123; client.delete() .guaranteed() // 如果删除失败，那么在会继续执行，直到成功 .deletingChildrenIfNeeded() // 如果有子节点，则递归删除 .withVersion(0) // 传入版本号，如果版本号错误则拒绝删除操作,并抛出 BadVersion 异常 .forPath(nodePath);&#125; 2.6 判断节点是否存在123456@Testpublic void existNode() throws Exception &#123; // 如果节点存在则返回其状态信息如果不存在则为 null Stat stat = client.checkExists().forPath(nodePath + "aa/bb/cc"); System.out.println("节点是否存在:" + !(stat == null));&#125; 三、监听事件3.1 创建一次性监听和 Zookeeper 原生监听一样，使用 usingWatcher 注册的监听是一次性的，即监听只会触发一次，触发后就销毁。示例如下： 123456789@Testpublic void DisposableWatch() throws Exception &#123; client.getData().usingWatcher(new CuratorWatcher() &#123; public void process(WatchedEvent event) &#123; System.out.println("节点" + event.getPath() + "发生了事件:" + event.getType()); &#125; &#125;).forPath(nodePath); Thread.sleep(1000 * 1000); //休眠以观察测试效果&#125; 3.2 创建永久监听Curator 还提供了创建永久监听的 API，其使用方式如下： 1234567891011121314151617@Testpublic void permanentWatch() throws Exception &#123; // 使用 NodeCache 包装节点，对其注册的监听作用于节点，且是永久性的 NodeCache nodeCache = new NodeCache(client, nodePath); // 通常设置为 true, 代表创建 nodeCache 时,就去获取对应节点的值并缓存 nodeCache.start(true); nodeCache.getListenable().addListener(new NodeCacheListener() &#123; public void nodeChanged() &#123; ChildData currentData = nodeCache.getCurrentData(); if (currentData != null) &#123; System.out.println("节点路径：" + currentData.getPath() + "数据：" + new String(currentData.getData())); &#125; &#125; &#125;); Thread.sleep(1000 * 1000); //休眠以观察测试效果&#125; 3.3 监听子节点这里以监听 /hadoop 下所有子节点为例，实现方式如下： 123456789101112131415161718192021222324252627282930313233343536373839@Testpublic void permanentChildrenNodesWatch() throws Exception &#123; // 第三个参数代表除了节点状态外，是否还缓存节点内容 PathChildrenCache childrenCache = new PathChildrenCache(client, "/hadoop", true); /* * StartMode 代表初始化方式: * NORMAL: 异步初始化 * BUILD_INITIAL_CACHE: 同步初始化 * POST_INITIALIZED_EVENT: 异步并通知,初始化之后会触发 INITIALIZED 事件 */ childrenCache.start(StartMode.POST_INITIALIZED_EVENT); List&lt;ChildData&gt; childDataList = childrenCache.getCurrentData(); System.out.println("当前数据节点的子节点列表："); childDataList.forEach(x -&gt; System.out.println(x.getPath())); childrenCache.getListenable().addListener(new PathChildrenCacheListener() &#123; public void childEvent(CuratorFramework client, PathChildrenCacheEvent event) &#123; switch (event.getType()) &#123; case INITIALIZED: System.out.println("childrenCache 初始化完成"); break; case CHILD_ADDED: // 需要注意的是: 即使是之前已经存在的子节点，也会触发该监听，因为会把该子节点加入 childrenCache 缓存中 System.out.println("增加子节点:" + event.getData().getPath()); break; case CHILD_REMOVED: System.out.println("删除子节点:" + event.getData().getPath()); break; case CHILD_UPDATED: System.out.println("被修改的子节点的路径:" + event.getData().getPath()); System.out.println("修改后的数据:" + new String(event.getData().getData())); break; &#125; &#125; &#125;); Thread.sleep(1000 * 1000); //休眠以观察测试效果&#125;]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
        <tag>Java客户端</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Zookeeper常用Shell命令]]></title>
    <url>%2F2019%2F08%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BZookeeper%E5%B8%B8%E7%94%A8Shell%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Zookeeper常用Shell命令一、节点增删改查1.1 启动服务和连接服务1.2 help命令1.3 查看节点列表1.4 新增节点1.5 查看节点1.6 更新节点1.7 删除节点二、监听器2.1 get path [watch]2.2 stat path [watch]2.3 ls\ls2 path [watch]三、 zookeeper 四字命令 一、节点增删改查1.1 启动服务和连接服务12345# 启动服务bin/zkServer.sh start#连接服务 不指定服务地址则默认连接到localhost:2181zkCli.sh -server hadoop001:2181 1.2 help命令使用 help 可以查看所有命令及格式。 1.3 查看节点列表查看节点列表有 ls path 和 ls2 path 两个命令，后者是前者的增强，不仅可以查看指定路径下的所有节点，还可以查看当前节点的信息。 123456789101112131415[zk: localhost:2181(CONNECTED) 0] ls /[cluster, controller_epoch, brokers, storm, zookeeper, admin, ...][zk: localhost:2181(CONNECTED) 1] ls2 /[cluster, controller_epoch, brokers, storm, zookeeper, admin, ....]cZxid = 0x0ctime = Thu Jan 01 08:00:00 CST 1970mZxid = 0x0mtime = Thu Jan 01 08:00:00 CST 1970pZxid = 0x130cversion = 19dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 0numChildren = 11 1.4 新增节点1create [-s] [-e] path data acl #其中-s 为有序节点，-e 临时节点 创建节点并写入数据： 1create /hadoop 123456 创建有序节点，此时创建的节点名为指定节点名 + 自增序号： 123456[zk: localhost:2181(CONNECTED) 23] create -s /a "aaa"Created /a0000000022[zk: localhost:2181(CONNECTED) 24] create -s /b "bbb"Created /b0000000023[zk: localhost:2181(CONNECTED) 25] create -s /c "ccc"Created /c0000000024 创建临时节点，临时节点会在会话过期后被删除： 12[zk: localhost:2181(CONNECTED) 26] create -e /tmp "tmp"Created /tmp 1.5 查看节点1. 获取节点数据12# 格式get path [watch] 12345678910111213[zk: localhost:2181(CONNECTED) 31] get /hadoop123456 #节点数据cZxid = 0x14bctime = Fri May 24 17:03:06 CST 2019mZxid = 0x14bmtime = Fri May 24 17:03:06 CST 2019pZxid = 0x14bcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 6numChildren = 0 节点各个属性如下表。其中一个重要的概念是 Zxid(ZooKeeper Transaction Id)，ZooKeeper 节点的每一次更改都具有唯一的 Zxid，如果 Zxid1 小于 Zxid2，则 Zxid1 的更改发生在 Zxid2 更改之前。 状态属性 说明 cZxid 数据节点创建时的事务 ID ctime 数据节点创建时的时间 mZxid 数据节点最后一次更新时的事务 ID mtime 数据节点最后一次更新时的时间 pZxid 数据节点的子节点最后一次被修改时的事务 ID cversion 子节点的更改次数 dataVersion 节点数据的更改次数 aclVersion 节点的 ACL 的更改次数 ephemeralOwner 如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0 dataLength 数据内容的长度 numChildren 数据节点当前的子节点个数 2. 查看节点状态可以使用 stat 命令查看节点状态，它的返回值和 get 命令类似，但不会返回节点数据。 123456789101112[zk: localhost:2181(CONNECTED) 32] stat /hadoopcZxid = 0x14bctime = Fri May 24 17:03:06 CST 2019mZxid = 0x14bmtime = Fri May 24 17:03:06 CST 2019pZxid = 0x14bcversion = 0dataVersion = 0aclVersion = 0ephemeralOwner = 0x0dataLength = 6numChildren = 0 1.6 更新节点更新节点的命令是 set，可以直接进行修改，如下： 123456789101112[zk: localhost:2181(CONNECTED) 33] set /hadoop 345cZxid = 0x14bctime = Fri May 24 17:03:06 CST 2019mZxid = 0x14cmtime = Fri May 24 17:13:05 CST 2019pZxid = 0x14bcversion = 0dataVersion = 1 # 注意更改后此时版本号为 1，默认创建时为 0aclVersion = 0ephemeralOwner = 0x0dataLength = 3numChildren = 0 也可以基于版本号进行更改，此时类似于乐观锁机制，当你传入的数据版本号 (dataVersion) 和当前节点的数据版本号不符合时，zookeeper 会拒绝本次修改： 12[zk: localhost:2181(CONNECTED) 34] set /hadoop 678 0version No is not valid : /hadoop #无效的版本号 1.7 删除节点删除节点的语法如下： 1delete path [version] 和更新节点数据一样，也可以传入版本号，当你传入的数据版本号 (dataVersion) 和当前节点的数据版本号不符合时，zookeeper 不会执行删除操作。 1234[zk: localhost:2181(CONNECTED) 36] delete /hadoop 0version No is not valid : /hadoop #无效的版本号[zk: localhost:2181(CONNECTED) 37] delete /hadoop 1[zk: localhost:2181(CONNECTED) 38] 要想删除某个节点及其所有后代节点，可以使用递归删除，命令为 rmr path。 二、监听器2.1 get path [watch]使用 get path [watch] 注册的监听器能够在节点内容发生改变的时候，向客户端发出通知。需要注意的是 zookeeper 的触发器是一次性的 (One-time trigger)，即触发一次后就会立即失效。 1234[zk: localhost:2181(CONNECTED) 4] get /hadoop watch[zk: localhost:2181(CONNECTED) 5] set /hadoop 45678WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/hadoop #节点值改变 2.2 stat path [watch]使用 stat path [watch] 注册的监听器能够在节点状态发生改变的时候，向客户端发出通知。 1234[zk: localhost:2181(CONNECTED) 7] stat /hadoop watch[zk: localhost:2181(CONNECTED) 8] set /hadoop 112233WATCHER::WatchedEvent state:SyncConnected type:NodeDataChanged path:/hadoop #节点值改变 2.3 ls\ls2 path [watch]使用 ls path [watch] 或 ls2 path [watch] 注册的监听器能够监听该节点下所有子节点的增加和删除操作。 12345[zk: localhost:2181(CONNECTED) 9] ls /hadoop watch[][zk: localhost:2181(CONNECTED) 10] create /hadoop/yarn "aaa"WATCHER::WatchedEvent state:SyncConnected type:NodeChildrenChanged path:/hadoop 三、 zookeeper 四字命令 命令 功能描述 conf 打印服务配置的详细信息。 cons 列出连接到此服务器的所有客户端的完整连接/会话详细信息。包括接收/发送的数据包数量，会话 ID，操作延迟，上次执行的操作等信息。 dump 列出未完成的会话和临时节点。这只适用于 Leader 节点。 envi 打印服务环境的详细信息。 ruok 测试服务是否处于正确状态。如果正确则返回“imok”，否则不做任何相应。 stat 列出服务器和连接客户端的简要详细信息。 wchs 列出所有 watch 的简单信息。 wchc 按会话列出服务器 watch 的详细信息。 wchp 按路径列出服务器 watch 的详细信息。 更多四字命令可以参阅官方文档：https://zookeeper.apache.org/doc/current/zookeeperAdmin.html 使用前需要使用 yum install nc 安装 nc 命令，使用示例如下： 1234567891011121314[root@hadoop001 bin]# echo stat | nc localhost 2181Zookeeper version: 3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 04:05 GMTClients: /0:0:0:0:0:0:0:1:50584[1](queued=0,recved=371,sent=371) /0:0:0:0:0:0:0:1:50656[0](queued=0,recved=1,sent=0)Latency min/avg/max: 0/0/19Received: 372Sent: 371Connections: 2Outstanding: 0Zxid: 0x150Mode: standaloneNode count: 167]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Shell</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Zookeeper单机环境和集群环境搭建]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BZookeeper%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E5%92%8C%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Zookeeper单机环境和集群环境搭建一、单机环境搭建1.1 下载1.2 解压1.3 配置环境变量1.4 修改配置1.5 启动1.6 验证二、集群环境搭建2.1 修改配置2.2 标识节点2.3 启动集群2.4 集群验证 一、单机环境搭建1.1 下载下载对应版本 Zookeeper，这里我下载的版本 3.4.14。官方下载地址：https://archive.apache.org/dist/zookeeper/ 1# wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz 1.2 解压1# tar -zxvf zookeeper-3.4.14.tar.gz 1.3 配置环境变量1# vim /etc/profile 添加环境变量： 12export ZOOKEEPER_HOME=/usr/app/zookeeper-3.4.14export PATH=$ZOOKEEPER_HOME/bin:$PATH 使得配置的环境变量生效： 1# source /etc/profile 1.4 修改配置进入安装目录的 conf/ 目录下，拷贝配置样本并进行修改： 1# cp zoo_sample.cfg zoo.cfg 指定数据存储目录和日志文件目录（目录不用预先创建，程序会自动创建），修改后完整配置如下： 1234567891011121314151617181920212223242526272829# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial# synchronization phase can takeinitLimit=10# The number of ticks that can pass between# sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just# example sakes.dataDir=/usr/local/zookeeper/datadataLogDir=/usr/local/zookeeper/log# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the# administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to &quot;0&quot; to disable auto purge feature#autopurge.purgeInterval=1 配置参数说明： tickTime：用于计算的基础时间单元。比如 session 超时：N*tickTime； initLimit：用于集群，允许从节点连接并同步到 master 节点的初始化连接时间，以 tickTime 的倍数来表示； syncLimit：用于集群， master 主节点与从节点之间发送消息，请求和应答时间长度（心跳机制）； dataDir：数据存储位置； dataLogDir：日志目录； clientPort：用于客户端连接的端口，默认 2181 1.5 启动由于已经配置过环境变量，直接使用下面命令启动即可： 1zkServer.sh start 1.6 验证使用 JPS 验证进程是否已经启动，出现 QuorumPeerMain 则代表启动成功。 12[root@hadoop001 bin]# jps3814 QuorumPeerMain 二、集群环境搭建为保证集群高可用，Zookeeper 集群的节点数最好是奇数，最少有三个节点，所以这里演示搭建一个三个节点的集群。这里我使用三台主机进行搭建，主机名分别为 hadoop001，hadoop002，hadoop003。 2.1 修改配置解压一份 zookeeper 安装包，修改其配置文件 zoo.cfg，内容如下。之后使用 scp 命令将安装包分发到三台服务器上： 123456789101112tickTime=2000initLimit=10syncLimit=5dataDir=/usr/local/zookeeper-cluster/data/dataLogDir=/usr/local/zookeeper-cluster/log/clientPort=2181# server.1 这个1是服务器的标识，可以是任意有效数字，标识这是第几个服务器节点，这个标识要写到dataDir目录下面myid文件里# 指名集群间通讯端口和选举端口server.1=hadoop001:2287:3387server.2=hadoop002:2287:3387server.3=hadoop003:2287:3387 2.2 标识节点分别在三台主机的 dataDir 目录下新建 myid 文件,并写入对应的节点标识。Zookeeper 集群通过 myid 文件识别集群节点，并通过上文配置的节点通信端口和选举端口来进行节点通信，选举出 Leader 节点。 创建存储目录： 12# 三台主机均执行该命令mkdir -vp /usr/local/zookeeper-cluster/data/ 创建并写入节点标识到 myid 文件： 123456# hadoop001主机echo "1" &gt; /usr/local/zookeeper-cluster/data/myid# hadoop002主机echo "2" &gt; /usr/local/zookeeper-cluster/data/myid# hadoop003主机echo "3" &gt; /usr/local/zookeeper-cluster/data/myid 2.3 启动集群分别在三台主机上，执行如下命令启动服务： 1/usr/app/zookeeper-cluster/zookeeper/bin/zkServer.sh start 2.4 集群验证启动后使用 zkServer.sh status 查看集群各个节点状态。如图所示：三个节点进程均启动成功，并且 hadoop002 为 leader 节点，hadoop001 和 hadoop003 为 follower 节点。]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Zookeeper简介及核心概念]]></title>
    <url>%2F2019%2F08%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BZookeeper%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Zookeeper简介及核心概念一、Zookeeper简介二、Zookeeper设计目标三、核心概念3.1 集群角色3.2 会话3.3 数据节点3.4 节点信息3.5 Watcher3.6 ACL四、ZAB协议4.1 ZAB协议与数据一致性4.2 ZAB协议的内容五、Zookeeper的典型应用场景5.1数据的发布/订阅5.2 命名服务5.3 Master选举5.4 分布式锁5.5 集群管理 一、Zookeeper简介Zookeeper 是一个开源的分布式协调服务，目前由 Apache 进行维护。Zookeeper 可以用于实现分布式系统中常见的发布/订阅、负载均衡、命令服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列等功能。它具有以下特性： 顺序一致性：从一个客户端发起的事务请求，最终都会严格按照其发起顺序被应用到 Zookeeper 中； 原子性：所有事务请求的处理结果在整个集群中所有机器上都是一致的；不存在部分机器应用了该事务，而另一部分没有应用的情况； 单一视图：所有客户端看到的服务端数据模型都是一致的； 可靠性：一旦服务端成功应用了一个事务，则其引起的改变会一直保留，直到被另外一个事务所更改； 实时性：一旦一个事务被成功应用后，Zookeeper 可以保证客户端立即可以读取到这个事务变更后的最新状态的数据。 二、Zookeeper设计目标Zookeeper 致力于为那些高吞吐的大型分布式系统提供一个高性能、高可用、且具有严格顺序访问控制能力的分布式协调服务。它具有以下四个目标： 2.1 目标一：简单的数据模型Zookeeper 通过树形结构来存储数据，它由一系列被称为 ZNode 的数据节点组成，类似于常见的文件系统。不过和常见的文件系统不同，Zookeeper 将数据全量存储在内存中，以此来实现高吞吐，减少访问延迟。 2.2 目标二：构建集群可以由一组 Zookeeper 服务构成 Zookeeper 集群，集群中每台机器都会单独在内存中维护自身的状态，并且每台机器之间都保持着通讯，只要集群中有半数机器能够正常工作，那么整个集群就可以正常提供服务。 2.3 目标三：顺序访问对于来自客户端的每个更新请求，Zookeeper 都会分配一个全局唯一的递增 ID，这个 ID 反映了所有事务请求的先后顺序。 2.4 目标四：高性能高可用ZooKeeper 将数据存全量储在内存中以保持高性能，并通过服务集群来实现高可用，由于 Zookeeper 的所有更新和删除都是基于事务的，所以其在读多写少的应用场景中有着很高的性能表现。 三、核心概念3.1 集群角色Zookeeper 集群中的机器分为以下三种角色： Leader ：为客户端提供读写服务，并维护集群状态，它是由集群选举所产生的； Follower ：为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态。同时也参与写操作“过半写成功”的策略和 Leader 的选举； Observer ：为客户端提供读写服务，并定期向 Leader 汇报自己的节点状态，但不参与写操作“过半写成功”的策略和 Leader 的选举，因此 Observer 可以在不影响写性能的情况下提升集群的读性能。 3.2 会话Zookeeper 客户端通过 TCP 长连接连接到服务集群，会话 (Session) 从第一次连接开始就已经建立，之后通过心跳检测机制来保持有效的会话状态。通过这个连接，客户端可以发送请求并接收响应，同时也可以接收到 Watch 事件的通知。 关于会话中另外一个核心的概念是 sessionTimeOut(会话超时时间)，当由于网络故障或者客户端主动断开等原因，导致连接断开，此时只要在会话超时时间之内重新建立连接，则之前创建的会话依然有效。 3.3 数据节点Zookeeper 数据模型是由一系列基本数据单元 Znode(数据节点) 组成的节点树，其中根节点为 /。每个节点上都会保存自己的数据和节点信息。Zookeeper 中节点可以分为两大类： 持久节点 ：节点一旦创建，除非被主动删除，否则一直存在； 临时节点 ：一旦创建该节点的客户端会话失效，则所有该客户端创建的临时节点都会被删除。 临时节点和持久节点都可以添加一个特殊的属性：SEQUENTIAL，代表该节点是否具有递增属性。如果指定该属性，那么在这个节点创建时，Zookeeper 会自动在其节点名称后面追加一个由父节点维护的递增数字。 3.4 节点信息每个 ZNode 节点在存储数据的同时，都会维护一个叫做 Stat 的数据结构，里面存储了关于该节点的全部状态信息。如下： 状态属性 说明 czxid 数据节点创建时的事务 ID ctime 数据节点创建时的时间 mzxid 数据节点最后一次更新时的事务 ID mtime 数据节点最后一次更新时的时间 pzxid 数据节点的子节点最后一次被修改时的事务 ID cversion 子节点的更改次数 version 节点数据的更改次数 aversion 节点的 ACL 的更改次数 ephemeralOwner 如果节点是临时节点，则表示创建该节点的会话的 SessionID；如果节点是持久节点，则该属性值为 0 dataLength 数据内容的长度 numChildren 数据节点当前的子节点个数 3.5 WatcherZookeeper 中一个常用的功能是 Watcher(事件监听器)，它允许用户在指定节点上针对感兴趣的事件注册监听，当事件发生时，监听器会被触发，并将事件信息推送到客户端。该机制是 Zookeeper 实现分布式协调服务的重要特性。 3.6 ACLZookeeper 采用 ACL(Access Control Lists) 策略来进行权限控制，类似于 UNIX 文件系统的权限控制。它定义了如下五种权限： CREATE：允许创建子节点； READ：允许从节点获取数据并列出其子节点； WRITE：允许为节点设置数据； DELETE：允许删除子节点； ADMIN：允许为节点设置权限。 四、ZAB协议4.1 ZAB协议与数据一致性ZAB 协议是 Zookeeper 专门设计的一种支持崩溃恢复的原子广播协议。通过该协议，Zookeepe 基于主从模式的系统架构来保持集群中各个副本之间数据的一致性。具体如下： Zookeeper 使用一个单一的主进程来接收并处理客户端的所有事务请求，并采用原子广播协议将数据状态的变更以事务 Proposal 的形式广播到所有的副本进程上去。如下图： 具体流程如下： 所有的事务请求必须由唯一的 Leader 服务来处理，Leader 服务将事务请求转换为事务 Proposal，并将该 Proposal 分发给集群中所有的 Follower 服务。如果有半数的 Follower 服务进行了正确的反馈，那么 Leader 就会再次向所有的 Follower 发出 Commit 消息，要求将前一个 Proposal 进行提交。 4.2 ZAB协议的内容ZAB 协议包括两种基本的模式，分别是崩溃恢复和消息广播： 1. 崩溃恢复当整个服务框架在启动过程中，或者当 Leader 服务器出现异常时，ZAB 协议就会进入恢复模式，通过过半选举机制产生新的 Leader，之后其他机器将从新的 Leader 上同步状态，当有过半机器完成状态同步后，就退出恢复模式，进入消息广播模式。 2. 消息广播ZAB 协议的消息广播过程使用的是原子广播协议。在整个消息的广播过程中，Leader 服务器会每个事物请求生成对应的 Proposal，并为其分配一个全局唯一的递增的事务 ID(ZXID)，之后再对其进行广播。具体过程如下： Leader 服务会为每一个 Follower 服务器分配一个单独的队列，然后将事务 Proposal 依次放入队列中，并根据 FIFO(先进先出) 的策略进行消息发送。Follower 服务在接收到 Proposal 后，会将其以事务日志的形式写入本地磁盘中，并在写入成功后反馈给 Leader 一个 Ack 响应。当 Leader 接收到超过半数 Follower 的 Ack 响应后，就会广播一个 Commit 消息给所有的 Follower 以通知其进行事务提交，之后 Leader 自身也会完成对事务的提交。而每一个 Follower 则在接收到 Commit 消息后，完成事务的提交。 五、Zookeeper的典型应用场景5.1数据的发布/订阅数据的发布/订阅系统，通常也用作配置中心。在分布式系统中，你可能有成千上万个服务节点，如果想要对所有服务的某项配置进行更改，由于数据节点过多，你不可逐台进行修改，而应该在设计时采用统一的配置中心。之后发布者只需要将新的配置发送到配置中心，所有服务节点即可自动下载并进行更新，从而实现配置的集中管理和动态更新。 Zookeeper 通过 Watcher 机制可以实现数据的发布和订阅。分布式系统的所有的服务节点可以对某个 ZNode 注册监听，之后只需要将新的配置写入该 ZNode，所有服务节点都会收到该事件。 5.2 命名服务在分布式系统中，通常需要一个全局唯一的名字，如生成全局唯一的订单号等，Zookeeper 可以通过顺序节点的特性来生成全局唯一 ID，从而可以对分布式系统提供命名服务。 5.3 Master选举分布式系统一个重要的模式就是主从模式 (Master/Salves)，Zookeeper 可以用于该模式下的 Matser 选举。可以让所有服务节点去竞争性地创建同一个 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，这样该服务节点就可以成为 Master 节点。 5.4 分布式锁可以通过 Zookeeper 的临时节点和 Watcher 机制来实现分布式锁，这里以排它锁为例进行说明： 分布式系统的所有服务节点可以竞争性地去创建同一个临时 ZNode，由于 Zookeeper 不能有路径相同的 ZNode，必然只有一个服务节点能够创建成功，此时可以认为该节点获得了锁。其他没有获得锁的服务节点通过在该 ZNode 上注册监听，从而当锁释放时再去竞争获得锁。锁的释放情况有以下两种： 当正常执行完业务逻辑后，客户端主动将临时 ZNode 删除，此时锁被释放； 当获得锁的客户端发生宕机时，临时 ZNode 会被自动删除，此时认为锁已经释放。 当锁被释放后，其他服务节点则再次去竞争性地进行创建，但每次都只有一个服务节点能够获取到锁，这就是排他锁。 5.5 集群管理Zookeeper 还能解决大多数分布式系统中的问题： 如可以通过创建临时节点来建立心跳检测机制。如果分布式系统的某个服务节点宕机了，则其持有的会话会超时，此时该临时节点会被删除，相应的监听事件就会被触发。 分布式系统的每个服务节点还可以将自己的节点状态写入临时节点，从而完成状态报告或节点工作进度汇报。 通过数据的订阅和发布功能，Zookeeper 还能对分布式系统进行模块的解耦和任务的调度。 通过监听机制，还能对分布式系统的服务节点进行动态上下线，从而实现服务的动态扩容。 参考资料 倪超 . 从 Paxos 到 Zookeeper——分布式一致性原理与实践 . 电子工业出版社 . 2015-02-01]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
        <tag>核心概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka核心组件和流程(日志管理器)]]></title>
    <url>%2F2019%2F08%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%92%8C%E6%B5%81%E7%A8%8B(%E6%97%A5%E5%BF%97%E7%AE%A1%E7%90%86%E5%99%A8)%2F</url>
    <content type="text"><![CDATA[上一节介绍了协调器。协调器主要负责消费者和kafka集群间的协调。那么消费者消费时，如何定位消息呢？消息是如何存储呢？本节将为你揭开答案。 3.1 日志的存储Kafka的消息以日志文件的形式进行存储。不同主题下不同分区的消息是分开存储的。同一个分区的不同副本也是以日志的形式，分布在不同的broker上存储。 这样看起来，日志的存储是以副本为单位的。在程序逻辑上，日志确实是以副本为单位的，每个副本对应一个log对象。但实际在物理上，一个log又划分为多个logSegment进行存储。 举个例子，创建一个topic名为test，拥有3个分区。为了简化例子，我们设定只有1个broker，1个副本。那么所有的分区副本都存储在同一个broker上。 第二章中，我们在kafka的配置文件中配置了log.dirs=/tmp/kafka-logs。此时在/tmp/kafka-logs下面会创建test-0，test-1，test-2三个文件夹，代表三个分区。命名规则为“topic名称-分区编号” 我们看test-0这个文件夹，注意里面的logSegment并不代表这个文件夹，logSegment代表逻辑上的一组文件，这组文件就是.log、.index、.timeindex这三个不同文件扩展名，但是同文件名的文件。 .log存储消息 .index存储消息的索引 .timeIndex，时间索引文件，通过时间戳做索引。 这三个文件配合使用，用来保存和消费时快速查找消息。 刚才说到同一个logSegment的三个文件，文件名是一样的。命名规则为.log文件中第一条消息的前一条消息偏移量，也称为基础偏移量，左边补0，补齐20位。比如说第一个LogSegement的日志文件名为00000000000000000000.log，假如存储了200条消息后，达到了log.segment.bytes配置的阈值（默认1个G），那么将会创建新的logSegment，文件名为00000000000000000200.log。以此类推。另外即使没有达到log.segment.bytes的阈值，而是达到了log.roll.ms或者log.roll.hours设置的时间触发阈值，同样会触发产生新的logSegment。 3.2 日志定位日志定位也就是消息定位，输入一个消息的offset，kafka如何定位到这条消息呢？ 日志定位的过程如下: 1、根据offset定位logSegment。（kafka将基础偏移量也就是logsegment的名称作为key存在concurrentSkipListMap中） 2、根据logSegment的index文件查找到距离目标offset最近的被索引的offset的position x。 3、找到logSegment的.log文件中的x位置，向下逐条查找，找到目标offset的消息。 结合下图中例子，我再做详细的讲解： 这里先说明一下.index文件的存储方式。.index文件中存储了消息的索引，存储内容是消息的offset及物理位置position。并不是每条消息都有自己的索引，kafka采用的是稀疏索引，说白了就是隔n条消息存一条索引数据。这样做比每一条消息都建索引，查找起来会慢，但是也极大的节省了存储空间。此例中我们假设跨度为2，实际kafka中跨度并不是固定条数，而是取决于消息累积字节数大小。 例子中consumer要消费offset=15的消息。我们假设目前可供消费的消息已经存储了三个logsegment，分别是00000000000000000，0000000000000000010，0000000000000000020。为了讲解方便，下面提到名称时，会把前面零去掉。 下面我们详细讲一下查找过程。 kafka收到查询offset=15的消息请求后，通过二分查找，从concurrentSkipListMap中找到对应的logsegment名称，也就是10。 从10.index中找到offset小于等于15的最大值，offset=14，它对应的position=340 从10.log文件中物理位置340，顺序往下扫描文件，找到offset=15的消息内容。 可以看到通过稀疏索引，kafka既加快了消息查找的速度，也顾及了存储的开销。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>日志管理器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka核心组件和流程(控制器)]]></title>
    <url>%2F2019%2F08%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%92%8C%E6%B5%81%E7%A8%8B(%E6%8E%A7%E5%88%B6%E5%99%A8)%2F</url>
    <content type="text"><![CDATA[Kafka主要的组件如下 控制器 协调器 日志管理器 副本管理器 我们将会逐个进行讲解，讲解过长还将保持前面章节的特点，多用有形的图表帮助读者理解。本篇博客先讲解控制器部分。 1、控制器在前一章的学习中，我们已经知道Kafka的集群由n个的broker所组成，每个broker就是一个kafka的实例或者称之为kafka的服务。其实控制器也是一个broker，控制器也叫leader broker。他除了具有一般broker的功能外，还负责分区leader的选取，也就是负责选举partition的leader replica。控制器是kafka核心中的核心，需要重点学习和理解。 控制器选举kafka每个broker启动的时候，都会实例化一个KafkaController，并将broker的id注册到zookeeper，这在第二章中已经通过例子做过讲解。集群在启动过程中，通过选举机制选举出其中一个broker作为leader，也就是前面所说的控制器。 包括集群启动在内，有三种情况触发控制器选举： 1、集群启动 2、控制器所在代理发生故障 3、zookeeper心跳感知，控制器与自己的session过期 按照惯例，先看图。我们根据下图来讲解集群启动时，控制器选举过程。 假设此集群有三个broker，同时启动。 （一）3个broker从zookeeper获取/controller临时节点信息。/controller存储的是选举出来的leader信息。此举是为了确认是否已经存在leader。 （二）如果还没有选举出leader，那么此节点是不存在的，返回-1。如果返回的不是-1，而是leader的json数据，那么说明已经有leader存在，选举结束。 （三）三个broker发现返回-1，了解到目前没有leader，于是均会触发向临时节点/controller写入自己的信息。最先写入的就会成为leader。 （四）假设broker 0的速度最快，他先写入了/controller节点，那么他就成为了leader。而broker1、broker2很不幸，因为晚了一步，他们在写/controller的过程中会抛出ZkNodeExistsException，也就是zk告诉他们，此节点已经存在了。 经过以上四步，broker 0成功写入/controller节点，其它broker写入失败了，所以broker 0成功当选leader。 此外zk中还有controller_epoch节点，存储了leader的变更次数，初始值为0，以后leader每变一次，该值+1。所有向控制器发起的请求，都会携带此值。如果控制器和自己内存中比较，请求值小，说明kafka集群已经发生了新的选举，此请求过期，此请求无效。如果请求值大于控制器内存的值，说明已经有新的控制器当选了，自己已经退位，请求无效。kafka通过controller_epoch保证集群控制器的唯一性及操作的一致性。 由此可见，Kafka控制器选举就是看谁先争抢到/controller节点写入自身信息。 控制器初始化控制器的初始化，其实是初始化控制器所用到的组件及监听器，准备元数据。 前面提到过每个broker都会实例化并启动一个KafkaController。KafkaController和他的组件关系，以及各个组件的介绍如下图： 图中箭头为组件层级关系，组件下面还会再初始化其他组件。可见控制器内部还是有些复杂的，主要有以下组件： 1、ControllerContext，此对象存储了控制器工作需要的所有上下文信息，包括存活的代理、所有主题及分区分配方案、每个分区的AR、leader、ISR等信息。 2、一系列的listener，通过对zookeeper的监听，触发相应的操作，黄色的框的均为listener 3、分区和副本状态机，管理分区和副本。 4、当前代理选举器ZookeeperLeaderElector，此选举器有上位和退位的相关回调方法。 5、分区leader选举器，PartitionLeaderSelector 6、主题删除管理器，TopicDeletetionManager 7、leader向broker批量通信的ControllerBrokerRequestBatch。缓存状态机处理后产生的request，然后统一发送出去。 8、控制器平衡操作的KafkaScheduler，仅在broker作为leader时有效。 图片是我根据资料所总结，个人认为对于理解kafkaController的全貌很有帮助。本章节后面讲到相应组件和流程时，还需要反复回来理解此图，思考组件所处的位置，对整体的作用。 故障转移故障转移其实就是leader所在broker发生故障，leader转移为其他的broker。转移的过程就是重新选举leader的过程。 重新选举leader后，需要为该broker注册相应权限，调用的是ZookeeperLeaderElector的onControllerFailover()方法。在这个方法中初始化和启动了一系列的组件来完成leader的各种操作。具体如下，其实和控制器初始化有很大的相似度。 1、注册分区管理的相关监听器 监听名称监听zookeeper节点作用PartitionsReassignedListener/admin/reassign_partitions节点变化将会引发分区重分配IsrChangeNotificationListener/isr_change_notification处理分区的ISR发生变化引发的操作PreferredReplicaElectionListener/admin/preferred_replica_election将优先副本选举为leader副本 2、注册主题管理的相关监听 监听名称监听zookeeper节点作用TopicChangeListener/brokers/topics监听主题发生变化时进行相应操作DeleteTopicsListener/admin/delete_topics完成服务器端删除主题的相应操作。否则客户端删除主题仅仅是表示删除 3、注册代理变化监听器 监听名称监听zookeeper节点作用BrokerChangeListener/brokers/ids代理发生增减的时候进行相应的处理 4、重新初始化ControllerContext， 5、启动控制器和其他代理之间通信的ControllerChannelManager 6、创建用于删除主题的TopicDeletionManager对象,并启动。 7、启动分区状态机和副本状态机 8、轮询每个主题，添加监听分区变化的PartitionModificationsListener 9、如果设置了分区平衡定时操作，那么创建分区平衡的定时任务，默认300秒检查并执行。 除了这些组件的启动外，onControllerFailover方法中还做了如下操作： 1、/controller_epoch值+1，并且更新到ControllerContext 2、检查是否出发分区重分配，并做相关操作 3、检查需要将优先副本选为leader，并做相关操作 4、向kafka集群所有代理发送更新元数据的请求。 下面来看代理下线的方法onControllerResignation 1、该方法中注销了控制器的权限。取消在zookeeper中对于分区、副本感知的相应监听器的监听。 2、关闭启动的各个组件 3、最后把ControllerContext中记录控制器版本的数值清零，并设置当前broker为RunnignAsBroker，变为普通的broker。 通过对控制器启动过程的学习，我们应该已经对kafka工作的原理有了了解，核心是监听zookeeper的相关节点，节点变化时触发相应的操作。其它的处理流程都是相类似的。本篇教程接下来做简要介绍，想要了解详情的，可以先找其它资料。我后续也会再补充更为详细的教程。 代理上下线有新的broker加入集群时，称为代理上线。反之，当broker关闭，推出集群时，称为代理下线。 代理上线： 1、新代理启动时向/brokers/ids写数据 2、BrokerChangeListener监听到变化。对新上线节点调用controllerChannelManager.addBroker()，完成新上线代理网络层初始化 3、调用KafkaController.onBrokerStartup()处理 3.1通过向所有代理发送UpdateMetadataRequest，告诉所有代理有新代理加入 3.2根据分配给新上线节点的副本集合，对副本状态做变迁。对分区也进行处理。 3.3触发一次leader选举，确认新加入的是否为分区leader 3.4轮询分配给新broker的副本，调用KafkaController.onPartitionReassignment()，执行分区副本分配 3.5恢复因新代理上线暂停的删除主题操作线程 代理下线： 1、查找下线节点集合 2、轮询下线节点，调用controllerChannelManager.removeBroker()，关闭每个下线节点网络连接。清空下线节点消息队列，关闭下线节点request请求 3、轮询下线节点，调用KafkaController.onBrokerFailure处理 3.1处理leader副本在下线节点上上的分区，重新选出leader副本，发送updateMetadataRequest请求。 3.2处理下线节点上的副本集合，做下线处理，从ISR集合中删除，不再同步，发送updateMetadataRequest请求。 4、向集群全部存活代理发送updateMetadataRequest请求 主题管理通过分区状态机及副本状态机来进行主题管理 1、创建主题 /brokers/topics下创建主题对应子节点 TopicChangeListener监听此节点 变化时获取重入锁ReentrantLock,调用handleChildChange方法进行处理。 通过对比zookeeper中/brokers/topics存储的主题集合及控制器的ControllerContext中缓存的主题集合的差集，得到新增的主题。反过来求差集，得到删除的主题。 接下来遍历新增的主题集合，进行主题操作的实质性操作。之前仅仅是在zookeeper中添加了主题。新增主题涉及的操作有分区、副本状态的转化、分区leader的分配、分区存储日志的创建等。 2、删除主题 /admin/delete_topics创建删除主题的子节点 DeleteTopicsListener监听此节点， 变化时获取重入锁ReentrantLock,进行处理 具体的删除逻辑再次就不再详述。 分区管理1、分区自动平衡 onControllerFailover方法中启动分区自动平衡任务。定时检查是否失去平衡。 自动平衡的操作就是把优先副本选为分区leader，AR中第一个副本为优先副本。 先查出所有可用副本，以分区AR头节点分组。 轮询代理节点，判断分区不平衡率是否超过10%(leader为非优先副本的分区/该代理分区总数)，则调用onPreferredReplicaElection()，让优先副本成为leader。达到自动平衡。 分区平衡操作的流程已经在第三章做了很详细的讲解，此处不再重复，可以参考kafka核心概念。 2、分区重分配 当zk节点/admin/reassign_partitions变化时，触发分区重分配操作。该节点存储分区重分配的方案。 通过计算主题分区原AR（OAR）和重新分配后的AR（RAR），分别做相应处理： 1、OAR+RAR：更新到该主题分区AR，并通知副本节点同步。leader_epoch+1 2、RAR-OAR：副本设为NewReplica。 3、（OAR+RAR）- RAR：需要下线的副本，做下线操作 具体流程不再详述 小结：关于控制器的相关知识点就先讲到这里，控制器初始化中的那张图需要充分去理解，理解了此图，对控制器内部的构造，以及控制器要做什么事情、如何做的，就已经掌握了。 你真的不关注一下嘛~]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>控制器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka核心组件和流程(协调器)]]></title>
    <url>%2F2019%2F08%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%92%8C%E6%B5%81%E7%A8%8B(%E5%8D%8F%E8%B0%83%E5%99%A8)%2F</url>
    <content type="text"><![CDATA[上一节介绍了kafka工作的核心组件–控制器。本节将介绍消费者密切相关的组件–协调器。它负责消费者的出入组工作。大家可以回想一下kafka核心概念中关于吃苹果的场景，如果我邀请了100个人过来吃苹果，如果没有人告诉每个吃苹果的人哪个是他的盘子，那岂不是要乱了套？协调器做的就是这个工作。当然还有更多。 2 协调器顾名思义，协调器负责协调工作。本节所讲的协调器，是用来协调消费者工作分配的。简单点说，就是消费者启动后，到可以正常消费前，这个阶段的初始化工作。消费者能够正常运转起来，全有赖于协调器。 主要的协调器有如下两个： 1、消费者协调器（ConsumerCoordinator） 2、组协调器（GroupCoordinator） 此外还有任务管理协调器（WorkCoordinator），用作kafka connect的works管理，本教程不做讲解。 kafka引入协调器有其历史过程，原来consumer信息依赖于zookeeper存储，当代理或消费者发生变化时，引发消费者平衡，此时消费者之间是互不透明的，每个消费者和zookeeper单独通信，容易造成羊群效应和脑裂问题。 为了解决这些问题，kafka引入了协调器。服务端引入组协调器（GroupCoordinator），消费者端引入消费者协调器（ConsumerCoordinator）。每个broker启动的时候，都会创建GroupCoordinator实例，管理部分消费组（集群负载均衡）和组下每个消费者消费的偏移量（offset）。每个consumer实例化时，同时实例化一个ConsumerCoordinator对象，负责同一个消费组下各个消费者和服务端组协调器之前的通信。如下图： 2.1 消费者协调器消费者协调器，可以看作是消费者做操作的代理类（其实并不是），消费者很多操作通过消费者协调器进行处理。 消费者协调器主要负责如下工作： 1、更新消费者缓存的MetaData 2、向组协调器申请加入组 3、消费者加入组后的相应处理 4、请求离开消费组 5、向组协调器提交偏移量 6、通过心跳，保持组协调器的连接感知。 7、被组协调器选为leader的消费者的协调器，负责消费者分区分配。分配结果发送给组协调器。 8、非leader的消费者，通过消费者协调器和组协调器同步分配结果。 消费者协调器主要依赖的组件和说明见下图： 可以看到这些组件和消费者协调器担负的工作是可以对照上的。 2.2 组协调器组协调器负责处理消费者协调器发过来的各种请求。它主要提供如下功能： 在与之连接的消费者中选举出消费者leader 下发leader消费者返回的消费者分区分配结果给所有的消费者 管理消费者的消费偏移量提交，保存在kafka的内部主题中 和消费者心跳保持，知道哪些消费者已经死掉，组中存活的消费者是哪些。 组协调器在broker启动的时候实例化，每个组协调器负责一部分消费组的管理。它主要依赖的组件见下图： 这些组件也是和组协调器的功能能够对应上的。具体内容不在详述。 2.3 消费者入组过程下图展示了消费者启动选取leader、入组的过程。 消费者入组的过程，很好的展示了消费者协调器和组协调器之间是如何配合工作的。leader consumer会承担分区分配的工作，这样kafka集群的压力会小很多。同组的consumer通过组协调器保持同步。消费者和分区的对应关系持久化在kafka内部主题。 2.4 消费偏移量管理消费者消费时，会在本地维护消费到的位置（offset），就是偏移量，这样下次消费才知道从哪里开始消费。如果整个环境没有变化，这样做就足够了。但一旦消费者平衡操作或者分区变化后，消费者不再对应原来的分区，而每个消费者的offset也没有同步到服务器，这样就无法接着前任的工作继续进行了。 因此只有把消费偏移量定期发送到服务器，由GroupCoordinator集中式管理，分区重分配后，各个消费者从GroupCoordinator读取自己对应分区的offset，在新的分区上继续前任的工作。 下图展示了不提交offset到服务端的问题： 开始时，consumer 0消费partition 0 和1，后来由于新的consumer 2入组，分区重新进行了分配。consumer 0不再消费partition2，而由consumer 2来消费partition 2，但由于consumer之间是不能通讯的，所有consumer2并不知道从哪里开始自己的消费。 因此consumer需要定期提交自己消费的offset到服务端，这样在重分区操作后，每个consumer都能在服务端查到分配给自己的partition所消费到的offset，继续消费。 由于kafka有高可用和横向扩展的特性，当有新的分区出现或者新的消费入组后，需要重新分配消费者对应的分区，所以如果偏移量提交的有问题，会重复消费或者丢消息。偏移量提交的时机和方式要格外注意！！ 下面两种情况分别会造成重复消费和丢消息： 如果提交的偏移量小于消费者最后一次消费的偏移量，那么再均衡后，两个offset之间的消息就会被重复消费 如果提交的偏移量大于消费者最后一次消费的偏移量，那么再均衡后，两个offset之间的消息就会丢失 以上两种情况是如何产生的呢？我们继续往下看。 2.4.1 偏移量有两种提交方式1、自动提交偏移量 设置 enable.auto.commit为true，设定好周期，默认5s。消费者每次调用轮询消息的poll() 方法时，会检查是否超过了5s没有提交偏移量，如果是，提交上一次轮询返回的偏移量。 这样做很方便，但是会带来重复消费的问题。假如最近一次偏移量提交3s后，触发了再均衡，服务器端存储的还是上次提交的偏移量，那么再均衡结束后，新的消费者会从最后一次提交的偏移量开始拉取消息，此3s内消费的消息会被重复消费。 2、手动提交偏移量 设置 enable.auto.commit为false。程序中手动调用commitSync()提交偏移量，此时提交的是poll方法返回的最新的偏移量。 我们来看下面两个提交时机： 如果poll完马上调用commitSync(),那么一旦处理到中间某条消息的时候异常，由于偏移量已经提交，那么出问题的消息位置到提交偏移量之间的消息就会丢失。 如果处理完所有消息后才调用commitSync()。有可能在处理到一半的时候发生再均衡，此时偏移量还未提交，那么再均衡后，会从上次提交的位置开始消费，造成重复消费。 比较起来，重复消费要比丢消息好一些，所以我们程序应采用第二种方式，同时消费逻辑中，要能够检查重复消费。 commitSync()是同步提交偏移量，主程序会一直阻塞，偏移量提交成功后才往下运行。这样会限制程序的吞吐量。如果降低提交频次，又很容易发生重复消费。 这里我们可以使用commitAsync()异步提交偏移量。只管提交，而不会等待broker返回提交结果 commitSync只要没有发生不可恢复错误，会进行重试，直到成功。而commitAsync不会进行重试，失败就是失败了。commitAsync不重试，是因为重试提交时，可能已经有其它更大偏移量已经提交成功了，如果此时重试提交成功，那么更小的偏移量会覆盖大的偏移量。那么如果此时发生再均衡，新的消费者将会重复消费消息。 commitAsync也支持回调，由于上述原因，回调中最好不要因为失败而重试提交。而是应该记录错误，以便后续分析和补偿。 2.4.2 偏移量提交的最佳实践关于偏移量的提交方式和时机，上文已经有了大量的讲解。但看完后好像还不知道应该怎么提交偏移量才是最合适的。是不是觉得无论怎么提交，都无法避免重复消费？没错，事实就是这样，我们只能采用合理的方式，最大可能的去降低发生此类问题的概率。此外做好补偿处理。 一般来说，偶尔的提交失败，不去重试，是没有问题的。因为一般是因为临时的问题而失败，后续的提交总会成功。如果我们在关闭消费者或者再均衡前，确保所有的消费者都能成功提交一次偏移量，也可以保证再均衡后，消费者能接着消费数据。 因此我们采用同步和异步混合的方式提交偏移量。 正常消费消息时，消费结束提交偏移量，采用异步方式 如果程序报错，finally中，提交偏移量，采用同步方式，确保提交成功 再均衡前的回调方法中，提交偏移量，采用同步方式，确保提交成功 这样既保证了吞吐量，也保证了提交偏移量的安全性。另外由于再均衡前提交偏移量，降低了重复消费可能。 kafka还提供了提交特定偏移量的方法。我们可以指定分区和offset进行提交。分区和offset的值可以从消息对象中取得。 另外，如果担心一次取回数据量太大，可能处理到一半的时候出现再均衡，导致偏移量没有提交，重复消费。那么可以每n条提交一次。 而当n=1时，也就是处理一条数据就提交一次，会把重复消费的可能降到最低。同时由于增加了和服务端的通讯，效率大大降低。 其实即使这样，也是可能重复消费的，试想如下场景： 消费者拉取到数据后，开始逻辑处理 处理第一条offset=2，成功了，提交offset=3 开始处理offset=3的消息，处理完成后，但提交offset=4前，此消费者突然意外挂掉了，所以也没能进入异常处理。偏移量没能成功提交。 消费者进行了再均衡，新的消费者接手此分区进行消费，取到的offset还是上一次提交的3，那么将会重复消费offset=3的消息。 所以我们应平衡重复消费发生的概率和程序的效率，来设置提交的时机。同时程序逻辑一定做好重复消费的检查工作！ 2.5 回顾本节从协调器讲起，首先介绍了消费者协调器和组协调器，以及他们是如何配合工作的。从消费偏移量的管理展开，详细介绍了偏移量的提交，及提交的最佳实践。本节没有涉及代码部分，所有知识点相关的代码将在最后一章中统一给出。现在的要求只是理解知识点。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>协调器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka核心组件和流程(副本管理器)]]></title>
    <url>%2F2019%2F08%2F13%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6%E5%92%8C%E6%B5%81%E7%A8%8B(%E5%89%AF%E6%9C%AC%E7%AE%A1%E7%90%86%E5%99%A8)%2F</url>
    <content type="text"><![CDATA[本章简单介绍了副本管理器，副本管理器负责分区及其副本的管理。副本管理器具体的工作流程可以参考牟大恩所著的《Kafka入门与实践》 副本管理器副本机制使得kafka整个集群中，只要有一个代理存活，就可以保证集群正常运行。这大大提高了Kafka的可靠性和稳定性。Kafka中代理的存活，需要满足以下两个条件： 存活的节点要维持和zookeeper的session连接，通过zookeeper的心跳机制实现 Follower副本要与leader副本保持同步，不能落后太多。 满足以上条件的节点在ISR中，一旦宕机，或者中断时间太长，Leader就会把同步副本从ISR中踢出。 所有节点中，leader节点负责接收客户端的读写操作，follower节点从leader复制数据。 副本管理器负责对副本管理。由于副本是分区的副本，所以对副本的管理体现在对分区的管理。 在第三章已经对分区和副本有了详细的讲解，这里再介绍两个重要的概念，LEO和HW。 LEO是Log End Offset缩写。表示每个分区副本的最后一条消息的位置，也就是说每个副本都有LEO。 HW是Hight Watermark缩写，他是一个分区所有副本中，最小的那个LEO。 看下图： 分区test-0有三个副本，每个副本的LEO就是自己最后一条消息的offset。可以看到最小的LEO是Replica2的，等于3，也就是说HW=3。这代表offset=4的消息还没有被所有副本复制，是无法被消费的。而offset&lt;=3的数据已经被所有副本复制，是可以被消费的。 副本管理器所承担的职责如下： 副本过期检查 追加消息 拉取消息 副本同步过程 副本角色转换 关闭副本]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>副本管理器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka编程实战]]></title>
    <url>%2F2019%2F08%2F11%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[本章通过实际例子，讲解了如何使用java进行kafka开发。 添加依赖： 12345&lt;dependency&gt;&lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;&lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;&lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt; 下面是创建主题的代码： 1234567891011121314151617181920212223public class TopicProcessor &#123;private static final String ZK_CONNECT=&quot;localhost:2181&quot;;private static final int SESSION_TIME_OUT=30000;private static final int CONNECT_OUT=30000;public static void createTopic(String topicName,int partitionNumber,int replicaNumber,Properties properties)&#123;ZkUtils zkUtils = null;try&#123;zkUtils=ZkUtils.apply(ZK_CONNECT,SESSION_TIME_OUT,CONNECT_OUT, JaasUtils.isZkSecurityEnabled());if(!AdminUtils.topicExists(zkUtils,topicName))&#123;AdminUtils.createTopic(zkUtils,topicName,partitionNumber,replicaNumber,properties,AdminUtils.createTopic$default$6());&#125;&#125;catch (Exception e)&#123;e.printStackTrace();&#125;finally &#123;zkUtils.close();&#125;&#125;public static void main(String[] args)&#123;createTopic(&quot;javatopic&quot;,1,1,new Properties());&#125;&#125; 首先定义了zookeeper相关连接信息。然后在createTopic中，先初始化ZkUtils，和zookeeper交互依赖于它。然后通过AdminUtils先判断是否存在你要创建的主题，如果不存在，则通过createTopic方法进行创建。传入参数包括主题名称，分区数量，副本数量等。 生产者生产消息生产者生产消息代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041public class MessageProducer &#123;private static final String TOPIC=&quot;education-info&quot;;private static final String BROKER_LIST=&quot;localhost:9092&quot;;private static KafkaProducer&lt;String,String&gt; producer = null;static&#123;Properties configs = initConfig();producer = new KafkaProducer&lt;String, String&gt;(configs);&#125;private static Properties initConfig()&#123;Properties properties = new Properties();properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG,BROKER_LIST);properties.put(ProducerConfig.ACKS_CONFIG,&quot;all&quot;);properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG,StringSerializer.class.getName());return properties;&#125;public static void main(String[] args)&#123;try&#123;String message = &quot;hello world&quot;;ProducerRecord&lt;String,String&gt; record = new ProducerRecord&lt;String,String&gt;(TOPIC,message);producer.send(record, new Callback() &#123;@Overridepublic void onCompletion(RecordMetadata metadata, Exception exception) &#123;if(null==exception)&#123;System.out.println(&quot;perfect!&quot;);&#125;if(null!=metadata)&#123;System.out.print(&quot;offset:&quot;+metadata.offset()+&quot;;partition:&quot;+metadata.partition());&#125;&#125;&#125;).get();&#125;catch (Exception e)&#123;e.printStackTrace();&#125;finally &#123;producer.close();&#125;&#125;&#125; 1、首先初始化KafkaProducer对象。 1producer = new KafkaProducer&lt;String, String&gt;(configs); 2、创建要发送的消息对象。 1ProducerRecord&lt;String,String&gt; record = new ProducerRecord&lt;String,String&gt;(TOPIC,message); 3、通过producer的send方法，发送消息 4、发送消息时，可以通过回调函数，取得消息发送的结果。异常发生时，对异常进行处理。 初始化producer时候,需要注意下面属性设置： 1properties.put(ProducerConfig.ACKS_CONFIG,&quot;all&quot;); 这里有三种值可供选择： 0，不等服务器响应，直接返回发送成功。速度最快，但是丢了消息是无法知道的 1，leader副本收到消息后返回成功 all，所有参与的副本都复制完成后返回成功。这样最安全，但是延迟最高。 消费者消费消息我们直接看代码 123456789101112131415161718192021222324252627282930313233343536373839404142public class MessageConsumer &#123;private static final String TOPIC=&quot;education-info&quot;;private static final String BROKER_LIST=&quot;localhost:9092&quot;;private static KafkaConsumer&lt;String,String&gt; kafkaConsumer = null;static &#123;Properties properties = initConfig();kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties);kafkaConsumer.subscribe(Arrays.asList(TOPIC));&#125;private static Properties initConfig()&#123;Properties properties = new Properties();properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,BROKER_LIST);properties.put(ConsumerConfig.GROUP_ID_CONFIG,&quot;test&quot;);properties.put(ConsumerConfig.CLIENT_ID_CONFIG,&quot;test&quot;);properties.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());properties.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,StringDeserializer.class.getName());return properties;&#125;public static void main(String[] args)&#123;try&#123;while(true)&#123;ConsumerRecords&lt;String,String&gt; records = kafkaConsumer.poll(100);for(ConsumerRecord record:records)&#123;try&#123;System.out.println(record.value());&#125;catch(Exception e)&#123;e.printStackTrace();&#125;&#125;&#125;&#125;catch(Exception e)&#123;e.printStackTrace();&#125;finally &#123;kafkaConsumer.close();&#125;&#125;&#125; 代码逻辑如下： 1、初始化消费者KafkaConsumer，并订阅主题。 12kafkaConsumer = new KafkaConsumer&lt;String, String&gt;(properties);kafkaConsumer.subscribe(Arrays.asList(TOPIC)); 2、循环拉取消息 1ConsumerRecords&lt;String,String&gt; records = kafkaConsumer.poll(100); poll方法传入的参数100，是等待broker返回数据的时间，如果超过100ms没有响应，则不再等待。 3、拉取回消息后，循环处理。 1234567for(ConsumerRecord record:records)&#123;try&#123;System.out.println(record.value());&#125;catch(Exception e)&#123;e.printStackTrace();&#125;&#125; 消费相关代码比较简单，不过这个版本没有处理偏移量提交。学习过第四章-协调器相关的同学应该还记得偏移量提交的问题。我曾说过最佳实践是同步和异步提交相结合，同时在特定的时间点，比如再均衡前进行手动提交。 加入偏移量提交，需要做如下修改： 1、enable.auto.commit设置为false 2、消费代码如下： 12345678910111213141516171819202122232425public static void main(String[] args)&#123;try&#123;while(true)&#123;ConsumerRecords&lt;String,String&gt; records =kafkaConsumer.poll(100);for(ConsumerRecord record:records)&#123;try&#123;System.out.println(record.value());&#125;catch(Exception e)&#123;e.printStackTrace();&#125;&#125;kafkaConsumer.commitAsync();&#125;&#125;catch(Exception e)&#123;e.printStackTrace();&#125;finally &#123;try&#123;kafkaConsumer.commitSync();&#125;finally &#123;kafkaConsumer.close();&#125;&#125;&#125; 3、订阅消息时，实现再均衡的回调方法，在此方法中手动提交偏移量 1234567kafkaConsumer.subscribe(Arrays.asList(TOPIC), new ConsumerRebalanceListener() &#123;@Overridepublic void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123;//再均衡之前和消费者停止读取消息之后调用kafkaConsumer.commitSync(currentOffsets);&#125;&#125;); 通过以上三步，我们把自动提交偏移量改为了手动提交。正常消费时，异步提交kafkaConsumer.commitAsync()。即使偶尔失败，也会被后续成功的提交覆盖掉。而在发生异常的时候，手动提交 kafkaConsumer.commitSync()。此外在步骤3中，我们通过实现再均衡时的回调方法，手动同步提交偏移量，确保了再均衡前偏移量提交成功。 以上面的最佳实践提交偏移量，既能保证消费时较高的效率，又能够尽量避免重复消费。不过由于重复消费无法100%避免，消费逻辑需要自己处理重复消费的判断。 你真的不关注一下嘛~]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>编程实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka安装和使用]]></title>
    <url>%2F2019%2F08%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[单机环境 官方建议使用JDK 1.8版本，因此本文使用的环境都是JDK1.8。关于JDK的安装，本文不再详述，默认Java环境已经具备。 由于Kafka依赖zookeeper，kafka通过zookeeper现实分布式系统的协调，所以我们需要先安装zookeeper。 接下来我们按照如下步骤，一步步来安装kafka： 1、下载zookeeper，解压。 下载地址：https://zookeeper.apache.org/releases.html#download 2、创建zookeeper配置文件 在zookeeper解压后的目录下找到conf文件夹，进入后，复制文件zoo_sample.cfg，并命名为zoo.cfg zoo.cfg中一共四个配置项，可以使用默认配置。 3、启动zookeeper。 进入zookeeper根目录执行 bin/zkServer.sh start 4、下载kafka，解压。 kafka 2.0版本下载地址：https://www.apache.org/dyn/closer.cgi?path=/kafka/2.0.0/kafka_2.11-2.0.0.tgz 5、修改kafka的配置文件 进入kafka根目录下的config文件夹下，打开server.properties,修改如下配置 zookeeper.connect=localhost:2181 broker.id=0 log.dirs=/tmp/kafka-logs zookeeper.connect是zookeeper的链接信息，broker.id是当前kafka实例的id，log.dirs是kafka存储消息内容的路径。 6、启动kafka 进入kafka根目录执行 bin/kafka-server-start.sh config/server.properties 此命令告诉kaka启动时使用config/server.properties配置项 启动kafka后，如果控制台没有报错信息，那么kafka应该已经启动成功了，我们可以通过查看zookeeper中相关节点值来确认。步骤如下： 1、启动zookeeper的client 进入zookeeper根目录下，执行 bin/zkCli.sh -server 127.0.0.1:2181。启动成功后如下图 2、输入命令 ls /brokers，回车，可以看到如下信息： 这些子节点存储的就是kafka集群管理的数据。broker是kafka的一个服务单元实例 3、我看看一下ids这个节点下的数据，输入命令 ls /brokers/ids，可以看到如下信息： 还记得我们在配置单机环境时，修改的kafka配置项broker.id=0 吗？这里的0就是表示那个kafka的实例已经加入了kafka集群。 集群环境集群环境的搭建也很简单，在单机环境的基础上，让多个单机连接到同一个zookeeper即可。需要注意两点： 1、每个实例设置不同的broker.id。 2、如果多个实例部署在同一台服务器，还要注意修改log.dirs为不同目录，确保消息存储时不会有冲突。集群环境的具体搭建，在此精简教程中不再做详细讨论。 发出你的第一条kafka消息 我们通过kafka带的工具来创建一个topic，然后尝试发送和消费一个消息，直观的去感受下kafka。 1、创建topic 进入kafka根目录，执行如下命令： 1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic study 执行成功后，创建了study这个topic，如下图所示： 此命令行有几个参数，分别指明了zookeeper的链接信息，分区和副本的数量等。关于分区和副本后续会仔细讲解，现在不用过多关注。 2、启动消费者 我们开启一个消费者并且订阅study这个topic，执行如下命令: 1bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic study --from-beginning 看到如下图，光标停留在最前面，没有任何信息输出，说明启动消费者成功，此时在等待新的消息。 3、开启生产者 新打开一个命令窗口，输入命令 1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic study 启动成功后，如下图，等待你输入新的消息。 4、发送你的第一条消息 在上面生产者的窗口输入一条消息 hello kafka,点击回车，如下图： 此时切换到消费者的窗口，可以看到消费者已经消费到这条消息，在窗口中打印了出来。 至此我们走完了一个发送消息的流程，可以看到我们经历了创建topic、启动生产者、消费者、生产者生产消息、消费者消费消息，这几个步骤。 小结：通过本章节学习，相信你已经能够成功搭建起kafka单机环境，甚至集群环境。然后通过kafka自带的工具，直观的感受了kafka运转的整个过程。接下来的章节我们将会进入kafka的核心领域，也是本教程的重点章节，只有理解了kafka内在的设计理念和原理，才能做到活学活用。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>安装使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka核心概念]]></title>
    <url>%2F2019%2F08%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[本章是学习kafka的核心章节，涵盖内容比较多，在理解上有一定的难度，需要反复阅读理解，才能参透Kafka的设计思想。 1、Kafka集群结构在第一章我给出过一个消息系统通用的结构图，也就是下图： 实际上kafka的结构图是有些区别的，现在我们看下面的图： producer和consumer想必大家都很熟悉，一个生产消息，一个消费掉消息。这里就不再做太多解释。 此图和第一张图可以看到有几个区别： 1、多了zookeeper集群，通过前几章的学习我们已经知道kafka是配合zookeeper进行工作的。 2、kafka集群中可以看到有若干个Broker，其中一个broker是leader，其他的broker是follower 3、consumer外面包裹了一层Consumer group。 我们先讲解一下Broker和consumer group的概念，以及Topic。 Broker一个Borker就是Kafka集群中的一个实例，或者说是一个服务单元。连接到同一个zookeeper的多个broker实例组成kafka的集群。在若干个broker中会有一个broker是leader，其余的broker为follower。leader在集群启动时候选举出来，负责和外部的通讯。当leader死掉的时候，follower们会再次通过选举，选择出新的leader，确保集群的正常工作。 Consumer GroupKafka和其它消息系统有一个不一样的设计，在consumer之上加了一层group。同一个group的consumer可以并行消费同一个topic的消息，但是同group的consumer，不会重复消费。这就好比多个consumer组成了一个团队，一起干活，当然干活的速度就上来了。group中的consumer是如何配合协调的，其实和topic的分区相关联，后面我们会详细论述。 如果同一个topic需要被多次消费，可以通过设立多个consumer group来实现。每个group分别消费，互不影响。 通过本节学习，我们从全局的层面了解了kafka的结构，接下来我们会深入到kafka内部，来看看它是怎么工作的。 Topickafka中消息订阅和发送都是基于某个topic。比如有个topic叫做NBA赛事信息，那么producer会把NBA赛事信息的消息发送到此topic下面。所有订阅此topic的consumer将会拉取到此topic下的消息。Topic就像一个特定主题的收件箱，producer往里丢，consumer取走。 2、Kafka核心概念简介kafka采用分区（Partition）的方式，使得消费者能够做到并行消费，从而大大提高了自己的吞吐能力。同时为了实现高可用，每个分区又有若干份副本（Replica），这样在某个broker挂掉的情况下，数据不会丢失。 接下来我们详细分析kafka是如何基于Partition和Replica工作的。 分区（Partition）大多数消息系统，同一个topic下的消息，存储在一个队列。分区的概念就是把这个队列划分为若干个小队列，每一个小队列就是一个分区，如下图： 这样做的好处是什么呢？其实从上图已经可以看出来。无分区时，一个topic只有一个消费者在消费这个消息队列。采用分区后，如果有两个分区，最多两个消费者同时消费，消费的速度肯定会更快。如果觉得不够快，可以加到四个分区，让四个消费者并行消费。分区的设计大大的提升了kafka的吞吐量！！ 我们再结合下图继续讲解Partition。 此图包含如下几个知识点： 1、一个partition只能被同组的一个consumer消费（图中只会有一个箭头指向一个partition） 2、同一个组里的一个consumer可以消费多个partition（图中第一个consumer消费Partition 0和3） 3、消费效率最高的情况是partition和consumer数量相同。这样确保每个consumer专职负责一个partition。 4、consumer数量不能大于partition数量。由于第一点的限制，当consumer多于partition时，就会有consumer闲置。 5、consumer group可以认为是一个订阅者的集群，其中的每个consumer负责自己所消费的分区 为了加深理解，我举个吃苹果的例子。 问题：有一篮子苹果，你如何把这一篮子苹果尽可能快的吃完？ 办法一： 我一个人，一个一个苹果吃，如下图。这样显然很慢，我吃完一个才能拿下一个。 办法二： 我再找两个人来一块吃，第一个人拿走一个去吃，然后第二个人拿一个去吃，接着第三个人拿一个去吃，如此循环。速度肯定快了，但是三个人还是会排队等待。三个人排队时间可能很短，但是如果叫了100个人帮忙吃呢？会有大量时间消耗在排队上。 办法三： 我还是找两个人来一块吃，但我把苹果提前分到三个盘子里，每人分一个盘子，自己吃自己的，这样不但能三个人同时吃苹果，还无须排队。速度显然是最快的。 办法三正是kafka所采用的设计方式，盘子就是partition，每个人就是一个consumer，每个苹果就是一条message。办法三每个盘子中苹果的消费是有序的，而办法二的消费是完全无序的。 相信通过这个例子你一定能充分理解partition的概念，以及为什么kafka会如此设计。 关于partition暂时说到这里，接下来介绍副本。 副本（Replica）提到副本，肯定就会想到正本。副本是正本的拷贝。在kafka中，正本和副本都称之为副本（Repalica），但存在leader和follower之分。活跃的称之为leader，其他的是follower。 每个分区的数据都会有多份副本，以此来保证Kafka的高可用。 Topic、partition、replica的关系如下图： topic下会划分多个partition，每个partition都有自己的replica，其中只有一个是leader replica，其余的是follower replica。 消息进来的时候会先存入leader replica，然后从leader replica复制到follower replica。只有复制全部完成时，consumer才可以消费此条消息。这是为了确保意外发生时，数据可以恢复。consumer的消费也是从leader replica读取的。 由此可见，leader replica做了大量的工作。所以如果不同partition的leader replica在kafka集群的broker上分布不均匀，就会造成负载不均衡。 kafka通过轮询算法保证leader replica是均匀分布在多个broker上。如下图。 可以看到每个partition的leader replica均匀的分布在三个broker上，follower replica也是均匀分布的。关于Replica，有如下知识点： 1、Replica均匀分配在Broker上，同一个partition的replica不会在同一个borker上 2、同一个partition的Replica数量不能多于broker数量。多个replica为了数据安全，一台server存多个replica没有意义。server挂掉，上面的副本都要挂掉。 3、分区的leader replica均衡分布在broker上。此时集群的负载是均衡的。这就叫做分区平衡 分区平衡是个很重要的概念，接下来我们就来讲解分区平衡。 分区平衡在讲分区平衡前，先讲几个概念： 1、AR： assigned replicas，已分配的副本。每个partition都有自己的AR列表，里面存储着这个partition最初分配的所有replica。注意AR列表不会变化，除非增加分区。 2、PR（优先replica）：AR列表中的第一个replica就是优先replica，而且永远是优先replica。最初，优先replica和leader replica是同一个replica。 3、ISR：in sync replicas，同步副本。每个partition都有自己的ISR列表。ISR是会根据同步情况动态变化的。 最初ISR列表和AR列表是一致的，但由于某个节点死掉，或者某个节点的follower replica落后leader replica太多，那么该节点就会被从ISR列表中移除。此时，ISR和AR就不再一致 接下来我们通过一个例子来理解分区平衡。 1、根据以上信息，一个拥有3个replica的partition，最初是下图的样子。 可以看到AR和ISR保持一致，并且初始时刻，优先副本和leader副本都指向replica 0. 2、接下来，replica 0所在的机器下线了，那么情况会变成如下图所示： 可以看到replica 0已经从ISR中移除掉了。同时，由于重新选举，leader副本变成了replica 1，而优先副本还是replica 0。优先副本是不会改变的。 由于最初时，leader副本在broker均匀分布，分区是平衡的。但此时，由于此partition的leader副本换成了另外一个，所以此时分区平衡已经被破坏。 3、replica 0所在的机器修复了，又重新上线，情况如下图： 可以看到replica 0重新回到ISR列表中，不过此时他没能恢复leader的身份。只能作为follower当一名小弟。此时分区依旧是不平衡的。那是否意味着分区永远都会不平衡下去呢？不是的。 4、kafka会定时触发分区平衡操作，也可以主动触发分区平衡。这就是所谓的分区平衡操作，操作完后如下图。 可以看到此时leader副本通过选举，会重新变回来replica 0，因为replica 0是优先副本，其实优先的含义就是选择leader时被优先选择。这样整个分区又回到了初始状态，而初始时，leader副本是均匀分布的。此时已经分区平衡了。 由此可见，分区平衡操作就是使leader副本和优先副本保持一致的操作。可以把优先副本理解为分区的平衡状态位，平衡操作就是让leader副本归位。 Partition的读和写通过之前的学习，我们知道topic下划分了多个partition，消息的生产和消费最终都是发生在partition之上。下图是一个三个partition的topic的读写示意。 我们先看右边的producer，可以看到写的时候，采用round-robin算法，轮询往每个partition写入。 而在消费者端，每个consumer都维护一个offset值，指向的是它所消费到的消息坐标。 我们先看group A的三个consumer，他们分别独立消费不同的三个partition。每个consumer维护了自己的offset。 我们再看group B，可以看到两个group是并行消费整个topic，同一条消息会被不同group消费到。 此处有如下知识点： 1、每个partition都是有序的不可变的。 2、Kafka可以保证partition的消费顺序，但不能保证topic消费顺序。 3、无论消费与否，保留周期默认两天（可配置）。 4、每个consumer维护的唯一元数据是offset，代表消费的位置，一般线性向后移动。 5、consumer也可以重置offset到之前的位置，可以以任何顺序消费，不一定线性后移。 回顾本章是理解kafka设计的核心，通过本章学习你应该理解如下知识点： producer consumer consumer group broker 分区（partition） 副本（replica） 分区平衡 消息读写]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>核心概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka流式简介]]></title>
    <url>%2F2019%2F08%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E6%B5%81%E5%BC%8F%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[kafka的定位 提到kafka，不太熟悉或者稍有接触的开发人员，第一想法可能会觉得它是一个消息系统。其实Kafka的定位并不止于此。 Kafka官方文档介绍说，Apache Kafka是一个分布式流平台，并给出了如下解释： 流平台有三个关键的能力： 发布订阅记录流，和消息队列或者企业新消息系统类似。 以可容错、持久的方式保存记录流 当记录流产生时就进行处理 Kafka通常用于应用中的两种广播类型： 在系统和应用间建立实时的数据管道，能够可信赖的获取数据。 建立实时的流应用，可以处理或者响应数据流。 由此可见，kafka给自身的定位并不只是一个消息系统，而是通过发布订阅消息这种机制实现了流平台。 其实不管kafka给自己的定位如何，他都逃脱不了发布订阅消息的底层机制。本文讲解的重点，也是kafka发布订阅消息的特性。 Kafka和大多数消息系统一样，搭建好kafka集群后，生产者向特定的topic生产消息，而消费者通过订阅topic，能够准实时的拉取到该topic新消息，进行消费。如下图： Kafka特性 kafka和有以下主要的特性： 消息持久化 高吞吐量 可扩展性 尤其是高吞吐量，是他的最大卖点。kafka之所以能够实现高吞吐量，是基于他自身优良的设计，及集群的可扩展性。后面章节会展开来分析。 Kafka应用场景 消息系统 日志系统 流处理]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka深入理解分区副本机制]]></title>
    <url>%2F2019%2F08%2F08%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%88%86%E5%8C%BA%E5%89%AF%E6%9C%AC%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[深入理解Kafka副本机制一、Kafka集群二、副本机制2.1 分区和副本2.2 ISR机制2.3 不完全的首领选举2.4 最少同步副本2.5 发送确认三、数据请求3.1 元数据请求机制3.2 数据可见性3.3 零拷贝四、物理存储4.1 分区分配4.2 分区数据保留规则4.3 文件格式 一、Kafka集群Kafka 使用 Zookeeper 来维护集群成员 (brokers) 的信息。每个 broker 都有一个唯一标识 broker.id，用于标识自己在集群中的身份，可以在配置文件 server.properties 中进行配置，或者由程序自动生成。下面是 Kafka brokers 集群自动创建的过程： 每一个 broker 启动的时候，它会在 Zookeeper 的 /brokers/ids 路径下创建一个 临时节点，并将自己的 broker.id 写入，从而将自身注册到集群； 当有多个 broker 时，所有 broker 会竞争性地在 Zookeeper 上创建 /controller 节点，由于 Zookeeper 上的节点不会重复，所以必然只会有一个 broker 创建成功，此时该 broker 称为 controller broker。它除了具备其他 broker 的功能外，还负责管理主题分区及其副本的状态。 当 broker 出现宕机或者主动退出从而导致其持有的 Zookeeper 会话超时时，会触发注册在 Zookeeper 上的 watcher 事件，此时 Kafka 会进行相应的容错处理；如果宕机的是 controller broker 时，还会触发新的 controller 选举。 二、副本机制为了保证高可用，kafka 的分区是多副本的，如果一个副本丢失了，那么还可以从其他副本中获取分区数据。但是这要求对应副本的数据必须是完整的，这是 Kafka 数据一致性的基础，所以才需要使用 controller broker 来进行专门的管理。下面将详解介绍 Kafka 的副本机制。 2.1 分区和副本Kafka 的主题被分为多个分区 ，分区是 Kafka 最基本的存储单位。每个分区可以有多个副本 (可以在创建主题时使用 replication-factor 参数进行指定)。其中一个副本是首领副本 (Leader replica)，所有的事件都直接发送给首领副本；其他副本是跟随者副本 (Follower replica)，需要通过复制来保持与首领副本数据一致，当首领副本不可用时，其中一个跟随者副本将成为新首领。 2.2 ISR机制每个分区都有一个 ISR(in-sync Replica) 列表，用于维护所有同步的、可用的副本。首领副本必然是同步副本，而对于跟随者副本来说，它需要满足以下条件才能被认为是同步副本： 与 Zookeeper 之间有一个活跃的会话，即必须定时向 Zookeeper 发送心跳； 在规定的时间内从首领副本那里低延迟地获取过消息。 如果副本不满足上面条件的话，就会被从 ISR 列表中移除，直到满足条件才会被再次加入。 这里给出一个主题创建的示例：使用 --replication-factor 指定副本系数为 3，创建成功后使用 --describe 命令可以看到分区 0 的有 0,1,2 三个副本，且三个副本都在 ISR 列表中，其中 1 为首领副本。 2.3 不完全的首领选举对于副本机制，在 broker 级别有一个可选的配置参数 unclean.leader.election.enable，默认值为 fasle，代表禁止不完全的首领选举。这是针对当首领副本挂掉且 ISR 中没有其他可用副本时，是否允许某个不完全同步的副本成为首领副本，这可能会导致数据丢失或者数据不一致，在某些对数据一致性要求较高的场景 (如金融领域)，这可能无法容忍的，所以其默认值为 false，如果你能够允许部分数据不一致的话，可以配置为 true。 2.4 最少同步副本ISR 机制的另外一个相关参数是 min.insync.replicas , 可以在 broker 或者主题级别进行配置，代表 ISR 列表中至少要有几个可用副本。这里假设设置为 2，那么当可用副本数量小于该值时，就认为整个分区处于不可用状态。此时客户端再向分区写入数据时候就会抛出异常 org.apache.kafka.common.errors.NotEnoughReplicasExceptoin: Messages are rejected since there are fewer in-sync replicas than required。 2.5 发送确认Kafka 在生产者上有一个可选的参数 ack，该参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入成功： acks=0 ：消息发送出去就认为已经成功了，不会等待任何来自服务器的响应； acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应； acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。 三、数据请求3.1 元数据请求机制在所有副本中，只有领导副本才能进行消息的读写处理。由于不同分区的领导副本可能在不同的 broker 上，如果某个 broker 收到了一个分区请求，但是该分区的领导副本并不在该 broker 上，那么它就会向客户端返回一个 Not a Leader for Partition 的错误响应。 为了解决这个问题，Kafka 提供了元数据请求机制。 首先集群中的每个 broker 都会缓存所有主题的分区副本信息，客户端会定期发送发送元数据请求，然后将获取的元数据进行缓存。定时刷新元数据的时间间隔可以通过为客户端配置 metadata.max.age.ms 来进行指定。有了元数据信息后，客户端就知道了领导副本所在的 broker，之后直接将读写请求发送给对应的 broker 即可。 如果在定时请求的时间间隔内发生的分区副本的选举，则意味着原来缓存的信息可能已经过时了，此时还有可能会收到 Not a Leader for Partition 的错误响应，这种情况下客户端会再次求发出元数据请求，然后刷新本地缓存，之后再去正确的 broker 上执行对应的操作，过程如下图： 3.2 数据可见性需要注意的是，并不是所有保存在分区首领上的数据都可以被客户端读取到，为了保证数据一致性，只有被所有同步副本 (ISR 中所有副本) 都保存了的数据才能被客户端读取到。 3.3 零拷贝Kafka 所有数据的写入和读取都是通过零拷贝来实现的。传统拷贝与零拷贝的区别如下： 传统模式下的四次拷贝与四次上下文切换以将磁盘文件通过网络发送为例。传统模式下，一般使用如下伪代码所示的方法先将文件数据读入内存，然后通过 Socket 将内存中的数据发送出去。 12buffer = File.readSocket.send(buffer) 这一过程实际上发生了四次数据拷贝。首先通过系统调用将文件数据读入到内核态 Buffer（DMA 拷贝），然后应用程序将内存态 Buffer 数据读入到用户态 Buffer（CPU 拷贝），接着用户程序通过 Socket 发送数据时将用户态 Buffer 数据拷贝到内核态 Buffer（CPU 拷贝），最后通过 DMA 拷贝将数据拷贝到 NIC Buffer。同时，还伴随着四次上下文切换，如下图所示： sendfile和transferTo实现零拷贝Linux 2.4+ 内核通过 sendfile 系统调用，提供了零拷贝。数据通过 DMA 拷贝到内核态 Buffer 后，直接通过 DMA 拷贝到 NIC Buffer，无需 CPU 拷贝。这也是零拷贝这一说法的来源。除了减少数据拷贝外，因为整个读文件到网络发送由一个 sendfile 调用完成，整个过程只有两次上下文切换，因此大大提高了性能。零拷贝过程如下图所示： 从具体实现来看，Kafka 的数据传输通过 TransportLayer 来完成，其子类 PlaintextTransportLayer 的 transferFrom 方法通过调用 Java NIO 中 FileChannel 的 transferTo 方法实现零拷贝，如下所示： 1234@Overridepublic long transferFrom(FileChannel fileChannel, long position, long count) throws IOException &#123; return fileChannel.transferTo(position, count, socketChannel);&#125; 注： transferTo 和 transferFrom 并不保证一定能使用零拷贝。实际上是否能使用零拷贝与操作系统相关，如果操作系统提供 sendfile 这样的零拷贝系统调用，则这两个方法会通过这样的系统调用充分利用零拷贝的优势，否则并不能通过这两个方法本身实现零拷贝。 四、物理存储4.1 分区分配在创建主题时，Kafka 会首先决定如何在 broker 间分配分区副本，它遵循以下原则： 在所有 broker 上均匀地分配分区副本； 确保分区的每个副本分布在不同的 broker 上； 如果使用了 broker.rack 参数为 broker 指定了机架信息，那么会尽可能的把每个分区的副本分配到不同机架的 broker 上，以避免一个机架不可用而导致整个分区不可用。 基于以上原因，如果你在一个单节点上创建一个 3 副本的主题，通常会抛出下面的异常： 12Error while executing topic command : org.apache.kafka.common.errors.InvalidReplicationFactor Exception: Replication factor: 3 larger than available brokers: 1. 4.2 分区数据保留规则保留数据是 Kafka 的一个基本特性， 但是 Kafka 不会一直保留数据，也不会等到所有消费者都读取了消息之后才删除消息。相反， Kafka 为每个主题配置了数据保留期限，规定数据被删除之前可以保留多长时间，或者清理数据之前可以保留的数据量大小。分别对应以下四个参数： log.retention.bytes ：删除数据前允许的最大数据量；默认值-1，代表没有限制； log.retention.ms：保存数据文件的毫秒数，如果未设置，则使用 log.retention.minutes 中的值，默认为 null； log.retention.minutes：保留数据文件的分钟数，如果未设置，则使用 log.retention.hours 中的值，默认为 null； log.retention.hours：保留数据文件的小时数，默认值为 168，也就是一周。 因为在一个大文件里查找和删除消息是很费时的，也很容易出错，所以 Kafka 把分区分成若干个片段，当前正在写入数据的片段叫作活跃片段。活动片段永远不会被删除。如果按照默认值保留数据一周，而且每天使用一个新片段，那么你就会看到，在每天使用一个新片段的同时会删除一个最老的片段，所以大部分时间该分区会有 7 个片段存在。 4.3 文件格式通常保存在磁盘上的数据格式与生产者发送过来消息格式是一样的。 如果生产者发送的是压缩过的消息，那么同一个批次的消息会被压缩在一起，被当作“包装消息”进行发送 (格式如下所示) ，然后保存到磁盘上。之后消费者读取后再自己解压这个包装消息，获取每条消息的具体信息。 参考资料 Neha Narkhede, Gwen Shapira ,Todd Palino(著) , 薛命灯 (译) . Kafka 权威指南 . 人民邮电出版社 . 2017-12-26 Kafka 高性能架构之道]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>分区副本</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka消费者详解]]></title>
    <url>%2F2019%2F08%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E6%B6%88%E8%B4%B9%E8%80%85%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Kafka消费者详解一、消费者和消费者群组二、分区再均衡三、创建Kafka消费者三、 自动提交偏移量四、手动提交偏移量4.1 同步提交4.2 异步提交4.3 同步加异步提交4.4 提交特定偏移量五、监听分区再均衡六、退出轮询七、独立的消费者附录 : Kafka消费者可选属性 一、消费者和消费者群组在 Kafka 中，消费者通常是消费者群组的一部分，多个消费者群组共同读取同一个主题时，彼此之间互不影响。Kafka 之所以要引入消费者群组这个概念是因为 Kafka 消费者经常会做一些高延迟的操作，比如把数据写到数据库或 HDFS ，或者进行耗时的计算，在这些情况下，单个消费者无法跟上数据生成的速度。此时可以增加更多的消费者，让它们分担负载，分别处理部分分区的消息，这就是 Kafka 实现横向伸缩的主要手段。 需要注意的是：同一个分区只能被同一个消费者群组里面的一个消费者读取，不可能存在同一个分区被同一个消费者群里多个消费者共同读取的情况，如图： 可以看到即便消费者 Consumer5 空闲了，但是也不会去读取任何一个分区的数据，这同时也提醒我们在使用时应该合理设置消费者的数量，以免造成闲置和额外开销。 二、分区再均衡因为群组里的消费者共同读取主题的分区，所以当一个消费者被关闭或发生崩溃时，它就离开了群组，原本由它读取的分区将由群组里的其他消费者来读取。同时在主题发生变化时 ， 比如添加了新的分区，也会发生分区与消费者的重新分配，分区的所有权从一个消费者转移到另一个消费者，这样的行为被称为再均衡。正是因为再均衡，所以消费费者群组才能保证高可用性和伸缩性。 消费者通过向群组协调器所在的 broker 发送心跳来维持它们和群组的从属关系以及它们对分区的所有权。只要消费者以正常的时间间隔发送心跳，就被认为是活跃的，说明它还在读取分区里的消息。消费者会在轮询消息或提交偏移量时发送心跳。如果消费者停止发送心跳的时间足够长，会话就会过期，群组协调器认为它已经死亡，就会触发再均衡。 三、创建Kafka消费者在创建消费者的时候以下以下三个选项是必选的： bootstrap.servers ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错； key.deserializer ：指定键的反序列化器； value.deserializer ：指定值的反序列化器。 除此之外你还需要指明你需要想订阅的主题，可以使用如下两个 API : consumer.subscribe(Collection\ topics) ：指明需要订阅的主题的集合； consumer.subscribe(Pattern pattern) ：使用正则来匹配需要订阅的集合。 最后只需要通过轮询 API(poll) 向服务器定时请求数据。一旦消费者订阅了主题，轮询就会处理所有的细节，包括群组协调、分区再均衡、发送心跳和获取数据，这使得开发者只需要关注从分区返回的数据，然后进行业务处理。 示例如下： 12345678910111213141516171819202122232425String topic = "Hello-Kafka";String group = "group1";Properties props = new Properties();props.put("bootstrap.servers", "hadoop001:9092");/*指定分组 ID*/props.put("group.id", group);props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props);/*订阅主题 (s)*/consumer.subscribe(Collections.singletonList(topic));try &#123; while (true) &#123; /*轮询获取数据*/ ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.printf("topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n", record.topic(), record.partition(), record.key(), record.value(), record.offset()); &#125; &#125;&#125; finally &#123; consumer.close();&#125; 本篇文章的所有示例代码可以从 Github 上进行下载：kafka-basis 三、 自动提交偏移量3.1 偏移量的重要性Kafka 的每一条消息都有一个偏移量属性，记录了其在分区中的位置，偏移量是一个单调递增的整数。消费者通过往一个叫作 ＿consumer_offset 的特殊主题发送消息，消息里包含每个分区的偏移量。 如果消费者一直处于运行状态，那么偏移量就没有什么用处。不过，如果有消费者退出或者新分区加入，此时就会触发再均衡。完成再均衡之后，每个消费者可能分配到新的分区，而不是之前处理的那个。为了能够继续之前的工作，消费者需要读取每个分区最后一次提交的偏移量，然后从偏移量指定的地方继续处理。 因为这个原因，所以如果不能正确提交偏移量，就可能会导致数据丢失或者重复出现消费，比如下面情况： 如果提交的偏移量小于客户端处理的最后一个消息的偏移量 ，那么处于两个偏移量之间的消息就会被重复消费； 如果提交的偏移量大于客户端处理的最后一个消息的偏移量，那么处于两个偏移量之间的消息将会丢失。 3.2 自动提交偏移量Kafka 支持自动提交和手动提交偏移量两种方式。这里先介绍比较简单的自动提交： 只需要将消费者的 enable.auto.commit 属性配置为 true 即可完成自动提交的配置。 此时每隔固定的时间，消费者就会把 poll() 方法接收到的最大偏移量进行提交，提交间隔由 auto.commit.interval.ms 属性进行配置，默认值是 5s。 使用自动提交是存在隐患的，假设我们使用默认的 5s 提交时间间隔，在最近一次提交之后的 3s 发生了再均衡，再均衡之后，消费者从最后一次提交的偏移量位置开始读取消息。这个时候偏移量已经落后了 3s ，所以在这 3s 内到达的消息会被重复处理。可以通过修改提交时间间隔来更频繁地提交偏移量，减小可能出现重复消息的时间窗，不过这种情况是无法完全避免的。基于这个原因，Kafka 也提供了手动提交偏移量的 API，使得用户可以更为灵活的提交偏移量。 四、手动提交偏移量用户可以通过将 enable.auto.commit 设为 false，然后手动提交偏移量。基于用户需求手动提交偏移量可以分为两大类： 手动提交当前偏移量：即手动提交当前轮询的最大偏移量； 手动提交固定偏移量：即按照业务需求，提交某一个固定的偏移量。 而按照 Kafka API，手动提交偏移量又可以分为同步提交和异步提交。 4.1 同步提交通过调用 consumer.commitSync() 来进行同步提交，不传递任何参数时提交的是当前轮询的最大偏移量。 12345678while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#125; /*同步提交*/ consumer.commitSync();&#125; 如果某个提交失败，同步提交还会进行重试，这可以保证数据能够最大限度提交成功，但是同时也会降低程序的吞吐量。基于这个原因，Kafka 还提供了异步提交的 API。 4.2 异步提交异步提交可以提高程序的吞吐量，因为此时你可以尽管请求数据，而不用等待 Broker 的响应。代码如下： 1234567891011121314151617while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#125; /*异步提交并定义回调*/ consumer.commitAsync(new OffsetCommitCallback() &#123; @Override public void onComplete(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, Exception exception) &#123; if (exception != null) &#123; System.out.println("错误处理"); offsets.forEach((x, y) -&gt; System.out.printf("topic = %s,partition = %d, offset = %s \n", x.topic(), x.partition(), y.offset())); &#125; &#125; &#125;);&#125; 异步提交存在的问题是，在提交失败的时候不会进行自动重试，实际上也不能进行自动重试。假设程序同时提交了 200 和 300 的偏移量，此时 200 的偏移量失败的，但是紧随其后的 300 的偏移量成功了，此时如果重试就会存在 200 覆盖 300 偏移量的可能。同步提交就不存在这个问题，因为在同步提交的情况下，300 的提交请求必须等待服务器返回 200 提交请求的成功反馈后才会发出。基于这个原因，某些情况下，需要同时组合同步和异步两种提交方式。 注：虽然程序不能在失败时候进行自动重试，但是我们是可以手动进行重试的，你可以通过一个 Map&lt;TopicPartition, Integer&gt; offsets 来维护你提交的每个分区的偏移量，然后当失败时候，你可以判断失败的偏移量是否小于你维护的同主题同分区的最后提交的偏移量，如果小于则代表你已经提交了更大的偏移量请求，此时不需要重试，否则就可以进行手动重试。 4.3 同步加异步提交下面这种情况，在正常的轮询中使用异步提交来保证吞吐量，但是因为在最后即将要关闭消费者了，所以此时需要用同步提交来保证最大限度的提交成功。 12345678910111213141516171819try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); &#125; // 异步提交 consumer.commitAsync(); &#125;&#125; catch (Exception e) &#123; e.printStackTrace();&#125; finally &#123; try &#123; // 因为即将要关闭消费者，所以要用同步提交保证提交成功 consumer.commitSync(); &#125; finally &#123; consumer.close(); &#125;&#125; 4.4 提交特定偏移量在上面同步和异步提交的 API 中，实际上我们都没有对 commit 方法传递参数，此时默认提交的是当前轮询的最大偏移量，如果你需要提交特定的偏移量，可以调用它们的重载方法。 1234/*同步提交特定偏移量*/commitSync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets) /*异步提交特定偏移量*/ commitAsync(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets, OffsetCommitCallback callback) 需要注意的是，因为你可以订阅多个主题，所以 offsets 中必须要包含所有主题的每个分区的偏移量，示例代码如下： 1234567891011121314151617try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); /*记录每个主题的每个分区的偏移量*/ TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition()); OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(record.offset()+1, "no metaData"); /*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/ offsets.put(topicPartition, offsetAndMetadata); &#125; /*提交特定偏移量*/ consumer.commitAsync(offsets, null); &#125;&#125; finally &#123; consumer.close();&#125; 五、监听分区再均衡因为分区再均衡会导致分区与消费者的重新划分，有时候你可能希望在再均衡前执行一些操作：比如提交已经处理但是尚未提交的偏移量，关闭数据库连接等。此时可以在订阅主题时候，调用 subscribe 的重载方法传入自定义的分区再均衡监听器。 1234 /*订阅指定集合内的所有主题*/subscribe(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener) /*使用正则匹配需要订阅的主题*/ subscribe(Pattern pattern, ConsumerRebalanceListener listener) 代码示例如下： 123456789101112131415161718192021222324252627282930313233Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets = new HashMap&lt;&gt;();consumer.subscribe(Collections.singletonList(topic), new ConsumerRebalanceListener() &#123; /*该方法会在消费者停止读取消息之后，再均衡开始之前就调用*/ @Override public void onPartitionsRevoked(Collection&lt;TopicPartition&gt; partitions) &#123; System.out.println("再均衡即将触发"); // 提交已经处理的偏移量 consumer.commitSync(offsets); &#125; /*该方法会在重新分配分区之后，消费者开始读取消息之前被调用*/ @Override public void onPartitionsAssigned(Collection&lt;TopicPartition&gt; partitions) &#123; &#125;&#125;);try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; record : records) &#123; System.out.println(record); TopicPartition topicPartition = new TopicPartition(record.topic(), record.partition()); OffsetAndMetadata offsetAndMetadata = new OffsetAndMetadata(record.offset() + 1, "no metaData"); /*TopicPartition 重写过 hashCode 和 equals 方法，所以能够保证同一主题和分区的实例不会被重复添加*/ offsets.put(topicPartition, offsetAndMetadata); &#125; consumer.commitAsync(offsets, null); &#125;&#125; finally &#123; consumer.close();&#125; 六 、退出轮询Kafka 提供了 consumer.wakeup() 方法用于退出轮询，它通过抛出 WakeupException 异常来跳出循环。需要注意的是，在退出线程时最好显示的调用 consumer.close() , 此时消费者会提交任何还没有提交的东西，并向群组协调器发送消息，告知自己要离开群组，接下来就会触发再均衡 ，而不需要等待会话超时。 下面的示例代码为监听控制台输出，当输入 exit 时结束轮询，关闭消费者并退出程序： 1234567891011121314151617181920212223242526272829303132/*调用 wakeup 优雅的退出*/final Thread mainThread = Thread.currentThread();new Thread(() -&gt; &#123; Scanner sc = new Scanner(System.in); while (sc.hasNext()) &#123; if ("exit".equals(sc.next())) &#123; consumer.wakeup(); try &#123; /*等待主线程完成提交偏移量、关闭消费者等操作*/ mainThread.join(); break; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125;&#125;).start();try &#123; while (true) &#123; ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;String, String&gt; rd : records) &#123; System.out.printf("topic = %s,partition = %d, key = %s, value = %s, offset = %d,\n", rd.topic(), rd.partition(), rd.key(), rd.value(), rd.offset()); &#125; &#125;&#125; catch (WakeupException e) &#123; //对于 wakeup() 调用引起的 WakeupException 异常可以不必处理&#125; finally &#123; consumer.close(); System.out.println("consumer 关闭");&#125; 七、独立的消费者因为 Kafka 的设计目标是高吞吐和低延迟，所以在 Kafka 中，消费者通常都是从属于某个群组的，这是因为单个消费者的处理能力是有限的。但是某些时候你的需求可能很简单，比如可能只需要一个消费者从一个主题的所有分区或者某个特定的分区读取数据，这个时候就不需要消费者群组和再均衡了， 只需要把主题或者分区分配给消费者，然后开始读取消息井提交偏移量即可。 在这种情况下，就不需要订阅主题， 取而代之的是消费者为自己分配分区。 一个消费者可以订阅主题（井加入消费者群组），或者为自己分配分区，但不能同时做这两件事情。 分配分区的示例代码如下： 12345678910111213141516171819202122List&lt;TopicPartition&gt; partitions = new ArrayList&lt;&gt;();List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic);/*可以指定读取哪些分区 如这里假设只读取主题的 0 分区*/for (PartitionInfo partition : partitionInfos) &#123; if (partition.partition()==0)&#123; partitions.add(new TopicPartition(partition.topic(), partition.partition())); &#125;&#125;// 为消费者指定分区consumer.assign(partitions);while (true) &#123; ConsumerRecords&lt;Integer, String&gt; records = consumer.poll(Duration.of(100, ChronoUnit.MILLIS)); for (ConsumerRecord&lt;Integer, String&gt; record : records) &#123; System.out.printf("partition = %s, key = %d, value = %s\n", record.partition(), record.key(), record.value()); &#125; consumer.commitSync();&#125; 附录 : Kafka消费者可选属性1. fetch.min.byte消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。 2. fetch.max.wait.msbroker 返回给消费者数据的等待时间，默认是 500ms。 3. max.partition.fetch.bytes该属性指定了服务器从每个分区返回给消费者的最大字节数，默认为 1MB。 4. session.timeout.ms消费者在被认为死亡之前可以与服务器断开连接的时间，默认是 3s。 5. auto.offset.reset该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理： latest (默认值) ：在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的最新记录）; earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。 6. enable.auto.commit是否自动提交偏移量，默认值是 true。为了避免出现重复消费和数据丢失，可以把它设置为 false。 7. client.id客户端 id，服务器用来识别消息的来源。 8. max.poll.records单次调用 poll() 方法能够返回的记录数量。 9. receive.buffer.bytes &amp; send.buffer.byte这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。 参考资料 Neha Narkhede, Gwen Shapira ,Todd Palino(著) , 薛命灯 (译) . Kafka 权威指南 . 人民邮电出版社 . 2017-12-26]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>消费者</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka生产者详解]]></title>
    <url>%2F2019%2F08%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E7%94%9F%E4%BA%A7%E8%80%85%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Kafka生产者详解一、生产者发送消息的过程二、创建生产者二、发送消息2.1 同步发送2.2 异步发送三、自定义分区器四、生产者其他属性 一、生产者发送消息的过程首先介绍一下 Kafka 生产者发送消息的过程： Kafka 会将发送消息包装为 ProducerRecord 对象， ProducerRecord 对象包含了目标主题和要发送的内容，同时还可以指定键和分区。在发送 ProducerRecord 对象前，生产者会先把键和值对象序列化成字节数组，这样它们才能够在网络上传输。 接下来，数据被传给分区器。如果之前已经在 ProducerRecord 对象里指定了分区，那么分区器就不会再做任何事情。如果没有指定分区 ，那么分区器会根据 ProducerRecord 对象的键来选择一个分区，紧接着，这条记录被添加到一个记录批次里，这个批次里的所有消息会被发送到相同的主题和分区上。有一个独立的线程负责把这些记录批次发送到相应的 broker 上。 服务器在收到这些消息时会返回一个响应。如果消息成功写入 Kafka，就返回一个 RecordMetaData 对象，它包含了主题和分区信息，以及记录在分区里的偏移量。如果写入失败，则会返回一个错误。生产者在收到错误之后会尝试重新发送消息，如果达到指定的重试次数后还没有成功，则直接抛出异常，不再重试。 二、创建生产者2.1 项目依赖本项目采用 Maven 构建，想要调用 Kafka 生产者 API，需要导入 kafka-clients 依赖，如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;2.2.0&lt;/version&gt;&lt;/dependency&gt; 2.2 创建生产者创建 Kafka 生产者时，以下三个属性是必须指定的： bootstrap.servers ：指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找 broker 的信息。不过建议至少要提供两个 broker 的信息作为容错； key.serializer ：指定键的序列化器； value.serializer ：指定值的序列化器。 创建的示例代码如下： 1234567891011121314151617181920212223public class SimpleProducer &#123; public static void main(String[] args) &#123; String topicName = "Hello-Kafka"; Properties props = new Properties(); props.put("bootstrap.servers", "hadoop001:9092"); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); /*创建生产者*/ Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 10; i++) &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, "hello" + i, "world" + i); /* 发送消息*/ producer.send(record); &#125; /*关闭生产者*/ producer.close(); &#125;&#125; 本篇文章的所有示例代码可以从 Github 上进行下载：kafka-basis 2.3 测试1. 启动KakfaKafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的： 12345# zookeeper启动命令bin/zkServer.sh start# 内置zookeeper启动命令bin/zookeeper-server-start.sh config/zookeeper.properties 启动单节点 kafka 用于测试： 1# bin/kafka-server-start.sh config/server.properties 2. 创建topic12345678# 创建用于测试主题bin/kafka-topics.sh --create \ --bootstrap-server hadoop001:9092 \ --replication-factor 1 --partitions 1 \ --topic Hello-Kafka# 查看所有主题 bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092 3. 启动消费者 启动一个控制台消费者用于观察写入情况，启动命令如下： 1# bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic Hello-Kafka --from-beginning 4. 运行项目此时可以看到消费者控制台，输出如下，这里 kafka-console-consumer 只会打印出值信息，不会打印出键信息。 2.4 可能出现的问题在这里可能出现的一个问题是：生产者程序在启动后，一直处于等待状态。这通常出现在你使用默认配置启动 Kafka 的情况下，此时需要对 server.properties 文件中的 listeners 配置进行更改： 12# hadoop001 为我启动kafka服务的主机名，你可以换成自己的主机名或者ip地址listeners=PLAINTEXT://hadoop001:9092 二、发送消息上面的示例程序调用了 send 方法发送消息后没有做任何操作，在这种情况下，我们没有办法知道消息发送的结果。想要知道消息发送的结果，可以使用同步发送或者异步发送来实现。 2.1 同步发送在调用 send 方法后可以接着调用 get() 方法，send 方法的返回值是一个 Future\对象，RecordMetadata 里面包含了发送消息的主题、分区、偏移量等信息。改写后的代码如下： 1234567891011for (int i = 0; i &lt; 10; i++) &#123; try &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, "k" + i, "world" + i); /*同步发送消息*/ RecordMetadata metadata = producer.send(record).get(); System.out.printf("topic=%s, partition=%d, offset=%s \n", metadata.topic(), metadata.partition(), metadata.offset()); &#125; catch (InterruptedException | ExecutionException e) &#123; e.printStackTrace(); &#125;&#125; 此时得到的输出如下：偏移量和调用次数有关，所有记录都分配到了 0 分区，这是因为在创建 Hello-Kafka 主题时候，使用 --partitions 指定其分区数为 1，即只有一个分区。 12345678910topic=Hello-Kafka, partition=0, offset=40 topic=Hello-Kafka, partition=0, offset=41 topic=Hello-Kafka, partition=0, offset=42 topic=Hello-Kafka, partition=0, offset=43 topic=Hello-Kafka, partition=0, offset=44 topic=Hello-Kafka, partition=0, offset=45 topic=Hello-Kafka, partition=0, offset=46 topic=Hello-Kafka, partition=0, offset=47 topic=Hello-Kafka, partition=0, offset=48 topic=Hello-Kafka, partition=0, offset=49 2.2 异步发送通常我们并不关心发送成功的情况，更多关注的是失败的情况，因此 Kafka 提供了异步发送和回调函数。 代码如下： 123456789101112131415for (int i = 0; i &lt; 10; i++) &#123; ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topicName, "k" + i, "world" + i); /*异步发送消息，并监听回调*/ producer.send(record, new Callback() &#123; @Override public void onCompletion(RecordMetadata metadata, Exception exception) &#123; if (exception != null) &#123; System.out.println("进行异常处理"); &#125; else &#123; System.out.printf("topic=%s, partition=%d, offset=%s \n", metadata.topic(), metadata.partition(), metadata.offset()); &#125; &#125; &#125;);&#125; 三、自定义分区器Kafka 有着默认的分区机制： 如果键值为 null， 则使用轮询 (Round Robin) 算法将消息均衡地分布到各个分区上； 如果键值不为 null，那么 Kafka 会使用内置的散列算法对键进行散列，然后分布到各个分区上。 某些情况下，你可能有着自己的分区需求，这时候可以采用自定义分区器实现。这里给出一个自定义分区器的示例： 3.1 自定义分区器12345678910111213141516171819202122232425/** * 自定义分区器 */public class CustomPartitioner implements Partitioner &#123; private int passLine; @Override public void configure(Map&lt;String, ?&gt; configs) &#123; /*从生产者配置中获取分数线*/ passLine = (Integer) configs.get("pass.line"); &#125; @Override public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) &#123; /*key 值为分数，当分数大于分数线时候，分配到 1 分区，否则分配到 0 分区*/ return (Integer) key &gt;= passLine ? 1 : 0; &#125; @Override public void close() &#123; System.out.println("分区器关闭"); &#125;&#125; 需要在创建生产者时指定分区器，和分区器所需要的配置参数： 1234567891011121314151617181920212223242526272829public class ProducerWithPartitioner &#123; public static void main(String[] args) &#123; String topicName = "Kafka-Partitioner-Test"; Properties props = new Properties(); props.put("bootstrap.servers", "hadoop001:9092"); props.put("key.serializer", "org.apache.kafka.common.serialization.IntegerSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); /*传递自定义分区器*/ props.put("partitioner.class", "com.heibaiying.producers.partitioners.CustomPartitioner"); /*传递分区器所需的参数*/ props.put("pass.line", 6); Producer&lt;Integer, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt;= 10; i++) &#123; String score = "score:" + i; ProducerRecord&lt;Integer, String&gt; record = new ProducerRecord&lt;&gt;(topicName, i, score); /*异步发送消息*/ producer.send(record, (metadata, exception) -&gt; System.out.printf("%s, partition=%d, \n", score, metadata.partition())); &#125; producer.close(); &#125;&#125; 3.2 测试需要创建一个至少有两个分区的主题： 1234bin/kafka-topics.sh --create \ --bootstrap-server hadoop001:9092 \ --replication-factor 1 --partitions 2 \ --topic Kafka-Partitioner-Test 此时输入如下，可以看到分数大于等于 6 分的都被分到 1 分区，而小于 6 分的都被分到了 0 分区。 123456789101112score:6, partition=1, score:7, partition=1, score:8, partition=1, score:9, partition=1, score:10, partition=1, score:0, partition=0, score:1, partition=0, score:2, partition=0, score:3, partition=0, score:4, partition=0, score:5, partition=0, 分区器关闭 四、生产者其他属性上面生产者的创建都仅指定了服务地址，键序列化器、值序列化器，实际上 Kafka 的生产者还有很多可配置属性，如下： 1. acksacks 参数指定了必须要有多少个分区副本收到消息，生产者才会认为消息写入是成功的： acks=0 ： 消息发送出去就认为已经成功了，不会等待任何来自服务器的响应； acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应； acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。 2. buffer.memory设置生产者内存缓冲区的大小。 3. compression.type默认情况下，发送的消息不会被压缩。如果想要进行压缩，可以配置此参数，可选值有 snappy，gzip，lz4。 4. retries发生错误后，消息重发的次数。如果达到设定值，生产者就会放弃重试并返回错误。 5. batch.size当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。 6. linger.ms该参数制定了生产者在发送批次之前等待更多消息加入批次的时间。 7. clent.id客户端 id,服务器用来识别消息的来源。 8. max.in.flight.requests.per.connection指定了生产者在收到服务器响应之前可以发送多少个消息。它的值越高，就会占用越多的内存，不过也会提升吞吐量，把它设置为 1 可以保证消息是按照发送的顺序写入服务器，即使发生了重试。 9. timeout.ms, request.timeout.ms &amp; metadata.fetch.timeout.ms timeout.ms 指定了 borker 等待同步副本返回消息的确认时间； request.timeout.ms 指定了生产者在发送数据时等待服务器返回响应的时间； metadata.fetch.timeout.ms 指定了生产者在获取元数据（比如分区首领是谁）时等待服务器返回响应的时间。 10. max.block.ms指定了在调用 send() 方法或使用 partitionsFor() 方法获取元数据时生产者的阻塞时间。当生产者的发送缓冲区已满，或者没有可用的元数据时，这些方法会阻塞。在阻塞时间达到 max.block.ms 时，生产者会抛出超时异常。 11. max.request.size该参数用于控制生产者发送的请求大小。它可以指发送的单个消息的最大值，也可以指单个请求里所有消息总的大小。例如，假设这个值为 1000K ，那么可以发送的单个最大消息为 1000K ，或者生产者可以在单个请求里发送一个批次，该批次包含了 1000 个消息，每个消息大小为 1K。 12. receive.buffer.bytes &amp; send.buffer.byte这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。 参考资料 Neha Narkhede, Gwen Shapira ,Todd Palino(著) , 薛命灯 (译) . Kafka 权威指南 . 人民邮电出版社 . 2017-12-26]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>生产者</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之基于Zookeeper搭建Kafka高可用集群]]></title>
    <url>%2F2019%2F08%2F06%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%9F%BA%E4%BA%8EZookeeper%E6%90%AD%E5%BB%BAKafka%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[基于Zookeeper搭建Kafka高可用集群一、Zookeeper集群搭建1.1 下载 &amp; 解压1.2 修改配置1.3 标识节点1.4 启动集群1.5 集群验证二、Kafka集群搭建2.1 下载解压2.2 拷贝配置文件2.3 修改配置2.4 启动集群2.5 创建测试主题 一、Zookeeper集群搭建为保证集群高可用，Zookeeper 集群的节点数最好是奇数，最少有三个节点，所以这里搭建一个三个节点的集群。 1.1 下载 &amp; 解压下载对应版本 Zookeeper，这里我下载的版本 3.4.14。官方下载地址：https://archive.apache.org/dist/zookeeper/ 1234# 下载wget https://archive.apache.org/dist/zookeeper/zookeeper-3.4.14/zookeeper-3.4.14.tar.gz# 解压tar -zxvf zookeeper-3.4.14.tar.gz 1.2 修改配置拷贝三份 zookeeper 安装包。分别进入安装目录的 conf 目录，拷贝配置样本 zoo_sample.cfg 为 zoo.cfg 并进行修改，修改后三份配置文件内容分别如下： zookeeper01 配置： 123456789101112tickTime=2000initLimit=10syncLimit=5dataDir=/usr/local/zookeeper-cluster/data/01dataLogDir=/usr/local/zookeeper-cluster/log/01clientPort=2181# server.1 这个1是服务器的标识，可以是任意有效数字，标识这是第几个服务器节点，这个标识要写到dataDir目录下面myid文件里# 指名集群间通讯端口和选举端口server.1=127.0.0.1:2287:3387server.2=127.0.0.1:2288:3388server.3=127.0.0.1:2289:3389 如果是多台服务器，则集群中每个节点通讯端口和选举端口可相同，IP 地址修改为每个节点所在主机 IP 即可。 zookeeper02 配置，与 zookeeper01 相比，只有 dataLogDir 和 dataLogDir 不同： 12345678910tickTime=2000initLimit=10syncLimit=5dataDir=/usr/local/zookeeper-cluster/data/02dataLogDir=/usr/local/zookeeper-cluster/log/02clientPort=2182server.1=127.0.0.1:2287:3387server.2=127.0.0.1:2288:3388server.3=127.0.0.1:2289:3389 zookeeper03 配置，与 zookeeper01，02 相比，也只有 dataLogDir 和 dataLogDir 不同： 12345678910tickTime=2000initLimit=10syncLimit=5dataDir=/usr/local/zookeeper-cluster/data/03dataLogDir=/usr/local/zookeeper-cluster/log/03clientPort=2183server.1=127.0.0.1:2287:3387server.2=127.0.0.1:2288:3388server.3=127.0.0.1:2289:3389 配置参数说明： tickTime：用于计算的基础时间单元。比如 session 超时：N*tickTime； initLimit：用于集群，允许从节点连接并同步到 master 节点的初始化连接时间，以 tickTime 的倍数来表示； syncLimit：用于集群， master 主节点与从节点之间发送消息，请求和应答时间长度（心跳机制）； dataDir：数据存储位置； dataLogDir：日志目录； clientPort：用于客户端连接的端口，默认 2181 1.3 标识节点分别在三个节点的数据存储目录下新建 myid 文件,并写入对应的节点标识。Zookeeper 集群通过 myid 文件识别集群节点，并通过上文配置的节点通信端口和选举端口来进行节点通信，选举出 leader 节点。 创建存储目录： 123456# dataDirmkdir -vp /usr/local/zookeeper-cluster/data/01# dataDirmkdir -vp /usr/local/zookeeper-cluster/data/02# dataDirmkdir -vp /usr/local/zookeeper-cluster/data/03 创建并写入节点标识到 myid 文件： 123456#server1echo "1" &gt; /usr/local/zookeeper-cluster/data/01/myid#server2echo "2" &gt; /usr/local/zookeeper-cluster/data/02/myid#server3echo "3" &gt; /usr/local/zookeeper-cluster/data/03/myid 1.4 启动集群分别启动三个节点： 123456# 启动节点1/usr/app/zookeeper-cluster/zookeeper01/bin/zkServer.sh start# 启动节点2/usr/app/zookeeper-cluster/zookeeper02/bin/zkServer.sh start# 启动节点3/usr/app/zookeeper-cluster/zookeeper03/bin/zkServer.sh start 1.5 集群验证使用 jps 查看进程，并且使用 zkServer.sh status 查看集群各个节点状态。如图三个节点进程均启动成功，并且两个节点为 follower 节点，一个节点为 leader 节点。 二、Kafka集群搭建2.1 下载解压Kafka 安装包官方下载地址：http://kafka.apache.org/downloads ，本用例下载的版本为 2.2.0，下载命令： 1234# 下载wget https://www-eu.apache.org/dist/kafka/2.2.0/kafka_2.12-2.2.0.tgz# 解压tar -xzf kafka_2.12-2.2.0.tgz 这里 j 解释一下 kafka 安装包的命名规则：以 kafka_2.12-2.2.0.tgz 为例，前面的 2.12 代表 Scala 的版本号（Kafka 采用 Scala 语言进行开发），后面的 2.2.0 则代表 Kafka 的版本号。 2.2 拷贝配置文件进入解压目录的 config 目录下 ，拷贝三份配置文件： 123# cp server.properties server-1.properties# cp server.properties server-2.properties# cp server.properties server-3.properties 2.3 修改配置分别修改三份配置文件中的部分配置，如下： server-1.properties： 12345678# The id of the broker. 集群中每个节点的唯一标识broker.id=0# 监听地址listeners=PLAINTEXT://hadoop001:9092# 数据的存储位置log.dirs=/usr/local/kafka-logs/00# Zookeeper连接地址zookeeper.connect=hadoop001:2181,hadoop001:2182,hadoop001:2183 server-2.properties： 1234broker.id=1listeners=PLAINTEXT://hadoop001:9093log.dirs=/usr/local/kafka-logs/01zookeeper.connect=hadoop001:2181,hadoop001:2182,hadoop001:2183 server-3.properties： 1234broker.id=2listeners=PLAINTEXT://hadoop001:9094log.dirs=/usr/local/kafka-logs/02zookeeper.connect=hadoop001:2181,hadoop001:2182,hadoop001:2183 这里需要说明的是 log.dirs 指的是数据日志的存储位置，确切的说，就是分区数据的存储位置，而不是程序运行日志的位置。程序运行日志的位置是通过同一目录下的 log4j.properties 进行配置的。 2.4 启动集群分别指定不同配置文件，启动三个 Kafka 节点。启动后可以使用 jps 查看进程，此时应该有三个 zookeeper 进程和三个 kafka 进程。 123bin/kafka-server-start.sh config/server-1.propertiesbin/kafka-server-start.sh config/server-2.propertiesbin/kafka-server-start.sh config/server-3.properties 2.5 创建测试主题创建测试主题： 123bin/kafka-topics.sh --create --bootstrap-server hadoop001:9092 \ --replication-factor 3 \ --partitions 1 --topic my-replicated-topic 创建后可以使用以下命令查看创建的主题信息： 1bin/kafka-topics.sh --describe --bootstrap-server hadoop001:9092 --topic my-replicated-topic 可以看到分区 0 的有 0,1,2 三个副本，且三个副本都是可用副本，都在 ISR(in-sync Replica 同步副本) 列表中，其中 1 为首领副本，此时代表集群已经搭建成功。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Kafka</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Kafka基本简介]]></title>
    <url>%2F2019%2F08%2F06%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BKafka%E5%9F%BA%E6%9C%AC%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Kafka简介一、Kafka简介二、Kafka核心概念2.1 Messages And Batches2.2 Topics And Partitions2.3 Producers And Consumers2.4 Brokers And Clusters 一、简介ApacheKafka 是一个分布式的流处理平台。它具有以下特点： 支持消息的发布和订阅，类似于 RabbtMQ、ActiveMQ 等消息队列； 支持数据实时处理； 能保证消息的可靠性投递； 支持消息的持久化存储，并通过多副本分布式的存储方案来保证消息的容错； 高吞吐率，单 Broker 可以轻松处理数千个分区以及每秒百万级的消息量。 二、基本概念2.1 Messages And BatchesKafka 的基本数据单元被称为 message(消息)，为减少网络开销，提高效率，多个消息会被放入同一批次 (Batch) 中后再写入。 2.2 Topics And PartitionsKafka 的消息通过 Topics(主题) 进行分类，一个主题可以被分为若干个 Partitions(分区)，一个分区就是一个提交日志 (commit log)。消息以追加的方式写入分区，然后以先入先出的顺序读取。Kafka 通过分区来实现数据的冗余和伸缩性，分区可以分布在不同的服务器上，这意味着一个 Topic 可以横跨多个服务器，以提供比单个服务器更强大的性能。 由于一个 Topic 包含多个分区，因此无法在整个 Topic 范围内保证消息的顺序性，但可以保证消息在单个分区内的顺序性。 2.3 Producers And Consumers1. 生产者生产者负责创建消息。一般情况下，生产者在把消息均衡地分布到在主题的所有分区上，而并不关心消息会被写到哪个分区。如果我们想要把消息写到指定的分区，可以通过自定义分区器来实现。 2. 消费者消费者是消费者群组的一部分，消费者负责消费消息。消费者可以订阅一个或者多个主题，并按照消息生成的顺序来读取它们。消费者通过检查消息的偏移量 (offset) 来区分读取过的消息。偏移量是一个不断递增的数值，在创建消息时，Kafka 会把它添加到其中，在给定的分区里，每个消息的偏移量都是唯一的。消费者把每个分区最后读取的偏移量保存在 Zookeeper 或 Kafka 上，如果消费者关闭或者重启，它还可以重新获取该偏移量，以保证读取状态不会丢失。 一个分区只能被同一个消费者群组里面的一个消费者读取，但可以被不同消费者群组中所组成的多个消费者共同读取。多个消费者群组中消费者共同读取同一个主题时，彼此之间互不影响。 2.4 Brokers And Clusters一个独立的 Kafka 服务器被称为 Broker。Broker 接收来自生产者的消息，为消息设置偏移量，并提交消息到磁盘保存。Broker 为消费者提供服务，对读取分区的请求做出响应，返回已经提交到磁盘的消息。 Broker 是集群 (Cluster) 的组成部分。每一个集群都会选举出一个 Broker 作为集群控制器 (Controller)，集群控制器负责管理工作，包括将分区分配给 Broker 和监控 Broker。 在集群中，一个分区 (Partition) 从属一个 Broker，该 Broker 被称为分区的首领 (Leader)。一个分区可以分配给多个 Brokers，这个时候会发生分区复制。这种复制机制为分区提供了消息冗余，如果有一个 Broker 失效，其他 Broker 可以接管领导权。 参考资料Neha Narkhede, Gwen Shapira ,Todd Palino(著) , 薛命灯 (译) . Kafka 权威指南 . 人民邮电出版社 . 2017-12-26]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spring+Mybtais+Phoenix整合]]></title>
    <url>%2F2019%2F07%2F31%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpring%2BMybtais%2BPhoenix%E6%95%B4%E5%90%88%2F</url>
    <content type="text"><![CDATA[Spring/Spring Boot 整合 Mybatis + Phoenix一、前言二、Spring + Mybatis + Phoenix2.1 项目结构2.2 主要依赖2.3 数据库配置文件2.4 配置数据源和会话工厂2.5 Mybtais参数配置2.6 查询接口2.7 单元测试三、SpringBoot + Mybatis + Phoenix3.1 项目结构3.2 主要依赖3.3 配置数据源3.4 新建查询接口3.5 单元测试附：建表语句 一、前言使用 Spring+Mybatis 操作 Phoenix 和操作其他的关系型数据库（如 Mysql，Oracle）在配置上是基本相同的，下面会分别给出 Spring/Spring Boot 整合步骤，完整代码见本仓库： Spring + Mybatis + Phoenix SpringBoot + Mybatis + Phoenix 二、Spring + Mybatis + Phoenix2.1 项目结构 2.2 主要依赖除了 Spring 相关依赖外，还需要导入 phoenix-core 和对应的 Mybatis 依赖包 1234567891011121314151617&lt;!--mybatis 依赖包--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.6&lt;/version&gt;&lt;/dependency&gt;&lt;!--phoenix core--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;4.14.0-cdh5.14.2&lt;/version&gt;&lt;/dependency&gt; 2.3 数据库配置文件在数据库配置文件 jdbc.properties 中配置数据库驱动和 zookeeper 地址 1234# 数据库驱动phoenix.driverClassName=org.apache.phoenix.jdbc.PhoenixDriver# zookeeper地址phoenix.url=jdbc:phoenix:192.168.0.105:2181 2.4 配置数据源和会话工厂1234567891011121314151617181920212223242526272829303132333435363738&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;beans xmlns="http://www.springframework.org/schema/beans" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:context="http://www.springframework.org/schema/context" xmlns:tx="http://www.springframework.org/schema/tx" xsi:schemaLocation="http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/context http://www.springframework.org/schema/context/spring-context-4.1.xsd http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd"&gt; &lt;!-- 开启注解包扫描--&gt; &lt;context:component-scan base-package="com.heibaiying.*"/&gt; &lt;!--指定配置文件的位置--&gt; &lt;context:property-placeholder location="classpath:jdbc.properties"/&gt; &lt;!--配置数据源--&gt; &lt;bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource"&gt; &lt;!--Phoenix 配置--&gt; &lt;property name="driverClassName" value="$&#123;phoenix.driverClassName&#125;"/&gt; &lt;property name="url" value="$&#123;phoenix.url&#125;"/&gt; &lt;/bean&gt; &lt;!--配置 mybatis 会话工厂 --&gt; &lt;bean id="sqlSessionFactory" class="org.mybatis.spring.SqlSessionFactoryBean"&gt; &lt;property name="dataSource" ref="dataSource"/&gt; &lt;!--指定 mapper 文件所在的位置--&gt; &lt;property name="mapperLocations" value="classpath*:/mappers/**/*.xml"/&gt; &lt;property name="configLocation" value="classpath:mybatisConfig.xml"/&gt; &lt;/bean&gt; &lt;!--扫描注册接口 --&gt; &lt;!--作用:从接口的基础包开始递归搜索，并将它们注册为 MapperFactoryBean(只有至少一种方法的接口才会被注册;, 具体类将被忽略)--&gt; &lt;bean class="org.mybatis.spring.mapper.MapperScannerConfigurer"&gt; &lt;!--指定会话工厂 --&gt; &lt;property name="sqlSessionFactoryBeanName" value="sqlSessionFactory"/&gt; &lt;!-- 指定 mybatis 接口所在的包 --&gt; &lt;property name="basePackage" value="com.heibaiying.dao"/&gt; &lt;/bean&gt;&lt;/beans&gt; 2.5 Mybtais参数配置新建 mybtais 配置文件，按照需求配置额外参数， 更多 settings 配置项可以参考官方文档 1234567891011121314&lt;?xml version="1.0" encoding="UTF-8" ?&gt;&lt;!DOCTYPE configuration PUBLIC "-//mybatis.org//DTD Config 3.0//EN" "http://mybatis.org/dtd/mybatis-3-config.dtd"&gt;&lt;!-- mybatis 配置文件 --&gt;&lt;configuration&gt; &lt;settings&gt; &lt;!-- 开启驼峰命名 --&gt; &lt;setting name="mapUnderscoreToCamelCase" value="true"/&gt; &lt;!-- 打印查询 sql --&gt; &lt;setting name="logImpl" value="STDOUT_LOGGING"/&gt; &lt;/settings&gt;&lt;/configuration&gt; 2.6 查询接口12345678910public interface PopulationDao &#123; List&lt;USPopulation&gt; queryAll(); void save(USPopulation USPopulation); USPopulation queryByStateAndCity(@Param("state") String state, @Param("city") String city); void deleteByStateAndCity(@Param("state") String state, @Param("city") String city);&#125; 123456789101112131415161718192021222324&lt;!DOCTYPE mapper PUBLIC "-//mybatis.org//DTD Mapper 3.0//EN" "http://mybatis.org/dtd/mybatis-3-mapper.dtd"&gt;&lt;mapper namespace="com.heibaiying.dao.PopulationDao"&gt; &lt;select id="queryAll" resultType="com.heibaiying.bean.USPopulation"&gt; SELECT * FROM us_population &lt;/select&gt; &lt;insert id="save"&gt; UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; ) &lt;/insert&gt; &lt;select id="queryByStateAndCity" resultType="com.heibaiying.bean.USPopulation"&gt; SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125; &lt;/select&gt; &lt;delete id="deleteByStateAndCity"&gt; DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125; &lt;/delete&gt;&lt;/mapper&gt; 2.7 单元测试123456789101112131415161718192021222324252627282930313233343536373839@RunWith(SpringRunner.class)@ContextConfiguration(&#123;"classpath:springApplication.xml"&#125;)public class PopulationDaoTest &#123; @Autowired private PopulationDao populationDao; @Test public void queryAll() &#123; List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll(); if (USPopulationList != null) &#123; for (USPopulation USPopulation : USPopulationList) &#123; System.out.println(USPopulation.getCity() + " " + USPopulation.getPopulation()); &#125; &#125; &#125; @Test public void save() &#123; populationDao.save(new USPopulation("TX", "Dallas", 66666)); USPopulation usPopulation = populationDao.queryByStateAndCity("TX", "Dallas"); System.out.println(usPopulation); &#125; @Test public void update() &#123; populationDao.save(new USPopulation("TX", "Dallas", 99999)); USPopulation usPopulation = populationDao.queryByStateAndCity("TX", "Dallas"); System.out.println(usPopulation); &#125; @Test public void delete() &#123; populationDao.deleteByStateAndCity("TX", "Dallas"); USPopulation usPopulation = populationDao.queryByStateAndCity("TX", "Dallas"); System.out.println(usPopulation); &#125;&#125; 三、SpringBoot + Mybatis + Phoenix3.1 项目结构 3.2 主要依赖1234567891011121314&lt;!--spring 1.5 x 以上版本对应 mybatis 1.3.x (1.3.1) 关于更多 spring-boot 与 mybatis 的版本对应可以参见 &lt;a href="http://www.mybatis.org/spring-boot-starter/mybatis-spring-boot-autoconfigure/"&gt;--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis.spring.boot&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring-boot-starter&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;!--phoenix core--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;4.14.0-cdh5.14.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; spring boot 与 mybatis 版本的对应关系： MyBatis-Spring-Boot-Starter 版本 MyBatis-Spring 版本 Spring Boot 版本 1.3.x (1.3.1) 1.3 or higher 1.5 or higher 1.2.x (1.2.1) 1.3 or higher 1.4 or higher 1.1.x (1.1.1) 1.3 or higher 1.3 or higher 1.0.x (1.0.2) 1.2 or higher 1.3 or higher 3.3 配置数据源在 application.yml 中配置数据源，spring boot 2.x 版本默认采用 Hikari 作为数据库连接池，Hikari 是目前 java 平台性能最好的连接池，性能好于 druid。 1234567891011121314151617181920212223242526272829303132spring: datasource: #zookeeper 地址 url: jdbc:phoenix:192.168.0.105:2181 driver-class-name: org.apache.phoenix.jdbc.PhoenixDriver # 如果不想配置对数据库连接池做特殊配置的话,以下关于连接池的配置就不是必须的 # spring-boot 2.X 默认采用高性能的 Hikari 作为连接池 更多配置可以参考 https://github.com/brettwooldridge/HikariCP#configuration-knobs-baby type: com.zaxxer.hikari.HikariDataSource hikari: # 池中维护的最小空闲连接数 minimum-idle: 10 # 池中最大连接数，包括闲置和使用中的连接 maximum-pool-size: 20 # 此属性控制从池返回的连接的默认自动提交行为。默认为 true auto-commit: true # 允许最长空闲时间 idle-timeout: 30000 # 此属性表示连接池的用户定义名称，主要显示在日志记录和 JMX 管理控制台中，以标识池和池配置。 默认值：自动生成 pool-name: custom-hikari #此属性控制池中连接的最长生命周期，值 0 表示无限生命周期，默认 1800000 即 30 分钟 max-lifetime: 1800000 # 数据库连接超时时间,默认 30 秒，即 30000 connection-timeout: 30000 # 连接测试 sql 这个地方需要根据数据库方言差异而配置 例如 oracle 就应该写成 select 1 from dual connection-test-query: SELECT 1# mybatis 相关配置mybatis: configuration: # 是否打印 sql 语句 调试的时候可以开启 log-impl: org.apache.ibatis.logging.stdout.StdOutImpl 3.4 新建查询接口上面 Spring+Mybatis 我们使用了 XML 的方式来写 SQL，为了体现 Mybatis 支持多种方式，这里使用注解的方式来写 SQL。 12345678910111213141516@Mapperpublic interface PopulationDao &#123; @Select("SELECT * from us_population") List&lt;USPopulation&gt; queryAll(); @Insert("UPSERT INTO us_population VALUES( #&#123;state&#125;, #&#123;city&#125;, #&#123;population&#125; )") void save(USPopulation USPopulation); @Select("SELECT * FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;") USPopulation queryByStateAndCity(String state, String city); @Delete("DELETE FROM us_population WHERE state=#&#123;state&#125; AND city = #&#123;city&#125;") void deleteByStateAndCity(String state, String city);&#125; 3.5 单元测试12345678910111213141516171819202122232425262728293031323334353637383940@RunWith(SpringRunner.class)@SpringBootTestpublic class PopulationTest &#123; @Autowired private PopulationDao populationDao; @Test public void queryAll() &#123; List&lt;USPopulation&gt; USPopulationList = populationDao.queryAll(); if (USPopulationList != null) &#123; for (USPopulation USPopulation : USPopulationList) &#123; System.out.println(USPopulation.getCity() + " " + USPopulation.getPopulation()); &#125; &#125; &#125; @Test public void save() &#123; populationDao.save(new USPopulation("TX", "Dallas", 66666)); USPopulation usPopulation = populationDao.queryByStateAndCity("TX", "Dallas"); System.out.println(usPopulation); &#125; @Test public void update() &#123; populationDao.save(new USPopulation("TX", "Dallas", 99999)); USPopulation usPopulation = populationDao.queryByStateAndCity("TX", "Dallas"); System.out.println(usPopulation); &#125; @Test public void delete() &#123; populationDao.deleteByStateAndCity("TX", "Dallas"); USPopulation usPopulation = populationDao.queryByStateAndCity("TX", "Dallas"); System.out.println(usPopulation); &#125;&#125; 附：建表语句上面单元测试涉及到的测试表的建表语句如下： 12345678910111213141516CREATE TABLE IF NOT EXISTS us_population ( state CHAR(2) NOT NULL, city VARCHAR NOT NULL, population BIGINT CONSTRAINT my_pk PRIMARY KEY (state, city)); -- 测试数据UPSERT INTO us_population VALUES('NY','New York',8143197);UPSERT INTO us_population VALUES('CA','Los Angeles',3844829);UPSERT INTO us_population VALUES('IL','Chicago',2842518);UPSERT INTO us_population VALUES('TX','Houston',2016582);UPSERT INTO us_population VALUES('PA','Philadelphia',1463281);UPSERT INTO us_population VALUES('AZ','Phoenix',1461575);UPSERT INTO us_population VALUES('TX','San Antonio',1256509);UPSERT INTO us_population VALUES('CA','San Diego',1255540);UPSERT INTO us_population VALUES('CA','San Jose',912332);]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>Spring+Mybtais+Phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hbase的SQL中间层_Phoenix]]></title>
    <url>%2F2019%2F07%2F31%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHbase%E7%9A%84SQL%E4%B8%AD%E9%97%B4%E5%B1%82_Phoenix%2F</url>
    <content type="text"><![CDATA[Hbase的SQL中间层——Phoenix一、Phoenix简介二、Phoenix安装2.1 下载并解压2.2 拷贝Jar包2.3 重启 Region Servers2.4 启动Phoenix2.5 启动结果三、Phoenix 简单使用3.1 创建表3.2 插入数据3.3 修改数据3.4 删除数据3.5 查询数据3.6 退出命令3.7 扩展四、Phoenix Java API4.1 引入Phoenix core JAR包4.2 简单的Java API实例 一、Phoenix简介Phoenix 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 Phoenix 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。Phoenix 的理念是 we put sql SQL back in NOSQL，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 Spring Data JPA 或 Mybatis 等常用的持久层框架来操作 HBase。 其次 Phoenix 的性能表现也非常优异，Phoenix 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 Phoenix 成为了 HBase 最优秀的 SQL 中间层。 二、Phoenix安装 我们可以按照官方安装说明进行安装，官方说明如下： download and expand our installation tar copy the phoenix server jar that is compatible with your HBase installation into the lib directory of every region server restart the region servers add the phoenix client jar to the classpath of your HBase client download and setup SQuirrel as your SQL client so you can issue adhoc SQL against your HBase cluster 2.1 下载并解压官方针对 Apache 版本和 CDH 版本的 HBase 均提供了安装包，按需下载即可。官方下载地址: http://phoenix.apache.org/download.html 1234# 下载wget http://mirror.bit.edu.cn/apache/phoenix/apache-phoenix-4.14.0-cdh5.14.2/bin/apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz# 解压tar tar apache-phoenix-4.14.0-cdh5.14.2-bin.tar.gz 2.2 拷贝Jar包按照官方文档的说明，需要将 phoenix server jar 添加到所有 Region Servers 的安装目录的 lib 目录下。 这里由于我搭建的是 HBase 伪集群，所以只需要拷贝到当前机器的 HBase 的 lib 目录下。如果是真实集群，则使用 scp 命令分发到所有 Region Servers 机器上。 1cp /usr/app/apache-phoenix-4.14.0-cdh5.14.2-bin/phoenix-4.14.0-cdh5.14.2-server.jar /usr/app/hbase-1.2.0-cdh5.15.2/lib 2.3 重启 Region Servers1234# 停止Hbasestop-hbase.sh# 启动Hbasestart-hbase.sh 2.4 启动Phoenix在 Phoenix 解压目录下的 bin 目录下执行如下命令，需要指定 Zookeeper 的地址： 如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则默认采用内置的 Zookeeper 服务，端口为 2181； 如果是 HBase 是集群模式并采用外置的 Zookeeper 集群，则按照自己的实际情况进行指定。 1# ./sqlline.py hadoop001:2181 2.5 启动结果启动后则进入了 Phoenix 交互式 SQL 命令行，可以使用 !table 或 !tables 查看当前所有表的信息 三、Phoenix 简单使用3.1 创建表12345CREATE TABLE IF NOT EXISTS us_population ( state CHAR(2) NOT NULL, city VARCHAR NOT NULL, population BIGINT CONSTRAINT my_pk PRIMARY KEY (state, city)); 新建的表会按照特定的规则转换为 HBase 上的表，关于表的信息，可以通过 Hbase Web UI 进行查看： ### 3.2 插入数据Phoenix 中插入数据采用的是 UPSERT 而不是 INSERT,因为 Phoenix 并没有更新操作，插入相同主键的数据就视为更新，所以 UPSERT 就相当于 UPDATE+INSERT12345678910UPSERT INTO us_population VALUES('NY','New York',8143197);UPSERT INTO us_population VALUES('CA','Los Angeles',3844829);UPSERT INTO us_population VALUES('IL','Chicago',2842518);UPSERT INTO us_population VALUES('TX','Houston',2016582);UPSERT INTO us_population VALUES('PA','Philadelphia',1463281);UPSERT INTO us_population VALUES('AZ','Phoenix',1461575);UPSERT INTO us_population VALUES('TX','San Antonio',1256509);UPSERT INTO us_population VALUES('CA','San Diego',1255540);UPSERT INTO us_population VALUES('TX','Dallas',1213825);UPSERT INTO us_population VALUES('CA','San Jose',912332);### 3.3 修改数据12-- 插入主键相同的数据就视为更新UPSERT INTO us_population VALUES('NY','New York',999999); ### 3.4 删除数据1DELETE FROM us_population WHERE city='Dallas'; ### 3.5 查询数据1234SELECT state as "州",count(city) as "市",sum(population) as "热度"FROM us_populationGROUP BY stateORDER BY sum(population) DESC; 3.6 退出命令1!quit 3.7 扩展从上面的操作中可以看出，Phoenix 支持大多数标准的 SQL 语法。关于 Phoenix 支持的语法、数据类型、函数、序列等详细信息，因为涉及内容很多，可以参考其官方文档，官方文档上有详细的说明： 语法 (Grammar) ：https://phoenix.apache.org/language/index.html 函数 (Functions) ：http://phoenix.apache.org/language/functions.html 数据类型 (Datatypes) ：http://phoenix.apache.org/language/datatypes.html 序列 (Sequences) :http://phoenix.apache.org/sequences.html 联结查询 (Joins) ：http://phoenix.apache.org/joins.html 四、Phoenix Java API因为 Phoenix 遵循 JDBC 规范，并提供了对应的数据库驱动 PhoenixDriver，这使得采用 Java 语言对其进行操作的时候，就如同对其他关系型数据库一样，下面给出基本的使用示例。 4.1 引入Phoenix core JAR包如果是 maven 项目，直接在 maven 中央仓库找到对应的版本，导入依赖即可： 123456&lt;!-- https://mvnrepository.com/artifact/org.apache.phoenix/phoenix-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.phoenix&lt;/groupId&gt; &lt;artifactId&gt;phoenix-core&lt;/artifactId&gt; &lt;version&gt;4.14.0-cdh5.14.2&lt;/version&gt; &lt;/dependency&gt; 如果是普通项目，则可以从 Phoenix 解压目录下找到对应的 JAR 包，然后手动引入： ### 4.2 简单的Java API实例1234567891011121314151617181920212223242526272829303132import java.sql.Connection;import java.sql.DriverManager;import java.sql.PreparedStatement;import java.sql.ResultSet;public class PhoenixJavaApi &#123; public static void main(String[] args) throws Exception &#123; // 加载数据库驱动 Class.forName("org.apache.phoenix.jdbc.PhoenixDriver"); /* * 指定数据库地址,格式为 jdbc:phoenix:Zookeeper 地址 * 如果 HBase 采用 Standalone 模式或者伪集群模式搭建，则 HBase 默认使用内置的 Zookeeper，默认端口为 2181 */ Connection connection = DriverManager.getConnection("jdbc:phoenix:192.168.200.226:2181"); PreparedStatement statement = connection.prepareStatement("SELECT * FROM us_population"); ResultSet resultSet = statement.executeQuery(); while (resultSet.next()) &#123; System.out.println(resultSet.getString("city") + " " + resultSet.getInt("population")); &#125; statement.close(); connection.close(); &#125;&#125;结果如下： 实际的开发中我们通常都是采用第三方框架来操作数据库，如 mybatis，Hibernate，Spring Data 等。关于 Phoenix 与这些框架的整合步骤参见下一篇文章：Spring/Spring Boot + Mybatis + Phoenix 参考资料 http://phoenix.apache.org/]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>Phoenix</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hbase容灾与备份]]></title>
    <url>%2F2019%2F07%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHbase%E5%AE%B9%E7%81%BE%E4%B8%8E%E5%A4%87%E4%BB%BD%2F</url>
    <content type="text"><![CDATA[Hbase容灾与备份一、前言二、CopyTable2.1 简介2.2 命令格式2.3 常用命令2.4 更多参数三、Export/Import3.1 简介3.2 命令格式3.3 常用命令四、Snapshot4.1 简介4.2 配置4.3 常用命令 一、前言本文主要介绍 Hbase 常用的三种简单的容灾备份方案，即CopyTable、Export/Import、Snapshot。分别介绍如下： 二、CopyTable2.1 简介CopyTable可以将现有表的数据复制到新表中，具有以下特点： 支持时间区间 、row 区间 、改变表名称 、改变列族名称 、以及是否 Copy 已被删除的数据等功能； 执行命令前，需先创建与原表结构相同的新表； CopyTable 的操作是基于 HBase Client API 进行的，即采用 scan 进行查询, 采用 put 进行写入。 2.2 命令格式1Usage: CopyTable [general options] [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] &lt;tablename&gt; 2.3 常用命令 同集群下 CopyTable 1hbase org.apache.hadoop.hbase.mapreduce.CopyTable --new.name=tableCopy tableOrig 不同集群下 CopyTable 12345678# 两表名称相同的情况hbase org.apache.hadoop.hbase.mapreduce.CopyTable \--peer.adr=dstClusterZK:2181:/hbase tableOrig# 也可以指新的表名hbase org.apache.hadoop.hbase.mapreduce.CopyTable \--peer.adr=dstClusterZK:2181:/hbase \--new.name=tableCopy tableOrig 下面是一个官方给的比较完整的例子，指定开始和结束时间，集群地址，以及只复制指定的列族： 12345hbase org.apache.hadoop.hbase.mapreduce.CopyTable \--starttime=1265875194289 \--endtime=1265878794289 \--peer.adr=server1,server2,server3:2181:/hbase \--families=myOldCf:myNewCf,cf2,cf3 TestTable 2.4 更多参数可以通过 --help 查看更多支持的参数 1# hbase org.apache.hadoop.hbase.mapreduce.CopyTable --help 三、Export/Import3.1 简介 Export 支持导出数据到 HDFS, Import 支持从 HDFS 导入数据。Export 还支持指定导出数据的开始时间和结束时间，因此可以用于增量备份。 Export 导出与 CopyTable 一样，依赖 HBase 的 scan 操作 3.2 命令格式12345# Exporthbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]# Inporthbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt; 导出的 outputdir 目录可以不用预先创建，程序会自动创建。导出完成后，导出文件的所有权将由执行导出命令的用户所拥有。 默认情况下，仅导出给定 Cell 的最新版本，而不管历史版本。要导出多个版本，需要将 &lt;versions&gt; 参数替换为所需的版本数。 3.3 常用命令 导出命令 1hbase org.apache.hadoop.hbase.mapreduce.Export tableName hdfs 路径/tableName.db 导入命令 1hbase org.apache.hadoop.hbase.mapreduce.Import tableName hdfs 路径/tableName.db 四、Snapshot4.1 简介HBase 的快照 (Snapshot) 功能允许您获取表的副本 (包括内容和元数据)，并且性能开销很小。因为快照存储的仅仅是表的元数据和 HFiles 的信息。快照的 clone 操作会从该快照创建新表，快照的 restore 操作会将表的内容还原到快照节点。clone 和 restore 操作不需要复制任何数据，因为底层 HFiles(包含 HBase 表数据的文件) 不会被修改，修改的只是表的元数据信息。 4.2 配置HBase 快照功能默认没有开启，如果要开启快照，需要在 hbase-site.xml 文件中添加如下配置项： 1234&lt;property&gt; &lt;name&gt;hbase.snapshot.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 4.3 常用命令快照的所有命令都需要在 Hbase Shell 交互式命令行中执行。 1. Take a Snapshot12# 拍摄快照hbase&gt; snapshot '表名', '快照名' 默认情况下拍摄快照之前会在内存中执行数据刷新。以保证内存中的数据包含在快照中。但是如果你不希望包含内存中的数据，则可以使用 SKIP_FLUSH 选项禁止刷新。 12# 禁止内存刷新hbase&gt; snapshot '表名', '快照名', &#123;SKIP_FLUSH =&gt; true&#125; 2. Listing Snapshots12# 获取快照列表hbase&gt; list_snapshots 3. Deleting Snapshots12# 删除快照hbase&gt; delete_snapshot '快照名' 4. Clone a table from snapshot12# 从现有的快照创建一张新表hbase&gt; clone_snapshot '快照名', '新表名' 5. Restore a snapshot将表恢复到快照节点，恢复操作需要先禁用表 12hbase&gt; disable '表名'hbase&gt; restore_snapshot '快照名' 这里需要注意的是：是如果 HBase 配置了基于 Replication 的主从复制，由于 Replication 在日志级别工作，而快照在文件系统级别工作，因此在还原之后，会出现副本与主服务器处于不同的状态的情况。这时候可以先停止同步，所有服务器还原到一致的数据点后再重新建立同步。 参考资料 Online Apache HBase Backups with CopyTable Apache HBase ™ Reference Guide]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>容灾备份</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hbase协处理器详解]]></title>
    <url>%2F2019%2F07%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHbase%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Hbase 协处理器一、简述二、协处理器类型2.1 Observer协处理器2.2 Endpoint协处理器三、协处理的加载方式四、静态加载与卸载4.1 静态加载4.2 静态卸载五、动态加载与卸载5.1 HBase Shell动态加载5.2 HBase Shell动态卸载5.3 Java API 动态加载5.4 Java API 动态卸载六、协处理器案例 一、简述在使用 HBase 时，如果你的数据量达到了数十亿行或数百万列，此时能否在查询中返回大量数据将受制于网络的带宽，即便网络状况允许，但是客户端的计算处理也未必能够满足要求。在这种情况下，协处理器（Coprocessors）应运而生。它允许你将业务计算代码放入在 RegionServer 的协处理器中，将处理好的数据再返回给客户端，这可以极大地降低需要传输的数据量，从而获得性能上的提升。同时协处理器也允许用户扩展实现 HBase 目前所不具备的功能，如权限校验、二级索引、完整性约束等。 二、协处理器类型2.1 Observer协处理器1. 功能Observer 协处理器类似于关系型数据库中的触发器，当发生某些事件的时候这类协处理器会被 Server 端调用。通常可以用来实现下面功能： 权限校验：在执行 Get 或 Put 操作之前，您可以使用 preGet 或 prePut 方法检查权限； 完整性约束： HBase 不支持关系型数据库中的外键功能，可以通过触发器在插入或者删除数据的时候，对关联的数据进行检查； 二级索引： 可以使用协处理器来维护二级索引。 2. 类型当前 Observer 协处理器有以下四种类型： RegionObserver :允许您观察 Region 上的事件，例如 Get 和 Put 操作。 RegionServerObserver :允许您观察与 RegionServer 操作相关的事件，例如启动，停止或执行合并，提交或回滚。 MasterObserver :允许您观察与 HBase Master 相关的事件，例如表创建，删除或 schema 修改。 WalObserver :允许您观察与预写日志（WAL）相关的事件。 3. 接口以上四种类型的 Observer 协处理器均继承自 Coprocessor 接口，这四个接口中分别定义了所有可用的钩子方法，以便在对应方法前后执行特定的操作。通常情况下，我们并不会直接实现上面接口，而是继承其 Base 实现类，Base 实现类只是简单空实现了接口中的方法，这样我们在实现自定义的协处理器时，就不必实现所有方法，只需要重写必要方法即可。 这里以 RegionObservers 为例，其接口类中定义了所有可用的钩子方法，下面截取了部分方法的定义，多数方法都是成对出现的，有 pre 就有 post： 4. 执行流程 客户端发出 put 请求 该请求被分派给合适的 RegionServer 和 region coprocessorHost 拦截该请求，然后在该表的每个 RegionObserver 上调用 prePut() 如果没有被 prePut() 拦截，该请求继续送到 region，然后进行处理 region 产生的结果再次被 CoprocessorHost 拦截，调用 postPut() 假如没有 postPut() 拦截该响应，最终结果被返回给客户端 如果大家了解 Spring，可以将这种执行方式类比于其 AOP 的执行原理即可，官方文档当中也是这样类比的： If you are familiar with Aspect Oriented Programming (AOP), you can think of a coprocessor as applying advice by intercepting a request and then running some custom code,before passing the request on to its final destination (or even changing the destination). 如果您熟悉面向切面编程（AOP），您可以将协处理器视为通过拦截请求然后运行一些自定义代码来使用 Advice，然后将请求传递到其最终目标（或者更改目标）。 2.2 Endpoint协处理器Endpoint 协处理器类似于关系型数据库中的存储过程。客户端可以调用 Endpoint 协处理器在服务端对数据进行处理，然后再返回。 以聚集操作为例，如果没有协处理器，当用户需要找出一张表中的最大数据，即 max 聚合操作，就必须进行全表扫描，然后在客户端上遍历扫描结果，这必然会加重了客户端处理数据的压力。利用 Coprocessor，用户可以将求最大值的代码部署到 HBase Server 端，HBase 将利用底层 cluster 的多个节点并发执行求最大值的操作。即在每个 Region 范围内执行求最大值的代码，将每个 Region 的最大值在 Region Server 端计算出来，仅仅将该 max 值返回给客户端。之后客户端只需要将每个 Region 的最大值进行比较而找到其中最大的值即可。 三、协处理的加载方式要使用我们自己开发的协处理器，必须通过静态（使用 HBase 配置）或动态（使用 HBase Shell 或 Java API）加载它。 静态加载的协处理器称之为 System Coprocessor（系统级协处理器）,作用范围是整个 HBase 上的所有表，需要重启 HBase 服务； 动态加载的协处理器称之为 Table Coprocessor（表处理器），作用于指定的表，不需要重启 HBase 服务。 其加载和卸载方式分别介绍如下。 四、静态加载与卸载4.1 静态加载静态加载分以下三步： 在 hbase-site.xml 定义需要加载的协处理器。 1234&lt;property&gt; &lt;name&gt;hbase.coprocessor.region.classes&lt;/name&gt; &lt;value&gt;org.myname.hbase.coprocessor.endpoint.SumEndPoint&lt;/value&gt;&lt;/property&gt; &lt;name&gt; 标签的值必须是下面其中之一： RegionObservers 和 Endpoints 协处理器：hbase.coprocessor.region.classes WALObservers 协处理器： hbase.coprocessor.wal.classes MasterObservers 协处理器：hbase.coprocessor.master.classes &lt;value&gt; 必须是协处理器实现类的全限定类名。如果为加载指定了多个类，则类名必须以逗号分隔。 将 jar(包含代码和所有依赖项) 放入 HBase 安装目录中的 lib 目录下； 重启 HBase。 4.2 静态卸载 从 hbase-site.xml 中删除配置的协处理器的\元素及其子元素； 从类路径或 HBase 的 lib 目录中删除协处理器的 JAR 文件（可选）； 重启 HBase。 五、动态加载与卸载使用动态加载协处理器，不需要重新启动 HBase。但动态加载的协处理器是基于每个表加载的，只能用于所指定的表。此外，在使用动态加载必须使表脱机（disable）以加载协处理器。动态加载通常有两种方式：Shell 和 Java API 。 以下示例基于两个前提： coprocessor.jar 包含协处理器实现及其所有依赖项。 JAR 包存放在 HDFS 上的路径为：hdfs：// \：\ / user / \ /coprocessor.jar 5.1 HBase Shell动态加载 使用 HBase Shell 禁用表 1hbase &gt; disable 'tableName' 使用如下命令加载协处理器 123hbase &gt; alter 'tableName', METHOD =&gt; 'table_att', 'Coprocessor'=&gt;'hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar| org.myname.hbase.Coprocessor.RegionObserverExample|1073741823|arg1=1,arg2=2' Coprocessor 包含由管道（|）字符分隔的四个参数，按顺序解释如下： JAR 包路径：通常为 JAR 包在 HDFS 上的路径。关于路径以下两点需要注意： 允许使用通配符，例如：hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/*.jar 来添加指定的 JAR 包； 可以使指定目录，例如：hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/ ，这会添加目录中的所有 JAR 包，但不会搜索子目录中的 JAR 包。 类名：协处理器的完整类名。 优先级：协处理器的优先级，遵循数字的自然序，即值越小优先级越高。可以为空，在这种情况下，将分配默认优先级值。 可选参数 ：传递的协处理器的可选参数。 启用表 1hbase &gt; enable 'tableName' 验证协处理器是否已加载 1hbase &gt; describe 'tableName' 协处理器出现在 TABLE_ATTRIBUTES 属性中则代表加载成功。 5.2 HBase Shell动态卸载 禁用表 1hbase&gt; disable 'tableName' 移除表协处理器 1hbase&gt; alter 'tableName', METHOD =&gt; 'table_att_unset', NAME =&gt; 'coprocessor$1' 启用表 1hbase&gt; enable 'tableName' 5.3 Java API 动态加载123456789101112131415161718TableName tableName = TableName.valueOf("users");String path = "hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar";Configuration conf = HBaseConfiguration.create();Connection connection = ConnectionFactory.createConnection(conf);Admin admin = connection.getAdmin();admin.disableTable(tableName);HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName);HColumnDescriptor columnFamily1 = new HColumnDescriptor("personalDet");columnFamily1.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily1);HColumnDescriptor columnFamily2 = new HColumnDescriptor("salaryDet");columnFamily2.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily2);hTableDescriptor.setValue("COPROCESSOR$1", path + "|"+ RegionObserverExample.class.getCanonicalName() + "|"+ Coprocessor.PRIORITY_USER);admin.modifyTable(tableName, hTableDescriptor);admin.enableTable(tableName); 在 HBase 0.96 及其以后版本中，HTableDescriptor 的 addCoprocessor() 方法提供了一种更为简便的加载方法。 1234567891011121314151617TableName tableName = TableName.valueOf("users");Path path = new Path("hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar");Configuration conf = HBaseConfiguration.create();Connection connection = ConnectionFactory.createConnection(conf);Admin admin = connection.getAdmin();admin.disableTable(tableName);HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName);HColumnDescriptor columnFamily1 = new HColumnDescriptor("personalDet");columnFamily1.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily1);HColumnDescriptor columnFamily2 = new HColumnDescriptor("salaryDet");columnFamily2.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily2);hTableDescriptor.addCoprocessor(RegionObserverExample.class.getCanonicalName(), path,Coprocessor.PRIORITY_USER, null);admin.modifyTable(tableName, hTableDescriptor);admin.enableTable(tableName); 5.4 Java API 动态卸载卸载其实就是重新定义表但不设置协处理器。这会删除所有表上的协处理器。 123456789101112131415TableName tableName = TableName.valueOf("users");String path = "hdfs://&lt;namenode&gt;:&lt;port&gt;/user/&lt;hadoop-user&gt;/coprocessor.jar";Configuration conf = HBaseConfiguration.create();Connection connection = ConnectionFactory.createConnection(conf);Admin admin = connection.getAdmin();admin.disableTable(tableName);HTableDescriptor hTableDescriptor = new HTableDescriptor(tableName);HColumnDescriptor columnFamily1 = new HColumnDescriptor("personalDet");columnFamily1.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily1);HColumnDescriptor columnFamily2 = new HColumnDescriptor("salaryDet");columnFamily2.setMaxVersions(3);hTableDescriptor.addFamily(columnFamily2);admin.modifyTable(tableName, hTableDescriptor);admin.enableTable(tableName); 六、协处理器案例这里给出一个简单的案例，实现一个类似于 Redis 中 append 命令的协处理器，当我们对已有列执行 put 操作时候，HBase 默认执行的是 update 操作，这里我们修改为执行 append 操作。 123456789# redis append 命令示例redis&gt; EXISTS mykey(integer) 0redis&gt; APPEND mykey "Hello"(integer) 5redis&gt; APPEND mykey " World"(integer) 11redis&gt; GET mykey "Hello World" 6.1 创建测试表12# 创建一张杂志表 有文章和图片两个列族hbase &gt; create 'magazine','article','picture' 6.2 协处理器编程 完整代码可见本仓库：hbase-observer-coprocessor 新建 Maven 工程，导入下面依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-common&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt; 继承 BaseRegionObserver 实现我们自定义的 RegionObserver,对相同的 article:content 执行 put 命令时，将新插入的内容添加到原有内容的末尾，代码如下： 12345678910111213141516171819202122232425262728293031public class AppendRegionObserver extends BaseRegionObserver &#123; private byte[] columnFamily = Bytes.toBytes("article"); private byte[] qualifier = Bytes.toBytes("content"); @Override public void prePut(ObserverContext&lt;RegionCoprocessorEnvironment&gt; e, Put put, WALEdit edit, Durability durability) throws IOException &#123; if (put.has(columnFamily, qualifier)) &#123; // 遍历查询结果，获取指定列的原值 Result rs = e.getEnvironment().getRegion().get(new Get(put.getRow())); String oldValue = ""; for (Cell cell : rs.rawCells()) if (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123; oldValue = Bytes.toString(CellUtil.cloneValue(cell)); &#125; // 获取指定列新插入的值 List&lt;Cell&gt; cells = put.get(columnFamily, qualifier); String newValue = ""; for (Cell cell : cells) &#123; if (CellUtil.matchingColumn(cell, columnFamily, qualifier)) &#123; newValue = Bytes.toString(CellUtil.cloneValue(cell)); &#125; &#125; // Append 操作 put.addColumn(columnFamily, qualifier, Bytes.toBytes(oldValue + newValue)); &#125; &#125;&#125; 6.3 打包项目使用 maven 命令进行打包，打包后的文件名为 hbase-observer-coprocessor-1.0-SNAPSHOT.jar 1# mvn clean package 6.4 上传JAR包到HDFS1234# 上传项目到HDFS上的hbase目录hadoop fs -put /usr/app/hbase-observer-coprocessor-1.0-SNAPSHOT.jar /hbase# 查看上传是否成功hadoop fs -ls /hbase 6.5 加载协处理器 加载协处理器前需要先禁用表 1hbase &gt; disable 'magazine' 加载协处理器 1hbase &gt; alter 'magazine', METHOD =&gt; 'table_att', 'Coprocessor'=&gt;'hdfs://hadoop001:8020/hbase/hbase-observer-coprocessor-1.0-SNAPSHOT.jar|com.heibaiying.AppendRegionObserver|1001|' 启用表 1hbase &gt; enable 'magazine' 查看协处理器是否加载成功 1hbase &gt; desc 'magazine' 协处理器出现在 TABLE_ATTRIBUTES 属性中则代表加载成功，如下图： 6.6 测试加载结果插入一组测试数据： 1234hbase &gt; put 'magazine', 'rowkey1','article:content','Hello'hbase &gt; get 'magazine','rowkey1','article:content'hbase &gt; put 'magazine', 'rowkey1','article:content','World'hbase &gt; get 'magazine','rowkey1','article:content' 可以看到对于指定列的值已经执行了 append 操作： 插入一组对照数据： 1234hbase &gt; put 'magazine', 'rowkey1','article:author','zhangsan'hbase &gt; get 'magazine','rowkey1','article:author'hbase &gt; put 'magazine', 'rowkey1','article:author','lisi'hbase &gt; get 'magazine','rowkey1','article:author' 可以看到对于正常的列还是执行 update 操作: 6.7 卸载协处理器 卸载协处理器前需要先禁用表 1hbase &gt; disable 'magazine' 卸载协处理器 1hbase &gt; alter 'magazine', METHOD =&gt; 'table_att_unset', NAME =&gt; 'coprocessor$1' 启用表 1hbase &gt; enable 'magazine' 查看协处理器是否卸载成功 1hbase &gt; desc 'magazine' 6.8 测试卸载结果依次执行下面命令可以测试卸载是否成功 123hbase &gt; get 'magazine','rowkey1','article:content'hbase &gt; put 'magazine', 'rowkey1','article:content','Hello'hbase &gt; get 'magazine','rowkey1','article:content' 参考资料 Apache HBase Coprocessors Apache HBase Coprocessor Introduction HBase 高階知識]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>协处理器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hbase过滤器详解]]></title>
    <url>%2F2019%2F07%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHbase%E8%BF%87%E6%BB%A4%E5%99%A8%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Hbase 过滤器详解一、HBase过滤器简介二、过滤器基础2.1 Filter接口和FilterBase抽象类2.2 过滤器分类三、比较过滤器3.1 比较运算符3.2 比较器3.3 比较过滤器种类3.4 DependentColumnFilter 四、专用过滤器4.1 单列列值过滤器 (SingleColumnValueFilter)4.2 单列列值排除器 (SingleColumnValueExcludeFilter) 4.3 行键前缀过滤器 (PrefixFilter)4.4 列名前缀过滤器 (ColumnPrefixFilter)4.5 分页过滤器 (PageFilter)4.6 时间戳过滤器 (TimestampsFilter)4.7 首次行键过滤器 (FirstKeyOnlyFilter)五、包装过滤器5.1 SkipFilter过滤器5.2 WhileMatchFilter过滤器六、FilterList 一、HBase过滤器简介Hbase 提供了种类丰富的过滤器（filter）来提高数据处理的效率，用户可以通过内置或自定义的过滤器来对数据进行过滤，所有的过滤器都在服务端生效，即谓词下推（predicate push down）。这样可以保证过滤掉的数据不会被传送到客户端，从而减轻网络传输和客户端处理的压力。 二、过滤器基础2.1 Filter接口和FilterBase抽象类Filter 接口中定义了过滤器的基本方法，FilterBase 抽象类实现了 Filter 接口。所有内置的过滤器则直接或者间接继承自 FilterBase 抽象类。用户只需要将定义好的过滤器通过 setFilter 方法传递给 Scan 或 put 的实例即可。 1setFilter(Filter filter) 123456// Scan 中定义的 setFilter@Override public Scan setFilter(Filter filter) &#123; super.setFilter(filter); return this; &#125; 123456 // Get 中定义的 setFilter@Override public Get setFilter(Filter filter) &#123; super.setFilter(filter); return this; &#125; FilterBase 的所有子类过滤器如下： 说明：上图基于当前时间点（2019.4）最新的 Hbase-2.1.4 ，下文所有说明均基于此版本。 2.2 过滤器分类HBase 内置过滤器可以分为三类：分别是比较过滤器，专用过滤器和包装过滤器。分别在下面的三个小节中做详细的介绍。 三、比较过滤器所有比较过滤器均继承自 CompareFilter。创建一个比较过滤器需要两个参数，分别是比较运算符和比较器实例。 1234public CompareFilter(final CompareOp compareOp,final ByteArrayComparable comparator) &#123; this.compareOp = compareOp; this.comparator = comparator; &#125; 3.1 比较运算符 LESS (&lt;) LESS_OR_EQUAL (&lt;=) EQUAL (=) NOT_EQUAL (!=) GREATER_OR_EQUAL (&gt;=) GREATER (&gt;) NO_OP (排除所有符合条件的值) 比较运算符均定义在枚举类 CompareOperator 中 12345678910@InterfaceAudience.Publicpublic enum CompareOperator &#123; LESS, LESS_OR_EQUAL, EQUAL, NOT_EQUAL, GREATER_OR_EQUAL, GREATER, NO_OP,&#125; 注意：在 1.x 版本的 HBase 中，比较运算符定义在 CompareFilter.CompareOp 枚举类中，但在 2.0 之后这个类就被标识为 @deprecated ，并会在 3.0 移除。所以 2.0 之后版本的 HBase 需要使用 CompareOperator 这个枚举类。 3.2 比较器所有比较器均继承自 ByteArrayComparable 抽象类，常用的有以下几种： BinaryComparator : 使用 Bytes.compareTo(byte []，byte []) 按字典序比较指定的字节数组。 BinaryPrefixComparator : 按字典序与指定的字节数组进行比较，但只比较到这个字节数组的长度。 RegexStringComparator : 使用给定的正则表达式与指定的字节数组进行比较。仅支持 EQUAL 和 NOT_EQUAL 操作。 SubStringComparator : 测试给定的子字符串是否出现在指定的字节数组中，比较不区分大小写。仅支持 EQUAL 和 NOT_EQUAL 操作。 NullComparator ：判断给定的值是否为空。 BitComparator ：按位进行比较。 BinaryPrefixComparator 和 BinaryComparator 的区别不是很好理解，这里举例说明一下： 在进行 EQUAL 的比较时，如果比较器传入的是 abcd 的字节数组，但是待比较数据是 abcdefgh： 如果使用的是 BinaryPrefixComparator 比较器，则比较以 abcd 字节数组的长度为准，即 efgh 不会参与比较，这时候认为 abcd 与 abcdefgh 是满足 EQUAL 条件的； 如果使用的是 BinaryComparator 比较器，则认为其是不相等的。 3.3 比较过滤器种类比较过滤器共有五个（Hbase 1.x 版本和 2.x 版本相同），见下图： RowFilter ：基于行键来过滤数据； FamilyFilterr ：基于列族来过滤数据； QualifierFilterr ：基于列限定符（列名）来过滤数据； ValueFilterr ：基于单元格 (cell) 的值来过滤数据； DependentColumnFilter ：指定一个参考列来过滤其他列的过滤器，过滤的原则是基于参考列的时间戳来进行筛选 。 前四种过滤器的使用方法相同，均只要传递比较运算符和运算器实例即可构建，然后通过 setFilter 方法传递给 scan： 123Filter filter = new RowFilter(CompareOperator.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes("xxx"))); scan.setFilter(filter); DependentColumnFilter 的使用稍微复杂一点，这里单独做下说明。 3.4 DependentColumnFilter可以把 DependentColumnFilter 理解为一个 valueFilter 和一个时间戳过滤器的组合。DependentColumnFilter 有三个带参构造器，这里选择一个参数最全的进行说明： 123DependentColumnFilter(final byte [] family, final byte[] qualifier, final boolean dropDependentColumn, final CompareOperator op, final ByteArrayComparable valueComparator) family ：列族 qualifier ：列限定符（列名） dropDependentColumn ：决定参考列是否被包含在返回结果内，为 true 时表示参考列被返回，为 false 时表示被丢弃 op ：比较运算符 valueComparator ：比较器 这里举例进行说明： 123456DependentColumnFilter dependentColumnFilter = new DependentColumnFilter( Bytes.toBytes("student"), Bytes.toBytes("name"), false, CompareOperator.EQUAL, new BinaryPrefixComparator(Bytes.toBytes("xiaolan"))); 首先会去查找 student:name 中值以 xiaolan 开头的所有数据获得 参考数据集，这一步等同于 valueFilter 过滤器； 其次再用参考数据集中所有数据的时间戳去检索其他列，获得时间戳相同的其他列的数据作为 结果数据集，这一步等同于时间戳过滤器； 最后如果 dropDependentColumn 为 true，则返回 参考数据集+结果数据集，若为 false，则抛弃参考数据集，只返回 结果数据集。 四、专用过滤器专用过滤器通常直接继承自 FilterBase，适用于范围更小的筛选规则。 4.1 单列列值过滤器 (SingleColumnValueFilter)基于某列（参考列）的值决定某行数据是否被过滤。其实例有以下方法： setFilterIfMissing(boolean filterIfMissing) ：默认值为 false，即如果该行数据不包含参考列，其依然被包含在最后的结果中；设置为 true 时，则不包含； setLatestVersionOnly(boolean latestVersionOnly) ：默认为 true，即只检索参考列的最新版本数据；设置为 false，则检索所有版本数据。 1234567SingleColumnValueFilter singleColumnValueFilter = new SingleColumnValueFilter( "student".getBytes(), "name".getBytes(), CompareOperator.EQUAL, new SubstringComparator("xiaolan"));singleColumnValueFilter.setFilterIfMissing(true);scan.setFilter(singleColumnValueFilter); 4.2 单列列值排除器 (SingleColumnValueExcludeFilter)SingleColumnValueExcludeFilter 继承自上面的 SingleColumnValueFilter，过滤行为与其相反。 4.3 行键前缀过滤器 (PrefixFilter)基于 RowKey 值决定某行数据是否被过滤。 12PrefixFilter prefixFilter = new PrefixFilter(Bytes.toBytes("xxx"));scan.setFilter(prefixFilter); 4.4 列名前缀过滤器 (ColumnPrefixFilter)基于列限定符（列名）决定某行数据是否被过滤。 12ColumnPrefixFilter columnPrefixFilter = new ColumnPrefixFilter(Bytes.toBytes("xxx")); scan.setFilter(columnPrefixFilter); 4.5 分页过滤器 (PageFilter)可以使用这个过滤器实现对结果按行进行分页，创建 PageFilter 实例的时候需要传入每页的行数。 1234public PageFilter(final long pageSize) &#123; Preconditions.checkArgument(pageSize &gt;= 0, "must be positive %s", pageSize); this.pageSize = pageSize; &#125; 下面的代码体现了客户端实现分页查询的主要逻辑，这里对其进行一下解释说明： 客户端进行分页查询，需要传递 startRow(起始 RowKey)，知道起始 startRow 后，就可以返回对应的 pageSize 行数据。这里唯一的问题就是，对于第一次查询，显然 startRow 就是表格的第一行数据，但是之后第二次、第三次查询我们并不知道 startRow，只能知道上一次查询的最后一条数据的 RowKey（简单称之为 lastRow）。 我们不能将 lastRow 作为新一次查询的 startRow 传入，因为 scan 的查询区间是[startRow，endRow) ，即前开后闭区间，这样 startRow 在新的查询也会被返回，这条数据就重复了。 同时在不使用第三方数据库存储 RowKey 的情况下，我们是无法通过知道 lastRow 的下一个 RowKey 的，因为 RowKey 的设计可能是连续的也有可能是不连续的。 由于 Hbase 的 RowKey 是按照字典序进行排序的。这种情况下，就可以在 lastRow 后面加上 0 ，作为 startRow 传入，因为按照字典序的规则，某个值加上 0 后的新值，在字典序上一定是这个值的下一个值，对于 HBase 来说下一个 RowKey 在字典序上一定也是等于或者大于这个新值的。 所以最后传入 lastRow+0，如果等于这个值的 RowKey 存在就从这个值开始 scan,否则从字典序的下一个 RowKey 开始 scan。 25 个字母以及数字字符，字典排序如下: &#39;0&#39; &lt; &#39;1&#39; &lt; &#39;2&#39; &lt; ... &lt; &#39;9&#39; &lt; &#39;a&#39; &lt; &#39;b&#39; &lt; ... &lt; &#39;z&#39; 分页查询主要实现逻辑： 12345678910111213141516171819202122232425262728byte[] POSTFIX = new byte[] &#123; 0x00 &#125;;Filter filter = new PageFilter(15);int totalRows = 0;byte[] lastRow = null;while (true) &#123; Scan scan = new Scan(); scan.setFilter(filter); if (lastRow != null) &#123; // 如果不是首行 则 lastRow + 0 byte[] startRow = Bytes.add(lastRow, POSTFIX); System.out.println("start row: " + Bytes.toStringBinary(startRow)); scan.withStartRow(startRow); &#125; ResultScanner scanner = table.getScanner(scan); int localRows = 0; Result result; while ((result = scanner.next()) != null) &#123; System.out.println(localRows++ + ": " + result); totalRows++; lastRow = result.getRow(); &#125; scanner.close(); //最后一页，查询结束 if (localRows == 0) break;&#125;System.out.println("total rows: " + totalRows); 需要注意的是在多台 Regin Services 上执行分页过滤的时候，由于并行执行的过滤器不能共享它们的状态和边界，所以有可能每个过滤器都会在完成扫描前获取了 PageCount 行的结果，这种情况下会返回比分页条数更多的数据，分页过滤器就有失效的可能。 4.6 时间戳过滤器 (TimestampsFilter)1234List&lt;Long&gt; list = new ArrayList&lt;&gt;();list.add(1554975573000L);TimestampsFilter timestampsFilter = new TimestampsFilter(list);scan.setFilter(timestampsFilter); 4.7 首次行键过滤器 (FirstKeyOnlyFilter)FirstKeyOnlyFilter 只扫描每行的第一列，扫描完第一列后就结束对当前行的扫描，并跳转到下一行。相比于全表扫描，其性能更好，通常用于行数统计的场景，因为如果某一行存在，则行中必然至少有一列。 12FirstKeyOnlyFilter firstKeyOnlyFilter = new FirstKeyOnlyFilter();scan.set(firstKeyOnlyFilter); 五、包装过滤器包装过滤器就是通过包装其他过滤器以实现某些拓展的功能。 5.1 SkipFilter过滤器SkipFilter 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，则拓展过滤整行数据。下面是一个使用示例： 12345// 定义 ValueFilter 过滤器Filter filter1 = new ValueFilter(CompareOperator.NOT_EQUAL, new BinaryComparator(Bytes.toBytes("xxx")));// 使用 SkipFilter 进行包装Filter filter2 = new SkipFilter(filter1); 5.2 WhileMatchFilter过滤器WhileMatchFilter 包装一个过滤器，当被包装的过滤器遇到一个需要过滤的 KeyValue 实例时，WhileMatchFilter 则结束本次扫描，返回已经扫描到的结果。下面是其使用示例： 1234567891011121314151617181920212223242526Filter filter1 = new RowFilter(CompareOperator.NOT_EQUAL, new BinaryComparator(Bytes.toBytes("rowKey4")));Scan scan = new Scan();scan.setFilter(filter1);ResultScanner scanner1 = table.getScanner(scan);for (Result result : scanner1) &#123; for (Cell cell : result.listCells()) &#123; System.out.println(cell); &#125;&#125;scanner1.close();System.out.println("--------------------");// 使用 WhileMatchFilter 进行包装Filter filter2 = new WhileMatchFilter(filter1);scan.setFilter(filter2);ResultScanner scanner2 = table.getScanner(scan);for (Result result : scanner1) &#123; for (Cell cell : result.listCells()) &#123; System.out.println(cell); &#125;&#125;scanner2.close(); 1234567891011121314rowKey0/student:name/1555035006994/Put/vlen=8/seqid=0rowKey1/student:name/1555035007019/Put/vlen=8/seqid=0rowKey2/student:name/1555035007025/Put/vlen=8/seqid=0rowKey3/student:name/1555035007037/Put/vlen=8/seqid=0rowKey5/student:name/1555035007051/Put/vlen=8/seqid=0rowKey6/student:name/1555035007057/Put/vlen=8/seqid=0rowKey7/student:name/1555035007062/Put/vlen=8/seqid=0rowKey8/student:name/1555035007068/Put/vlen=8/seqid=0rowKey9/student:name/1555035007073/Put/vlen=8/seqid=0--------------------rowKey0/student:name/1555035006994/Put/vlen=8/seqid=0rowKey1/student:name/1555035007019/Put/vlen=8/seqid=0rowKey2/student:name/1555035007025/Put/vlen=8/seqid=0rowKey3/student:name/1555035007037/Put/vlen=8/seqid=0 可以看到被包装后，只返回了 rowKey4 之前的数据。 六、FilterList以上都是讲解单个过滤器的作用，当需要多个过滤器共同作用于一次查询的时候，就需要使用 FilterList。FilterList 支持通过构造器或者 addFilter 方法传入多个过滤器。 12345678// 构造器传入public FilterList(final Operator operator, final List&lt;Filter&gt; filters)public FilterList(final List&lt;Filter&gt; filters)public FilterList(final Filter... filters)// 方法传入 public void addFilter(List&lt;Filter&gt; filters) public void addFilter(Filter filter) 多个过滤器组合的结果由 operator 参数定义 ，其可选参数定义在 Operator 枚举类中。只有 MUST_PASS_ALL 和 MUST_PASS_ONE 两个可选的值： MUST_PASS_ALL ：相当于 AND，必须所有的过滤器都通过才认为通过； MUST_PASS_ONE ：相当于 OR，只有要一个过滤器通过则认为通过。 1234567@InterfaceAudience.Public public enum Operator &#123; /** !AND */ MUST_PASS_ALL, /** !OR */ MUST_PASS_ONE &#125; 使用示例如下： 123456789101112131415161718List&lt;Filter&gt; filters = new ArrayList&lt;Filter&gt;();Filter filter1 = new RowFilter(CompareOperator.GREATER_OR_EQUAL, new BinaryComparator(Bytes.toBytes("XXX")));filters.add(filter1);Filter filter2 = new RowFilter(CompareOperator.LESS_OR_EQUAL, new BinaryComparator(Bytes.toBytes("YYY")));filters.add(filter2);Filter filter3 = new QualifierFilter(CompareOperator.EQUAL, new RegexStringComparator("ZZZ"));filters.add(filter3);FilterList filterList = new FilterList(filters);Scan scan = new Scan();scan.setFilter(filterList); 参考资料HBase: The Definitive Guide _&gt; Chapter 4. Client API: Advanced Features]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>过滤器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hbase_Java_API]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHbase_Java_API%2F</url>
    <content type="text"><![CDATA[HBase Java API 的基本使用一、简述二、Java API 1.x 基本使用三、Java API 2.x 基本使用四、正确连接Hbase 一、简述截至到目前 (2019.04)，HBase 有两个主要的版本，分别是 1.x 和 2.x ，两个版本的 Java API 有所不同，1.x 中某些方法在 2.x 中被标识为 @deprecated 过时。所以下面关于 API 的样例，我会分别给出 1.x 和 2.x 两个版本。完整的代码见本仓库： Java API 1.x Examples Java API 2.x Examples 同时你使用的客户端的版本必须与服务端版本保持一致，如果用 2.x 版本的客户端代码去连接 1.x 版本的服务端，会抛出 NoSuchColumnFamilyException 等异常。 二、Java API 1.x 基本使用2.1 新建Maven工程，导入项目依赖要使用 Java API 操作 HBase，需要引入 hbase-client。这里选取的 HBase Client 的版本为 1.2.0。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.2.0&lt;/version&gt;&lt;/dependency&gt; 2.2 API 基本使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252public class HBaseUtils &#123; private static Connection connection; static &#123; Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.property.clientPort", "2181"); // 如果是集群 则主机名用逗号分隔 configuration.set("hbase.zookeeper.quorum", "hadoop001"); try &#123; connection = ConnectionFactory.createConnection(configuration); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 创建 HBase 表 * * @param tableName 表名 * @param columnFamilies 列族的数组 */ public static boolean createTable(String tableName, List&lt;String&gt; columnFamilies) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); if (admin.tableExists(tableName)) &#123; return false; &#125; HTableDescriptor tableDescriptor = new HTableDescriptor(TableName.valueOf(tableName)); columnFamilies.forEach(columnFamily -&gt; &#123; HColumnDescriptor columnDescriptor = new HColumnDescriptor(columnFamily); columnDescriptor.setMaxVersions(1); tableDescriptor.addFamily(columnDescriptor); &#125;); admin.createTable(tableDescriptor); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除 hBase 表 * * @param tableName 表名 */ public static boolean deleteTable(String tableName) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); // 删除表前需要先禁用表 admin.disableTable(tableName); admin.deleteTable(tableName); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param qualifier 列标识 * @param value 数据 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, String qualifier, String value) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value)); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param pairList 列标识和值的集合 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue()))); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 根据 rowKey 获取指定行的数据 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static Result getRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); return table.get(get); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 获取指定行指定列 (cell) 的最新版本的数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamily 列族 * @param qualifier 列标识 */ public static String getCell(String tableName, String rowKey, String columnFamily, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); if (!get.isCheckExistenceOnly()) &#123; get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); Result result = table.get(get); byte[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); return Bytes.toString(resultValue); &#125; else &#123; return null; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索全表 * * @param tableName 表名 */ public static ResultScanner getScanner(String tableName) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param startRowKey 起始 RowKey * @param endRowKey 终止 RowKey * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, String startRowKey, String endRowKey, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.setStartRow(Bytes.toBytes(startRowKey)); scan.setStopRow(Bytes.toBytes(endRowKey)); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 删除指定行记录 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static boolean deleteRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); table.delete(delete); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除指定行的指定列 * * @param tableName 表名 * @param rowKey 唯一标识 * @param familyName 列族 * @param qualifier 列标识 */ public static boolean deleteColumn(String tableName, String rowKey, String familyName, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier)); table.delete(delete); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125;&#125; 2.3 单元测试以单元测试的方式对上面封装的 API 进行测试。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293public class HBaseUtilsTest &#123; private static final String TABLE_NAME = "class"; private static final String TEACHER = "teacher"; private static final String STUDENT = "student"; @Test public void createTable() &#123; // 新建表 List&lt;String&gt; columnFamilies = Arrays.asList(TEACHER, STUDENT); boolean table = HBaseUtils.createTable(TABLE_NAME, columnFamilies); System.out.println("表创建结果:" + table); &#125; @Test public void insertData() &#123; List&lt;Pair&lt;String, String&gt;&gt; pairs1 = Arrays.asList(new Pair&lt;&gt;("name", "Tom"), new Pair&lt;&gt;("age", "22"), new Pair&lt;&gt;("gender", "1")); HBaseUtils.putRow(TABLE_NAME, "rowKey1", STUDENT, pairs1); List&lt;Pair&lt;String, String&gt;&gt; pairs2 = Arrays.asList(new Pair&lt;&gt;("name", "Jack"), new Pair&lt;&gt;("age", "33"), new Pair&lt;&gt;("gender", "2")); HBaseUtils.putRow(TABLE_NAME, "rowKey2", STUDENT, pairs2); List&lt;Pair&lt;String, String&gt;&gt; pairs3 = Arrays.asList(new Pair&lt;&gt;("name", "Mike"), new Pair&lt;&gt;("age", "44"), new Pair&lt;&gt;("gender", "1")); HBaseUtils.putRow(TABLE_NAME, "rowKey3", STUDENT, pairs3); &#125; @Test public void getRow() &#123; Result result = HBaseUtils.getRow(TABLE_NAME, "rowKey1"); if (result != null) &#123; System.out.println(Bytes .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes("name")))); &#125; &#125; @Test public void getCell() &#123; String cell = HBaseUtils.getCell(TABLE_NAME, "rowKey2", STUDENT, "age"); System.out.println("cell age :" + cell); &#125; @Test public void getScanner() &#123; ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME); if (scanner != null) &#123; scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + "-&gt;" + Bytes .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes("name"))))); scanner.close(); &#125; &#125; @Test public void getScannerWithFilter() &#123; FilterList filterList = new FilterList(FilterList.Operator.MUST_PASS_ALL); SingleColumnValueFilter nameFilter = new SingleColumnValueFilter(Bytes.toBytes(STUDENT), Bytes.toBytes("name"), CompareOperator.EQUAL, Bytes.toBytes("Jack")); filterList.addFilter(nameFilter); ResultScanner scanner = HBaseUtils.getScanner(TABLE_NAME, filterList); if (scanner != null) &#123; scanner.forEach(result -&gt; System.out.println(Bytes.toString(result.getRow()) + "-&gt;" + Bytes .toString(result.getValue(Bytes.toBytes(STUDENT), Bytes.toBytes("name"))))); scanner.close(); &#125; &#125; @Test public void deleteColumn() &#123; boolean b = HBaseUtils.deleteColumn(TABLE_NAME, "rowKey2", STUDENT, "age"); System.out.println("删除结果: " + b); &#125; @Test public void deleteRow() &#123; boolean b = HBaseUtils.deleteRow(TABLE_NAME, "rowKey2"); System.out.println("删除结果: " + b); &#125; @Test public void deleteTable() &#123; boolean b = HBaseUtils.deleteTable(TABLE_NAME); System.out.println("删除结果: " + b); &#125;&#125; 三、Java API 2.x 基本使用3.1 新建Maven工程，导入项目依赖这里选取的 HBase Client 的版本为最新的 2.1.4。 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;2.1.4&lt;/version&gt;&lt;/dependency&gt; 3.2 API 的基本使用2.x 版本相比于 1.x 废弃了一部分方法，关于废弃的方法在源码中都会指明新的替代方法，比如，在 2.x 中创建表时：HTableDescriptor 和 HColumnDescriptor 等类都标识为废弃，取而代之的是使用 TableDescriptorBuilder 和 ColumnFamilyDescriptorBuilder 来定义表和列族。 以下为 HBase 2.x 版本 Java API 的使用示例： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253public class HBaseUtils &#123; private static Connection connection; static &#123; Configuration configuration = HBaseConfiguration.create(); configuration.set("hbase.zookeeper.property.clientPort", "2181"); // 如果是集群 则主机名用逗号分隔 configuration.set("hbase.zookeeper.quorum", "hadoop001"); try &#123; connection = ConnectionFactory.createConnection(configuration); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; /** * 创建 HBase 表 * * @param tableName 表名 * @param columnFamilies 列族的数组 */ public static boolean createTable(String tableName, List&lt;String&gt; columnFamilies) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); if (admin.tableExists(TableName.valueOf(tableName))) &#123; return false; &#125; TableDescriptorBuilder tableDescriptor = TableDescriptorBuilder.newBuilder(TableName.valueOf(tableName)); columnFamilies.forEach(columnFamily -&gt; &#123; ColumnFamilyDescriptorBuilder cfDescriptorBuilder = ColumnFamilyDescriptorBuilder.newBuilder(Bytes.toBytes(columnFamily)); cfDescriptorBuilder.setMaxVersions(1); ColumnFamilyDescriptor familyDescriptor = cfDescriptorBuilder.build(); tableDescriptor.setColumnFamily(familyDescriptor); &#125;); admin.createTable(tableDescriptor.build()); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除 hBase 表 * * @param tableName 表名 */ public static boolean deleteTable(String tableName) &#123; try &#123; HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); // 删除表前需要先禁用表 admin.disableTable(TableName.valueOf(tableName)); admin.deleteTable(TableName.valueOf(tableName)); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param qualifier 列标识 * @param value 数据 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, String qualifier, String value) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(qualifier), Bytes.toBytes(value)); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 插入数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamilyName 列族名 * @param pairList 列标识和值的集合 */ public static boolean putRow(String tableName, String rowKey, String columnFamilyName, List&lt;Pair&lt;String, String&gt;&gt; pairList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Put put = new Put(Bytes.toBytes(rowKey)); pairList.forEach(pair -&gt; put.addColumn(Bytes.toBytes(columnFamilyName), Bytes.toBytes(pair.getKey()), Bytes.toBytes(pair.getValue()))); table.put(put); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 根据 rowKey 获取指定行的数据 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static Result getRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); return table.get(get); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 获取指定行指定列 (cell) 的最新版本的数据 * * @param tableName 表名 * @param rowKey 唯一标识 * @param columnFamily 列族 * @param qualifier 列标识 */ public static String getCell(String tableName, String rowKey, String columnFamily, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Get get = new Get(Bytes.toBytes(rowKey)); if (!get.isCheckExistenceOnly()) &#123; get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); Result result = table.get(get); byte[] resultValue = result.getValue(Bytes.toBytes(columnFamily), Bytes.toBytes(qualifier)); return Bytes.toString(resultValue); &#125; else &#123; return null; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索全表 * * @param tableName 表名 */ public static ResultScanner getScanner(String tableName) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 检索表中指定数据 * * @param tableName 表名 * @param startRowKey 起始 RowKey * @param endRowKey 终止 RowKey * @param filterList 过滤器 */ public static ResultScanner getScanner(String tableName, String startRowKey, String endRowKey, FilterList filterList) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Scan scan = new Scan(); scan.withStartRow(Bytes.toBytes(startRowKey)); scan.withStopRow(Bytes.toBytes(endRowKey)); scan.setFilter(filterList); return table.getScanner(scan); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; /** * 删除指定行记录 * * @param tableName 表名 * @param rowKey 唯一标识 */ public static boolean deleteRow(String tableName, String rowKey) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); table.delete(delete); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125; /** * 删除指定行指定列 * * @param tableName 表名 * @param rowKey 唯一标识 * @param familyName 列族 * @param qualifier 列标识 */ public static boolean deleteColumn(String tableName, String rowKey, String familyName, String qualifier) &#123; try &#123; Table table = connection.getTable(TableName.valueOf(tableName)); Delete delete = new Delete(Bytes.toBytes(rowKey)); delete.addColumn(Bytes.toBytes(familyName), Bytes.toBytes(qualifier)); table.delete(delete); table.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return true; &#125;&#125; 四、正确连接Hbase在上面的代码中，在类加载时就初始化了 Connection 连接，并且之后的方法都是复用这个 Connection，这时我们可能会考虑是否可以使用自定义连接池来获取更好的性能表现？实际上这是没有必要的。 首先官方对于 Connection 的使用说明如下： 1234567891011121314Connection Pooling For applications which require high-end multithreaded access (e.g., web-servers or application servers that may serve many application threads in a single JVM), you can pre-create a Connection, as shown in the following example:对于高并发多线程访问的应用程序（例如，在单个 JVM 中存在的为多个线程服务的 Web 服务器或应用程序服务器）， 您只需要预先创建一个 Connection。例子如下：// Create a connection to the cluster.Configuration conf = HBaseConfiguration.create();try (Connection connection = ConnectionFactory.createConnection(conf); Table table = connection.getTable(TableName.valueOf(tablename))) &#123; // use table as needed, the table returned is lightweight&#125; 之所以能这样使用，这是因为 Connection 并不是一个简单的 socket 连接，接口文档 中对 Connection 的表述是： 1234567A cluster connection encapsulating lower level individual connections to actual servers and a connection to zookeeper. Connections are instantiated through the ConnectionFactory class. The lifecycle of the connection is managed by the caller, who has to close() the connection to release the resources. Connection 是一个集群连接，封装了与多台服务器（Matser/Region Server）的底层连接以及与 zookeeper 的连接。 连接通过 ConnectionFactory 类实例化。连接的生命周期由调用者管理，调用者必须使用 close() 关闭连接以释放资源。 之所以封装这些连接，是因为 HBase 客户端需要连接三个不同的服务角色： Zookeeper ：主要用于获取 meta 表的位置信息，Master 的信息； HBase Master ：主要用于执行 HBaseAdmin 接口的一些操作，例如建表等； HBase RegionServer ：用于读、写数据。 Connection 对象和实际的 Socket 连接之间的对应关系如下图： 上面两张图片引用自博客：连接 HBase 的正确姿势 在 HBase 客户端代码中，真正对应 Socket 连接的是 RpcConnection 对象。HBase 使用 PoolMap 这种数据结构来存储客户端到 HBase 服务器之间的连接。PoolMap 的内部有一个 ConcurrentHashMap 实例，其 key 是 ConnectionId(封装了服务器地址和用户 ticket)，value 是一个 RpcConnection 对象的资源池。当 HBase 需要连接一个服务器时，首先会根据 ConnectionId 找到对应的连接池，然后从连接池中取出一个连接对象。 123456789101112@InterfaceAudience.Privatepublic class PoolMap&lt;K, V&gt; implements Map&lt;K, V&gt; &#123; private PoolType poolType; private int poolMaxSize; private Map&lt;K, Pool&lt;V&gt;&gt; pools = new ConcurrentHashMap&lt;&gt;(); public PoolMap(PoolType poolType) &#123; this.poolType = poolType; &#125; ..... HBase 中提供了三种资源池的实现，分别是 Reusable，RoundRobin 和 ThreadLocal。具体实现可以通 hbase.client.ipc.pool.type 配置项指定，默认为 Reusable。连接池的大小也可以通过 hbase.client.ipc.pool.size 配置项指定，默认为 1，即每个 Server 1 个连接。也可以通过修改配置实现： 123config.set("hbase.client.ipc.pool.type",...);config.set("hbase.client.ipc.pool.size",...);connection = ConnectionFactory.createConnection(config); 由此可以看出 HBase 中 Connection 类已经实现了对连接的管理功能，所以我们不必在 Connection 上在做额外的管理。 另外，Connection 是线程安全的，但 Table 和 Admin 却不是线程安全的，因此正确的做法是一个进程共用一个 Connection 对象，而在不同的线程中使用单独的 Table 和 Admin 对象。Table 和 Admin 的获取操作 getTable() 和 getAdmin() 都是轻量级，所以不必担心性能的消耗，同时建议在使用完成后显示的调用 close() 方法来关闭它们。 参考资料 连接 HBase 的正确姿势 Apache HBase ™ Reference Guide]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hbase_Shell]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHbase_Shell%2F</url>
    <content type="text"><![CDATA[Hbase 常用 Shell 命令一、基本命令1.1 获取帮助1.2 查看服务器状态1.3 查看版本信息二、关于表的操作2.1 查看所有表2.2 创建表2.3 查看表的基本信息2.4 表的启用/禁用2.5 检查表是否存在2.6 删除表三、增删改3.1 添加列族3.2 删除列族3.3 更改列族存储版本的限制3.4 插入数据3.5 获取指定行、指定行中的列族、列的信息3.6 删除指定行、指定行中的列四、查询4.1Get查询4.2 查询整表数据4.3 查询指定列簇的数据4.4 条件查询4.5 条件过滤 一、基本命令打开 Hbase Shell： 1# hbase shell 1.1 获取帮助1234# 获取帮助help# 获取命令的详细信息help 'status' 1.2 查看服务器状态1status 1.3 查看版本信息1version 二、关于表的操作2.1 查看所有表1list 2.2 创建表 命令格式： create ‘表名称’, ‘列族名称 1’,’列族名称 2’,’列名称 N’ 12# 创建一张名为Student的表,包含基本信息（baseInfo）、学校信息（schoolInfo）两个列族create 'Student','baseInfo','schoolInfo' 2.3 查看表的基本信息 命令格式：desc ‘表名’ 1describe 'Student' 2.4 表的启用/禁用enable 和 disable 可以启用/禁用这个表,is_enabled 和 is_disabled 来检查表是否被禁用 12345678# 禁用表disable 'Student'# 检查表是否被禁用is_disabled 'Student'# 启用表enable 'Student'# 检查表是否被启用is_enabled 'Student' 2.5 检查表是否存在1exists 'Student' 2.6 删除表1234# 删除表前需要先禁用表disable 'Student'# 删除表drop 'Student' 三、增删改3.1 添加列族 命令格式： alter ‘表名’, ‘列族名’ 1alter 'Student', 'teacherInfo' 3.2 删除列族 命令格式：alter ‘表名’, {NAME =&gt; ‘列族名’, METHOD =&gt; ‘delete’} 1alter 'Student', &#123;NAME =&gt; 'teacherInfo', METHOD =&gt; 'delete'&#125; 3.3 更改列族存储版本的限制默认情况下，列族只存储一个版本的数据，如果需要存储多个版本的数据，则需要修改列族的属性。修改后可通过 desc 命令查看。 1alter 'Student',&#123;NAME=&gt;'baseInfo',VERSIONS=&gt;3&#125; 3.4 插入数据 命令格式：put ‘表名’, ‘行键’,’列族:列’,’值’ 注意：如果新增数据的行键值、列族名、列名与原有数据完全相同，则相当于更新操作 12345678910111213141516171819put 'Student', 'rowkey1','baseInfo:name','tom'put 'Student', 'rowkey1','baseInfo:birthday','1990-01-09'put 'Student', 'rowkey1','baseInfo:age','29'put 'Student', 'rowkey1','schoolInfo:name','Havard'put 'Student', 'rowkey1','schoolInfo:localtion','Boston'put 'Student', 'rowkey2','baseInfo:name','jack'put 'Student', 'rowkey2','baseInfo:birthday','1998-08-22'put 'Student', 'rowkey2','baseInfo:age','21'put 'Student', 'rowkey2','schoolInfo:name','yale'put 'Student', 'rowkey2','schoolInfo:localtion','New Haven'put 'Student', 'rowkey3','baseInfo:name','maike'put 'Student', 'rowkey3','baseInfo:birthday','1995-01-22'put 'Student', 'rowkey3','baseInfo:age','24'put 'Student', 'rowkey3','schoolInfo:name','yale'put 'Student', 'rowkey3','schoolInfo:localtion','New Haven'put 'Student', 'wrowkey4','baseInfo:name','maike-jack' 3.5 获取指定行、指定行中的列族、列的信息123456# 获取指定行中所有列的数据信息get 'Student','rowkey3'# 获取指定行中指定列族下所有列的数据信息get 'Student','rowkey3','baseInfo'# 获取指定行中指定列的数据信息get 'Student','rowkey3','baseInfo:name' 3.6 删除指定行、指定行中的列1234# 删除指定行delete 'Student','rowkey3'# 删除指定行中指定列的数据delete 'Student','rowkey3','baseInfo:name' 四、查询hbase 中访问数据有两种基本的方式： 按指定 rowkey 获取数据：get 方法； 按指定条件获取数据：scan 方法。 scan 可以设置 begin 和 end 参数来访问一个范围内所有的数据。get 本质上就是 begin 和 end 相等的一种特殊的 scan。 4.1Get查询123456# 获取指定行中所有列的数据信息get 'Student','rowkey3'# 获取指定行中指定列族下所有列的数据信息get 'Student','rowkey3','baseInfo'# 获取指定行中指定列的数据信息get 'Student','rowkey3','baseInfo:name' 4.2 查询整表数据1scan 'Student' 4.3 查询指定列簇的数据1scan 'Student', &#123;COLUMN=&gt;'baseInfo'&#125; 4.4 条件查询12# 查询指定列的数据scan 'Student', &#123;COLUMNS=&gt; 'baseInfo:birthday'&#125; 除了列 （COLUMNS） 修饰词外，HBase 还支持 Limit（限制查询结果行数），STARTROW（ROWKEY 起始行，会先根据这个 key 定位到 region，再向后扫描）、STOPROW(结束行)、TIMERANGE（限定时间戳范围）、VERSIONS（版本数）、和 FILTER（按条件过滤行）等。 如下代表从 rowkey2 这个 rowkey 开始，查找下两个行的最新 3 个版本的 name 列的数据： 1scan 'Student', &#123;COLUMNS=&gt; 'baseInfo:name',STARTROW =&gt; 'rowkey2',STOPROW =&gt; 'wrowkey4',LIMIT=&gt;2, VERSIONS=&gt;3&#125; 4.5 条件过滤Filter 可以设定一系列条件来进行过滤。如我们要查询值等于 24 的所有数据： 1scan 'Student', FILTER=&gt;"ValueFilter(=,'binary:24')" 值包含 yale 的所有数据： 1scan 'Student', FILTER=&gt;"ValueFilter(=,'substring:yale')" 列名中的前缀为 birth 的： 1scan 'Student', FILTER=&gt;"ColumnPrefixFilter('birth')" FILTER 中支持多个过滤条件通过括号、AND 和 OR 进行组合： 12# 列名中的前缀为birth且列值中包含1998的数据scan 'Student', FILTER=&gt;"ColumnPrefixFilter('birth') AND ValueFilter ValueFilter(=,'substring:1998')" PrefixFilter 用于对 Rowkey 的前缀进行判断： 1scan 'Student', FILTER=&gt;"PrefixFilter('wr')"]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之HBase集群环境搭建]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHBase%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[HBase集群环境配置一、集群规划二、前置条件三、集群搭建3.1 下载并解压3.2 配置环境变量3.3 集群配置3.4 HDFS客户端配置3.5 安装包分发四、启动集群4.1 启动ZooKeeper集群4.2 启动Hadoop集群4.3 启动HBase集群4.5 查看服务 一、集群规划这里搭建一个 3 节点的 HBase 集群，其中三台主机上均为 Regin Server。同时为了保证高可用，除了在 hadoop001 上部署主 Master 服务外，还在 hadoop002 上部署备用的 Master 服务。Master 服务由 Zookeeper 集群进行协调管理，如果主 Master 不可用，则备用 Master 会成为新的主 Master。 二、前置条件HBase 的运行需要依赖 Hadoop 和 JDK(HBase 2.0+ 对应 JDK 1.8+) 。同时为了保证高可用，这里我们不采用 HBase 内置的 Zookeeper 服务，而采用外置的 Zookeeper 集群。相关搭建步骤可以参阅： Linux 环境下 JDK 安装 Zookeeper 单机环境和集群环境搭建 Hadoop 集群环境搭建 三、集群搭建3.1 下载并解压下载并解压，这里我下载的是 CDH 版本 HBase，下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 1# tar -zxvf hbase-1.2.0-cdh5.15.2.tar.gz 3.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export HBASE_HOME=usr/app/hbase-1.2.0-cdh5.15.2export PATH=$HBASE_HOME/bin:$PATH 使得配置的环境变量立即生效： 1# source /etc/profile 3.3 集群配置进入 ${HBASE_HOME}/conf 目录下，修改配置： 1. hbase-env.sh1234# 配置JDK安装位置export JAVA_HOME=/usr/java/jdk1.8.0_201# 不使用内置的zookeeper服务export HBASE_MANAGES_ZK=false 2. hbase-site.xml1234567891011121314151617&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定 hbase 以分布式集群的方式运行 --&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定 hbase 在 HDFS 上的存储位置 --&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定 zookeeper 的地址--&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. regionservers123hadoop001hadoop002hadoop003 4. backup-masters1hadoop002 backup-masters 这个文件是不存在的，需要新建，主要用来指明备用的 master 节点，可以是多个，这里我们以 1 个为例。 3.4 HDFS客户端配置这里有一个可选的配置：如果您在 Hadoop 集群上进行了 HDFS 客户端配置的更改，比如将副本系数 dfs.replication 设置成 5，则必须使用以下方法之一来使 HBase 知道，否则 HBase 将依旧使用默认的副本系数 3 来创建文件： Add a pointer to your HADOOP_CONF_DIR to the HBASE_CLASSPATH environment variable in hbase-env.sh. Add a copy of hdfs-site.xml (or hadoop-site.xml) or, better, symlinks, under ${HBASE_HOME}/conf, or if only a small set of HDFS client configurations, add them to hbase-site.xml. 以上是官方文档的说明，这里解释一下： 第一种 ：将 Hadoop 配置文件的位置信息添加到 hbase-env.sh 的 HBASE_CLASSPATH 属性，示例如下： 1export HBASE_CLASSPATH=usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop 第二种 ：将 Hadoop 的 hdfs-site.xml 或 hadoop-site.xml 拷贝到 ${HBASE_HOME}/conf 目录下，或者通过符号链接的方式。如果采用这种方式的话，建议将两者都拷贝或建立符号链接，示例如下： 12345# 拷贝cp core-site.xml hdfs-site.xml /usr/app/hbase-1.2.0-cdh5.15.2/conf/# 使用符号链接ln -s /usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop/core-site.xmlln -s /usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop/hdfs-site.xml 注：hadoop-site.xml 这个配置文件现在叫做 core-site.xml 第三种 ：如果你只有少量更改，那么直接配置到 hbase-site.xml 中即可。 3.5 安装包分发将 HBase 的安装包分发到其他服务器，分发后建议在这两台服务器上也配置一下 HBase 的环境变量。 12scp -r /usr/app/hbase-1.2.0-cdh5.15.2/ hadoop002:usr/app/scp -r /usr/app/hbase-1.2.0-cdh5.15.2/ hadoop003:usr/app/ 四、启动集群4.1 启动ZooKeeper集群分别到三台服务器上启动 ZooKeeper 服务： 1zkServer.sh start 4.2 启动Hadoop集群1234# 启动dfs服务start-dfs.sh# 启动yarn服务start-yarn.sh 4.3 启动HBase集群进入 hadoop001 的 ${HBASE_HOME}/bin，使用以下命令启动 HBase 集群。执行此命令后，会在 hadoop001 上启动 Master 服务，在 hadoop002 上启动备用 Master 服务，在 regionservers 文件中配置的所有节点启动 region server 服务。 1start-hbase.sh 4.5 查看服务访问 HBase 的 Web-UI 界面，这里我安装的 HBase 版本为 1.2，访问端口为 60010，如果你安装的是 2.0 以上的版本，则访问端口号为 16010。可以看到 Master 在 hadoop001 上，三个 Regin Servers 分别在 hadoop001，hadoop002，和 hadoop003 上，并且还有一个 Backup Matser 服务在 hadoop002 上。 hadoop002 上的 HBase 出于备用状态：]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之HBase单机环境搭建]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHBase%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[HBase基本环境搭建一、安装前置条件说明二、Standalone 模式三、伪集群模式安装（Pseudo-Distributed） 一、安装前置条件说明1.1 JDK版本说明HBase 需要依赖 JDK 环境，同时 HBase 2.0+ 以上版本不再支持 JDK 1.7 ，需要安装 JDK 1.8+ 。JDK 安装方式见本仓库： Linux 环境下 JDK 安装 1.2 Standalone模式和伪集群模式的区别 在 Standalone 模式下，所有守护进程都运行在一个 jvm 进程/实例中； 在伪分布模式下，HBase 仍然在单个主机上运行，但是每个守护进程 (HMaster，HRegionServer 和 ZooKeeper) 则分别作为一个单独的进程运行。 说明：两种模式任选其一进行部署即可，对于开发测试来说区别不大。 二、Standalone 模式2.1 下载并解压从官方网站 下载所需要版本的二进制安装包，并进行解压： 1# tar -zxvf hbase-2.1.4-bin.tar.gz 2.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export HBASE_HOME=/usr/app/hbase-2.1.4export PATH=$HBASE_HOME/bin:$PATH 使得配置的环境变量生效： 1# source /etc/profile 2.3 进行HBase相关配置修改安装目录下的 conf/hbase-env.sh,指定 JDK 的安装路径： 12# The java implementation to use. Java 1.8+ required.export JAVA_HOME=/usr/java/jdk1.8.0_201 修改安装目录下的 conf/hbase-site.xml，增加如下配置： 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;file:///home/hbase/rootdir&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/zookeeper/dataDir&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.unsafe.stream.capability.enforce&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; hbase.rootdir: 配置 hbase 数据的存储路径； hbase.zookeeper.property.dataDir: 配置 zookeeper 数据的存储路径； hbase.unsafe.stream.capability.enforce: 使用本地文件系统存储，不使用 HDFS 的情况下需要禁用此配置，设置为 false。 2.4 启动HBase由于已经将 HBase 的 bin 目录配置到环境变量，直接使用以下命令启动： 1# start-hbase.sh 2.5 验证启动是否成功验证方式一 ：使用 jps 命令查看 HMaster 进程是否启动。 123[root@hadoop001 hbase-2.1.4]# jps16336 Jps15500 HMaster 验证方式二 ：访问 HBaseWeb UI 页面，默认端口为 16010 。 三、伪集群模式安装（Pseudo-Distributed）3.1 Hadoop单机伪集群安装这里我们采用 HDFS 作为 HBase 的存储方案，需要预先安装 Hadoop。Hadoop 的安装方式单独整理至： Hadoop 单机伪集群搭建 3.2 Hbase版本选择HBase 的版本必须要与 Hadoop 的版本兼容，不然会出现各种 Jar 包冲突。这里我 Hadoop 安装的版本为 hadoop-2.6.0-cdh5.15.2，为保持版本一致，选择的 HBase 版本为 hbase-1.2.0-cdh5.15.2 。所有软件版本如下： Hadoop 版本： hadoop-2.6.0-cdh5.15.2 HBase 版本： hbase-1.2.0-cdh5.15.2 JDK 版本：JDK 1.8 3.3 软件下载解压下载后进行解压，下载地址：http://archive.cloudera.com/cdh5/cdh/5/ 1# tar -zxvf hbase-1.2.0-cdh5.15.2.tar.gz 3.4 配置环境变量1# vim /etc/profile 添加环境变量： 12export HBASE_HOME=/usr/app/hbase-1.2.0-cdh5.15.2export PATH=$HBASE_HOME/bin:$PATH 使得配置的环境变量生效： 1# source /etc/profile 3.5 进行HBase相关配置1.修改安装目录下的 conf/hbase-env.sh,指定 JDK 的安装路径： 12# The java implementation to use. Java 1.7+ required.export JAVA_HOME=/usr/java/jdk1.8.0_201 2.修改安装目录下的 conf/hbase-site.xml，增加如下配置 (hadoop001 为主机名)： 1234567891011121314151617&lt;configuration&gt; &lt;!--指定 HBase 以分布式模式运行--&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;!--指定 HBase 数据存储路径为 HDFS 上的 hbase 目录,hbase 目录不需要预先创建，程序会自动创建--&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020/hbase&lt;/value&gt; &lt;/property&gt; &lt;!--指定 zookeeper 数据的存储位置--&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/home/zookeeper/dataDir&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3.修改安装目录下的 conf/regionservers，指定 region servers 的地址，修改后其内容如下： 1hadoop001 3.6 启动1# bin/start-hbase.sh 3.7 验证启动是否成功验证方式一 ：使用 jps 命令查看进程。其中 HMaster，HRegionServer 是 HBase 的进程，HQuorumPeer 是 HBase 内置的 Zookeeper 的进程，其余的为 HDFS 和 YARN 的进程。 123456789101112[root@hadoop001 conf]# jps28688 NodeManager25824 GradleDaemon10177 Jps22083 HRegionServer20534 DataNode20807 SecondaryNameNode18744 Main20411 NameNode21851 HQuorumPeer28573 ResourceManager21933 HMaster 验证方式二 ：访问 HBase Web UI 界面，需要注意的是 1.2 版本的 HBase 的访问端口为 60010]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hbase系统架构及数据结构]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHbase%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Hbase系统架构及数据结构一、基本概念1.1 Row Key (行键)1.2 Column Family（列族）1.3 Column Qualifier (列限定符)1.4 Column(列)1.5 Cell1.6 Timestamp(时间戳)二、存储结构2.1 Regions2.2 Region Server三、Hbase系统架构3.1 系统架构3.2 组件间的协作四、数据的读写流程简述4.1 写入数据的流程4.2 读取数据的流程 一、基本概念一个典型的 Hbase Table 表如下： 1.1 Row Key (行键)Row Key 是用来检索记录的主键。想要访问 HBase Table 中的数据，只有以下三种方式： 通过指定的 Row Key 进行访问； 通过 Row Key 的 range 进行访问，即访问指定范围内的行； 进行全表扫描。 Row Key 可以是任意字符串，存储时数据按照 Row Key 的字典序进行排序。这里需要注意以下两点： 因为字典序对 Int 排序的结果是 1,10,100,11,12,13,14,15,16,17,18,19,2,20,21,…,9,91,92,93,94,95,96,97,98,99。如果你使用整型的字符串作为行键，那么为了保持整型的自然序，行键必须用 0 作左填充。 行的一次读写操作时原子性的 (不论一次读写多少列)。 1.2 Column Family（列族）HBase 表中的每个列，都归属于某个列族。列族是表的 Schema 的一部分，所以列族需要在创建表时进行定义。列族的所有列都以列族名作为前缀，例如 courses:history，courses:math 都属于 courses 这个列族。 1.3 Column Qualifier (列限定符)列限定符，你可以理解为是具体的列名，例如 courses:history，courses:math 都属于 courses 这个列族，它们的列限定符分别是 history 和 math。需要注意的是列限定符不是表 Schema 的一部分，你可以在插入数据的过程中动态创建列。 1.4 Column(列)HBase 中的列由列族和列限定符组成，它们由 :(冒号) 进行分隔，即一个完整的列名应该表述为 列族名 ：列限定符。 1.5 CellCell 是行，列族和列限定符的组合，并包含值和时间戳。你可以等价理解为关系型数据库中由指定行和指定列确定的一个单元格，但不同的是 HBase 中的一个单元格是由多个版本的数据组成的，每个版本的数据用时间戳进行区分。 1.6 Timestamp(时间戳)HBase 中通过 row key 和 column 确定的为一个存储单元称为 Cell。每个 Cell 都保存着同一份数据的多个版本。版本通过时间戳来索引，时间戳的类型是 64 位整型，时间戳可以由 HBase 在数据写入时自动赋值，也可以由客户显式指定。每个 Cell 中，不同版本的数据按照时间戳倒序排列，即最新的数据排在最前面。 二、存储结构2.1 RegionsHBase Table 中的所有行按照 Row Key 的字典序排列。HBase Tables 通过行键的范围 (row key range) 被水平切分成多个 Region, 一个 Region 包含了在 start key 和 end key 之间的所有行。 每个表一开始只有一个 Region，随着数据不断增加，Region 会不断增大，当增大到一个阀值的时候，Region 就会等分为两个新的 Region。当 Table 中的行不断增多，就会有越来越多的 Region。 Region 是 HBase 中分布式存储和负载均衡的最小单元。这意味着不同的 Region 可以分布在不同的 Region Server 上。但一个 Region 是不会拆分到多个 Server 上的。 2.2 Region ServerRegion Server 运行在 HDFS 的 DataNode 上。它具有以下组件： WAL(Write Ahead Log，预写日志)：用于存储尚未进持久化存储的数据记录，以便在发生故障时进行恢复。 BlockCache：读缓存。它将频繁读取的数据存储在内存中，如果存储不足，它将按照 最近最少使用原则 清除多余的数据。 MemStore：写缓存。它存储尚未写入磁盘的新数据，并会在数据写入磁盘之前对其进行排序。每个 Region 上的每个列族都有一个 MemStore。 HFile ：将行数据按照 Key\Values 的形式存储在文件系统上。 Region Server 存取一个子表时，会创建一个 Region 对象，然后对表的每个列族创建一个 Store 实例，每个 Store 会有 0 个或多个 StoreFile 与之对应，每个 StoreFile 则对应一个 HFile，HFile 就是实际存储在 HDFS 上的文件。 三、Hbase系统架构3.1 系统架构HBase 系统遵循 Master/Salve 架构，由三种不同类型的组件组成： Zookeeper 保证任何时候，集群中只有一个 Master； 存贮所有 Region 的寻址入口； 实时监控 Region Server 的状态，将 Region Server 的上线和下线信息实时通知给 Master； 存储 HBase 的 Schema，包括有哪些 Table，每个 Table 有哪些 Column Family 等信息。 Master 为 Region Server 分配 Region ； 负责 Region Server 的负载均衡 ； 发现失效的 Region Server 并重新分配其上的 Region； GFS 上的垃圾文件回收； 处理 Schema 的更新请求。 Region Server Region Server 负责维护 Master 分配给它的 Region ，并处理发送到 Region 上的 IO 请求； Region Server 负责切分在运行过程中变得过大的 Region。 3.2 组件间的协作 HBase 使用 ZooKeeper 作为分布式协调服务来维护集群中的服务器状态。 Zookeeper 负责维护可用服务列表，并提供服务故障通知等服务： 每个 Region Server 都会在 ZooKeeper 上创建一个临时节点，Master 通过 Zookeeper 的 Watcher 机制对节点进行监控，从而可以发现新加入的 Region Server 或故障退出的 Region Server； 所有 Masters 会竞争性地在 Zookeeper 上创建同一个临时节点，由于 Zookeeper 只能有一个同名节点，所以必然只有一个 Master 能够创建成功，此时该 Master 就是主 Master，主 Master 会定期向 Zookeeper 发送心跳。备用 Masters 则通过 Watcher 机制对主 HMaster 所在节点进行监听； 如果主 Master 未能定时发送心跳，则其持有的 Zookeeper 会话会过期，相应的临时节点也会被删除，这会触发定义在该节点上的 Watcher 事件，使得备用的 Master Servers 得到通知。所有备用的 Master Servers 在接到通知后，会再次去竞争性地创建临时节点，完成主 Master 的选举。 四、数据的读写流程简述4.1 写入数据的流程 Client 向 Region Server 提交写请求； Region Server 找到目标 Region； Region 检查数据是否与 Schema 一致； 如果客户端没有指定版本，则获取当前系统时间作为数据版本； 将更新写入 WAL Log； 将更新写入 Memstore； 判断 Memstore 存储是否已满，如果存储已满则需要 flush 为 Store Hfile 文件。 更为详细写入流程可以参考：HBase － 数据写入流程解析 4.2 读取数据的流程以下是客户端首次读写 HBase 上数据的流程： 客户端从 Zookeeper 获取 META 表所在的 Region Server； 客户端访问 META 表所在的 Region Server，从 META 表中查询到访问行键所在的 Region Server，之后客户端将缓存这些信息以及 META 表的位置； 客户端从行键所在的 Region Server 上获取数据。 如果再次读取，客户端将从缓存中获取行键所在的 Region Server。这样客户端就不需要再次查询 META 表，除非 Region 移动导致缓存失效，这样的话，则将会重新查询并更新缓存。 注：META 表是 HBase 中一张特殊的表，它保存了所有 Region 的位置信息，META 表自己的位置信息则存储在 ZooKeeper 上。 更为详细读取数据流程参考： HBase 原理－数据读取流程解析 HBase 原理－迟到的‘数据读取流程部分细节 参考资料本篇文章内容主要参考自官方文档和以下两篇博客，图片也主要引用自以下两篇博客： HBase Architectural Components Hbase 系统架构及数据结构 官方文档： Apache HBase ™ Reference Guide]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>数据结构</tag>
        <tag>系统架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hbase简介]]></title>
    <url>%2F2019%2F07%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHbase%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[HBase简介一、Hadoop的局限二、HBase简介三、HBase Table四、Phoenix 一、Hadoop的局限HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。 要想明白为什么产生 HBase，就需要先了解一下 Hadoop 存在的限制？Hadoop 可以通过 HDFS 来存储结构化、半结构甚至非结构化的数据，它是传统数据库的补充，是海量数据存储的最佳方法，它针对大文件的存储，批量访问和流式访问都做了优化，同时也通过多副本解决了容灾问题。 但是 Hadoop 的缺陷在于它只能执行批处理，并且只能以顺序方式访问数据，这意味着即使是最简单的工作，也必须搜索整个数据集，无法实现对数据的随机访问。实现数据的随机访问是传统的关系型数据库所擅长的，但它们却不能用于海量数据的存储。在这种情况下，必须有一种新的方案来解决海量数据存储和随机访问的问题，HBase 就是其中之一 (HBase，Cassandra，couchDB，Dynamo 和 MongoDB 都能存储海量数据并支持随机访问)。 注：数据结构分类： 结构化数据：即以关系型数据库表形式管理的数据； 半结构化数据：非关系模型的，有基本固定结构模式的数据，例如日志文件、XML 文档、JSON 文档、Email 等； 非结构化数据：没有固定模式的数据，如 WORD、PDF、PPT、EXL，各种格式的图片、视频等。 二、HBase简介HBase 是一个构建在 Hadoop 文件系统之上的面向列的数据库管理系统。 HBase 是一种类似于 Google’s Big Table 的数据模型，它是 Hadoop 生态系统的一部分，它将数据存储在 HDFS 上，客户端可以通过 HBase 实现对 HDFS 上数据的随机访问。它具有以下特性： 不支持复杂的事务，只支持行级事务，即单行数据的读写都是原子性的； 由于是采用 HDFS 作为底层存储，所以和 HDFS 一样，支持结构化、半结构化和非结构化的存储； 支持通过增加机器进行横向扩展； 支持数据分片； 支持 RegionServers 之间的自动故障转移； 易于使用的 Java 客户端 API； 支持 BlockCache 和布隆过滤器； 过滤器支持谓词下推。 三、HBase TableHBase 是一个面向 列 的数据库管理系统，这里更为确切的而说，HBase 是一个面向 列族 的数据库管理系统。表 schema 仅定义列族，表具有多个列族，每个列族可以包含任意数量的列，列由多个单元格（cell ）组成，单元格可以存储多个版本的数据，多个版本数据以时间戳进行区分。 下图为 HBase 中一张表的： RowKey 为行的唯一标识，所有行按照 RowKey 的字典序进行排序； 该表具有两个列族，分别是 personal 和 office; 其中列族 personal 拥有 name、city、phone 三个列，列族 office 拥有 tel、addres 两个列。 图片引用自 : HBase 是列式存储数据库吗 https://www.iteblog.com/archives/2498.html Hbase 的表具有以下特点： 容量大：一个表可以有数十亿行，上百万列； 面向列：数据是按照列存储，每一列都单独存放，数据即索引，在查询时可以只访问指定列的数据，有效地降低了系统的 I/O 负担； 稀疏性：空 (null) 列并不占用存储空间，表可以设计的非常稀疏 ； 数据多版本：每个单元中的数据可以有多个版本，按照时间戳排序，新的数据在最上面； 存储类型：所有数据的底层存储格式都是字节数组 (byte[])。 四、PhoenixPhoenix 是 HBase 的开源 SQL 中间层，它允许你使用标准 JDBC 的方式来操作 HBase 上的数据。在 Phoenix 之前，如果你要访问 HBase，只能调用它的 Java API，但相比于使用一行 SQL 就能实现数据查询，HBase 的 API 还是过于复杂。Phoenix 的理念是 we put sql SQL back in NOSQL，即你可以使用标准的 SQL 就能完成对 HBase 上数据的操作。同时这也意味着你可以通过集成 Spring Data JPA 或 Mybatis 等常用的持久层框架来操作 HBase。 其次 Phoenix 的性能表现也非常优异，Phoenix 查询引擎会将 SQL 查询转换为一个或多个 HBase Scan，通过并行执行来生成标准的 JDBC 结果集。它通过直接使用 HBase API 以及协处理器和自定义过滤器，可以为小型数据查询提供毫秒级的性能，为千万行数据的查询提供秒级的性能。同时 Phoenix 还拥有二级索引等 HBase 不具备的特性，因为以上的优点，所以 Phoenix 成为了 HBase 最优秀的 SQL 中间层。 参考资料 HBase - Overview]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink杂文系列State]]></title>
    <url>%2F2019%2F07%2F23%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E6%9D%82%E6%96%87%E7%B3%BB%E5%88%97State%2F</url>
    <content type="text"><![CDATA[实际问题在流计算场景中，数据会源源不断的流入Apache Flink系统，每条数据进入Apache Flink系统都会触发计算。如果我们想进行一个Count聚合计算，那么每次触发计算是将历史上所有流入的数据重新新计算一次，还是每次计算都是在上一次计算结果之上进行增量计算呢？答案是肯定的，Apache Flink是基于上一次的计算结果进行增量计算的。那么问题来了: “上一次的计算结果保存在哪里，保存在内存可以吗？”，答案是否定的，如果保存在内存，在由于网络，硬件等原因造成某个计算节点失败的情况下，上一次计算结果会丢失，在节点恢复的时候，就需要将历史上所有数据（可能十几天，上百天的数据）重新计算一次，所以为了避免这种灾难性的问题发生，Apache Flink 会利用State存储计算结果。本篇将会为大家介绍Apache Flink State的相关内容。 什么是State这个问题似乎有些”弱智”？不管问题的答案是否显而易见，但我还是想简单说一下在Apache Flink里面什么是State？State是指流计算过程中计算节点的中间计算结果或元数据属性，比如 在aggregation过程中要在state中记录中间聚合结果，比如 Apache Kafka 作为数据源时候，我们也要记录已经读取记录的offset，这些State数据在计算过程中会进行持久化(插入或更新)。所以Apache Flink中的State就是与时间相关的，Apache Flink任务的内部数据（计算数据和元数据属性）的快照。 为什么需要State与批计算相比，State是流计算特有的，批计算没有failover机制，要么成功，要么重新计算。流计算在 大多数场景 下是增量计算，数据逐条处理（大多数场景)，每次计算是在上一次计算结果之上进行处理的，这样的机制势必要将上一次的计算结果进行存储（生产模式要持久化），另外由于 机器，网络，脏数据等原因导致的程序错误，在重启job时候需要从成功的检查点(checkpoint，后面篇章会专门介绍)进行state的恢复。增量计算，Failover这些机制都需要state的支撑。 State 实现Apache Flink内部有四种state的存储实现，具体如下： 基于内存的HeapStateBackend - 在debug模式使用，不 建议在生产模式下应用； 基于HDFS的FsStateBackend - 分布式文件持久化，每次读写都产生网络IO，整体性能不佳； 基于RocksDB的RocksDBStateBackend - 本地文件+异步HDFS持久化； 还有一个是基于Niagara(Alibaba内部实现)NiagaraStateBackend - 分布式持久化- 在Alibaba生产环境应用； State 持久化逻辑Apache Flink版本选择用RocksDB+HDFS的方式进行State的存储，State存储分两个阶段，首先本地存储到RocksDB，然后异步的同步到远程的HDFS。 这样而设计既消除了HeapStateBackend的局限（内存大小，机器坏掉丢失等），也减少了纯分布式存储的网络IO开销。 -State.resources/11AD7C2A-A1DD-4238-8226-AED47EF6F446.png) State 分类Apache Flink 内部按照算子和数据分组角度将State划分为如下两类： KeyedState - 这里面的key是我们在SQL语句中对应的GroupBy/PartitioneBy里面的字段，key的值就是groupby/PartitionBy字段组成的Row的字节数组，每一个key都有一个属于自己的State，key与key之间的State是不可见的； OperatorState - Apache Flink内部的Source Connector的实现中就会用OperatorState来记录source数据读取的offset。 State 扩容重新分配Apache Flink是一个大规模并行分布式系统，允许大规模的有状态流处理。 为了可伸缩性，Apache Flink作业在逻辑上被分解成operator graph，并且每个operator的执行被物理地分解成多个并行运算符实例。 从概念上讲，Apache Flink中的每个并行运算符实例都是一个独立的任务，可以在自己的机器上调度到网络连接的其他机器运行。 Apache Flink的DAG图中只有边相连的节点🈶网络通信，也就是整个DAG在垂直方向有网络IO，在水平方向如下图的stateful节点之间没有网络通信，这种模型也保证了每个operator实例维护一份自己的state，并且保存在本地磁盘（远程异步同步）。通过这种设计，任务的所有状态数据都是本地的，并且状态访问不需要任务之间的网络通信。 避免这种流量对于像Apache Flink这样的大规模并行分布式系统的可扩展性至关重要。 如上我们知道Apache Flink中State有OperatorState和KeyedState，那么在进行扩容时候（增加并发）State如何分配呢？比如：外部Source有5个partition，在Apache Flink上面由Srouce的1个并发扩容到2个并发，中间Stateful Operation 节点由2个并发并扩容的3个并发，如下图所示: -State.resources/60A3963C-B15F-456F-9B37-69C022B6491D.png) 在Apache Flink中对不同类型的State有不同的扩容方法，接下来我们分别介绍。 OperatorState对扩容的处理我们选取Apache Flink中某个具体Connector实现实例进行介绍，以MetaQ为例，MetaQ以topic方式订阅数据，每个topic会有N&gt;0个分区，以上图为例，加上我们订阅的MetaQ的topic有5个分区，那么当我们source由1个并发调整为2个并发时候，State是怎么恢复的呢？state 恢复的方式与Source中OperatorState的存储结构有必然关系，我们先看MetaQSource的实现是如何存储State的。首先MetaQSource 实现了ListCheckpointed，其中的T是Tuple2&lt;InputSplit,Long&gt;，我们在看ListCheckpointed接口的内部定义如下： 12345public interface ListCheckpointed&lt;T extends Serializable&gt;; &#123;List&lt;T&gt; snapshotState(long var1, long var3) throws Exception;void restoreState(List&amp;lt;T&amp;gt; var1) throws Exception;&#125; 我们发现 snapshotState方法的返回值是一个List,T是Tuple2&lt;InputSplit,Long&gt;，也就是snapshotState方法返回List&lt;Tuple2&lt;InputSplit,Long&gt;&gt;,这个类型说明state的存储是一个包含partiton和offset信息的列表，InputSplit代表一个分区，Long代表当前partition读取的offset。InputSplit有一个方法如下： 123public interface InputSplit extends Serializable &#123; int getSplitNumber();&#125; 也就是说，InputSplit我们可以理解为是一个Partition索引，有了这个数据结构我们在看看上面图所示的case是如何工作的？当Source的并行度是1的时候，所有打partition数据都在同一个线程中读取，所有partition的state也在同一个state中维护，State存储信息格式如下： -State.resources/736E827E-E4C1-4412-8F0C-F51AECB68329.png) 如果我们现在将并发调整为2，那么我们5个分区的State将会在2个独立的任务（线程）中进行维护，在内部实现中我们有如下算法进行分配每个Task所处理和维护partition的State信息，如下： 123456List&lt;Integer&gt; assignedPartitions = new LinkedList&lt;&gt;();for (int i = 0; i &lt; partitions; i++) &#123; if (i % consumerCount == consumerIndex) &#123; assignedPartitions.add(i); &#125;&#125; 这个求mod的算法，决定了每个并发所处理和维护partition的State信息，针对我们当前的case具体的存储情况如下： -State.resources/436E7A11-0256-4373-A3D4-ED5A4363B0E3.png) 那么到现在我们发现上面扩容后State得以很好的分配得益于OperatorState采用了List的数据结构的设计。另外大家注意一个问题，相信大家已经发现上面分配partition的算法有一个限制，那就是Source的扩容（并发数）是否可以超过Source物理存储的partition数量呢？答案是否定的，不能。目前Apache Flink的做法是提前报错，即使不报错也是资源的浪费，因为超过partition数量的并发永远分配不到待管理的partition。 KeyedState对扩容的处理对于KeyedState最容易想到的是hash(key) mod parallelism(operator) 方式分配state，就和OperatorState一样，这种分配方式大多数情况是恢复的state不是本地已有的state，需要一次网络拷贝，这种效率比较低，OperatorState采用这种简单的方式进行处理是因为OperatorState的state一般都比较小，网络拉取的成本很小，对于KeyedState往往很大，我们会有更好的选择，在Apache Flink中采用的是Key-Groups方式进行分配。 什么是Key-GroupsKey-Groups 是Apache Flink中对keyed state按照key进行分组的方式，每个key-group中会包含N&gt;0个key，一个key-group是State分配的原子单位。在Apache Flink中关于Key-Group的对象是 KeyGroupRange, 如下： 12345678public class KeyGroupRange implements KeyGroupsList, Serializable &#123; ... ... private final int startKeyGroup; private final int endKeyGroup; ... ...&#125; KeyGroupRange两个重要的属性就是 startKeyGroup和endKeyGroup，定义了startKeyGroup和endKeyGroup属性后Operator上面的Key-Group的个数也就确定了。 什么决定Key-Groups的个数key-group的数量在job启动前必须是确定的且运行中不能改变。由于key-group是state分配的原子单位，而每个operator并行实例至少包含一个key-group，因此operator的最大并行度不能超过设定的key-group的个数，那么在Apache Flink的内部实现上key-group的数量就是最大并行度的值。 GroupRange.of(0, maxParallelism)如何决定key属于哪个Key-Group确定好GroupRange之后，如何决定每个Key属于哪个Key-Group呢？我们采取的是取mod的方式，在KeyGroupRangeAssignment中的assignToKeyGroup方法会将key划分到指定的key-group中，如下： 123456789101112public static int assignToKeyGroup(Object key, int maxParallelism) &#123; return computeKeyGroupForKeyHash(key.hashCode(), maxParallelism);&#125;public static int computeKeyGroupForKeyHash(int keyHash, int maxParallelism) &#123; return HashPartitioner.INSTANCE.partition(keyHash, maxParallelism);&#125;@Overridepublic int partition(T key, int numPartitions) &#123; return MathUtils.murmurHash(Objects.hashCode(key)) % numPartitions;&#125; 如上实现我们了解到分配Key到指定的key-group的逻辑是利用key的hashCode和maxParallelism进行取余操作来分配的。如下图当parallelism=2,maxParallelism=10的情况下流上key与key-group的对应关系如下图所示： -State.resources/90D10775-4713-43B6-B7FE-F77334D29212.png) 如上图key(a)的hashCode是97，与最大并发10取余后是7，被分配到了KG-7中，流上每个event都会分配到KG-0至KG-9其中一个Key-Group中。每个Operator实例如何获取Key-Groups 了解了Key-Groups概念和如何分配每个Key到指定的Key-Groups之后，我们看看如何计算每个Operator实例所处理的Key-Groups。 在KeyGroupRangeAssignment的computeKeyGroupRangeForOperatorIndex方法描述了分配算法： 123456789101112131415161718192021222324252627282930public static KeyGroupRange computeKeyGroupRangeForOperatorIndex( int maxParallelism, int parallelism, int operatorIndex) &#123; GroupRange splitRange = GroupRange.of(0, maxParallelism).getSplitRange(parallelism, operatorIndex); int startGroup = splitRange.getStartGroup(); int endGroup = splitRange.getEndGroup();return new KeyGroupRange(startGroup, endGroup - 1);&#125;public GroupRange getSplitRange(int numSplits, int splitIndex) &#123; ... final int numGroupsPerSplit = getNumGroups() / numSplits; final int numFatSplits = getNumGroups() % numSplits; int startGroupForThisSplit; int endGroupForThisSplit; if (splitIndex &amp;lt; numFatSplits) &#123; startGroupForThisSplit = getStartGroup() + splitIndex * (numGroupsPerSplit + 1); endGroupForThisSplit = startGroupForThisSplit + numGroupsPerSplit + 1; &#125; else &#123; startGroupForThisSplit = getStartGroup() + splitIndex * numGroupsPerSplit + numFatSplits; endGroupForThisSplit = startGroupForThisSplit + numGroupsPerSplit; &#125; if (startGroupForThisSplit &amp;gt;= endGroupForThisSplit) &#123; return GroupRange.emptyGroupRange(); &#125; else &#123; return new GroupRange(startGroupForThisSplit, endGroupForThisSplit); &#125;&#125; 上面代码的核心逻辑是先计算每个Operator实例至少分配的Key-Group个数，将不能整除的部分N个，平均分给前N个实例。最终每个Operator实例管理的Key-Groups会在GroupRange中表示，本质是一个区间值；下面我们就上图的case，说明一下如何进行分配以及扩容后如何重新分配。假设上面的Stateful Operation节点的最大并行度maxParallelism的值是10，也就是我们一共有10个Key-Group，当我们并发是2的时候和并发是3的时候分配的情况如下图： -State.resources/FBDBA73F-4927-4834-8284-4893707EA6FB.png) 如上算法我们发现在进行扩容时候，大部分state还是落到本地的，如Task0只有KG-4被分出去，其他的还是保持在本地。同时我们也发现，一个job如果修改了maxParallelism的值那么会直接影响到Key-Groups的数量和key的分配，也会打乱所有的Key-Group的分配，目前在Apache Flink系统中统一将maxParallelism的默认值调整到4096，最大程度的避免无法扩容的情况发生。 小结本篇简单介绍了Apache Flink中State的概念，并重点介绍了OperatorState和KeyedState在扩容时候的处理方式。Apache Flink State是支撑Apache Flink中failover，增量计算，Window等重要机制和功能的核心设施。后续介绍failover，增量计算，Window等相关篇章中也会涉及State的利用，当涉及到本篇没有覆盖的内容时候再补充介绍。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>State</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink杂文系列Watermark]]></title>
    <url>%2F2019%2F07%2F23%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E6%9D%82%E6%96%87%E7%B3%BB%E5%88%97Watermark%2F</url>
    <content type="text"><![CDATA[实际问题（乱序）在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图： 那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？ Apache Flink的时间类型开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图： ProcessingTime 是数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中ProcessingTime具有不确定性，相同数据流多次运行有可能产生不同的计算结果。 IngestionTime IngestionTime是数据进入Apache Flink框架的时间，是在Source Operator中设置的。与ProcessingTime相比可以提供更可预测的结果，因为IngestionTime的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于ProcessingTime同一数据在流经不同窗口算子会有不同的处理时间戳。 EventTime EventTime是事件在设备上产生时候携带的。在进入Apache Flink框架之前EventTime通常要嵌入到记录中，并且EventTime也可以从记录中提取出来。在实际的网上购物订单等业务场景中，大多会使用EventTime来进行数据计算。 开篇描述的问题和本篇要介绍的Watermark所涉及的时间类型均是指EventTime类型。 什么是WatermarkWatermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示: Watermark的产生方式目前Apache Flink 有两种生产Watermark的方式，如下： Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。 Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。 所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。 Watermark的接口定义对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下： Periodic Watermarks - AssignerWithPeriodicWatermarks 1234567891011121314151617181920212223/** * Returns the current watermark. This method is periodically called by the * system to retrieve the current watermark. The method may return &#123;@code null&#125; to * indicate that no new Watermark is available. * * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp * is larger than that of the previously emitted watermark (to preserve the contract of * ascending watermarks). If the current watermark is still * identical to the previous one, no progress in EventTime has happened since * the previous call to this method. If a null value is returned, or theTimestamp * of the returned watermark is smaller than that of the last emitted one, then no * new watermark will be generated. * * &amp;lt;p&amp;gt;The interval in which this method is called and Watermarks are generated * depends on &#123;@link ExecutionConfig#getAutoWatermarkInterval()&#125;. * * @see org.Apache.flink.streaming.api.watermark.Watermark * @see ExecutionConfig#getAutoWatermarkInterval() * * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit. */ @Nullable Watermark getCurrentWatermark(); Punctuated Watermarks -AssignerWithPunctuatedWatermarks 1234567891011121314151617181920public interface AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt; extendsTimestampAssigner&amp;lt;T&amp;gt; &#123;/** * Asks this implementation if it wants to emit a watermark. This method is called right after * the &#123;@link #extractTimestamp(Object, long)&#125; method. * * &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp * is larger than that of the previously emitted watermark (to preserve the contract of * ascending watermarks). If a null value is returned, or theTimestamp of the returned * watermark is smaller than that of the last emitted one, then no new watermark will * be generated. * * &amp;lt;p&amp;gt;For an example how to use this method, see the documentation of * &#123;@link AssignerWithPunctuatedWatermarks this class&#125;. * * @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit. */ @NullableWatermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);&#125; AssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner 1234567891011121314151617public interfaceTimestampAssigner&amp;lt;T&amp;gt; extends Function &#123;/** * Assigns aTimestamp to an element, in milliseconds since the Epoch. * * &amp;lt;p&amp;gt;The method is passed the previously assignedTimestamp of the element. * That previousTimestamp may have been assigned from a previous assigner, * by ingestionTime. If the element did not carry aTimestamp before, this value is * &#123;@code Long.MIN_VALUE&#125;. * * @param element The element that theTimestamp is wil be assigned to. * @param previousElementTimestamp The previous internalTimestamp of the element, * or a negative value, if noTimestamp has been assigned, yet. * @return The newTimestamp. */long extractTimestamp(T element, long previousElementTimestamp);&#125; 从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。 Watermark解决如上问题从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。 回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。 当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下： 上面对应的DDL(Alibaba 企业版的Flink分支)定义如下： 1234567CREATE TABLE source( ..., Event_timeTimeStamp, WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0) ) with ( ...); 如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下： 上面对应的DDL(Alibaba 内部的DDL语法，目前正在和社区讨论)定义如下： 1234567CREATE TABLE source( ..., Event_timeTimeStamp, WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000) ) with ( ...); 上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s. 多流的Watermark处理在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示： Apache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图: 小结本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>杂文系列</tag>
        <tag>Watermark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink杂文系列概述]]></title>
    <url>%2F2019%2F07%2F23%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E6%9D%82%E6%96%87%E7%B3%BB%E5%88%97%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[-概述.resources/49D66A77-779B-468F-9BCB-6846609484DA.png) 摘要：Apache Flink 的命脉 “命脉” 即生命与血脉，常喻极为重要的事物。系列的首篇，首篇的首段不聊Apache Flink的历史，不聊Apache Flink的架构，不聊Apache Flink的功能特性，我们用一句话聊聊什么是 Apache Flink 的命脉？我的答案是：Apache Flink 是以”批是流的特例”的认知进行系统设计的。 “命脉” 即生命与血脉，常喻极为重要的事物。系列的首篇，首篇的首段不聊Apache Flink的历史，不聊Apache Flink的架构，不聊Apache Flink的功能特性，我们用一句话聊聊什么是 Apache Flink 的命脉？我的答案是：Apache Flink 是以”批是流的特例”的认知进行系统设计的。 我们经常听说 “天下武功，唯快不破”，大概意思是说 “任何一种武功的招数都是有拆招的，唯有速度快，快到对手根本来不及反应，你就将对手KO了，对手没有机会拆招，所以唯快不破”。 那么这与Apache Flink有什么关系呢？Apache Flink是Native Streaming(纯流式)计算引擎，在实时计算场景最关心的就是”快”,也就是 “低延时”。 就目前最热的两种流计算引擎Apache Spark和Apache Flink而言，谁最终会成为No1呢？单从 “低延时” 的角度看，Spark是Micro Batching(微批式)模式，最低延迟Spark能达到0.5~2秒左右，Flink是Native Streaming(纯流式)模式，最低延时能达到微秒。很显然是相对较晚出道的 Apache Flink 后来者居上。 那么为什么Apache Flink能做到如此之 “快”呢？根本原因是Apache Flink 设计之初就认为 “批是流的特例”，整个系统是Native Streaming设计，每来一条数据都能够触发计算。相对于需要靠时间来积攒数据Micro Batching模式来说，在架构上就已经占据了绝对优势。 那么为什么关于流计算会有两种计算模式呢？归其根本是因为对流计算的认知不同，是”流是批的特例” 和 “批是流的特例” 两种不同认知产物。 Micro Batching 模式 Micro-Batching 计算模式认为 “流是批的特例”， 流计算就是将连续不断的批进行持续计算，如果批足够小那么就有足够小的延时，在一定程度上满足了99%的实时计算场景。那么那1%为啥做不到呢？这就是架构的魅力，在Micro-Batching模式的架构实现上就有一个自然流数据流入系统进行攒批的过程，这在一定程度上就增加了延时。具体如下示意图： -概述.resources/3BF00033-D856-49C2-A301-E6DB65B22EAE.png) 很显然Micro-Batching模式有其天生的低延时瓶颈，但任何事物的存在都有两面性，在大数据计算的发展历史上，最初Hadoop上的MapReduce就是优秀的批模式计算框架，Micro-Batching在设计和实现上可以借鉴很多成熟实践。 Native Streaming 模式 Native Streaming 计算模式认为 “”批是流的特”, 这个认知更贴切流的概念，比如一些监控类的消息流，数据库操作的binlog，实时的支付交易信息等等自然流数据都是一条，一条的流入。Native Streaming 计算模式每条数据的到来都进行计算，这种计算模式显得更自然，并且延时性能达到更低。具体如下示意图： -概述.resources/68B36353-346D-4E32-9AD8-8AE91F3FE461.png) 很明显Native Streaming模式占据了流计算领域 “低延时” 的核心竞争力，当然Native Streaming模式的实现框架是一个历史先河，第一个实现Native Streaming模式的流计算框架是第一个吃螃蟹的人，需要面临更多的挑战，后续章节我们会慢慢介绍。当然Native Streaming模式的框架实现上面很容易实现Micro-Batching和Batching模式的计算，Apache Flink就是Native Streaming计算模式的流批统一的计算引擎。 Apache Flink 按不同的需求支持Local，Cluster，Cloud三种部署模式，同时Apache Flink在部署上能够与其他成熟的生态产品进行完美集成，如 Cluster模式下可以利用YARN(Yet Another Resource Negotiator）/Mesos集成进行资源管理，在Cloud部署模式下可以与GCE(Google Compute Engine), EC2(Elastic Compute Cloud)进行集成。 Local 模式 该模式下Apache Flink 整体运行在Single JVM中，在开发学习中使用，同时也可以安装到很多端类设备上。参考 Cluster模式 该模式是典型的投产的集群模式，Apache Flink 既可以Standalone的方式进行部署，也可以与其他资源管理系统进行集成部署，比如与YARN进行集成。Standalone Cluster 参考 YARN Cluster 参考这种部署模式是典型的Master/Slave模式，我们以Standalone Cluster模式为例示意如下： -概述.resources/79ACC2B0-BF5E-4DC4-99DE-9C4DF6007A3F.png) 其中JM(JobManager)是Master，TM(TaskManager)是Slave，这种Master/Slave模式有一个典型的问题就是SPOF(single point of failure), SPOF如何解决呢？Apache Flink 又提供了HA(High Availability)方案，也就是提供多个Master，在任何时候总有一个JM服役，N(N&gt;=1)个JM候选,进而解决SPOF问题，示意如下： -概述.resources/5F99E16C-1172-4E31-989B-DA7C0800D476.png) 在实际的生产环境我们都会配置HA方案，目前Alibaba内部使用的也是基于YARN Cluster的HA方案。 Cloud 模式 该模式主要是与成熟的云产品进行集成，Apache Flink官网介绍了Google的GCE 参考，Amazon的EC2 参考，在Alibaba我们也可以将Apache Flink部署到Alibaba的ECS(Elastic Compute Service)。 什么是容错 容错(Fault Tolerance) 是指容忍故障，在故障发生时能够自动检测出来并使系统能够自动回复正常运行。当出现某些指定的网络故障、硬件故障、软件错误时，系统仍能执行规定的一组程序，或者说程序不会因系统中的故障而中止，并且执行结果也不会因系统故障而引起计算差错。 容错的处理模式 在一个分布式系统中由于单个进程或者节点宕机都有可能导致整个Job失败，那么容错机制除了要保证在遇到非预期情况系统能够”运行”外，还要求能”正确运行”,也就是数据能按预期的处理方式进行处理，保证计算结果的正确性。计算结果的正确性取决于系统对每一条计算数据处理机制，一般有如下三种处理机制： At Most Once：最多消费一次，这种处理机制会存在数据丢失的可能。 At Least Once：最少消费一次，这种处理机制数据不会丢失，但是有可能重复消费。 Exactly Once：精确一次，无论何种情况下，数据都只会消费一次，这种机制是对数据准确性的最高要求，在金融支付，银行账务等领域必须采用这种模式。 Apache Flink的容错机制 Apache Flink的Job会涉及到3个部分，外部数据源(External Input), Flink内部数据处理(Flink Data Flow)和外部输出(External Output)。如下示意图: -概述.resources/C810C9BC-45F5-4ADA-8408-7FF57C0C31DB.png) 目前Apache Flink 支持两种数据容错机制： At Least Once Exactly Once 其中 Exactly Once 是最严格的容错机制，该模式要求每条数据必须处理且仅处理一次。那么对于这种严格容错机制，一个完整的Flink Job容错要做到 End-to-End 的 容错必须结合三个部分进行联合处理，根据上图我们考虑三个场景： 系统内部容错 Apache Flink利用Checkpointing机制来处理容错，Checkpointing的理论基础 Stephan 在 Lightweight Asynchronous Snapshots for Distributed Dataflows 进行了细节描述，该机制源于有K. MANI CHANDY和LESLIE LAMPORT 发表的 Determining-Global-States-of-a-Distributed-System Paper。Apache Flink 基于Checkpointing机制对Flink Data Flow实现了At Least Once 和 Exactly Once 两种容错处理模式。 Apache Flink Checkpointing的内部实现会利用 Barriers，StateBackend等后续章节会详细介绍的技术来将数据的处理进行Marker。Apache Flink会利用Barrier将整个流进行标记切分，如下示意图： -概述.resources/72919AF2-F467-4306-BE2B-042FA56E4DD4.png) 这样Apache Flink的每个Operator都会记录当前成功处理的Checkpoint，如果发生错误，就会从上一个成功的Checkpoint开始继续处理后续数据。比如 Soruce Operator会将读取外部数据源的Position实时的记录到Checkpoint中，失败时候会从Checkpoint中读取成功的position继续精准的消费数据。每个算子会在Checkpoint中记录自己恢复时候必须的数据，比如流的原始数据和中间计算结果等信息，在恢复的时候从Checkpoint中读取并持续处理流数据。 外部Source容错 Apache Flink 要做到 End-to-End 的 Exactly Once 需要外部Source的支持，比如上面我们说过 Apache Flink的Checkpointing机制会在Source节点记录读取的Position，那就需要外部数据提供读取的Position和支持根据Position进行数据读取。 外部Sink容错 Apache Flink 要做到 End-to-End 的 Exactly Once 相对比较困难，如上场景三所述，当Sink Operator节点宕机，重新恢复时候根据Apache Flink 内部系统容错 exactly once的保证,系统会回滚到上次成功的Checkpoin继续写入，但是上次成功Checkpoint之后当前Checkpoint未完成之前已经把一部分新数据写入到kafka了. Apache Flink自上次成功的Checkpoint继续写入kafka，就造成了kafka再次接收到一份同样的来自Sink Operator的数据,进而破坏了End-to-End 的 Exactly Once 语义(重复写入就变成了At Least Once了)，如果要解决这一问题，Apache Flink 利用Two phase commit(两阶段提交)的方式来进行处理。本质上是Sink Operator 需要感知整体Checkpoint的完成，并在整体Checkpoint完成时候将计算结果写入Kafka。 批与流是两种不同的数据处理模式，如Apache Storm只支持流模式的数据处理，Apache Spark只支持批(Micro Batching)模式的数据处理。那么Apache Flink 是如何做到既支持流处理模式也支持批处理模式呢？ 统一的数据传输层 开篇我们就介绍Apache Flink 的 “命脉”是以”批是流的特例”为导向来进行引擎的设计的，系统设计成为 “Native Streaming”的模式进行数据处理。那么Apache FLink将批模式执行的任务看做是流式处理任务的特殊情况，只是在数据上批是有界的(有限数量的元素)。 Apache Flink 在网络传输层面有两种数据传输模式： PIPELINED模式 - 即一条数据被处理完成以后，立刻传输到下一个节点进行处理。 BATCH 模式 - 即一条数据被处理完成后，并不会立刻传输到下一个节点进行处理，而是写入到缓存区，如果缓存写满就持久化到本地硬盘上，最后当所有数据都被处理完成后，才将数据传输到下一个节点进行处理。 对于批任务而言同样可以利用PIPELINED模式，比如我要做count统计，利用PIPELINED模式能拿到更好的执行性能。只有在特殊情况，比如SortMergeJoin，这时候我们需要全局数据排序，才需要BATCH模式。大部分情况流与批可用统一的传输策略，只有特殊情况，才将批看做是流的一个特例继续特殊处理。 统一任务调度层 Apache Flink 在任务调度上流与批共享统一的资源和任务调度机制（后续章节会详细介绍）。 统一的用户API层 Apache Flink 在DataStremAPI和DataSetAPI基础上，为用户提供了流批统一的上层TableAPI和SQL，在语法和语义上流批进行高度统一。(其中DataStremAPI和DataSetAPI对流和批进行了分别抽象，这一点并不优雅，在Alibaba内部对其进行了统一抽象）。 求同存异 Apache Flink 是流批统一的计算引擎，并不意味着流与批的任务都走统一的code path，在对底层的具体算子的实现也是有各自的处理的，在具体功能上面会根据不同的特性区别处理。比如 批没有Checkpoint机制，流上不能做SortMergeJoin。 组件栈 我们上面内容已经介绍了很多Apache Flink的各种组件，下面我们整体概览一下全貌，如下： -概述.resources/624AD4C0-351D-42C0-ACEA-E30851223B5F.png) TableAPI和SQL都建立在DataSetAPI和DataStreamAPI的基础之上，那么TableAPI和SQL是如何转换为DataStream和DataSet的呢？ TableAPI&amp;SQL到DataStrem&amp;DataSet的架构 TableAPI&amp;SQL最终会经过Calcite优化之后转换为DataStream和DataSet，具体转换示意如下： -概述.resources/DC074C98-3D57-4997-AAF6-896BEF272F84.png) 对于流任务最终会转换成DataStream，对于批任务最终会转换成DataSet。 ANSI-SQL的支持 Apache Flink 之所以利用ANSI-SQL作为用户统一的开发语言，是因为SQL有着非常明显的优点，如下： -概述.resources/7C067E43-57A2-4A83-BE9C-3C769DDFF6C6.png) Declarative - 用户只需要表达我想要什么，不用关心如何计算。 Optimized - 查询优化器可以为用户的 SQL 生成最优的执行计划，获取最好的查询性能。 Understandable - SQL语言被不同领域的人所熟知，用SQL 作为跨团队的开发语言可以很大地提高效率。 Stable - SQL 是一个拥有几十年历史的语言，是一个非常稳定的语言，很少有变动。 Unify - Apache Flink在引擎上对流与批进行统一，同时又利用ANSI-SQL在语法和语义层面进行统一。 无限扩展的优化机制 Apache Flink 利用Apache Calcite对SQL进行解析和优化，Apache Calcite采用Calcite是开源的一套查询引擎，实现了两套Planner： HepPlanner - 是RBO(Rule Base Optimize)模式，基于规则的优化。 VolcanoPlanner - 是CBO(Cost Base Optimize)模式，基于成本的优化。 Flink SQL会利用Calcite解析优化之后，最终转换为底层的DataStrem和Dataset。上图中 Batch rules和Stream rules可以根据优化需要无限添加优化规则。 Apache Flink 优秀的架构就像一座摩天大厦的地基一样为Apache Flink 持久的生命力打下了良好的基础，为打造Apache Flink丰富的功能生态留下无限的空间。 类库 CEP - 复杂事件处理类库，核心是一个状态机，广泛应用于事件驱动的监控预警类业务场景。 ML - 机器学习类库，机器学习主要是识别数据中的关系、趋势和模式，一般应用在预测类业务场景。 GELLY - 图计算类库，图计算更多的是考虑边和点的概念，一般被用来解决网状关系的业务场景。 算子 Apache Flink 提供了丰富的功能算子，对于数据流的处理来讲，可以分为单流处理(一个数据源)和多流处理(多个数据源)。 多流操作 如上通过UION和JOIN我们可以将多流最终变成单流，Apache Flink 在单流上提供了更多的操作算子。 单流操作 将多流变成单流之后，我们按数据输入输出的不同归类如下： 类型 输入 输出 Table/SQL算子 DataStream/DataSet算子Scalar Function 1 1 Built-in &amp; UDF, MapTable Function 1 N(N&gt;=0) Built-in &amp; UDTF FlatMapAggregate Function N(N&gt;=0) 1 Built-in &amp; UDAF Reduce 如上表格对单流上面操作做简单归类，除此之外还可以做 过滤，排序，窗口等操作，我们后续章节会逐一介绍。 存在的问题 Apache Flink 目前的架构还存在很大的优化空间，比如前面提到的DataStreamAPI和DataSetAPI其实是流与批在API层面不统一的体现，同时看具体实现会发现DataStreamAPI会生成Transformation tree然后生成StreamGraph，最后生成JobGraph，底层对应StreamTask，但DataSetAPI会形成Operator tree，flink-optimize模块会对Batch Plan进行优化，形成Optimized Plan 后形成JobGraph,最后形成BatchTask。具体示意如下： -概述.resources/1DEA43A8-381D-451C-9B99-9106B2B058B5.png) 这种情况其实 DataStreamAPI到Runtime 和 DataSetAPI到Runtime的实现上并没有得到最大程度的统一和复用。在这一点上面Aalibab 企业版的Flink在架构和实现上都进行了进一步优化。 组件栈 Alibaba 对Apache Flink进行了大量的架构优化，如下架构是一直努力的方向，大部分功能还在持续开发中，具体如下： -概述.resources/928F452D-B175-4740-A139-8186EEDAC99C.png) 如上架构我们发现较大的变化是： QP/QE/QO - 我们增加了QP/QE/QO层，在这一层进行统一的流和批的查询优化和底层算子的转换。 DAG API - 我们在Runtime层面统一抽象API接口，在API层对流与批进行统一。 TableAPI&amp;SQL到Runtime的架构 Apache Flink执行层是流批统一的设计，在API和算子设计上面我们尽量达到流批的共享，在TableAPI和SQL层无论是流任务还是批任务最终都转换为统一的底层实现。这个层面最核心的变化是批最终也会生成StreamGraph，执行层运行Stream Task，如下： -概述.resources/61F69F73-CEE1-43F4-BB88-177A115FC62E.png) 本篇概要的介绍了”批是流的特例”这一设计观点是Apache Flink的”命脉”，它决定了Apache Flink的运行模式是纯流式的，这在实时计算场景的”低延迟”需求上，相对于Micro Batching模式占据了架构的绝对优势，同时概要的向大家介绍了Apache Flink的部署模式，容错处理，引擎的统一性和Apache Flink的架构，最后和大家分享了Apache Flink的优化架构。 本篇没有对具体技术进行详细展开，大家只要对Apache Flink有初步感知，头脑中知道Alibaba对Apache Flink进行了架构优化，增加了众多功能就可以了，至于Apache Flink的具体技术细节和实现原理，以及Alibaba对Apache Flink做了哪些架构优化和增加了哪些功能后续章节会展开介绍！]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>杂文系列</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink_Standalone_Cluster]]></title>
    <url>%2F2019%2F07%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink_Standalone_Cluster%2F</url>
    <content type="text"><![CDATA[Flink Standalone Cluster一、部署模式二、单机模式2.1 安装部署2.2 作业提交2.3 停止作业2.4 停止 Flink 三、Standalone Cluster3.1 前置条件3.2 搭建步骤3.3 可选配置四、Standalone Cluster HA4.1 前置条件4.2 搭建步骤4.3 常见异常 一、部署模式Flink 支持使用多种部署模式来满足不同规模应用的需求，常见的有单机模式，Standalone Cluster 模式，同时 Flink 也支持部署在其他第三方平台上，如 YARN，Mesos，Docker，Kubernetes 等。以下主要介绍其单机模式和 Standalone Cluster 模式的部署。 二、单机模式单机模式是一种开箱即用的模式，可以在单台服务器上运行，适用于日常的开发和调试。具体操作步骤如下： 2.1 安装部署1. 前置条件 Flink 的运行依赖 JAVA 环境，故需要预先安装好 JDK，具体步骤可以参考：Linux 环境下 JDK 安装 2. 下载 &amp; 解压 &amp; 运行 Flink 所有版本的安装包可以直接从其官网进行下载，这里我下载的 Flink 的版本为 1.9.1 ，要求的 JDK 版本为 1.8.x +。 下载后解压到指定目录： 1tar -zxvf flink-1.9.1-bin-scala_2.12.tgz -C /usr/app 不需要进行任何配置，直接使用以下命令就可以启动单机版本的 Flink： 1bin/start-cluster.sh 3. WEB UI 界面 Flink 提供了 WEB 界面用于直观的管理 Flink 集群，访问端口为 8081： Flink 的 WEB UI 界面支持大多数常用功能，如提交作业，取消作业，查看各个节点运行情况，查看作业执行情况等，大家可以在部署完成后，进入该页面进行详细的浏览。 2.2 作业提交启动后可以运行安装包中自带的词频统计案例，具体步骤如下： 1. 开启端口 1nc -lk 9999 2. 提交作业 1bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9999 该 JAR 包的源码可以在 Flink 官方的 GitHub 仓库中找到，地址为 ：SocketWindowWordCount ，可选传参有 hostname， port，对应的词频数据需要使用空格进行分割。 3. 输入测试数据 1a a b b c c c a e 4. 查看控制台输出 可以通过 WEB UI 的控制台查看作业统运行情况： 也可以通过 WEB 控制台查看到统计结果： 2.3 停止作业可以直接在 WEB 界面上点击对应作业的 Cancel Job 按钮进行取消，也可以使用命令行进行取消。使用命令行进行取消时，需要先获取到作业的 JobId，可以使用 flink list 命令查看，输出如下： 123456[root@hadoop001 flink-1.9.1]# ./bin/flink listWaiting for response...------------------ Running/Restarting Jobs -------------------05.11.2019 08:19:53 : ba2b1cc41a5e241c32d574c93de8a2bc : Socket Window WordCount (RUNNING)--------------------------------------------------------------No scheduled jobs. 获取到 JobId 后，就可以使用 flink cancel 命令取消作业： 1bin/flink cancel ba2b1cc41a5e241c32d574c93de8a2bc 2.4 停止 Flink命令如下： 1bin/stop-cluster.sh 三、Standalone ClusterStandalone Cluster 模式是 Flink 自带的一种集群模式，具体配置步骤如下： 3.1 前置条件使用该模式前，需要确保所有服务器间都已经配置好 SSH 免密登录服务。这里我以三台服务器为例，主机名分别为 hadoop001，hadoop002，hadoop003 , 其中 hadoop001 为 master 节点，其余两台为 slave 节点，搭建步骤如下： 3.2 搭建步骤修改 conf/flink-conf.yaml 中 jobmanager 节点的通讯地址为 hadoop001: 1jobmanager.rpc.address: hadoop001 修改 conf/slaves 配置文件，将 hadoop002 和 hadoop003 配置为 slave 节点： 12hadoop002hadoop003 将配置好的 Flink 安装包分发到其他两台服务器上： 12scp -r /usr/app/flink-1.9.1 hadoop002:/usr/appscp -r /usr/app/flink-1.9.1 hadoop003:/usr/app 在 hadoop001 上使用和单机模式相同的命令来启动集群： 1bin/start-cluster.sh 此时控制台输出如下： 启动完成后可以使用 Jps 命令或者通过 WEB 界面来查看是否启动成功。 3.3 可选配置除了上面介绍的 jobmanager.rpc.address 是必选配置外，Flink h还支持使用其他可选参数来优化集群性能，主要如下： jobmanager.heap.size：JobManager 的 JVM 堆内存大小，默认为 1024m 。 taskmanager.heap.size：Taskmanager 的 JVM 堆内存大小，默认为 1024m 。 taskmanager.numberOfTaskSlots：Taskmanager 上 slots 的数量，通常设置为 CPU 核心的数量，或其一半。 parallelism.default：任务默认的并行度。 io.tmp.dirs：存储临时文件的路径，如果没有配置，则默认采用服务器的临时目录，如 LInux 的 /tmp 目录。 更多配置可以参考 Flink 的官方手册：Configuration 四、Standalone Cluster HA上面我们配置的 Standalone 集群实际上只有一个 JobManager，此时是存在单点故障的，所以官方提供了 Standalone Cluster HA 模式来实现集群高可用。 4.1 前置条件在 Standalone Cluster HA 模式下，集群可以由多个 JobManager，但只有一个处于 active 状态，其余的则处于备用状态，Flink 使用 ZooKeeper 来选举出 Active JobManager，并依赖其来提供一致性协调服务，所以需要预先安装 ZooKeeper 。 另外在高可用模式下，还需要使用分布式文件系统来持久化存储 JobManager 的元数据，最常用的就是 HDFS，所以 Hadoop 也需要预先安装。关于 Hadoop 集群和 ZooKeeper 集群的搭建可以参考： Hadoop 集群环境搭建 Zookeeper 单机环境和集群环境搭建 4.2 搭建步骤修改 conf/flink-conf.yaml 文件，增加如下配置： 12345678910# 配置使用zookeeper来开启高可用模式high-availability: zookeeper# 配置zookeeper的地址，采用zookeeper集群时，可以使用逗号来分隔多个节点地址high-availability.zookeeper.quorum: hadoop003:2181# 在zookeeper上存储flink集群元信息的路径high-availability.zookeeper.path.root: /flink# 集群idhigh-availability.cluster-id: /standalone_cluster_one# 持久化存储JobManager元数据的地址，zookeeper上存储的只是指向该元数据的指针信息high-availability.storageDir: hdfs://hadoop001:8020/flink/recovery 修改 conf/masters 文件，将 hadoop001 和 hadoop002 都配置为 master 节点： 12hadoop001:8081hadoop002:8081 确保 Hadoop 和 ZooKeeper 已经启动后，使用以下命令来启动集群： 1bin/start-cluster.sh 此时输出如下： 可以看到集群已经以 HA 的模式启动，此时还需要在各个节点上使用 jps 命令来查看进程是否启动成功，正常情况如下： 只有 hadoop001 和 hadoop002 的 JobManager 进程，hadoop002 和 hadoop003 上的 TaskManager 进程都已经完全启动，才表示 Standalone Cluster HA 模式搭建成功。 4.3 常见异常如果进程没有启动，可以通过查看 log 目录下的日志来定位错误，常见的一个错误如下： 12345678910112019-11-05 09:18:35,877 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint - Shutting StandaloneSessionClusterEntrypoint down with application status FAILED. Diagnosticsjava.io.IOException: Could not create FileSystem for highly available storage (high-availability.storageDir).......Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 'hdfs'. The scheme is not directly supported by Flink and no Hadoop file system to support this scheme could be loaded......Caused by: org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Hadoop is not in the classpath/dependencies....... 可以看到是因为在 classpath 目录下找不到 Hadoop 的相关依赖，此时需要检查是否在环境变量中配置了 Hadoop 的安装路径，如果路径已经配置但仍然存在上面的问题，可以从 Flink 官网下载对应版本的 Hadoop 组件包： 下载完成后，将该 JAR 包上传至所有 Flink 安装目录的 lib 目录即可。 参考资料 Standalone Cluster JobManager High Availability (HA)]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>Standalone</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink状态管理与检查点机制]]></title>
    <url>%2F2019%2F07%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E7%8A%B6%E6%80%81%E7%AE%A1%E7%90%86%E4%B8%8E%E6%A3%80%E6%9F%A5%E7%82%B9%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[Flink 状态管理一、状态分类2.1 算子状态2.2 键控状态二、状态编程2.1 键控状态2.2 状态有效期2.3 算子状态三、检查点机制3.1 CheckPoints3.2 开启检查点3.3 保存点机制四、状态后端4.1 状态管理器分类4.2 配置方式 一、状态分类相对于其他流计算框架，Flink 一个比较重要的特性就是其支持有状态计算。即你可以将中间的计算结果进行保存，并提供给后续的计算使用： 具体而言，Flink 又将状态 (State) 分为 Keyed State 与 Operator State： 2.1 算子状态算子状态 (Operator State)：顾名思义，状态是和算子进行绑定的，一个算子的状态不能被其他算子所访问到。官方文档上对 Operator State 的解释是：each operator state is bound to one parallel operator instance，所以更为确切的说一个算子状态是与一个并发的算子实例所绑定的，即假设算子的并行度是 2，那么其应有两个对应的算子状态： 2.2 键控状态键控状态 (Keyed State) ：是一种特殊的算子状态，即状态是根据 key 值进行区分的，Flink 会为每类键值维护一个状态实例。如下图所示，每个颜色代表不同 key 值，对应四个不同的状态实例。需要注意的是键控状态只能在 KeyedStream 上进行使用，我们可以通过 stream.keyBy(...) 来得到 KeyedStream 。 二、状态编程2.1 键控状态Flink 提供了以下数据格式来管理和存储键控状态 (Keyed State)： ValueState：存储单值类型的状态。可以使用 update(T) 进行更新，并通过 T value() 进行检索。 ListState：存储列表类型的状态。可以使用 add(T) 或 addAll(List) 添加元素；并通过 get() 获得整个列表。 ReducingState：用于存储经过 ReduceFunction 计算后的结果，使用 add(T) 增加元素。 AggregatingState：用于存储经过 AggregatingState 计算后的结果，使用 add(IN) 添加元素。 FoldingState：已被标识为废弃，会在未来版本中移除，官方推荐使用 AggregatingState 代替。 MapState：维护 Map 类型的状态。 以上所有增删改查方法不必硬记，在使用时通过语法提示来调用即可。这里给出一个具体的使用示例：假设我们正在开发一个监控系统，当监控数据超过阈值一定次数后，需要发出报警信息。这里之所以要达到一定次数，是因为由于偶发原因，偶尔一次超过阈值并不能代表什么，故需要达到一定次数后才触发报警，这就需要使用到 Flink 的状态编程。相关代码如下： 123456789101112131415161718192021222324252627282930313233343536373839public class ThresholdWarning extends RichFlatMapFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, List&lt;Long&gt;&gt;&gt; &#123; // 通过ListState来存储非正常数据的状态 private transient ListState&lt;Long&gt; abnormalData; // 需要监控的阈值 private Long threshold; // 触发报警的次数 private Integer numberOfTimes; ThresholdWarning(Long threshold, Integer numberOfTimes) &#123; this.threshold = threshold; this.numberOfTimes = numberOfTimes; &#125; @Override public void open(Configuration parameters) &#123; // 通过状态名称(句柄)获取状态实例，如果不存在则会自动创建 abnormalData = getRuntimeContext().getListState( new ListStateDescriptor&lt;&gt;("abnormalData", Long.class)); &#125; @Override public void flatMap(Tuple2&lt;String, Long&gt; value, Collector&lt;Tuple2&lt;String, List&lt;Long&gt;&gt;&gt; out) throws Exception &#123; Long inputValue = value.f1; // 如果输入值超过阈值，则记录该次不正常的数据信息 if (inputValue &gt;= threshold) &#123; abnormalData.add(inputValue); &#125; ArrayList&lt;Long&gt; list = Lists.newArrayList(abnormalData.get().iterator()); // 如果不正常的数据出现达到一定次数，则输出报警信息 if (list.size() &gt;= numberOfTimes) &#123; out.collect(Tuple2.of(value.f0 + " 超过指定阈值 ", list)); // 报警信息输出后，清空状态 abnormalData.clear(); &#125; &#125;&#125; 调用自定义的状态监控，这里我们使用 a，b 来代表不同类型的监控数据，分别对其数据进行监控： 1234567891011final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();DataStreamSource&lt;Tuple2&lt;String, Long&gt;&gt; tuple2DataStreamSource = env.fromElements( Tuple2.of("a", 50L), Tuple2.of("a", 80L), Tuple2.of("a", 400L), Tuple2.of("a", 100L), Tuple2.of("a", 200L), Tuple2.of("a", 200L), Tuple2.of("b", 100L), Tuple2.of("b", 200L), Tuple2.of("b", 200L), Tuple2.of("b", 500L), Tuple2.of("b", 600L), Tuple2.of("b", 700L));tuple2DataStreamSource .keyBy(0) .flatMap(new ThresholdWarning(100L, 3)) // 超过100的阈值3次后就进行报警 .printToErr();env.execute("Managed Keyed State"); 输出如下结果如下： 2.2 状态有效期以上任何类型的 keyed state 都支持配置有效期 (TTL) ，示例如下： 1234567891011StateTtlConfig ttlConfig = StateTtlConfig // 设置有效期为 10 秒 .newBuilder(Time.seconds(10)) // 设置有效期更新规则，这里设置为当创建和写入时，都重置其有效期到规定的10秒 .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite) /*设置只要值过期就不可见，另外一个可选值是ReturnExpiredIfNotCleanedUp， 代表即使值过期了，但如果还没有被物理删除，就是可见的*/ .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired) .build();ListStateDescriptor&lt;Long&gt; descriptor = new ListStateDescriptor&lt;&gt;("abnormalData", Long.class);descriptor.enableTimeToLive(ttlConfig); 2.3 算子状态相比于键控状态，算子状态目前支持的存储类型只有以下三种： ListState：存储列表类型的状态。 UnionListState：存储列表类型的状态，与 ListState 的区别在于：如果并行度发生变化，ListState 会将该算子的所有并发的状态实例进行汇总，然后均分给新的 Task；而 UnionListState 只是将所有并发的状态实例汇总起来，具体的划分行为则由用户进行定义。 BroadcastState：用于广播的算子状态。 这里我们继续沿用上面的例子，假设此时我们不需要区分监控数据的类型，只要有监控数据超过阈值并达到指定的次数后，就进行报警，代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758public class ThresholdWarning extends RichFlatMapFunction&lt;Tuple2&lt;String, Long&gt;, Tuple2&lt;String, List&lt;Tuple2&lt;String, Long&gt;&gt;&gt;&gt; implements CheckpointedFunction &#123; // 非正常数据 private List&lt;Tuple2&lt;String, Long&gt;&gt; bufferedData; // checkPointedState private transient ListState&lt;Tuple2&lt;String, Long&gt;&gt; checkPointedState; // 需要监控的阈值 private Long threshold; // 次数 private Integer numberOfTimes; ThresholdWarning(Long threshold, Integer numberOfTimes) &#123; this.threshold = threshold; this.numberOfTimes = numberOfTimes; this.bufferedData = new ArrayList&lt;&gt;(); &#125; @Override public void initializeState(FunctionInitializationContext context) throws Exception &#123; // 注意这里获取的是OperatorStateStore checkPointedState = context.getOperatorStateStore(). getListState(new ListStateDescriptor&lt;&gt;("abnormalData", TypeInformation.of(new TypeHint&lt;Tuple2&lt;String, Long&gt;&gt;() &#123; &#125;))); // 如果发生重启，则需要从快照中将状态进行恢复 if (context.isRestored()) &#123; for (Tuple2&lt;String, Long&gt; element : checkPointedState.get()) &#123; bufferedData.add(element); &#125; &#125; &#125; @Override public void flatMap(Tuple2&lt;String, Long&gt; value, Collector&lt;Tuple2&lt;String, List&lt;Tuple2&lt;String, Long&gt;&gt;&gt;&gt; out) &#123; Long inputValue = value.f1; // 超过阈值则进行记录 if (inputValue &gt;= threshold) &#123; bufferedData.add(value); &#125; // 超过指定次数则输出报警信息 if (bufferedData.size() &gt;= numberOfTimes) &#123; // 顺便输出状态实例的hashcode out.collect(Tuple2.of(checkPointedState.hashCode() + "阈值警报！", bufferedData)); bufferedData.clear(); &#125; &#125; @Override public void snapshotState(FunctionSnapshotContext context) throws Exception &#123; // 在进行快照时，将数据存储到checkPointedState checkPointedState.clear(); for (Tuple2&lt;String, Long&gt; element : bufferedData) &#123; checkPointedState.add(element); &#125; &#125;&#125; 调用自定义算子状态，这里需要将并行度设置为 1： 1234567891011121314final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 开启检查点机制env.enableCheckpointing(1000);// 设置并行度为1DataStreamSource&lt;Tuple2&lt;String, Long&gt;&gt; tuple2DataStreamSource = env.setParallelism(1).fromElements( Tuple2.of("a", 50L), Tuple2.of("a", 80L), Tuple2.of("a", 400L), Tuple2.of("a", 100L), Tuple2.of("a", 200L), Tuple2.of("a", 200L), Tuple2.of("b", 100L), Tuple2.of("b", 200L), Tuple2.of("b", 200L), Tuple2.of("b", 500L), Tuple2.of("b", 600L), Tuple2.of("b", 700L));tuple2DataStreamSource .flatMap(new ThresholdWarning(100L, 3)) .printToErr();env.execute("Managed Keyed State");&#125; 此时输出如下： 在上面的调用代码中，我们将程序的并行度设置为 1，可以看到三次输出中状态实例的 hashcode 全是一致的，证明它们都同一个状态实例。假设将并行度设置为 2，此时输出如下： 可以看到此时两次输出中状态实例的 hashcode 是不一致的，代表它们不是同一个状态实例，这也就是上文提到的，一个算子状态是与一个并发的算子实例所绑定的。同时这里只输出两次，是因为在并发处理的情况下，线程 1 可能拿到 5 个非正常值，线程 2 可能拿到 4 个非正常值，因为要大于 3 次才能输出，所以在这种情况下就会出现只输出两条记录的情况，所以需要将程序的并行度设置为 1。 三、检查点机制3.1 CheckPoints为了使 Flink 的状态具有良好的容错性，Flink 提供了检查点机制 (CheckPoints) 。通过检查点机制，Flink 定期在数据流上生成 checkpoint barrier ，当某个算子收到 barrier 时，即会基于当前状态生成一份快照，然后再将该 barrier 传递到下游算子，下游算子接收到该 barrier 后，也基于当前状态生成一份快照，依次传递直至到最后的 Sink 算子上。当出现异常后，Flink 就可以根据最近的一次的快照数据将所有算子恢复到先前的状态。 3.2 开启检查点默认情况下，检查点机制是关闭的，需要在程序中进行开启： 1234567891011121314151617// 开启检查点机制，并指定状态检查点之间的时间间隔env.enableCheckpointing(1000); // 其他可选配置如下：// 设置语义env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);// 设置两个检查点之间的最小时间间隔env.getCheckpointConfig().setMinPauseBetweenCheckpoints(500);// 设置执行Checkpoint操作时的超时时间env.getCheckpointConfig().setCheckpointTimeout(60000);// 设置最大并发执行的检查点的数量env.getCheckpointConfig().setMaxConcurrentCheckpoints(1);// 将检查点持久化到外部存储env.getCheckpointConfig().enableExternalizedCheckpoints( ExternalizedCheckpointCleanup.RETAIN_ON_CANCELLATION);// 如果有更近的保存点时，是否将作业回退到该检查点env.getCheckpointConfig().setPreferCheckpointForRecovery(true); 3.3 保存点机制保存点机制 (Savepoints) 是检查点机制的一种特殊的实现，它允许你通过手工的方式来触发 Checkpoint，并将结果持久化存储到指定路径中，主要用于避免 Flink 集群在重启或升级时导致状态丢失。示例如下： 12# 触发指定id的作业的Savepoint，并将结果存储到指定目录下bin/flink savepoint :jobId [:targetDirectory] 更多命令和配置可以参考官方文档：savepoints 四、状态后端4.1 状态管理器分类默认情况下，所有的状态都存储在 JVM 的堆内存中，在状态数据过多的情况下，这种方式很有可能导致内存溢出，因此 Flink 该提供了其它方式来存储状态数据，这些存储方式统一称为状态后端 (或状态管理器)： 主要有以下三种： 1. MemoryStateBackend默认的方式，即基于 JVM 的堆内存进行存储，主要适用于本地开发和调试。 2. FsStateBackend基于文件系统进行存储，可以是本地文件系统，也可以是 HDFS 等分布式文件系统。 需要注意而是虽然选择使用了 FsStateBackend ，但正在进行的数据仍然是存储在 TaskManager 的内存中的，只有在 checkpoint 时，才会将状态快照写入到指定文件系统上。 3. RocksDBStateBackendRocksDBStateBackend 是 Flink 内置的第三方状态管理器，采用嵌入式的 key-value 型数据库 RocksDB 来存储正在进行的数据。等到 checkpoint 时，再将其中的数据持久化到指定的文件系统中，所以采用 RocksDBStateBackend 时也需要配置持久化存储的文件系统。之所以这样做是因为 RocksDB 作为嵌入式数据库安全性比较低，但比起全文件系统的方式，其读取速率更快；比起全内存的方式，其存储空间更大，因此它是一种比较均衡的方案。 4.2 配置方式Flink 支持使用两种方式来配置后端管理器： 第一种方式：基于代码方式进行配置，只对当前作业生效： 1234// 配置 FsStateBackendenv.setStateBackend(new FsStateBackend("hdfs://namenode:40010/flink/checkpoints"));// 配置 RocksDBStateBackendenv.setStateBackend(new RocksDBStateBackend("hdfs://namenode:40010/flink/checkpoints")); 配置 RocksDBStateBackend 时，需要额外导入下面的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-statebackend-rocksdb_2.11&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt;&lt;/dependency&gt; 第二种方式：基于 flink-conf.yaml 配置文件的方式进行配置，对所有部署在该集群上的作业都生效： 12state.backend: filesystemstate.checkpoints.dir: hdfs://namenode:40010/flink/checkpoints 注：本篇文章所有示例代码下载地址：flink-state-management 参考资料 Working with State Checkpointing Savepoints State Backends Fabian Hueske , Vasiliki Kalavri . 《Stream Processing with Apache Flink》. O’Reilly Media . 2019-4-30]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>状态管理</tag>
        <tag>检查点机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink_Windows]]></title>
    <url>%2F2019%2F07%2F19%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink_Windows%2F</url>
    <content type="text"><![CDATA[Flink Windows一、窗口概念二、Time Windows2.1 Tumbling Windows2.2 Sliding Windows2.3 Session Windows2.4 Global Windows三、Count Windows 一、窗口概念在大多数场景下，我们需要统计的数据流都是无界的，因此我们无法等待整个数据流终止后才进行统计。通常情况下，我们只需要对某个时间范围或者数量范围内的数据进行统计分析：如每隔五分钟统计一次过去一小时内所有商品的点击量；或者每发生1000次点击后，都去统计一下每个商品点击率的占比。在 Flink 中，我们使用窗口 (Window) 来实现这类功能。按照统计维度的不同，Flink 中的窗口可以分为 时间窗口 (Time Windows) 和 计数窗口 (Count Windows) 。 二、Time WindowsTime Windows 用于以时间为维度来进行数据聚合，具体分为以下四类： 2.1 Tumbling Windows滚动窗口 (Tumbling Windows) 是指彼此之间没有重叠的窗口。例如：每隔1小时统计过去1小时内的商品点击量，那么 1 天就只能分为 24 个窗口，每个窗口彼此之间是不存在重叠的，具体如下： 这里我们以词频统计为例，给出一个具体的用例，代码如下： 12345678910111213final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 接收socket上的数据输入DataStreamSource&lt;String&gt; streamSource = env.socketTextStream("hadoop001", 9999, "\n", 3);streamSource.flatMap(new FlatMapFunction&lt;String, Tuple2&lt;String, Long&gt;&gt;() &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Long&gt;&gt; out) throws Exception &#123; String[] words = value.split("\t"); for (String word : words) &#123; out.collect(new Tuple2&lt;&gt;(word, 1L)); &#125; &#125;&#125;).keyBy(0).timeWindow(Time.seconds(3)).sum(1).print(); //每隔3秒统计一次每个单词出现的数量env.execute("Flink Streaming"); 测试结果如下： 2.2 Sliding Windows滑动窗口用于滚动进行聚合分析，例如：每隔 6 分钟统计一次过去一小时内所有商品的点击量，那么统计窗口彼此之间就是存在重叠的，即 1天可以分为 240 个窗口。图示如下： 可以看到 window 1 - 4 这四个窗口彼此之间都存在着时间相等的重叠部分。想要实现滑动窗口，只需要在使用 timeWindow 方法时额外传递第二个参数作为滚动时间即可，具体如下： 12// 每隔3秒统计一次过去1分钟内的数据timeWindow(Time.minutes(1),Time.seconds(3)) 2.3 Session Windows当用户在进行持续浏览时，可能每时每刻都会有点击数据，例如在活动区间内，用户可能频繁的将某类商品加入和移除购物车，而你只想知道用户本次浏览最终的购物车情况，此时就可以在用户持有的会话结束后再进行统计。想要实现这类统计，可以通过 Session Windows 来进行实现。 具体的实现代码如下： 1234// 以处理时间为衡量标准，如果10秒内没有任何数据输入，就认为会话已经关闭，此时触发统计window(ProcessingTimeSessionWindows.withGap(Time.seconds(10)))// 以事件时间为衡量标准 window(EventTimeSessionWindows.withGap(Time.seconds(10))) 2.4 Global Windows最后一个窗口是全局窗口， 全局窗口会将所有 key 相同的元素分配到同一个窗口中，其通常配合触发器 (trigger) 进行使用。如果没有相应触发器，则计算将不会被执行。 这里继续以上面词频统计的案例为例，示例代码如下： 12// 当单词累计出现的次数每达到10次时，则触发计算，计算整个窗口内该单词出现的总数window(GlobalWindows.create()).trigger(CountTrigger.of(10)).sum(1).print(); 三、Count WindowsCount Windows 用于以数量为维度来进行数据聚合，同样也分为滚动窗口和滑动窗口，实现方式也和时间窗口完全一致，只是调用的 API 不同，具体如下： 1234// 滚动计数窗口，每1000次点击则计算一次countWindow(1000)// 滑动计数窗口，每10次点击发生后，则计算过去1000次点击的情况countWindow(1000,10) 实际上计数窗口内部就是调用的我们上一部分介绍的全局窗口来实现的，其源码如下： 12345678910public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size) &#123; return window(GlobalWindows.create()).trigger(PurgingTrigger.of(CountTrigger.of(size)));&#125;public WindowedStream&lt;T, KEY, GlobalWindow&gt; countWindow(long size, long slide) &#123; return window(GlobalWindows.create()) .evictor(CountEvictor.of(size)) .trigger(CountTrigger.of(slide));&#125; 参考资料Flink Windows： https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/windows.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>Windows</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink_Data_Sink]]></title>
    <url>%2F2019%2F07%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink_Data_Sink%2F</url>
    <content type="text"><![CDATA[Flink Sink一、Data Sinks1.1 writeAsText1.2 writeAsCsv1.3 print printToErr1.4 writeUsingOutputFormat1.5 writeToSocket二、Streaming Connectors三、整合 Kafka Sink3.1 addSink3.2 创建输出主题3.3 启动消费者3.4 测试结果四、自定义 Sink4.1 导入依赖4.2 自定义 Sink4.3 使用自定义 Sink4.4 测试结果 一、Data Sinks在使用 Flink 进行数据处理时，数据经 Data Source 流入，然后通过系列 Transformations 的转化，最终可以通过 Sink 将计算结果进行输出，Flink Data Sinks 就是用于定义数据流最终的输出位置。Flink 提供了几个较为简单的 Sink API 用于日常的开发，具体如下： 1.1 writeAsTextwriteAsText 用于将计算结果以文本的方式并行地写入到指定文件夹下，除了路径参数是必选外，该方法还可以通过指定第二个参数来定义输出模式，它有以下两个可选值： WriteMode.NO_OVERWRITE：当指定路径上不存在任何文件时，才执行写出操作； WriteMode.OVERWRITE：不论指定路径上是否存在文件，都执行写出操作；如果原来已有文件，则进行覆盖。 使用示例如下： 1streamSource.writeAsText("D:\\out", FileSystem.WriteMode.OVERWRITE); 以上写出是以并行的方式写出到多个文件，如果想要将输出结果全部写出到一个文件，需要设置其并行度为 1： 1streamSource.writeAsText("D:\\out", FileSystem.WriteMode.OVERWRITE).setParallelism(1); 1.2 writeAsCsvwriteAsCsv 用于将计算结果以 CSV 的文件格式写出到指定目录，除了路径参数是必选外，该方法还支持传入输出模式，行分隔符，和字段分隔符三个额外的参数，其方法定义如下： 1writeAsCsv(String path, WriteMode writeMode, String rowDelimiter, String fieldDelimiter) 1.3 print \ printToErrprint \ printToErr 是测试当中最常用的方式，用于将计算结果以标准输出流或错误输出流的方式打印到控制台上。 1.4 writeUsingOutputFormat采用自定义的输出格式将计算结果写出，上面介绍的 writeAsText 和 writeAsCsv 其底层调用的都是该方法，源码如下： 12345public DataStreamSink&lt;T&gt; writeAsText(String path, WriteMode writeMode) &#123; TextOutputFormat&lt;T&gt; tof = new TextOutputFormat&lt;&gt;(new Path(path)); tof.setWriteMode(writeMode); return writeUsingOutputFormat(tof);&#125; 1.5 writeToSocketwriteToSocket 用于将计算结果以指定的格式写出到 Socket 中，使用示例如下： 1streamSource.writeToSocket("192.168.0.226", 9999, new SimpleStringSchema()); 二、Streaming Connectors除了上述 API 外，Flink 中还内置了系列的 Connectors 连接器，用于将计算结果输入到常用的存储系统或者消息中间件中，具体如下： Apache Kafka (支持 source 和 sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) Hadoop FileSystem (sink) RabbitMQ (source/sink) Apache NiFi (source/sink) Google PubSub (source/sink) 除了内置的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink Sink 相关的连接器如下： Apache ActiveMQ (source/sink) Apache Flume (sink) Redis (sink) Akka (sink) 这里接着在 Data Sources 章节介绍的整合 Kafka Source 的基础上，将 Kafka Sink 也一并进行整合，具体步骤如下。 三、整合 Kafka Sink3.1 addSinkFlink 提供了 addSink 方法用来调用自定义的 Sink 或者第三方的连接器，想要将计算结果写出到 Kafka，需要使用该方法来调用 Kafka 的生产者 FlinkKafkaProducer，具体代码如下： 12345678910111213141516171819202122232425final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();// 1.指定Kafka的相关配置属性Properties properties = new Properties();properties.setProperty("bootstrap.servers", "192.168.200.0:9092");// 2.接收Kafka上的数据DataStream&lt;String&gt; stream = env .addSource(new FlinkKafkaConsumer&lt;&gt;("flink-stream-in-topic", new SimpleStringSchema(), properties));// 3.定义计算结果到 Kafka ProducerRecord 的转换KafkaSerializationSchema&lt;String&gt; kafkaSerializationSchema = new KafkaSerializationSchema&lt;String&gt;() &#123; @Override public ProducerRecord&lt;byte[], byte[]&gt; serialize(String element, @Nullable Long timestamp) &#123; return new ProducerRecord&lt;&gt;("flink-stream-out-topic", element.getBytes()); &#125;&#125;;// 4. 定义Flink Kafka生产者FlinkKafkaProducer&lt;String&gt; kafkaProducer = new FlinkKafkaProducer&lt;&gt;("flink-stream-out-topic", kafkaSerializationSchema, properties, FlinkKafkaProducer.Semantic.AT_LEAST_ONCE, 5);// 5. 将接收到输入元素*2后写出到Kafkastream.map((MapFunction&lt;String, String&gt;) value -&gt; value + value).addSink(kafkaProducer);env.execute("Flink Streaming"); 3.2 创建输出主题创建用于输出测试的主题： 12345678bin/kafka-topics.sh --create \ --bootstrap-server hadoop001:9092 \ --replication-factor 1 \ --partitions 1 \ --topic flink-stream-out-topic# 查看所有主题 bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092 3.3 启动消费者启动一个 Kafka 消费者，用于查看 Flink 程序的输出情况： 1bin/kafka-console-consumer.sh --bootstrap-server hadoop001:9092 --topic flink-stream-out-topic 3.4 测试结果在 Kafka 生产者上发送消息到 Flink 程序，观察 Flink 程序转换后的输出情况，具体如下： 可以看到 Kafka 生成者发出的数据已经被 Flink 程序正常接收到，并经过转换后又输出到 Kafka 对应的 Topic 上。 四、自定义 Sink除了使用内置的第三方连接器外，Flink 还支持使用自定义的 Sink 来满足多样化的输出需求。想要实现自定义的 Sink ，需要直接或者间接实现 SinkFunction 接口。通常情况下，我们都是实现其抽象类 RichSinkFunction，相比于 SinkFunction ，其提供了更多的与生命周期相关的方法。两者间的关系如下： 这里我们以自定义一个 FlinkToMySQLSink 为例，将计算结果写出到 MySQL 数据库中，具体步骤如下： 4.1 导入依赖首先需要导入 MySQL 相关的依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;8.0.16&lt;/version&gt;&lt;/dependency&gt; 4.2 自定义 Sink继承自 RichSinkFunction，实现自定义的 Sink ： 123456789101112131415161718192021222324252627282930313233343536public class FlinkToMySQLSink extends RichSinkFunction&lt;Employee&gt; &#123; private PreparedStatement stmt; private Connection conn; @Override public void open(Configuration parameters) throws Exception &#123; Class.forName("com.mysql.cj.jdbc.Driver"); conn = DriverManager.getConnection("jdbc:mysql://192.168.0.229:3306/employees" + "?characterEncoding=UTF-8&amp;serverTimezone=UTC&amp;useSSL=false", "root", "123456"); String sql = "insert into emp(name, age, birthday) values(?, ?, ?)"; stmt = conn.prepareStatement(sql); &#125; @Override public void invoke(Employee value, Context context) throws Exception &#123; stmt.setString(1, value.getName()); stmt.setInt(2, value.getAge()); stmt.setDate(3, value.getBirthday()); stmt.executeUpdate(); &#125; @Override public void close() throws Exception &#123; super.close(); if (stmt != null) &#123; stmt.close(); &#125; if (conn != null) &#123; conn.close(); &#125; &#125;&#125; 4.3 使用自定义 Sink想要使用自定义的 Sink，同样是需要调用 addSink 方法，具体如下： 12345678final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();Date date = new Date(System.currentTimeMillis());DataStreamSource&lt;Employee&gt; streamSource = env.fromElements( new Employee("hei", 10, date), new Employee("bai", 20, date), new Employee("ying", 30, date));streamSource.addSink(new FlinkToMySQLSink());env.execute(); 4.4 测试结果启动程序，观察数据库写入情况： 数据库成功写入，代表自定义 Sink 整合成功。 以上所有用例的源码见本仓库：flink-kafka-integration 参考资料 data-sinks： https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sinks Streaming Connectors：https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html Apache Kafka Connector： https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>Data_Sink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink_Data_Transformation]]></title>
    <url>%2F2019%2F07%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink_Data_Transformation%2F</url>
    <content type="text"><![CDATA[Flink Transformation一、Transformations 分类二、DataStream Transformations2.1 Map [DataStream → DataStream] 2.2 FlatMap [DataStream → DataStream]2.3 Filter [DataStream → DataStream]2.4 KeyBy 和 Reduce2.5 Aggregations [KeyedStream → DataStream]2.6 Union [DataStream* → DataStream]2.7 Connect [DataStream,DataStream → ConnectedStreams]2.8 Split 和 Select2.9 project [DataStream → DataStream]三、物理分区3.1 Random partitioning [DataStream → DataStream]3.2 Rebalancing [DataStream → DataStream]3.3 Rescaling [DataStream → DataStream]3.4 Broadcasting [DataStream → DataStream]3.5 Custom partitioning [DataStream → DataStream]四、任务链和资源组4.1 startNewChain4.2 disableChaining4.3 slotSharingGroup 一、Transformations 分类Flink 的 Transformations 操作主要用于将一个和多个 DataStream 按需转换成新的 DataStream。它主要分为以下三类： DataStream Transformations：进行数据流相关转换操作； Physical partitioning：物理分区。Flink 提供的底层 API ，允许用户定义数据的分区规则； Task chaining and resource groups：任务链和资源组。允许用户进行任务链和资源组的细粒度的控制。 以下分别对其主要 API 进行介绍： 二、DataStream Transformations2.1 Map [DataStream → DataStream]对一个 DataStream 中的每个元素都执行特定的转换操作： 123DataStream&lt;Integer&gt; integerDataStream = env.fromElements(1, 2, 3, 4, 5);integerDataStream.map((MapFunction&lt;Integer, Object&gt;) value -&gt; value * 2).print();// 输出 2,4,6,8,10 2.2 FlatMap [DataStream → DataStream]FlatMap 与 Map 类似，但是 FlatMap 中的一个输入元素可以被映射成一个或者多个输出元素，示例如下： 12345678910111213String string01 = "one one one two two";String string02 = "third third third four";DataStream&lt;String&gt; stringDataStream = env.fromElements(string01, string02);stringDataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; for (String s : value.split(" ")) &#123; out.collect(s); &#125; &#125;&#125;).print();// 输出每一个独立的单词，为节省排版，这里去掉换行，后文亦同one one one two two third third third four 2.3 Filter [DataStream → DataStream]用于过滤符合条件的数据： 1env.fromElements(1, 2, 3, 4, 5).filter(x -&gt; x &gt; 3).print(); 2.4 KeyBy 和 Reduce KeyBy [DataStream → KeyedStream] ：用于将相同 Key 值的数据分到相同的分区中； Reduce [KeyedStream → DataStream] ：用于对数据执行归约计算。 如下例子将数据按照 key 值分区后，滚动进行求和计算： 12345678910111213DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; tuple2DataStream = env.fromElements(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("a", 2), new Tuple2&lt;&gt;("b", 3), new Tuple2&lt;&gt;("b", 5));KeyedStream&lt;Tuple2&lt;String, Integer&gt;, Tuple&gt; keyedStream = tuple2DataStream.keyBy(0);keyedStream.reduce((ReduceFunction&lt;Tuple2&lt;String, Integer&gt;&gt;) (value1, value2) -&gt; new Tuple2&lt;&gt;(value1.f0, value1.f1 + value2.f1)).print();// 持续进行求和计算，输出：(a,1)(a,3)(b,3)(b,8) KeyBy 操作存在以下两个限制： KeyBy 操作用于用户自定义的 POJOs 类型时，该自定义类型必须重写 hashCode 方法； KeyBy 操作不能用于数组类型。 2.5 Aggregations [KeyedStream → DataStream]Aggregations 是官方提供的聚合算子，封装了常用的聚合操作，如上利用 Reduce 进行求和的操作也可以利用 Aggregations 中的 sum 算子重写为下面的形式： 1tuple2DataStream.keyBy(0).sum(1).print(); 除了 sum 外，Flink 还提供了 min , max , minBy，maxBy 等常用聚合算子： 123456789101112// 滚动计算指定key的最小值，可以通过index或者fieldName来指定keykeyedStream.min(0);keyedStream.min("key");// 滚动计算指定key的最大值keyedStream.max(0);keyedStream.max("key");// 滚动计算指定key的最小值，并返回其对应的元素keyedStream.minBy(0);keyedStream.minBy("key");// 滚动计算指定key的最大值，并返回其对应的元素keyedStream.maxBy(0);keyedStream.maxBy("key"); 2.6 Union [DataStream* → DataStream]用于连接两个或者多个元素类型相同的 DataStream 。当然一个 DataStream 也可以与其本生进行连接，此时该 DataStream 中的每个元素都会被获取两次： 123456DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(new Tuple2&lt;&gt;("a", 1), new Tuple2&lt;&gt;("a", 2));DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource02 = env.fromElements(new Tuple2&lt;&gt;("b", 1), new Tuple2&lt;&gt;("b", 2));streamSource01.union(streamSource02);streamSource01.union(streamSource01,streamSource02); 2.7 Connect [DataStream,DataStream → ConnectedStreams]Connect 操作用于连接两个或者多个类型不同的 DataStream ，其返回的类型是 ConnectedStreams ，此时被连接的多个 DataStreams 可以共享彼此之间的数据状态。但是需要注意的是由于不同 DataStream 之间的数据类型是不同的，如果想要进行后续的计算操作，还需要通过 CoMap 或 CoFlatMap 将 ConnectedStreams 转换回 DataStream： 12345678910111213141516171819DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource01 = env.fromElements(new Tuple2&lt;&gt;("a", 3), new Tuple2&lt;&gt;("b", 5));DataStreamSource&lt;Integer&gt; streamSource02 = env.fromElements(2, 3, 9);// 使用connect进行连接ConnectedStreams&lt;Tuple2&lt;String, Integer&gt;, Integer&gt; connect = streamSource01.connect(streamSource02);connect.map(new CoMapFunction&lt;Tuple2&lt;String, Integer&gt;, Integer, Integer&gt;() &#123; @Override public Integer map1(Tuple2&lt;String, Integer&gt; value) throws Exception &#123; return value.f1; &#125; @Override public Integer map2(Integer value) throws Exception &#123; return value; &#125;&#125;).map(x -&gt; x * 100).print();// 输出：300 500 200 900 300 2.8 Split 和 Select Split [DataStream → SplitStream]：用于将一个 DataStream 按照指定规则进行拆分为多个 DataStream，需要注意的是这里进行的是逻辑拆分，即 Split 只是将数据贴上不同的类型标签，但最终返回的仍然只是一个 SplitStream； Select [SplitStream → DataStream]：想要从逻辑拆分的 SplitStream 中获取真实的不同类型的 DataStream，需要使用 Select 算子，示例如下： 12345678910111213DataStreamSource&lt;Integer&gt; streamSource = env.fromElements(1, 2, 3, 4, 5, 6, 7, 8);// 标记SplitStream&lt;Integer&gt; split = streamSource.split(new OutputSelector&lt;Integer&gt;() &#123; @Override public Iterable&lt;String&gt; select(Integer value) &#123; List&lt;String&gt; output = new ArrayList&lt;String&gt;(); output.add(value % 2 == 0 ? "even" : "odd"); return output; &#125;&#125;);// 获取偶数数据集split.select("even").print();// 输出 2,4,6,8 2.9 project [DataStream → DataStream]project 主要用于获取 tuples 中的指定字段集，示例如下： 12345678DataStreamSource&lt;Tuple3&lt;String, Integer, String&gt;&gt; streamSource = env.fromElements( new Tuple3&lt;&gt;("li", 22, "2018-09-23"), new Tuple3&lt;&gt;("ming", 33, "2020-09-23"));streamSource.project(0,2).print();// 输出(li,2018-09-23)(ming,2020-09-23) 三、物理分区物理分区 (Physical partitioning) 是 Flink 提供的底层的 API，允许用户采用内置的分区规则或者自定义的分区规则来对数据进行分区，从而避免数据在某些分区上过于倾斜，常用的分区规则如下： 3.1 Random partitioning [DataStream → DataStream]随机分区 (Random partitioning) 用于随机的将数据分布到所有下游分区中，通过 shuffle 方法来进行实现： 1dataStream.shuffle(); 3.2 Rebalancing [DataStream → DataStream]Rebalancing 采用轮询的方式将数据进行分区，其适合于存在数据倾斜的场景下，通过 rebalance 方法进行实现： 1dataStream.rebalance(); 3.3 Rescaling [DataStream → DataStream]当采用 Rebalancing 进行分区平衡时，其实现的是全局性的负载均衡，数据会通过网络传输到其他节点上并完成分区数据的均衡。 而 Rescaling 则是低配版本的 rebalance，它不需要额外的网络开销，它只会对上下游的算子之间进行重新均衡，通过 rescale 方法进行实现： 1dataStream.rescale(); ReScale 这个单词具有重新缩放的意义，其对应的操作也是如此，具体如下：如果上游 operation 并行度为 2，而下游的 operation 并行度为 6，则其中 1 个上游的 operation 会将元素分发到 3 个下游 operation，另 1 个上游 operation 则会将元素分发到另外 3 个下游 operation。反之亦然，如果上游的 operation 并行度为 6，而下游 operation 并行度为 2，则其中 3 个上游 operation 会将元素分发到 1 个下游 operation，另 3 个上游 operation 会将元素分发到另外 1 个下游operation： 3.4 Broadcasting [DataStream → DataStream]将数据分发到所有分区上。通常用于小数据集与大数据集进行关联的情况下，此时可以将小数据集广播到所有分区上，避免频繁的跨分区关联，通过 broadcast 方法进行实现： 1dataStream.broadcast(); 3.5 Custom partitioning [DataStream → DataStream]Flink 运行用户采用自定义的分区规则来实现分区，此时需要通过实现 Partitioner 接口来自定义分区规则，并指定对应的分区键，示例如下： 12345678910111213141516171819202122 DataStreamSource&lt;Tuple2&lt;String, Integer&gt;&gt; streamSource = env.fromElements(new Tuple2&lt;&gt;("Hadoop", 1), new Tuple2&lt;&gt;("Spark", 1), new Tuple2&lt;&gt;("Flink-streaming", 2), new Tuple2&lt;&gt;("Flink-batch", 4), new Tuple2&lt;&gt;("Storm", 4), new Tuple2&lt;&gt;("HBase", 3));streamSource.partitionCustom(new Partitioner&lt;String&gt;() &#123; @Override public int partition(String key, int numPartitions) &#123; // 将第一个字段包含flink的Tuple2分配到同一个分区 return key.toLowerCase().contains("flink") ? 0 : 1; &#125;&#125;, 0).print();// 输出如下：1&gt; (Flink-streaming,2)1&gt; (Flink-batch,4)2&gt; (Hadoop,1)2&gt; (Spark,1)2&gt; (Storm,4)2&gt; (HBase,3) 四、任务链和资源组任务链和资源组 ( Task chaining and resource groups ) 也是 Flink 提供的底层 API，用于控制任务链和资源分配。默认情况下，如果操作允许 (例如相邻的两次 map 操作) ，则 Flink 会尝试将它们在同一个线程内进行，从而可以获取更好的性能。但是 Flink 也允许用户自己来控制这些行为，这就是任务链和资源组 API： 4.1 startNewChainstartNewChain 用于基于当前 operation 开启一个新的任务链。如下所示，基于第一个 map 开启一个新的任务链，此时前一个 map 和 后一个 map 将处于同一个新的任务链中，但它们与 filter 操作则分别处于不同的任务链中： 1someStream.filter(...).map(...).startNewChain().map(...); 4.2 disableChaining disableChaining 操作用于禁止将其他操作与当前操作放置于同一个任务链中，示例如下： 1someStream.map(...).disableChaining(); 4.3 slotSharingGroupslot 是任务管理器 (TaskManager) 所拥有资源的固定子集，每个操作 (operation) 的子任务 (sub task) 都需要获取 slot 来执行计算，但每个操作所需要资源的大小都是不相同的，为了更好地利用资源，Flink 允许不同操作的子任务被部署到同一 slot 中。slotSharingGroup 用于设置操作的 slot 共享组 (slot sharing group) ，Flink 会将具有相同 slot 共享组的操作放到同一个 slot 中 。示例如下： 1someStream.filter(...).slotSharingGroup("slotSharingGroupName"); 参考资料Flink Operators： https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/stream/operators/]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>Data_Transformation</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink_Data_Source]]></title>
    <url>%2F2019%2F07%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink_Data_Source%2F</url>
    <content type="text"><![CDATA[Flink Data Source一、内置 Data Source1.1 基于文件构建1.2 基于集合构建1.3 基于 Socket 构建二、自定义 Data Source2.1 SourceFunction2.2 ParallelSourceFunction 和 RichParallelSourceFunction三、Streaming Connectors3.1 内置连接器3.2 整合 Kakfa3.3 整合测试 一、内置 Data SourceFlink Data Source 用于定义 Flink 程序的数据来源，Flink 官方提供了多种数据获取方法，用于帮助开发者简单快速地构建输入流，具体如下： 1.1 基于文件构建1. readTextFile(path)：按照 TextInputFormat 格式读取文本文件，并将其内容以字符串的形式返回。示例如下： 1env.readTextFile(filePath).print(); 2. readFile(fileInputFormat, path) ：按照指定格式读取文件。 3. readFile(inputFormat, filePath, watchType, interval, typeInformation)：按照指定格式周期性的读取文件。其中各个参数的含义如下： inputFormat：数据流的输入格式。 filePath：文件路径，可以是本地文件系统上的路径，也可以是 HDFS 上的文件路径。 watchType：读取方式，它有两个可选值，分别是 FileProcessingMode.PROCESS_ONCE 和 FileProcessingMode.PROCESS_CONTINUOUSLY：前者表示对指定路径上的数据只读取一次，然后退出；后者表示对路径进行定期地扫描和读取。需要注意的是如果 watchType 被设置为 PROCESS_CONTINUOUSLY，那么当文件被修改时，其所有的内容 (包含原有的内容和新增的内容) 都将被重新处理，因此这会打破 Flink 的 exactly-once 语义。 interval：定期扫描的时间间隔。 typeInformation：输入流中元素的类型。 使用示例如下： 12345678final String filePath = "D:\\log4j.properties";final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.readFile(new TextInputFormat(new Path(filePath)), filePath, FileProcessingMode.PROCESS_ONCE, 1, BasicTypeInfo.STRING_TYPE_INFO).print();env.execute(); 1.2 基于集合构建1. fromCollection(Collection)：基于集合构建，集合中的所有元素必须是同一类型。示例如下： 1env.fromCollection(Arrays.asList(1,2,3,4,5)).print(); 2. fromElements(T …)： 基于元素构建，所有元素必须是同一类型。示例如下： 1env.fromElements(1,2,3,4,5).print(); 3. generateSequence(from, to)：基于给定的序列区间进行构建。示例如下： 1env.generateSequence(0,100); 4. fromCollection(Iterator, Class)：基于迭代器进行构建。第一个参数用于定义迭代器，第二个参数用于定义输出元素的类型。使用示例如下： 1env.fromCollection(new CustomIterator(), BasicTypeInfo.INT_TYPE_INFO).print(); 其中 CustomIterator 为自定义的迭代器，这里以产生 1 到 100 区间内的数据为例，源码如下。需要注意的是自定义迭代器除了要实现 Iterator 接口外，还必须要实现序列化接口 Serializable ，否则会抛出序列化失败的异常： 1234567891011121314151617import java.io.Serializable;import java.util.Iterator;public class CustomIterator implements Iterator&lt;Integer&gt;, Serializable &#123; private Integer i = 0; @Override public boolean hasNext() &#123; return i &lt; 100; &#125; @Override public Integer next() &#123; i++; return i; &#125;&#125; 5. fromParallelCollection(SplittableIterator, Class)：方法接收两个参数，第二个参数用于定义输出元素的类型，第一个参数 SplittableIterator 是迭代器的抽象基类，它用于将原始迭代器的值拆分到多个不相交的迭代器中。 1.3 基于 Socket 构建Flink 提供了 socketTextStream 方法用于构建基于 Socket 的数据流，socketTextStream 方法有以下四个主要参数： hostname：主机名； port：端口号，设置为 0 时，表示端口号自动分配； delimiter：用于分隔每条记录的分隔符； maxRetry：当 Socket 临时关闭时，程序的最大重试间隔，单位为秒。设置为 0 时表示不进行重试；设置为负值则表示一直重试。示例如下： 1env.socketTextStream("192.168.0.229", 9999, "\n", 3).print(); 二、自定义 Data Source2.1 SourceFunction除了内置的数据源外，用户还可以使用 addSource 方法来添加自定义的数据源。自定义的数据源必须要实现 SourceFunction 接口，这里以产生 [0 , 1000) 区间内的数据为例，代码如下： 123456789101112131415161718192021final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.addSource(new SourceFunction&lt;Long&gt;() &#123; private long count = 0L; private volatile boolean isRunning = true; public void run(SourceContext&lt;Long&gt; ctx) &#123; while (isRunning &amp;&amp; count &lt; 1000) &#123; // 通过collect将输入发送出去 ctx.collect(count); count++; &#125; &#125; public void cancel() &#123; isRunning = false; &#125;&#125;).print();env.execute(); 2.2 ParallelSourceFunction 和 RichParallelSourceFunction上面通过 SourceFunction 实现的数据源是不具有并行度的，即不支持在得到的 DataStream 上调用 setParallelism(n) 方法，此时会抛出如下的异常： 1Exception in thread "main" java.lang.IllegalArgumentException: Source: 1 is not a parallel source 如果你想要实现具有并行度的输入流，则需要实现 ParallelSourceFunction 或 RichParallelSourceFunction 接口，其与 SourceFunction 的关系如下图： ParallelSourceFunction 直接继承自 ParallelSourceFunction，具有并行度的功能。RichParallelSourceFunction 则继承自 AbstractRichFunction，同时实现了 ParallelSourceFunction 接口，所以其除了具有并行度的功能外，还提供了额外的与生命周期相关的方法，如 open() ，closen() 。 三、Streaming Connectors3.1 内置连接器除了自定义数据源外， Flink 还内置了多种连接器，用于满足大多数的数据收集场景。当前内置连接器的支持情况如下： Apache Kafka (支持 source 和 sink) Apache Cassandra (sink) Amazon Kinesis Streams (source/sink) Elasticsearch (sink) Hadoop FileSystem (sink) RabbitMQ (source/sink) Apache NiFi (source/sink) Twitter Streaming API (source) Google PubSub (source/sink) 除了上述的连接器外，你还可以通过 Apache Bahir 的连接器扩展 Flink。Apache Bahir 旨在为分布式数据分析系统 (如 Spark，Flink) 等提供功能上的扩展，当前其支持的与 Flink 相关的连接器如下： Apache ActiveMQ (source/sink) Apache Flume (sink) Redis (sink) Akka (sink) Netty (source) 随着 Flink 的不断发展，可以预见到其会支持越来越多类型的连接器，关于连接器的后续发展情况，可以查看其官方文档：Streaming Connectors 。在所有 DataSource 连接器中，使用的广泛的就是 Kafka，所以这里我们以其为例，来介绍 Connectors 的整合步骤。 3.2 整合 Kakfa1. 导入依赖整合 Kafka 时，一定要注意所使用的 Kafka 的版本，不同版本间所需的 Maven 依赖和开发时所调用的类均不相同，具体如下： Maven 依赖 Flink 版本 Consumer and Producer 类的名称 Kafka 版本 flink-connector-kafka-0.8_2.11 1.0.0 + FlinkKafkaConsumer08 FlinkKafkaProducer08 0.8.x flink-connector-kafka-0.9_2.11 1.0.0 + FlinkKafkaConsumer09 FlinkKafkaProducer09 0.9.x flink-connector-kafka-0.10_2.11 1.2.0 + FlinkKafkaConsumer010 FlinkKafkaProducer010 0.10.x flink-connector-kafka-0.11_2.11 1.4.0 + FlinkKafkaConsumer011 FlinkKafkaProducer011 0.11.x flink-connector-kafka_2.11 1.7.0 + FlinkKafkaConsumer FlinkKafkaProducer &gt;= 1.0.0 这里我使用的 Kafka 版本为 kafka_2.12-2.2.0，添加的依赖如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.9.0&lt;/version&gt;&lt;/dependency&gt; 2. 代码开发这里以最简单的场景为例，接收 Kafka 上的数据并打印，代码如下： 123456789final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();Properties properties = new Properties();// 指定Kafka的连接位置properties.setProperty("bootstrap.servers", "hadoop001:9092");// 指定监听的主题，并定义Kafka字节消息到Flink对象之间的转换规则DataStream&lt;String&gt; stream = env .addSource(new FlinkKafkaConsumer&lt;&gt;("flink-stream-in-topic", new SimpleStringSchema(), properties));stream.print();env.execute("Flink Streaming"); 3.3 整合测试1. 启动 KakfaKafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的： 12345# zookeeper启动命令bin/zkServer.sh start# 内置zookeeper启动命令bin/zookeeper-server-start.sh config/zookeeper.properties 启动单节点 kafka 用于测试： 1# bin/kafka-server-start.sh config/server.properties 2. 创建 Topic123456789# 创建用于测试主题bin/kafka-topics.sh --create \ --bootstrap-server hadoop001:9092 \ --replication-factor 1 \ --partitions 1 \ --topic flink-stream-in-topic# 查看所有主题 bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092 3. 启动 Producer这里 启动一个 Kafka 生产者，用于发送测试数据： 1bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic flink-stream-in-topic 4. 测试结果在 Producer 上输入任意测试数据，之后观察程序控制台的输出： 程序控制台的输出如下： 可以看到已经成功接收并打印出相关的数据。 参考资料 data-sources：https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/datastream_api.html#data-sources Streaming Connectors：https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/index.html Apache Kafka Connector： https://ci.apache.org/projects/flink/flink-docs-release-1.9/dev/connectors/kafka.html]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>Data_Source</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink开发环境搭建]]></title>
    <url>%2F2019%2F07%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Flink 开发环境搭建一、安装 Scala 插件二、Flink 项目初始化2.1 使用官方脚本构建2.2 使用 IDEA 构建三、项目结构3.1 项目结构3.2 主要依赖四、词频统计案例4.1 批处理示例4.2 流处理示例五、使用 Scala Shell 一、安装 Scala 插件Flink 分别提供了基于 Java 语言和 Scala 语言的 API ，如果想要使用 Scala 语言来开发 Flink 程序，可以通过在 IDEA 中安装 Scala 插件来提供语法提示，代码高亮等功能。打开 IDEA , 依次点击 File =&gt; settings =&gt; plugins 打开插件安装页面，搜索 Scala 插件并进行安装，安装完成后，重启 IDEA 即可生效。 二、Flink 项目初始化2.1 使用官方脚本构建Flink 官方支持使用 Maven 和 Gradle 两种构建工具来构建基于 Java 语言的 Flink 项目；支持使用 SBT 和 Maven 两种构建工具来构建基于 Scala 语言的 Flink 项目。 这里以 Maven 为例进行说明，因为其可以同时支持 Java 语言和 Scala 语言项目的构建。需要注意的是 Flink 1.9 只支持 Maven 3.0.4 以上的版本，Maven 安装完成后，可以通过以下两种方式来构建项目： 1. 直接基于 Maven Archetype 构建 直接使用下面的 mvn 语句来进行构建，然后根据交互信息的提示，依次输入 groupId , artifactId 以及包名等信息后等待初始化的完成： 1234$ mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-java \ -DarchetypeVersion=1.9.0 注：如果想要创建基于 Scala 语言的项目，只需要将 flink-quickstart-java 换成 flink-quickstart-scala 即可，后文亦同。 2. 使用官方脚本快速构建 为了更方便的初始化项目，官方提供了快速构建脚本，可以直接通过以下命令来进行调用： 1$ curl https://flink.apache.org/q/quickstart.sh | bash -s 1.9.0 该方式其实也是通过执行 maven archetype 命令来进行初始化，其脚本内容如下： 1234567891011PACKAGE=quickstartmvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-java \ -DarchetypeVersion=$&#123;1:-1.8.0&#125; \ -DgroupId=org.myorg.quickstart \ -DartifactId=$PACKAGE \ -Dversion=0.1 \ -Dpackage=org.myorg.quickstart \ -DinteractiveMode=false 可以看到相比于第一种方式，该种方式只是直接指定好了 groupId ，artifactId ，version 等信息而已。 2.2 使用 IDEA 构建如果你使用的是开发工具是 IDEA ，可以直接在项目创建页面选择 Maven Flink Archetype 进行项目初始化： 如果你的 IDEA 没有上述 Archetype， 可以通过点击右上角的 ADD ARCHETYPE ，来进行添加，依次填入所需信息，这些信息都可以从上述的 archetype:generate 语句中获取。点击 OK 保存后，该 Archetype 就会一直存在于你的 IDEA 中，之后每次创建项目时，只需要直接选择该 Archetype 即可： 选中 Flink Archetype ，然后点击 NEXT 按钮，之后的所有步骤都和正常的 Maven 工程相同。 三、项目结构3.1 项目结构创建完成后的自动生成的项目结构如下： 其中 BatchJob 为批处理的样例代码，源码如下： 123456789import org.apache.flink.api.scala._object BatchJob &#123; def main(args: Array[String]) &#123; val env = ExecutionEnvironment.getExecutionEnvironment .... env.execute("Flink Batch Scala API Skeleton") &#125;&#125; getExecutionEnvironment 代表获取批处理的执行环境，如果是本地运行则获取到的就是本地的执行环境；如果在集群上运行，得到的就是集群的执行环境。如果想要获取流处理的执行环境，则只需要将 ExecutionEnvironment 替换为 StreamExecutionEnvironment， 对应的代码样例在 StreamingJob 中： 123456789import org.apache.flink.streaming.api.scala._object StreamingJob &#123; def main(args: Array[String]) &#123; val env = StreamExecutionEnvironment.getExecutionEnvironment ... env.execute("Flink Streaming Scala API Skeleton") &#125;&#125; 需要注意的是对于流处理项目 env.execute() 这句代码是必须的，否则流处理程序就不会被执行，但是对于批处理项目则是可选的。 3.2 主要依赖基于 Maven 骨架创建的项目主要提供了以下核心依赖：其中 flink-scala 用于支持开发批处理程序 ；flink-streaming-scala 用于支持开发流处理程序 ；scala-library 用于提供 Scala 语言所需要的类库。如果在使用 Maven 骨架创建时选择的是 Java 语言，则默认提供的则是 flink-java 和 flink-streaming-java 依赖。 12345678910111213141516171819202122&lt;!-- Apache Flink dependencies --&gt;&lt;!-- These dependencies are provided, because they should not be packaged into the JAR file. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt;&lt;!-- Scala Library, provided by Flink as well. --&gt;&lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt;&lt;/dependency&gt; 需要特别注意的以上依赖的 scope 标签全部被标识为 provided ，这意味着这些依赖都不会被打入最终的 JAR 包。因为 Flink 的安装包中已经提供了这些依赖，位于其 lib 目录下，名为 flink-dist_*.jar ，它包含了 Flink 的所有核心类和依赖： scope 标签被标识为 provided 会导致你在 IDEA 中启动项目时会抛出 ClassNotFoundException 异常。基于这个原因，在使用 IDEA 创建项目时还自动生成了以下 profile 配置： 1234567891011121314151617181920212223242526272829303132333435&lt;!-- This profile helps to make things run out of the box in IntelliJ --&gt;&lt;!-- Its adds Flink's core classes to the runtime class path. --&gt;&lt;!-- Otherwise they are missing in IntelliJ, because the dependency is 'provided' --&gt;&lt;profiles&gt; &lt;profile&gt; &lt;id&gt;add-dependencies-for-IDEA&lt;/id&gt; &lt;activation&gt; &lt;property&gt; &lt;name&gt;idea.version&lt;/name&gt; &lt;/property&gt; &lt;/activation&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-streaming-scala_$&#123;scala.binary.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;flink.version&#125;&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;$&#123;scala.version&#125;&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;/profile&gt;&lt;/profiles&gt; 在 id 为 add-dependencies-for-IDEA 的 profile 中，所有的核心依赖都被标识为 compile，此时你可以无需改动任何代码，只需要在 IDEA 的 Maven 面板中勾选该 profile，即可直接在 IDEA 中运行 Flink 项目： 四、词频统计案例项目创建完成后，可以先书写一个简单的词频统计的案例来尝试运行 Flink 项目，以下以 Scala 语言为例，分别介绍流处理程序和批处理程序的编程示例： 4.1 批处理示例123456789101112131415import org.apache.flink.api.scala._object WordCountBatch &#123; def main(args: Array[String]): Unit = &#123; val benv = ExecutionEnvironment.getExecutionEnvironment val dataSet = benv.readTextFile("D:\\wordcount.txt") dataSet.flatMap &#123; _.toLowerCase.split(",")&#125; .filter (_.nonEmpty) .map &#123; (_, 1) &#125; .groupBy(0) .sum(1) .print() &#125;&#125; 其中 wordcount.txt 中的内容如下： 1234a,a,a,a,ab,b,bc,cd,d 本机不需要配置其他任何的 Flink 环境，直接运行 Main 方法即可，结果如下： 4.2 流处理示例1234567891011121314151617181920import org.apache.flink.streaming.api.scala._import org.apache.flink.streaming.api.windowing.time.Timeobject WordCountStreaming &#123; def main(args: Array[String]): Unit = &#123; val senv = StreamExecutionEnvironment.getExecutionEnvironment val dataStream: DataStream[String] = senv.socketTextStream("192.168.0.229", 9999, '\n') dataStream.flatMap &#123; line =&gt; line.toLowerCase.split(",") &#125; .filter(_.nonEmpty) .map &#123; word =&gt; (word, 1) &#125; .keyBy(0) .timeWindow(Time.seconds(3)) .sum(1) .print() senv.execute("Streaming WordCount") &#125;&#125; 这里以监听指定端口号上的内容为例，使用以下命令来开启端口服务： 1nc -lk 9999 之后输入测试数据即可观察到流处理程序的处理情况。 五、使用 Scala Shell对于日常的 Demo 项目，如果你不想频繁地启动 IDEA 来观察测试结果，可以像 Spark 一样，直接使用 Scala Shell 来运行程序，这对于日常的学习来说，效果更加直观，也更省时。Flink 安装包的下载地址如下： 1https://flink.apache.org/downloads.html Flink 大多数版本都提供有 Scala 2.11 和 Scala 2.12 两个版本的安装包可供下载： 下载完成后进行解压即可，Scala Shell 位于安装目录的 bin 目录下，直接使用以下命令即可以本地模式启动： 1./start-scala-shell.sh local 命令行启动完成后，其已经提供了批处理 （benv 和 btenv）和流处理（senv 和 stenv）的运行环境，可以直接运行 Scala Flink 程序，示例如下： 最后解释一个常见的异常：这里我使用的 Flink 版本为 1.9.1，启动时会抛出如下异常。这里因为按照官方的说明，目前所有 Scala 2.12 版本的安装包暂时都不支持 Scala Shell，所以如果想要使用 Scala Shell，只能选择 Scala 2.11 版本的安装包。 12[root@hadoop001 bin]# ./start-scala-shell.sh local错误: 找不到或无法加载主类 org.apache.flink.api.scala.FlinkShell]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink核心概念综述]]></title>
    <url>%2F2019%2F07%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Flink 核心概念综述一、Flink 简介二、Flink 核心架构2.1 API &amp; Libraries 层2.2 Runtime 核心层2.3 物理部署层三、Flink 分层 API3.1 SQL &amp; Table API3.2 DataStream &amp; DataSet API3.3 Stateful Stream Processing四、Flink 集群架构4.1 核心组件4.2 Task &amp; SubTask4.3 资源管理4.4 组件通讯五、Flink 的优点 一、Flink 简介Apache Flink 诞生于柏林工业大学的一个研究性项目，原名 StratoSphere 。2014 年，由 StratoSphere 项目孵化出 Flink，并于同年捐赠 Apache，之后成为 Apache 的顶级项目。2019 年 1 年，阿里巴巴收购了 Flink 的母公司 Data Artisans，并宣布开源内部的 Blink，Blink 是阿里巴巴基于 Flink 优化后的版本，增加了大量的新功能，并在性能和稳定性上进行了各种优化，经历过阿里内部多种复杂业务的挑战和检验。同时阿里巴巴也表示会逐步将这些新功能和特性 Merge 回社区版本的 Flink 中，因此 Flink 成为目前最为火热的大数据处理框架。 简单来说，Flink 是一个分布式的流处理框架，它能够对有界和无界的数据流进行高效的处理。Flink 的核心是流处理，当然它也能支持批处理，Flink 将批处理看成是流处理的一种特殊情况，即数据流是有明确界限的。这和 Spark Streaming 的思想是完全相反的，Spark Streaming 的核心是批处理，它将流处理看成是批处理的一种特殊情况， 即把数据流进行极小粒度的拆分，拆分为多个微批处理。 Flink 有界数据流和无界数据流： Spark Streaming 数据流的拆分： 二、Flink 核心架构Flink 采用分层的架构设计，从而保证各层在功能和职责上的清晰。如下图所示，由上而下分别是 API &amp; Libraries 层、Runtime 核心层以及物理部署层： 2.1 API &amp; Libraries 层这一层主要提供了编程 API 和 顶层类库： 编程 API : 用于进行流处理的 DataStream API 和用于进行批处理的 DataSet API； 顶层类库：包括用于复杂事件处理的 CEP 库；用于结构化数据查询的 SQL &amp; Table 库，以及基于批处理的机器学习库 FlinkML 和 图形处理库 Gelly。 2.2 Runtime 核心层这一层是 Flink 分布式计算框架的核心实现层，包括作业转换，任务调度，资源分配，任务执行等功能，基于这一层的实现，可以在流式引擎下同时运行流处理程序和批处理程序。 2.3 物理部署层Flink 的物理部署层，用于支持在不同平台上部署运行 Flink 应用。 三、Flink 分层 API在上面介绍的 API &amp; Libraries 这一层，Flink 又进行了更为具体的划分。具体如下： 按照如上的层次结构，API 的一致性由下至上依次递增，接口的表现能力由下至上依次递减，各层的核心功能如下： 3.1 SQL &amp; Table APISQL &amp; Table API 同时适用于批处理和流处理，这意味着你可以对有界数据流和无界数据流以相同的语义进行查询，并产生相同的结果。除了基本查询外， 它还支持自定义的标量函数，聚合函数以及表值函数，可以满足多样化的查询需求。 3.2 DataStream &amp; DataSet APIDataStream &amp; DataSet API 是 Flink 数据处理的核心 API，支持使用 Java 语言或 Scala 语言进行调用，提供了数据读取，数据转换和数据输出等一系列常用操作的封装。 3.3 Stateful Stream ProcessingStateful Stream Processing 是最低级别的抽象，它通过 Process Function 函数内嵌到 DataStream API 中。 Process Function 是 Flink 提供的最底层 API，具有最大的灵活性，允许开发者对于时间和状态进行细粒度的控制。 四、Flink 集群架构4.1 核心组件按照上面的介绍，Flink 核心架构的第二层是 Runtime 层， 该层采用标准的 Master - Slave 结构， 其中，Master 部分又包含了三个核心组件：Dispatcher、ResourceManager 和 JobManager，而 Slave 则主要是 TaskManager 进程。它们的功能分别如下： JobManagers (也称为 masters) ：JobManagers 接收由 Dispatcher 传递过来的执行程序，该执行程序包含了作业图 (JobGraph)，逻辑数据流图 (logical dataflow graph) 及其所有的 classes 文件以及第三方类库 (libraries) 等等 。紧接着 JobManagers 会将 JobGraph 转换为执行图 (ExecutionGraph)，然后向 ResourceManager 申请资源来执行该任务，一旦申请到资源，就将执行图分发给对应的 TaskManagers 。因此每个作业 (Job) 至少有一个 JobManager；高可用部署下可以有多个 JobManagers，其中一个作为 leader，其余的则处于 standby 状态。 TaskManagers (也称为 workers) : TaskManagers 负责实际的子任务 (subtasks) 的执行，每个 TaskManagers 都拥有一定数量的 slots。Slot 是一组固定大小的资源的合集 (如计算能力，存储空间)。TaskManagers 启动后，会将其所拥有的 slots 注册到 ResourceManager 上，由 ResourceManager 进行统一管理。 Dispatcher：负责接收客户端提交的执行程序，并传递给 JobManager 。除此之外，它还提供了一个 WEB UI 界面，用于监控作业的执行情况。 ResourceManager ：负责管理 slots 并协调集群资源。ResourceManager 接收来自 JobManager 的资源请求，并将存在空闲 slots 的 TaskManagers 分配给 JobManager 执行任务。Flink 基于不同的部署平台，如 YARN , Mesos，K8s 等提供了不同的资源管理器，当 TaskManagers 没有足够的 slots 来执行任务时，它会向第三方平台发起会话来请求额外的资源。 4.2 Task &amp; SubTask上面我们提到：TaskManagers 实际执行的是 SubTask，而不是 Task，这里解释一下两者的区别： 在执行分布式计算时，Flink 将可以链接的操作 (operators) 链接到一起，这就是 Task。之所以这样做， 是为了减少线程间切换和缓冲而导致的开销，在降低延迟的同时可以提高整体的吞吐量。 但不是所有的 operator 都可以被链接，如下 keyBy 等操作会导致网络 shuffle 和重分区，因此其就不能被链接，只能被单独作为一个 Task。 简单来说，一个 Task 就是一个可以链接的最小的操作链 (Operator Chains) 。如下图，source 和 map 算子被链接到一块，因此整个作业就只有三个 Task： 解释完 Task ，我们在解释一下什么是 SubTask，其准确的翻译是： A subtask is one parallel slice of a task，即一个 Task 可以按照其并行度拆分为多个 SubTask。如上图，source &amp; map 具有两个并行度，KeyBy 具有两个并行度，Sink 具有一个并行度，因此整个虽然只有 3 个 Task，但是却有 5 个 SubTask。Jobmanager 负责定义和拆分这些 SubTask，并将其交给 Taskmanagers 来执行，每个 SubTask 都是一个单独的线程。 4.3 资源管理理解了 SubTasks ，我们再来看看其与 Slots 的对应情况。一种可能的分配情况如下： 这时每个 SubTask 线程运行在一个独立的 TaskSlot， 它们共享所属的 TaskManager 进程的TCP 连接（通过多路复用技术）和心跳信息 (heartbeat messages)，从而可以降低整体的性能开销。此时看似是最好的情况，但是每个操作需要的资源都是不尽相同的，这里假设该作业 keyBy 操作所需资源的数量比 Sink 多很多 ，那么此时 Sink 所在 Slot 的资源就没有得到有效的利用。 基于这个原因，Flink 允许多个 subtasks 共享 slots，即使它们是不同 tasks 的 subtasks，但只要它们来自同一个 Job 就可以。假设上面 souce &amp; map 和 keyBy 的并行度调整为 6，而 Slot 的数量不变，此时情况如下： 可以看到一个 Task Slot 中运行了多个 SubTask 子任务，此时每个子任务仍然在一个独立的线程中执行，只不过共享一组 Sot 资源而已。那么 Flink 到底如何确定一个 Job 至少需要多少个 Slot 呢？Flink 对于这个问题的处理很简单，默认情况一个 Job 所需要的 Slot 的数量就等于其 Operation 操作的最高并行度。如下， A，B，D 操作的并行度为 4，而 C，E 操作的并行度为 2，那么此时整个 Job 就需要至少四个 Slots 来完成。通过这个机制，Flink 就可以不必去关心一个 Job 到底会被拆分为多少个 Tasks 和 SubTasks。 4.4 组件通讯Flink 的所有组件都基于 Actor System 来进行通讯。Actor system是多种角色的 actor 的容器，它提供调度，配置，日志记录等多种服务，并包含一个可以启动所有 actor 的线程池，如果 actor 是本地的，则消息通过共享内存进行共享，但如果 actor 是远程的，则通过 RPC 的调用来传递消息。 五、Flink 的优点最后基于上面的介绍，来总结一下 Flink 的优点： Flink 是基于事件驱动 (Event-driven) 的应用，能够同时支持流处理和批处理； 基于内存的计算，能够保证高吞吐和低延迟，具有优越的性能表现； 支持精确一次 (Exactly-once) 语意，能够完美地保证一致性和正确性； 分层 API ，能够满足各个层次的开发需求； 支持高可用配置，支持保存点机制，能够提供安全性和稳定性上的保证； 多样化的部署方式，支持本地，远端，云端等多种部署方案； 具有横向扩展架构，能够按照用户的需求进行动态扩容； 活跃度极高的社区和完善的生态圈的支持。 参考资料 Dataflow Programming Model Distributed Runtime Environment Component Stack Fabian Hueske , Vasiliki Kalavri . 《Stream Processing with Apache Flink》. O’Reilly Media . 2019-4-30]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>核心概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink消费Kafka写入Mysql]]></title>
    <url>%2F2019%2F07%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E6%B6%88%E8%B4%B9Kafka%E5%86%99%E5%85%A5Mysql%2F</url>
    <content type="text"><![CDATA[本文介绍消费Kafka的消息实时写入Mysql maven新增依赖： 12345&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.39&lt;/version&gt;&lt;/dependency&gt; 2.重写RichSinkFunction,实现一个Mysql Sink 123456789101112131415161718192021222324252627public class MysqlSink extends RichSinkFunction&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; &#123;private Connection connection;private PreparedStatement preparedStatement;String username = &quot;&quot;;String password = &quot;&quot;;String drivername = &quot;&quot;; //配置改成自己的配置String dburl = &quot;&quot;;@Overridepublic void invoke(Tuple3&lt;Integer, String, Integer&gt; value) throws Exception &#123; Class.forName(drivername); connection = DriverManager.getConnection(dburl, username, password); String sql = &quot;replace into table(id,num,price) values(?,?,?)&quot;; //假设mysql 有3列 id,num,price preparedStatement = connection.prepareStatement(sql); preparedStatement.setInt(1, value.f0); preparedStatement.setString(2, value.f1); preparedStatement.setInt(3, value.f2); preparedStatement.executeUpdate(); if (preparedStatement != null) &#123; preparedStatement.close(); &#125; if (connection != null) &#123; connection.close(); &#125;&#125;&#125; Flink主类 12345678910111213141516171819202122232425262728public class MysqlSinkTest &#123;public static void main(String[] args) throws Exception &#123;StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();Properties properties = new Properties();properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;);// 1,abc,100 类似这样的数据，当然也可以是很复杂的json数据，去做解析FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(&quot;test&quot;, new SimpleStringSchema(), properties);env.getConfig().disableSysoutLogging(); //设置此可以屏蔽掉日记打印情况env.getConfig().setRestartStrategy( RestartStrategies.fixedDelayRestart(5, 5000));env.enableCheckpointing(2000);DataStream&lt;String&gt; stream = env .addSource(consumer);DataStream&lt;Tuple3&lt;Integer, String, Integer&gt;&gt; sourceStream = stream.filter((FilterFunction&lt;String&gt;) value -&gt; StringUtils.isNotBlank(value)) .map((MapFunction&lt;String, Tuple3&lt;Integer, String, Integer&gt;&gt;) value -&gt; &#123; String[] args1 = value.split(&quot;,&quot;); return new Tuple3&lt;Integer, String, Integer&gt;(Integer .valueOf(args1[0]), args1[1],Integer .valueOf(args1[2]));&#125;);sourceStream.addSink(new MysqlSink());env.execute(&quot;data to mysql start&quot;);&#125;&#125;]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>消费Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink-Redis-Sink]]></title>
    <url>%2F2019%2F07%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink-Redis-Sink%2F</url>
    <content type="text"><![CDATA[简介流式计算中，我们经常有一些场景是消费Kafka数据，进行处理，然后存储到其他的数据库或者缓存或者重新发送回其他的消息队列中。本文讲述一个简单的Redis作为Sink的案例。后续，我们会补充完善，比如落入Hbase，Kafka，Mysql等。 关于Redis SinkFlink提供了封装好的写入Redis的包给我们用，首先我们要新增一个依赖：12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-redis_2.10&lt;/artifactId&gt; &lt;version&gt;1.1.5&lt;/version&gt;&lt;/dependency&gt; 然后我们实现一个自己的RedisSinkExample： 1234567891011121314//指定Redis setpublic static final class RedisSinkExample implements RedisMapper&lt;Tuple2&lt;String,Integer&gt;&gt; &#123;public RedisCommandDescription getCommandDescription() &#123; return new RedisCommandDescription(RedisCommand.SET, null);&#125;public String getKeyFromData(Tuple2&lt;String, Integer&gt; data) &#123; return data.f0;&#125;public String getValueFromData(Tuple2&lt;String, Integer&gt; data) &#123; return data.f1.toString();&#125;&#125; 我们用最简单的单机Redis的SET命令进行演示。 完整的代码如下，实现一个读取Kafka的消息，然后进行WordCount，并把结果更新到redis中： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class RedisSinkTest &#123;public static void main(String[] args) throws Exception&#123;StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime);env.enableCheckpointing(2000);env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);//连接kafkaProperties properties = new Properties();properties.setProperty(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;);FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(&quot;test&quot;, new SimpleStringSchema(), properties);consumer.setStartFromEarliest();DataStream&lt;String&gt; stream = env.addSource(consumer);DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts = stream.flatMap(new LineSplitter()).keyBy(0).sum(1);//实例化FlinkJedisPoolConfig 配置redisFlinkJedisPoolConfig conf = new FlinkJedisPoolConfig.Builder().setHost(&quot;127.0.0.1&quot;).setPort(&quot;6379&quot;).build();//实例化RedisSink，并通过flink的addSink的方式将flink计算的结果插入到rediscounts.addSink(new RedisSink&lt;&gt;(conf,new RedisSinkExample()));env.execute(&quot;WordCount From Kafka To Redis&quot;);&#125;//public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123;@Overridepublic void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) &#123; String[] tokens = value.toLowerCase().split(&quot;\\W+&quot;); for (String token : tokens) &#123; if (token.length() &gt; 0) &#123; out.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125;&#125;&#125;//指定Redis setpublic static final class RedisSinkExample implements RedisMapper&lt;Tuple2&lt;String,Integer&gt;&gt; &#123;public RedisCommandDescription getCommandDescription() &#123; return new RedisCommandDescription(RedisCommand.SET, null);&#125;public String getKeyFromData(Tuple2&lt;String, Integer&gt; data) &#123; return data.f0;&#125;public String getValueFromData(Tuple2&lt;String, Integer&gt; data) &#123; return data.f1.toString();&#125;&#125;&#125;// 预告，后续更新写入Hbase和Mysql案例代码。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>Redis-Sink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink实战项目之实时热销排行]]></title>
    <url>%2F2019%2F07%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E5%AE%9E%E6%88%98%E9%A1%B9%E7%9B%AE%E4%B9%8B%E5%AE%9E%E6%97%B6%E7%83%AD%E9%94%80%E6%8E%92%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[需求某个图书网站，希望看到双十一秒杀期间实时的热销排行榜单。我们可以将“实时热门商品”翻译成程序员更好理解的需求:每隔5秒钟输出最近一小时内点击量最多的前 N 个商品/图书. 需求分解将这个需求进行分解我们大概要做这么几件事情： 告诉 Flink 框架基于时间做窗口，我们这里用processingTime，不用自带时间戳 过滤出图书点击行为数据 按一小时的窗口大小，每5秒钟统计一次，做滑动窗口聚合（Sliding Window） 聚合，输出窗口中点击量前N名的商品 代码实现向Kafka发消息模拟购买事件123456789101112131415161718192021public class KafkaProducer &#123; public static void main(String[] args) throws Exception&#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; text = env.addSource(new MyNoParalleSource()).setParallelism(1); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;); //new FlinkKafkaProducer(&quot;topn&quot;,new KeyedSerializationSchemaWrapper(new SimpleStringSchema()),properties,FlinkKafkaProducer.Semantic.EXACTLY_ONCE); FlinkKafkaProducer&lt;String&gt; producer = new FlinkKafkaProducer(&quot;topn&quot;,new SimpleStringSchema(),properties);/* //event-timestamp事件的发生时间 producer.setWriteTimestampToKafka(true);*/ text.addSink(producer); env.execute(); &#125;&#125;// 其中的：MyNoParalleSource 是作者自己实现的一个并行度为1的发送器，用来向kafka发送数据： 123456789101112131415161718192021222324252627282930313233343536public class MyNoParalleSource implements SourceFunction&lt;String&gt; &#123;//1 //private long count = 1L; private boolean isRunning = true; /** * 主要的方法 * 启动一个source * 大部分情况下，都需要在这个run方法中实现一个循环，这样就可以循环产生数据了 * * @param ctx * @throws Exception */ @Override public void run(SourceContext&lt;String&gt; ctx) throws Exception &#123; while(isRunning)&#123; //图书的排行榜 List&lt;String&gt; books = new ArrayList&lt;&gt;(); books.add(&quot;Pyhton从入门到放弃&quot;);//10 books.add(&quot;Java从入门到放弃&quot;);//8 books.add(&quot;Php从入门到放弃&quot;);//5 books.add(&quot;C++从入门到放弃&quot;);//3 books.add(&quot;Scala从入门到放弃&quot;);//0-4 int i = new Random().nextInt(5); ctx.collect(books.get(i)); //每1秒产生一条数据 Thread.sleep(1000); &#125; &#125; //取消一个cancel的时候会调用的方法 @Override public void cancel() &#123; isRunning = false; &#125;&#125; 可见，我们每过1秒向Kafka的topn这个topic随机发送一本书的名字用来模拟购买行为。 整体实现代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119public class TopN &#123; public static void main(String[] args) throws Exception&#123; /** * * 书1 书2 书3 * （书1,1） (书2，1) （书3,1） * * */ //每隔5秒钟 计算过去1小时 的 Top 3 商品 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); env.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); //以processtime作为时间语义 Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;127.0.0.1:9092&quot;); FlinkKafkaConsumer&lt;String&gt; input = new FlinkKafkaConsumer&lt;&gt;(&quot;topn&quot;, new SimpleStringSchema(), properties); //从最早开始消费 位点 input.setStartFromEarliest(); DataStream&lt;String&gt; stream = env .addSource(input); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; ds = stream .flatMap(new LineSplitter()); //将输入语句split成一个一个单词并初始化count值为1的Tuple2&lt;String, Integer&gt;类型 DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; wcount = ds .keyBy(0) .window(SlidingProcessingTimeWindows.of(Time.seconds(600),Time.seconds(5))) //key之后的元素进入一个总时间长度为600s,每5s向后滑动一次的滑动窗口 .sum(1);// 将相同的key的元素第二个count值相加 wcount .windowAll(TumblingProcessingTimeWindows.of(Time.seconds(5)))//(shu1, xx) (shu2,xx).... //所有key元素进入一个5s长的窗口（选5秒是因为上游窗口每5s计算一轮数据，topN窗口一次计算只统计一个窗口时间内的变化） .process(new TopNAllFunction(3)) .print();//redis sink redis -&gt; 接口 env.execute(); &#125;// private static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) &#123; // normalize and split the line //String[] tokens = value.toLowerCase().split(&quot;\\W+&quot;); // emit the pairs /*for (String token : tokens) &#123; if (token.length() &gt; 0) &#123; out.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125;*/ //（书1,1） (书2，1) （书3,1） out.collect(new Tuple2&lt;String, Integer&gt;(value, 1)); &#125; &#125; private static class TopNAllFunction extends ProcessAllWindowFunction&lt;Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt; &#123; private int topSize = 3; public TopNAllFunction(int topSize) &#123; this.topSize = topSize; &#125; public void process( ProcessAllWindowFunction&lt;Tuple2&lt;String, Integer&gt;, String, TimeWindow&gt;.Context arg0, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; input, Collector&lt;String&gt; out) throws Exception &#123; TreeMap&lt;Integer, Tuple2&lt;String, Integer&gt;&gt; treemap = new TreeMap&lt;Integer, Tuple2&lt;String, Integer&gt;&gt;( new Comparator&lt;Integer&gt;() &#123; @Override public int compare(Integer y, Integer x) &#123; return (x &lt; y) ? -1 : 1; &#125; &#125;); //treemap按照key降序排列，相同count值不覆盖 for (Tuple2&lt;String, Integer&gt; element : input) &#123; treemap.put(element.f1, element); if (treemap.size() &gt; topSize) &#123; //只保留前面TopN个元素 treemap.pollLastEntry(); &#125; &#125; for (Map.Entry&lt;Integer, Tuple2&lt;String, Integer&gt;&gt; entry : treemap .entrySet()) &#123; out.collect(&quot;=================\n热销图书列表:\n&quot;+ new Timestamp(System.currentTimeMillis()) + treemap.toString() + &quot;\n===============\n&quot;); &#125; &#125; &#125;&#125;// 查看输出：12345678=================热销图书列表:2019-03-05 22:32:40.004&#123;8=(Java从入门到放弃,8), 7=(C++从入门到放弃,7), 5=(Php从入门到放弃,5)&#125;================================热销图书列表:2019-03-05 22:32:45.004&#123;8=(Java从入门到放弃,8), 7=(C++从入门到放弃,7), 5=(Php从入门到放弃,5)&#125;===============]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>实时实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink-Table-SQL]]></title>
    <url>%2F2019%2F07%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink-Table-SQL%2F</url>
    <content type="text"><![CDATA[简介Apache Flink具有两个关系API - 表API和SQL - 用于统一流和批处理。Table API是Scala和Java的语言集成查询API，允许以非常直观的方式组合来自关系运算符的查询，Table API和SQL接口彼此紧密集成，以及Flink的DataStream和DataSet API。您可以轻松地在基于API构建的所有API和库之间切换。例如，您可以使用CEP库从DataStream中提取模式，然后使用Table API分析模式，或者可以在预处理上运行Gelly图算法之前使用SQL查询扫描，过滤和聚合批处理表数据。 Flink SQL的编程模型创建一个TableEnvironmentTableEnvironment是Table API和SQL集成的核心概念，它主要负责: 1、在内部目录中注册一个Table 2、注册一个外部目录 3、执行SQL查询 4、注册一个用户自定义函数(标量、表及聚合) 5、将DataStream或者DataSet转换成Table 6、持有ExecutionEnvironment或者StreamExecutionEnvironment的引用一个Table总是会绑定到一个指定的TableEnvironment中，相同的查询不同的TableEnvironment是无法通过join、union合并在一起。TableEnvironment有一个在内部通过表名组织起来的表目录，Table API或者SQL查询可以访问注册在目录中的表，并通过名称来引用它们。 在目录中注册表TableEnvironment允许通过各种源来注册一个表: 1、一个已存在的Table对象，通常是Table API或者SQL查询的结果 Table projTable = tableEnv.scan(“X”).select(…); 2、TableSource，可以访问外部数据如文件、数据库或者消息系统 TableSource csvSource = new CsvTableSource(“/path/to/file”, …); 3、DataStream或者DataSet程序中的DataStream或者DataSet //将DataSet转换为Table Table table= tableEnv.fromDataSet(tableset); 注册TableSink注册TableSink可用于将 Table API或SQL查询的结果发送到外部存储系统，例如数据库，键值存储，消息队列或文件系统（在不同的编码中，例如，CSV，Apache [Parquet] ，Avro，ORC]，……）: 1TableSink csvSink = new CsvTableSink(&quot;/path/to/file&quot;, ...); 123 2、 String[] fieldNames = &#123;&quot;a&quot;, &quot;b&quot;, &quot;c&quot;&#125;; TypeInformation[] fieldTypes = &#123;Types.INT, Types.STRING, Types.LONG&#125;; tableEnv.registerTableSink(&quot;CsvSinkTable&quot;, fieldNames, fieldTypes, csvSink); 实战案例一基于Flink SQL的WordCount: 1234567891011121314151617181920212223242526272829303132333435363738394041public class WordCountSQL &#123; public static void main(String[] args) throws Exception&#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tEnv = TableEnvironment.getTableEnvironment(env); List list = new ArrayList(); String wordsStr = &quot;Hello Flink Hello TOM&quot;; String[] words = wordsStr.split(&quot;\\W+&quot;); for(String word : words)&#123; WC wc = new WC(word, 1); list.add(wc); &#125; DataSet&lt;WC&gt; input = env.fromCollection(list); tEnv.registerDataSet(&quot;WordCount&quot;, input, &quot;word, frequency&quot;); Table table = tEnv.sqlQuery( &quot;SELECT word, SUM(frequency) as frequency FROM WordCount GROUP BY word&quot;); DataSet&lt;WC&gt; result = tEnv.toDataSet(table, WC.class); result.print(); &#125;//main public static class WC &#123; public String word;//hello public long frequency;//1 // public constructor to make it a Flink POJO public WC() &#123;&#125; public WC(String word, long frequency) &#123; this.word = word; this.frequency = frequency; &#125; @Override public String toString() &#123; return &quot;WC &quot; + word + &quot; &quot; + frequency; &#125; &#125;&#125; 输出如下： 123WC TOM 1WC Hello 2WC Flink 1 实战案例二本例稍微复杂，首先读取一个文件中的内容进行统计，并写入到另外一个文件中： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class SQLTest &#123; public static void main(String[] args) throws Exception&#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); BatchTableEnvironment tableEnv = BatchTableEnvironment.getTableEnvironment(env); env.setParallelism(1); DataSource&lt;String&gt; input = env.readTextFile(&quot;test.txt&quot;); input.print(); //转换成dataset DataSet&lt;Orders&gt; topInput = input.map(new MapFunction&lt;String, Orders&gt;() &#123; @Override public Orders map(String s) throws Exception &#123; String[] splits = s.split(&quot; &quot;); return new Orders(Integer.valueOf(splits[0]), String.valueOf(splits[1]),String.valueOf(splits[2]), Double.valueOf(splits[3])); &#125; &#125;); //将DataSet转换为Table Table order = tableEnv.fromDataSet(topInput); //orders表名 tableEnv.registerTable(&quot;Orders&quot;,order); Table tapiResult = tableEnv.scan(&quot;Orders&quot;).select(&quot;name&quot;); tapiResult.printSchema(); Table sqlQuery = tableEnv.sqlQuery(&quot;select name, sum(price) as total from Orders group by name order by total desc&quot;); //转换回dataset DataSet&lt;Result&gt; result = tableEnv.toDataSet(sqlQuery, Result.class); //将dataset map成tuple输出 /*result.map(new MapFunction&lt;Result, Tuple2&lt;String,Double&gt;&gt;() &#123; @Override public Tuple2&lt;String, Double&gt; map(Result result) throws Exception &#123; String name = result.name; Double total = result.total; return Tuple2.of(name,total); &#125; &#125;).print();*/ TableSink sink = new CsvTableSink(&quot;SQLTEST.txt&quot;, &quot;|&quot;); //writeToSink /*sqlQuery.writeToSink(sink); env.execute();*/ String[] fieldNames = &#123;&quot;name&quot;, &quot;total&quot;&#125;; TypeInformation[] fieldTypes = &#123;Types.STRING, Types.DOUBLE&#125;; tableEnv.registerTableSink(&quot;SQLTEST&quot;, fieldNames, fieldTypes, sink); sqlQuery.insertInto(&quot;SQLTEST&quot;); env.execute(); &#125; /** * 源数据的映射类 */ public static class Orders &#123; /** * 序号，姓名，书名，价格 */ public Integer id; public String name; public String book; public Double price; public Orders() &#123; super(); &#125; public Orders(Integer id, String name, String book, Double price) &#123; this.id = id; this.name = name; this.book = book; this.price = price; &#125; &#125; /** * 统计结果对应的类 */ public static class Result &#123; public String name; public Double total; public Result() &#123;&#125; &#125; &#125;// 以上所有代码，大家在公众号回复Flink即可下载，可以直接本地运行，方便大家调试]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>Table-SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink-Kafka-Connector]]></title>
    <url>%2F2019%2F07%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink-Kafka-Connector%2F</url>
    <content type="text"><![CDATA[简介Flink-kafka-connector用来做什么？ Kafka中的partition机制和Flink的并行度机制结合，实现数据恢复Kafka可以作为Flink的source和sink任务失败，通过设置kafka的offset来恢复应用 kafka简单介绍关于kafka，我们会有专题文章介绍，这里简单介绍几个必须知道的概念。 1.生产者（Producer） 顾名思义，生产者就是生产消息的组件，它的主要工作就是源源不断地生产出消息，然后发送给消息队列。生产者可以向消息队列发送各种类型的消息，如狭义的字符串消息，也可以发送二进制消息。生产者是消息队列的数据源，只有通过生产者持续不断地向消息队列发送消息，消息队列才能不断处理消息。2.消费者（Consumer） 所谓消费者，指的是不断消费（获取）消息的组件，它获取消息的来源就是消息队列（即Kafka本身）。换句话说，生产者不断向消息队列发送消息，而消费者则不断从消息队列中获取消息。3.主题（Topic） 主题是Kafka中一个极为重要的概念。首先，主题是一个逻辑上的概念，它用于从逻辑上来归类与存储消息本身。多个生产者可以向一个Topic发送消息，同时也可以有多个消费者消费一个Topic中的消息。Topic还有分区和副本的概念。Topic与消息这两个概念之间密切相关，Kafka中的每一条消息都归属于某一个Topic，而一个Topic下面可以有任意数量的消息。 kafka简单操作启动zk：nohup bin/zookeeper-server-start.sh config/zookeeper.properties &amp; 启动server: nohup bin/kafka-server-start.sh config/server.properties &amp; 创建一个topic：bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic test 查看topic：bin/kafka-topics.sh –list –zookeeper localhost:2181 发送数据：bin/kafka-console-producer.sh –broker-list localhost:9092 –topic test 启动一个消费者：bin/kafka-console-consumer.sh –bootstrap-server localhost:9092 –topic test –from-beginning 删除topic： bin/kafka-topics.sh –delete –zookeeper localhost:2181 –topic topn Flink消费Kafka注意事项 setStartFromGroupOffsets()【默认消费策略】 默认读取上次保存的offset信息 如果是应用第一次启动，读取不到上次的offset信息，则会根据这个参数auto.offset.reset的值来进行消费数据 setStartFromEarliest()从最早的数据开始进行消费，忽略存储的offset信息 setStartFromLatest()从最新的数据进行消费，忽略存储的offset信息 setStartFromSpecificOffsets(Map&lt;KafkaTopicPartition, Long&gt;)从指定位置进行消费 当checkpoint机制开启的时候，KafkaConsumer会定期把kafka的offset信息还有其他operator的状态信息一块保存起来。当job失败重启的时候，Flink会从最近一次的checkpoint中进行恢复数据，重新消费kafka中的数据。 为了能够使用支持容错的kafka Consumer，需要开启checkpointenv.enableCheckpointing(5000); // 每5s checkpoint一次 搭建Kafka单机环境我本地安装了一个kafka_2.11-2.1.0版本的kafka 启动Zookeeper和kafka server:123启动zk：nohup bin/zookeeper-server-start.sh config/zookeeper.properties &amp;启动server: nohup bin/kafka-server-start.sh config/server.properties &amp; 创建一个topic:1bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test 实战案例 所有代码，我放在了我的公众号，回复Flink可以下载 海量【java和大数据的面试题+视频资料】整理在公众号，关注后可以下载~ 更多大数据技术欢迎和作者一起探讨~ Kafka作为Flink Sink首先pom依赖：12345&lt;dependency&gt; &lt;groupId&gt;org.apache.flink&lt;/groupId&gt; &lt;artifactId&gt;flink-connector-kafka_2.11&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt; 向kafka写入数据： 123456789101112131415161718192021public class KafkaProducer &#123; public static void main(String[] args) throws Exception&#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); DataStreamSource&lt;String&gt; text = env.addSource(new MyNoParalleSource()).setParallelism(1); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); //new FlinkKafkaProducer(&quot;topn&quot;,new KeyedSerializationSchemaWrapper(new SimpleStringSchema()),properties,FlinkKafkaProducer.Semantic.EXACTLY_ONCE); FlinkKafkaProducer&lt;String&gt; producer = new FlinkKafkaProducer(&quot;test&quot;,new SimpleStringSchema(),properties);/* //event-timestamp事件的发生时间 producer.setWriteTimestampToKafka(true);*/ text.addSink(producer); env.execute(); &#125;&#125;// 大家这里特别注意，我们实现了一个并行度为1的MyNoParalleSource来生产数据，代码如下： 12345678910111213141516171819202122232425262728293031323334353637//使用并行度为1的sourcepublic class MyNoParalleSource implements SourceFunction&lt;String&gt; &#123;//1 //private long count = 1L; private boolean isRunning = true; /** * 主要的方法 * 启动一个source * 大部分情况下，都需要在这个run方法中实现一个循环，这样就可以循环产生数据了 * * @param ctx * @throws Exception */ @Override public void run(SourceContext&lt;String&gt; ctx) throws Exception &#123; while(isRunning)&#123; //图书的排行榜 List&lt;String&gt; books = new ArrayList&lt;&gt;(); books.add(&quot;Pyhton从入门到放弃&quot;);//10 books.add(&quot;Java从入门到放弃&quot;);//8 books.add(&quot;Php从入门到放弃&quot;);//5 books.add(&quot;C++从入门到放弃&quot;);//3 books.add(&quot;Scala从入门到放弃&quot;);//0-4 int i = new Random().nextInt(5); ctx.collect(books.get(i)); //每2秒产生一条数据 Thread.sleep(2000); &#125; &#125; //取消一个cancel的时候会调用的方法 @Override public void cancel() &#123; isRunning = false; &#125;&#125; 代码实现了一个发送器，来发送书名&lt;Pyhton从入门到放弃&gt;&lt;Java从入门到放弃&gt;等… 然后右键运行我们的程序，控制台输出如下： 开始源源不断的生产数据了。 然后我们用命令去查看一下 kafka test这个topic： 1bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning 输出如下： Kafka作为Flink Source直接上代码： 12345678910111213141516171819public class KafkaConsumer &#123; public static void main(String[] args) throws Exception&#123; StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); Properties properties = new Properties(); properties.setProperty(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); FlinkKafkaConsumer&lt;String&gt; consumer = new FlinkKafkaConsumer&lt;&gt;(&quot;test&quot;, new SimpleStringSchema(), properties); //从最早开始消费 consumer.setStartFromEarliest(); DataStream&lt;String&gt; stream = env .addSource(consumer); stream.print(); //stream.map(); env.execute(); &#125;&#125;// 控制台输出如下： 将我们之前发往kafka的消息全部打印出来了。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>Kafka连接器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Broadcast广播变量]]></title>
    <url>%2F2019%2F07%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BBroadcast%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[广播变量简介在Flink中，同一个算子可能存在若干个不同的并行实例，计算过程可能不在同一个Slot中进行，不同算子之间更是如此，因此不同算子的计算数据之间不能像Java数组之间一样互相访问，而广播变量Broadcast便是解决这种情况的。 我们可以把广播变量理解为是一个公共的共享变量，我们可以把一个dataset 数据集广播出去，然后不同的task在节点上都能够获取到，这个数据在每个节点上只会存在一份 用法123456789101：初始化数据 DataSet&lt;Integer&gt; num = env.fromElements(1, 2, 3) 2：广播数据 .withBroadcastSet(toBroadcast, &quot;num&quot;); 3：获取数据 Collection&lt;Integer&gt; broadcastSet = getRuntimeContext().getBroadcastVariable(&quot;num&quot;); 注意： 1：广播出去的变量存在于每个节点的内存中，所以这个数据集不能太大。因为广播出去的数据，会常驻内存，除非程序执行结束 2：广播变量在初始化广播出去以后不支持修改，这样才能保证每个节点的数据都是一致的。 注意事项使用广播状态，task 之间不会相互通信只有广播的一边可以修改广播状态的内容。用户必须保证所有 operator 并发实例上对广播状态的 修改行为都是一致的。或者说，如果不同的并发实例拥有不同的广播状态内容，将导致不一致的结果。 广播状态中事件的顺序在各个并发实例中可能不尽相同广播流的元素保证了将所有元素（最终）都发给下游所有的并发实例，但是元素的到达的顺序可能在并发实例之间并不相同。因此，对广播状态的修改不能依赖于输入数据的顺序。 所有operator task都会快照下他们的广播状态在checkpoint时，所有的 task 都会 checkpoint 下他们的广播状态，随着并发度的增加，checkpoint 的大小也会随之增加 广播变量存在内存中广播出去的变量存在于每个节点的内存中，所以这个数据集不能太大，百兆左右可以接受，Gb不能接受 案例12345678910111213141516171819202122232425public class BroadCastTest &#123; public static void main(String[] args) throws Exception&#123; ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); //1.封装一个DataSet DataSet&lt;Integer&gt; broadcast = env.fromElements(1, 2, 3); DataSet&lt;String&gt; data = env.fromElements(&quot;a&quot;, &quot;b&quot;); data.map(new RichMapFunction&lt;String, String&gt;() &#123; private List list = new ArrayList(); @Override public void open(Configuration parameters) throws Exception &#123; // 3. 获取广播的DataSet数据 作为一个Collection Collection&lt;Integer&gt; broadcastSet = getRuntimeContext().getBroadcastVariable(&quot;number&quot;); list.addAll(broadcastSet); &#125; @Override public String map(String value) throws Exception &#123; return value + &quot;: &quot;+ list; &#125; &#125;).withBroadcastSet(broadcast, &quot;number&quot;) // 2. 广播的broadcast .printToErr();//打印到err方便查看 &#125;&#125; 输出结果： 12a: [1, 2, 3]b: [1, 2, 3]]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>广播变量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之时间戳和水印]]></title>
    <url>%2F2019%2F07%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%97%B6%E9%97%B4%E6%88%B3%E5%92%8C%E6%B0%B4%E5%8D%B0%2F</url>
    <content type="text"><![CDATA[实际问题（乱序）在介绍Watermark相关内容之前我们先抛出一个具体的问题，在实际的流式计算中数据到来的顺序对计算结果的正确性有至关重要的影响，比如：某数据源中的某些数据由于某种原因(如：网络原因，外部存储自身原因)会有5秒的延时，也就是在实际时间的第1秒产生的数据有可能在第5秒中产生的数据之后到来(比如到Window处理节点).选具体某个delay的元素来说，假设在一个5秒的Tumble窗口(详见Window介绍章节)，有一个EventTime是 11秒的数据，在第16秒时候到来了。图示第11秒的数据，在16秒到来了，如下图： 那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？Apache Flink的时间类型开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图： 那么对于一个Count聚合的Tumble(5s)的window，上面的情况如何处理才能window2=4，window3=2 呢？ Apache Flink的时间类型开篇我们描述的问题是一个很常见的TimeWindow中数据乱序的问题，乱序是相对于事件产生时间和到达Apache Flink 实际处理算子的顺序而言的，关于时间在Apache Flink中有如下三种时间类型，如下图： ProcessingTime 是数据流入到具体某个算子时候相应的系统时间。ProcessingTime 有最好的性能和最低的延迟。但在分布式计算环境中ProcessingTime具有不确定性，相同数据流多次运行有可能产生不同的计算结果。 IngestionTime IngestionTime是数据进入Apache Flink框架的时间，是在Source Operator中设置的。与ProcessingTime相比可以提供更可预测的结果，因为IngestionTime的时间戳比较稳定(在源处只记录一次)，同一数据在流经不同窗口操作时将使用相同的时间戳，而对于ProcessingTime同一数据在流经不同窗口算子会有不同的处理时间戳。 EventTime EventTime是事件在设备上产生时候携带的。在进入Apache Flink框架之前EventTime通常要嵌入到记录中，并且EventTime也可以从记录中提取出来。在实际的网上购物订单等业务场景中，大多会使用EventTime来进行数据计算。 开篇描述的问题和本篇要介绍的Watermark所涉及的时间类型均是指EventTime类型。 什么是WatermarkWatermark是Apache Flink为了处理EventTime 窗口计算提出的一种机制,本质上也是一种时间戳，由Apache Flink Source或者自定义的Watermark生成器按照需求Punctuated或者Periodic两种方式生成的一种系统Event，与普通数据流Event一样流转到对应的下游算子，接收到Watermark Event的算子以此不断调整自己管理的EventTime clock。 Apache Flink 框架保证Watermark单调递增，算子接收到一个Watermark时候，框架知道不会再有任何小于该Watermark的时间戳的数据元素到来了，所以Watermark可以看做是告诉Apache Flink框架数据流已经处理到什么位置(时间维度)的方式。 Watermark的产生和Apache Flink内部处理逻辑如下图所示: Watermark的产生方式目前Apache Flink 有两种生产Watermark的方式，如下： Punctuated - 数据流中每一个递增的EventTime都会产生一个Watermark。 在实际的生产中Punctuated方式在TPS很高的场景下会产生大量的Watermark在一定程度上对下游算子造成压力，所以只有在实时性要求非常高的场景才会选择Punctuated的方式进行Watermark的生成。 Periodic - 周期性的（一定时间间隔或者达到一定的记录条数）产生一个Watermark。在实际的生产中Periodic的方式必须结合时间和积累条数两个维度继续周期性产生Watermark，否则在极端情况下会有很大的延时。 所以Watermark的生成方式需要根据业务场景的不同进行不同的选择。 Watermark的接口定义对应Apache Flink Watermark两种不同的生成方式，我们了解一下对应的接口定义，如下： Periodic Watermarks - AssignerWithPeriodicWatermarks 1234567891011121314151617181920212223/*** Returns the current watermark. This method is periodically called by the* system to retrieve the current watermark. The method may return &#123;@code null&#125; to* indicate that no new Watermark is available.** &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp* is larger than that of the previously emitted watermark (to preserve the contract of* ascending watermarks). If the current watermark is still* identical to the previous one, no progress in EventTime has happened since* the previous call to this method. If a null value is returned, or theTimestamp* of the returned watermark is smaller than that of the last emitted one, then no* new watermark will be generated.** &amp;lt;p&amp;gt;The interval in which this method is called and Watermarks are generated* depends on &#123;@link ExecutionConfig#getAutoWatermarkInterval()&#125;.** @see org.Apache.flink.streaming.api.watermark.Watermark* @see ExecutionConfig#getAutoWatermarkInterval()** @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.*/@NullableWatermark getCurrentWatermark(); Punctuated Watermarks - AssignerWithPunctuatedWatermarks 1234567891011121314151617181920public interface AssignerWithPunctuatedWatermarks&amp;lt;T&amp;gt; extendsTimestampAssigner&amp;lt;T&amp;gt; &#123;/*** Asks this implementation if it wants to emit a watermark. This method is called right after* the &#123;@link #extractTimestamp(Object, long)&#125; method.** &amp;lt;p&amp;gt;The returned watermark will be emitted only if it is non-null and itsTimestamp* is larger than that of the previously emitted watermark (to preserve the contract of* ascending watermarks). If a null value is returned, or theTimestamp of the returned* watermark is smaller than that of the last emitted one, then no new watermark will* be generated.** &amp;lt;p&amp;gt;For an example how to use this method, see the documentation of* &#123;@link AssignerWithPunctuatedWatermarks this class&#125;.** @return &#123;@code Null&#125;, if no watermark should be emitted, or the next watermark to emit.*/@NullableWatermark checkAndGetNextWatermark(T lastElement, long extractedTimestamp);&#125; AssignerWithPunctuatedWatermarks 继承了TimestampAssigner接口 -TimestampAssigner 1234567891011121314151617public interfaceTimestampAssigner&amp;lt;T&amp;gt; extends Function &#123;/*** Assigns aTimestamp to an element, in milliseconds since the Epoch.** &amp;lt;p&amp;gt;The method is passed the previously assignedTimestamp of the element.* That previousTimestamp may have been assigned from a previous assigner,* by ingestionTime. If the element did not carry aTimestamp before, this value is* &#123;@code Long.MIN_VALUE&#125;.** @param element The element that theTimestamp is wil be assigned to.* @param previousElementTimestamp The previous internalTimestamp of the element,* or a negative value, if noTimestamp has been assigned, yet.* @return The newTimestamp.*/long extractTimestamp(T element, long previousElementTimestamp);&#125; 从接口定义可以看出，Watermark可以在Event(Element)中提取EventTime，进而定义一定的计算逻辑产生Watermark的时间戳。 Watermark解决如上问题从上面的Watermark生成接口和Apache Flink内部对Periodic Watermark的实现来看，Watermark的时间戳可以和Event中的EventTime 一致，也可以自己定义任何合理的逻辑使得Watermark的时间戳不等于Event中的EventTime，Event中的EventTime自产生那一刻起就不可以改变了，不受Apache Flink框架控制，而Watermark的产生是在Apache Flink的Source节点或实现的Watermark生成器计算产生(如上Apache Flink内置的 Periodic Watermark实现), Apache Flink内部对单流或多流的场景有统一的Watermark处理。 回过头来我们在看看Watermark机制如何解决上面的问题，上面的问题在于如何将迟来的EventTime 位11的元素正确处理。要解决这个问题我们还需要先了解一下EventTime window是如何触发的？ EventTime window 计算条件是当Window计算的Timer时间戳 小于等于 当前系统的Watermak的时间戳时候进行计算。 当Watermark的时间戳等于Event中携带的EventTime时候，上面场景（Watermark=EventTime)的计算结果如下： 上面对应的DDL(Alibaba 企业版的Flink分支)定义如下： 1234567CREATE TABLE source(...,Event_timeTimeStamp,WATERMARK wk1 FOR Event_time as withOffset(Event_time, 0) ) with (...); 如果想正确处理迟来的数据可以定义Watermark生成策略为 Watermark = EventTime -5s， 如下： 上面对应的DDL(Alibaba 内部的DDL语法，目前正在和社区讨论)定义如下： 1234567CREATE TABLE source(...,Event_timeTimeStamp,WATERMARK wk1 FOR Event_time as withOffset(Event_time, 5000) ) with (...); 上面正确处理的根源是我们采取了 延迟触发 window 计算 的方式正确处理了 Late Event. 与此同时，我们发现window的延时触发计算，也导致了下游的LATENCY变大，本例子中下游得到window的结果就延迟了5s. 多流的Watermark处理在实际的流计算中往往一个job中会处理多个Source的数据，对Source的数据进行GroupBy分组，那么来自不同Source的相同key值会shuffle到同一个处理节点，并携带各自的Watermark，Apache Flink内部要保证Watermark要保持单调递增，多个Source的Watermark汇聚到一起时候可能不是单调自增的，这样的情况Apache Flink内部是如何处理的呢？如下图所示： Apache Flink内部实现每一个边上只能有一个递增的Watermark， 当出现多流携带Eventtime汇聚到一起(GroupBy or Union)时候，Apache Flink会选择所有流入的Eventtime中最小的一个向下游流出。从而保证watermark的单调递增和保证数据的完整性.如下图: 本节以一个流计算常见的乱序问题介绍了Apache Flink如何利用Watermark机制来处理乱序问题. 本篇内容在一定程度上也体现了EventTime Window中的Trigger机制依赖了Watermark(后续Window篇章会介绍)。Watermark机制是流计算中处理乱序，正确处理Late Event的核心手段。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>时间戳</tag>
        <tag>水印</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink集群的高可用(搭建篇补充)]]></title>
    <url>%2F2019%2F07%2F06%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E9%9B%86%E7%BE%A4%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8(%E6%90%AD%E5%BB%BA%E7%AF%87%E8%A1%A5%E5%85%85)%2F</url>
    <content type="text"><![CDATA[概述JobManager 协调每个 Flink 部署。它负责调度和资源管理。默认情况下，每个 Flink 集群只有一个 JobManager 实例。 这会产生单点故障(SPOF)：如果 JobManager 崩溃，则无法提交新作业并且导致运行中的作业运行失败。使用 JobManager 高可用性模式，可以避免这个问题，从而消除 SPOF。您可以为Standalone和 YARN 集群配置高可用性。 Standalone集群高可用性针对 Standalone 集群的 JobManager 高可用性的一般概念是，任何时候都有一个 主 JobManager 和多个备 JobManagers，以便在主节点失败时有备 JobManagers 来接管集群。这保证了没有单点故障，一旦备 JobManager 接管集群，作业就可以正常运行。主备 JobManager 实例之间没有明显的区别。每个 JobManager 都可以充当主备节点。例如，请考虑以下三个 JobManager 实例的设置: .resources/6278EDED-A65A-4539-A16D-7BCD9FE77864.png) 配置要启用 JobManager 高可用性，您必须将高可用性模式设置为 zookeeper，配置 zookeeper quorum 将所有 JobManager 主机及其 web UI 端口写入配置文件。Flink利用 ZooKeeper 在所有正在运行的 JobManager 实例之间进行分布式协调。 ZooKeeper 是独立于 Flink 的服务，通过 Leader 选举和轻量级一致状态存储提供高可靠的分布式协调。 Masters文件 (masters服务器)要启动HA集群，请在以下位置配置Master文件 conf/masters:masters文件：masters文件包含启动 jobmanager 的所有主机和 web 用户界面绑定的端口。 123jobManagerAddress1:webUIPort1 [...]jobManagerAddressX:webUIPortX 默认情况下，job manager选一个随机端口作为进程随机通信端口。您可以通过 high-availability.jobmanager.port 键修改此设置。此配置接受单个端口（例如50010），范围（50000-50025）或两者的组合（50010,50011,50020-50025,50050-50075）。 配置文件（flink-conf.yaml）要启动HA集群，请将以下配置键添加到 conf/flink-conf.yaml: 高可用性模式（必需）：在 conf/flink-conf.yaml 中，必须将高可用性模式设置为zookeeper，以打开高可用模式。或者将此选项设置为工厂类的 FQN，Flink 通过创建 HighAvailabilityServices 实例使用。 1high-availability: zookeeper Zookeeper quorum（必需）： ZooKeeper quorum 是 ZooKeeper 服务器的复制组，它提供分布式协调服务。 1high-availability.zookeeper.quorum:address1:2181[,...],addressX:2181 每个 addressX:port 都是一个 ZooKeeper 服务器的ip及其端口，Flink 可以在指定的地址和端口访问zookeeper。 ZooKeeper root （推荐）： ZooKeeper 根节点，在该节点下放置所有集群节点。 1high-availability.zookeeper.path.root: /flink ZooKeeper cluster-id（推荐）： ZooKeeper的cluster-id节点，在该节点下放置集群的所有相关数据。 1high-availability.cluster-id: /default_ns # important: customize per cluster 重要： 在运行 YARN 或其他群集管理器中运行时，不要手动设置此值。在这些情况下，将根据应用程序 ID 自动生成 cluster-id。 手动设置 cluster-id 会覆盖 YARN 中的自动生成的 ID。反过来，使用 -z CLI 选项指定 cluster-id 会覆盖手动配置。如果在裸机上运行多个 Flink HA 集群，则必须为每个集群手动配置单独的 cluster-id。 存储目录（必需）： JobManager 元数据保存在文件系统 storageDir 中，在 ZooKeeper 中仅保存了指向此状态的指针。 1high-availability.storageDir: hdfs:///flink/recovery 该storageDir 中保存了 JobManager 恢复状态所需的所有元数据。配置 master 文件和 ZooKeeper quorum 之后，您可以使用提供的集群启动脚本。它们将启动 HA 群集。请注意，启动 Flink HA 集群前，必须启动 Zookeeper 集群，并确保为要启动的每个 HA 群集配置单独的 ZooKeeper 根路径。 示例：具有2个 JobManager 的 Standalone 集群 在conf/flink-conf.yaml 中配置高可用模式和 ZooKeeper quorum： 12345high-availability: zookeeperhigh-availability.zookeeper.quorum: localhost:2181high-availability.zookeeper.path.root: /flinkhigh-availability.cluster-id: /cluster_onehigh-availability.storageDir: hdfs:///flink/recovery 在 conf/master 中配置 master: 12localhost:8081localhost:8082 在 conf/zoo.cfg 中配置 ZooKeeper 服务（目前，每台机器只能运行一个 ZooKeeper 进程） 1server.0=localhost:2888:3888 启动 ZooKeeper quorum： 12$ bin/start-zookeeper-quorum.shStarting zookeeper daemon on host localhost. 启动 Flink HA 集群： 12345$ bin/start-cluster.shStarting HA cluster with 2 masters and 1 peers in ZooKeeper quorum.Starting jobmanager daemon on host localhost.Starting jobmanager daemon on host localhost.Starting taskmanager daemon on host localhost. 停止 Zookeeper quorum 和集群： 123456$ bin/stop-cluster.shStopping taskmanager daemon (pid: 7647) on localhost.Stopping jobmanager daemon (pid: 7495) on host localhost.Stopping jobmanager daemon (pid: 7349) on host localhost.$ bin/stop-zookeeper-quorum.shStopping zookeeper daemon (pid: 7101) on host localhost. YARN 集群的高可用性在运行高可用性 YARN 集群时，我们不会运行多个 JobManager (ApplicationMaster) 实例，而只运行一个，该JobManager实例失败时，YARN会将其重新启动。Yarn的具体行为取决于您使用的 YARN 版本。 配置Application Master最大重试次数（yarn-site.xml）在YARN 配置文件 yarn-site.xml 中，需要配置 application master 的最大重试次数： 1234567&lt;property&gt; &lt;name&gt;yarn.resourcemanager.am.max-attempts&lt;/name&gt; &lt;value&gt;4&lt;/value&gt; &lt;description&gt; The maximum number of application master execution attempts. &lt;/description&gt;&lt;/property&gt; 当前 YARN 版本的默认值是2(表示允许单个JobManager失败两次)。 Application Attempts（flink-conf.yaml）：除了HA配置(参考上文)之外，您还必须配置最大重试次数 conf/flink-conf.yaml: 1yarn.application-attempts: 10 这意味着在如果程序启动失败，YARN会再重试9次（9 次重试 + 1次启动）。如果 YARN 操作需要，如果启动10次作业还失败，yarn才会将该任务的状态置为失败。如果抢占，节点硬件故障或重启，NodeManager 重新同步等操作需要，YARN继续尝试启动应用。 这些重启不计入 yarn.application-attempts 个数中。重要的是要注意 yarn.resourcemanager.am.max-attempts 为yarn中程序重启上限。因此， Flink 中设置的程序尝试次数不能超过 YARN 的集群设置。 示例：高可用的YARN Session1.配置 HA 模式和 ZooKeeper 集群在 conf/flink-conf.yaml 中：12345high-availability: zookeeperhigh-availability.zookeeper.quorum: localhost:2181high-availability.storageDir: hdfs:///flink/recoveryhigh-availability.zookeeper.path.root: /flinkyarn.application-attempts: 10 配置 ZooKeeper 服务在 conf/zoo.cfg 中(目前每台机器只能运行一个 ZooKeeper 进程)： 1server.0=localhost:2888:3888 启动 ZooKeeper 集群： 12$ bin/start-zookeeper-quorum.shStarting zookeeper daemon on host localhost. 启动 HA 集群： 1$ bin / yarn-session.sh -n 2 配置 Zookeeper 安全性如果 ZooKeeper 使用 Kerberos 以安全模式运行，flink-conf.yaml 根据需要覆盖以下配置：1234567zookeeper.sasl.service-name: zookeeper # 默认设置是 “zookeeper” 。如果 ZooKeeper 集群配置了# 不同的服务名称，那么可以在这里提供。zookeeper.sasl.login-context-name: Client # 默认设置是 “Client”。该值配置需要匹配# &quot;security.kerberos.login.contexts&quot;中的其中一个值。 有关 Kerberos 安全性的 Flink 配置的更多信息，请参阅 此处。您还可以在 此处 找到关于 Flink 内部如何设置基于 kerberos 的安全性的详细信息。 Bootstrap ZooKeeper如果您没有正在运行的ZooKeeper，则可以使用Flink程序附带的脚本。这是一个 ZooKeeper 配置模板 conf/zoo.cfg。您可以为主机配置为使用 server.X 条目运行 ZooKeeper，其中 X 是每个服务器的唯一IP:123server.X=addressX:peerPort:leaderPort[...]server.Y=addressY:peerPort:leaderPort 该脚本 bin/start-zookeeper-quorum.sh 将在每个配置的主机上启动 ZooKeeper 服务器。 Flink wrapper 会启动 ZooKeeper 服务，该 wraper 从 conf/zoo.cfg 中读取配置，并设置一些必需的配置项。在生产设置中，建议您使用自己安装的 ZooKeeper。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>高可用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink中的Time]]></title>
    <url>%2F2019%2F07%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E4%B8%AD%E7%9A%84Time%2F</url>
    <content type="text"><![CDATA[时间时间类型 Flink中的时间与现实世界中的时间是不一致的，在flink中被划分为事件时间，摄入时间，处理时间三种。 如果以EventTime为基准来定义时间窗口将形成EventTimeWindow,要求消息本身就应该携带EventTime 如果以IngesingtTime为基准来定义时间窗口将形成IngestingTimeWindow,以source的systemTime为准。 如果以ProcessingTime基准来定义时间窗口将形成ProcessingTimeWindow，以operator的systemTime为准。 时间详解Processing Time Processing Time 是指事件被处理时机器的系统时间。 当流程序在 Processing Time 上运行时，所有基于时间的操作(如时间窗口)将使用当时机器的系统时间。每小时 Processing Time 窗口将包括在系统时钟指示整个小时之间到达特定操作的所有事件。 例如，如果应用程序在上午 9:15 开始运行，则第一个每小时 Processing Time 窗口将包括在上午 9:15 到上午 10:00 之间处理的事件，下一个窗口将包括在上午 10:00 到 11:00 之间处理的事件。 Processing Time 是最简单的 “Time” 概念，不需要流和机器之间的协调，它提供了最好的性能和最低的延迟。但是，在分布式和异步的环境下，Processing Time 不能提供确定性，因为它容易受到事件到达系统的速度（例如从消息队列）、事件在系统内操作流动的速度以及中断的影响。 Event Time Event Time 是事件发生的时间，一般就是数据本身携带的时间。这个时间通常是在事件到达 Flink 之前就确定的，并且可以从每个事件中获取到事件时间戳。在 Event Time 中，时间取决于数据，而跟其他没什么关系。Event Time 程序必须指定如何生成 Event Time 水印，这是表示 Event Time 进度的机制。 完美的说，无论事件什么时候到达或者其怎么排序，最后处理 Event Time 将产生完全一致和确定的结果。但是，除非事件按照已知顺序（按照事件的时间）到达，否则处理 Event Time 时将会因为要等待一些无序事件而产生一些延迟。由于只能等待一段有限的时间，因此就难以保证处理 Event Time 将产生完全一致和确定的结果。 假设所有数据都已到达， Event Time 操作将按照预期运行，即使在处理无序事件、延迟事件、重新处理历史数据时也会产生正确且一致的结果。 例如，每小时事件时间窗口将包含带有落入该小时的事件时间戳的所有记录，无论它们到达的顺序如何。 请注意，有时当 Event Time 程序实时处理实时数据时，它们将使用一些 Processing Time 操作，以确保它们及时进行。 Ingestion Time Ingestion Time 是事件进入 Flink 的时间。 在源操作处，每个事件将源的当前时间作为时间戳，并且基于时间的操作（如时间窗口）会利用这个时间戳。 Ingestion Time 在概念上位于 Event Time 和 Processing Time 之间。 与 Processing Time 相比，它稍微贵一些，但结果更可预测。因为 Ingestion Time 使用稳定的时间戳（在源处分配一次），所以对事件的不同窗口操作将引用相同的时间戳，而在 Processing Time 中，每个窗口操作符可以将事件分配给不同的窗口（基于机器系统时间和到达延迟）。 与 Event Time 相比，Ingestion Time 程序无法处理任何无序事件或延迟数据，但程序不必指定如何生成水印。 在 Flink 中，，Ingestion Time 与 Event Time 非常相似，但 Ingestion Time 具有自动分配时间戳和自动生成水印功能。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>Time</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink中的窗口]]></title>
    <url>%2F2019%2F07%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E4%B8%AD%E7%9A%84%E7%AA%97%E5%8F%A3%2F</url>
    <content type="text"><![CDATA[窗口窗口类型 flink支持两种划分窗口的方式（time和count） 如果根据时间划分窗口，那么它就是一个time-window 如果根据数据划分窗口，那么它就是一个count-window flink支持窗口的两个重要属性（size和interval） 如果size=interval,那么就会形成tumbling-window(无重叠数据) 如果size&gt;interval,那么就会形成sliding-window(有重叠数据) 如果size&lt;interval,那么这种窗口将会丢失数据。比如每5秒钟，统计过去3秒的通过路口汽车的数据，将会漏掉2秒钟的数据。 通过组合可以得出四种基本窗口： time-tumbling-window 无重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5)) time-sliding-window 有重叠数据的时间窗口，设置方式举例：timeWindow(Time.seconds(5), Time.seconds(3)) count-tumbling-window无重叠数据的数量窗口，设置方式举例：countWindow(5) count-sliding-window 有重叠数据的数量窗口，设置方式举例：countWindow(5,3) flink支持在stream上的通过key去区分多个窗口 窗口的实现方式上一张经典图： Tumbling Time Window 假如我们需要统计每一分钟中用户购买的商品的总数，需要将用户的行为事件按每一分钟进行切分，这种切分被成为翻滚时间窗口（Tumbling Time Window）。翻滚窗口能将数据流切分成不重叠的窗口，每一个事件只能属于一个窗口。 123456789// 用户id和购买数量 streamval counts: DataStream[(Int, Int)] = ...val tumblingCnts: DataStream[(Int, Int)] = counts // 用userId分组 .keyBy(0) // 1分钟的翻滚窗口宽度 .timeWindow(Time.minutes(1)) // 计算购买数量 .sum(1) Sliding Time Window 我们可以每30秒计算一次最近一分钟用户购买的商品总数。这种窗口我们称为滑动时间窗口（Sliding Time Window）。在滑窗中，一个元素可以对应多个窗口。通过使用 DataStream API，我们可以这样实现： 1234val slidingCnts: DataStream[(Int, Int)] = buyCnts .keyBy(0) .timeWindow(Time.minutes(1), Time.seconds(30)) .sum(1) Tumbling Count Window 当我们想要每100个用户购买行为事件统计购买总数，那么每当窗口中填满100个元素了，就会对窗口进行计算，这种窗口我们称之为翻滚计数窗口（Tumbling Count Window），上图所示窗口大小为3个。通过使用 DataStream API，我们可以这样实现： 12345678910// Stream of (userId, buyCnts)val buyCnts: DataStream[(Int, Int)] = ...val tumblingCnts: DataStream[(Int, Int)] = buyCnts // key stream by sensorId .keyBy(0) // tumbling count window of 100 elements size .countWindow(100) // compute the buyCnt sum .sum(1) Session Window 在这种用户交互事件流中，我们首先想到的是将事件聚合到会话窗口中（一段用户持续活跃的周期），由非活跃的间隙分隔开。如上图所示，就是需要计算每个用户在活跃期间总共购买的商品数量，如果用户30秒没有活动则视为会话断开（假设raw data stream是单个用户的购买行为流）。Session Window 的示例代码如下： 12345678// Stream of (userId, buyCnts)val buyCnts: DataStream[(Int, Int)] = ... val sessionCnts: DataStream[(Int, Int)] = vehicleCnts .keyBy(0) // session window based on a 30 seconds session gap interval .window(ProcessingTimeSessionWindows.withGap(Time.seconds(30))) .sum(1) 一般而言，window 是在无限的流上定义了一个有限的元素集合。这个集合可以是基于时间的，元素个数的，时间和个数结合的，会话间隙的，或者是自定义的。Flink 的 DataStream API 提供了简洁的算子来满足常用的窗口操作，同时提供了通用的窗口机制来允许用户自己定义窗口分配逻辑。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>窗口</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink分布式缓存]]></title>
    <url>%2F2019%2F07%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98%2F</url>
    <content type="text"><![CDATA[分布式缓存Flink提供了一个分布式缓存，类似于hadoop，可以使用户在并行函数中很方便的读取本地文件，并把它放在taskmanager节点中，防止task重复拉取。 此缓存的工作机制如下：程序注册一个文件或者目录(本地或者远程文件系统，例如hdfs或者s3)，通过ExecutionEnvironment注册缓存文件并为它起一个名称。当程序执行，Flink自动将文件或者目录复制到所有taskmanager节点的本地文件系统，仅会执行一次。用户可以通过这个指定的名称查找文件或者目录，然后从taskmanager节点的本地文件系统访问它。 示例在ExecutionEnvironment中注册一个文件： 123456//获取运行环境ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();//1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试env.registerCachedFile(&quot;/Users/wangzhiwu/WorkSpace/quickstart/text&quot;,&quot;a.txt&quot;); 在用户函数中访问缓存文件或者目录(这里是一个map函数)。这个函数必须继承RichFunction,因为它需要使用RuntimeContext读取数据: 1234567891011121314151617181920212223242526DataSet&lt;String&gt; result = data.map(new RichMapFunction&lt;String, String&gt;() &#123; private ArrayList&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); //2：使用文件 File myFile = getRuntimeContext().getDistributedCache().getFile(&quot;a.txt&quot;); List&lt;String&gt; lines = FileUtils.readLines(myFile); for (String line : lines) &#123; this.dataList.add(line); System.err.println(&quot;分布式缓存为:&quot; + line); &#125; &#125; @Override public String map(String value) throws Exception &#123; //在这里就可以使用dataList System.err.println(&quot;使用datalist：&quot; + dataList + &quot;------------&quot; +value); //业务逻辑 return dataList +&quot;：&quot; + value; &#125; &#125;); result.printToErr(); &#125; 完整代码如下,仔细看注释： 12345678910111213141516171819202122232425262728293031323334353637383940public class DisCacheTest &#123; public static void main(String[] args) throws Exception&#123; //获取运行环境 ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); //1：注册一个文件,可以使用hdfs上的文件 也可以是本地文件进行测试 //text 中有4个单词:hello flink hello FLINK env.registerCachedFile(&quot;/Users/wangzhiwu/WorkSpace/quickstart/text&quot;,&quot;a.txt&quot;); DataSource&lt;String&gt; data = env.fromElements(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;); DataSet&lt;String&gt; result = data.map(new RichMapFunction&lt;String, String&gt;() &#123; private ArrayList&lt;String&gt; dataList = new ArrayList&lt;String&gt;(); @Override public void open(Configuration parameters) throws Exception &#123; super.open(parameters); //2：使用文件 File myFile = getRuntimeContext().getDistributedCache().getFile(&quot;a.txt&quot;); List&lt;String&gt; lines = FileUtils.readLines(myFile); for (String line : lines) &#123; this.dataList.add(line); System.err.println(&quot;分布式缓存为:&quot; + line); &#125; &#125; @Override public String map(String value) throws Exception &#123; //在这里就可以使用dataList System.err.println(&quot;使用datalist：&quot; + dataList + &quot;------------&quot; +value); //业务逻辑 return dataList +&quot;：&quot; + value; &#125; &#125;); result.printToErr(); &#125;&#125;// 输出结果如下：1234[hello, flink, hello, FLINK]：a[hello, flink, hello, FLINK]：b[hello, flink, hello, FLINK]：c[hello, flink, hello, FLINK]：d]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>分布式缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink重启策略]]></title>
    <url>%2F2019%2F07%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[概述 Flink支持不同的重启策略，以在故障发生时控制作业如何重启 集群在启动时会伴随一个默认的重启策略，在没有定义具体重启策略时会使用该默认策略。 如果在工作提交时指定了一个重启策略，该策略会覆盖集群的默认策略默认的重启策略可以通过 Flink 的配置文件 flink-conf.yaml 指定。配置参数 restart-strategy 定义了哪个策略被使用。 常用的重启： 1.策略固定间隔 (Fixed delay) 2.失败率 (Failure rate) 3.无重启 (No restart) 如果没有启用 checkpointing，则使用无重启 (no restart) 策略。如果启用了 checkpointing，但没有配置重启策略，则使用固定间隔 (fixed-delay) 策略 重启策略可以在flink-conf.yaml中配置，表示全局的配置。也可以在应用代码中动态指定，会覆盖全局配置 固定间隔第一种：全局配置 flink-conf.yaml123restart-strategy: fixed-delay restart-strategy.fixed-delay.attempts: 3 restart-strategy.fixed-delay.delay: 10 s 第二种：应用代码设置： 12env.setRestartStrategy(RestartStrategies.fixedDelayRestart( 3,// 尝试重启的次数 Time.of(10, TimeUnit.SECONDS) // 间隔 )); 失败率 失败率重启策略在Job失败后会重启，但是超过失败率后，Job会最终被认定失败。在两个连续的重启尝试之间，重启策略会等待一个固定的时间 下面配置是5分钟内若失败了3次则认为该job失败，重试间隔为10s 第一种：全局配置 flink-conf.yaml1234 restart-strategy: failure-rate restart-strategy.failure-rate.max-failures-per-interval: 3 restart-strategy.failure-rate.failure-rate-interval: 5 min restart-strategy.failure-rate.delay: 10 s 第二种：应用代码设置 12 env.setRestartStrategy(RestartStrategies.failureRateRestart( 3,//一个时间段内的最大失败次数 Time.of(5, TimeUnit.MINUTES), // 衡量失败次数的是时间段 Time.of(10, TimeUnit.SECONDS) // 间隔 )); 无重启策略第一种：全局配置 flink-conf.yaml 1restart-strategy: none 第二种：应用代码设置1ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); env.setRestartStrategy(RestartStrategies.noRestart()); 实际代码演示1234567891011121314151617181920public class RestartTest &#123; public static void main(String[] args) &#123; //获取flink的运行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // 每隔1000 ms进行启动一个检查点【设置checkpoint的周期】 env.enableCheckpointing(1000); // 间隔10秒 重启3次 env.setRestartStrategy(RestartStrategies.fixedDelayRestart(3,Time.seconds(10))); //5分钟内若失败了3次则认为该job失败，重试间隔为10s env.setRestartStrategy(RestartStrategies.failureRateRestart(3,Time.of(5,TimeUnit.MINUTES),Time.of(10,TimeUnit.SECONDS))); //不重试 env.setRestartStrategy(RestartStrategies.noRestart()); &#125;//&#125;]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>重启策略</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink集群部署]]></title>
    <url>%2F2019%2F07%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[部署方式一般来讲有三种方式： Local Standalone Flink On Yarn/Mesos/K8s… 单机模式参考上一篇Flink从入门到放弃(入门篇2)-本地环境搭建&amp;构建第一个Flink应用 Standalone模式部署我们基于CentOS7虚拟机搭建一个3个节点的集群： 角色分配：123Master: 192.168.246.134Slave: 192.168.246.135Slave: 192.168.246.136 123192.168.246.134 jobmanager192.168.246.135 taskmanager192.168.246.136 taskmanager 假设三台机器都存在：用户root 密码为123 123192.168.246.134 master192.168.246.135 slave1192.168.246.136 slave2 三台机器首先要做ssh免登，具体方法很简单，可以百度。 下载一个包到本地：这里我选择了1.7.2版本+Hadoop2.8+Scala2.11版本然后，分发123456789101112scp flink-1.7.2-bin-hadoop28-scala_2.11.tgz root@192.168.246.13X:~scp jdk-8u11-linux-x64.tar.gz root@192.168.246.13X:~注意：X代表4、5、6，分发到3台机器修改解压后目录属主：Chown -R root:root flink/Chown -R root:root jdk8/export JAVA_HOME=/root/jdk8export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 分别修改master和slave的flink-conf.yaml文件123456789101112131415161718Vim flink/conf/flink-conf.yaml##配置master节点ipjobmanager.rpc.address: 192.168.1.100##配置slave节点可用内存，单位MBtaskmanager.heap.mb: 25600##配置每个节点的可用slot，1 核CPU对应 1 slot##the number of available CPUs per machine taskmanager.numberOfTaskSlots: 30##默认并行度 1 slot资源parallelism.default: 1修改slave节点配置文件slaves：192.168.246.135192.168.246.136 启动集群：12345##在master节点上执行此脚本，就可以启动集群，前提要保证master节点到slaver节点可以免密登录，##因为它的启动过程是：先在master节点启动jobmanager进程，然后ssh到各slaver节点启动taskmanager进程./bin/start-cluster.sh停止集群：./bin/stop-cluster.sh Flink on yarn集群部署Yarn的简介： ResourceManagerResourceManager 负责整个集群的资源管理和分配，是一个全局的资源管理系统。 NodeManager 以心跳的方式向 ResourceManager 汇报资源使用情况（目前主要是 CPU 和内存的使用情况）。RM 只接受 NM 的资源回报信息，对于具体的资源处理则交给 NM 自己处理。 NodeManagerNodeManager 是每个节点上的资源和任务管理器，它是管理这台机器的代理，负责该节点程序的运行，以及该节点资源的管理和监控。YARN 集群每个节点都运行一个NodeManager。NodeManager 定时向 ResourceManager 汇报本节点资源（CPU、内存）的使用情况和Container 的运行状态。当 ResourceManager 宕机时 NodeManager 自动连接 RM 备用节点。NodeManager 接收并处理来自 ApplicationMaster 的 Container 启动、停止等各种请求。 ApplicationMaster负责与 RM 调度器协商以获取资源（用 Container 表示）。将得到的任务进一步分配给内部的任务(资源的二次分配)。与 NM 通信以启动/停止任务。监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务 Flink on yarn 集群启动步骤 步骤1 用户向YARN中提交应用程序，其中包括ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。 步骤2 ResourceManager为该应用程序分配第一个Container，并与对应的Node-Manager通信，要求它在这个Container中启动应用程序的ApplicationMaster。 步骤3 ApplicationMaster首先向ResourceManager注册，这样用户可以直接通过ResourceManager查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。 步骤4 ApplicationMaster采用轮询的方式通过RPC协议向ResourceManager申请和领取资源。 步骤5 一旦ApplicationMaster申请到资源后，便与对应的NodeManager通信，要求它启动任务。 步骤6 NodeManager为任务设置好运行环境（包括环境变量、JAR包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。 步骤7 各个任务通过某个RPC协议向ApplicationMaster汇报自己的状态和进度，以让ApplicationMaster随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。 在应用程序运行过程中，用户可随时通过RPC向ApplicationMaster查询应用程序的当前运行状态。 步骤8 应用程序运行完成后，ApplicationMaster向ResourceManager注销并关闭自己 on yarn 集群部署设置Hadoop环境变量：12[root@hadoop2 flink-1.7.2]# vi /etc/profileexport HADOOP_CONF_DIR=这里是你自己的hadoop路径 bin/yarn-session.sh -h 查看使用方法: 在启动的是可以指定TaskManager的个数以及内存(默认是1G)，也可以指定JobManager的内存，但是JobManager的个数只能是一个 我们开启动一个YARN session：1./bin/yarn-session.sh -n 4 -tm 8192 -s 8 上面命令启动了4个TaskManager，每个TaskManager内存为8G且占用了8个核(是每个TaskManager，默认是1个核)。在启动YARN session的时候会加载conf/flink-config.yaml配置文件，我们可以根据自己的需求去修改里面的相关参数. YARN session启动之后就可以使用bin/flink来启动提交作业: 例如：1./bin/flink run -c com.demo.wangzhiwu.WordCount $DEMO_DIR/target/flink-demo-1.0.SNAPSHOT.jar --port 9000 flink run的用法如下：1234567用法: run [OPTIONS] &lt;jar-file&gt; &lt;arguments&gt; &quot;run&quot; 操作参数: -c,--class &lt;classname&gt; 如果没有在jar包中指定入口类，则需要在这里通过这个参数指定 -m,--jobmanager &lt;host:port&gt; 指定需要连接的jobmanager(主节点)地址 使用这个参数可以指定一个不同于配置文件中的jobmanager -p,--parallelism &lt;parallelism&gt; 指定程序的并行度。可以覆盖配置文件中的默认值。 使用run 命令向yarn集群提交一个job。客户端可以确定jobmanager的地址。当然，你也可以通过-m参数指定jobmanager。jobmanager的地址在yarn控制台上可以看到。 值得注意的是： 上面的YARN session是在Hadoop YARN环境下启动一个Flink cluster集群，里面的资源是可以共享给其他的Flink作业。我们还可以在YARN上启动一个Flink作业。这里我们还是使用./bin/flink，但是不需要事先启动YARN session：123./bin/flink run -m yarn-cluster -yn 2 ./examples/batch/WordCount.jar \ --input hdfs://user/hadoop/input.txt \ --output hdfs://user/hadoop/output.txt 上面的命令同样会启动一个类似于YARN session启动的页面。其中的-yn是指TaskManager的个数，必须要指定。 后台运行 yarn session如果你不希望flink yarn client一直运行，也可以启动一个后台运行的yarn session。使用这个参数：-d 或者 –detached在这种情况下，flink yarn client将会只提交任务到集群然后关闭自己。注意：在这种情况下，无法使用flink停止yarn session。必须使用yarn工具来停止yarn session 1yarn application -kill &lt;applicationId&gt; flink on yarn的故障恢复flink 的 yarn 客户端通过下面的配置参数来控制容器的故障恢复。这些参数可以通过conf/flink-conf.yaml 或者在启动yarn session的时候通过-D参数来指定。 yarn.reallocate-failed：这个参数控制了flink是否应该重新分配失败的taskmanager容器。默认是true。 yarn.maximum-failed-containers：applicationMaster可以接受的容器最大失败次数，达到这个参数，就会认为yarn session失败。默认这个次数和初始化请求的taskmanager数量相等(-n 参数指定的)。 yarn.application-attempts：applicationMaster重试的次数。如果这个值被设置为1(默认就是1)，当application master失败的时候，yarn session也会失败。设置一个比较大的值的话，yarn会尝试重启applicationMaster。 日志文件查看在某种情况下，flink yarn session 部署失败是由于它自身的原因，用户必须依赖于yarn的日志来进行分析。最有用的就是yarn log aggregation 。启动它，用户必须在yarn-site.xml文件中设置yarn.log-aggregation-enable 属性为true。一旦启用了，用户可以通过下面的命令来查看一个失败的yarn session的所有详细日志。 1yarn logs -applicationId &lt;application ID&gt; 完。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>集群部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink实战(DataStreamAPI)]]></title>
    <url>%2F2019%2F07%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E5%AE%9E%E6%88%98(DataStreamAPI)%2F</url>
    <content type="text"><![CDATA[DataStream算子将一个或多个DataStream转换为新DataStream。程序可以将多个转换组合成复杂的数据流拓扑。DataStreamAPI和DataSetAPI主要的区别在于Transformation部分。 DataStream Transformationmap DataStream→DataStream用一个数据元生成一个数据元。一个map函数，它将输入流的值加倍：1234567DataStream&lt;Integer&gt; dataStream = //...dataStream.map(new MapFunction&lt;Integer, Integer&gt;() &#123; @Override public Integer map(Integer value) throws Exception &#123; return 2 * value; &#125;&#125;); FlatMap DataStream→DataStream 采用一个数据元并生成零个，一个或多个数据元。将句子分割为单词的flatmap函数： 123456789dataStream.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; @Override public void flatMap(String value, Collector&lt;String&gt; out) throws Exception &#123; for(String word: value.split(&quot; &quot;))&#123; out.collect(word); &#125; &#125;&#125;); Filter DataStream→DataStream计算每个数据元的布尔函数，并保存函数返回true的数据元。过滤掉零值的过滤器： 123456dataStream.filter(new FilterFunction&lt;Integer&gt;() &#123; @Override public boolean filter(Integer value) throws Exception &#123; return value != 0; &#125;&#125;); KeyBy DataStream→KeyedStream 逻辑上将流分区为不相交的分区。具有相同Keys的所有记录都分配给同一分区。在内部，keyBy（）是使用散列分区实现的。指定键有不同的方法。 此转换返回KeyedStream，其中包括使用被Keys化状态所需的KeyedStream。 12dataStream.keyBy(&quot;someKey&quot;) // Key by field &quot;someKey&quot;dataStream.keyBy(0) // Key by the first element of a Tuple 🌺注意： 如果出现以下情况，则类型不能成为key： 它是POJO类型但不覆盖hashCode（）方法并依赖于Object.hashCode（）实现 任何类型的数组 ReduceKeyedStream→DataStream 将当前数据元与最后一个Reduce的值组合并发出新值。例如：reduce函数，用于创建部分和的流： 1234567keyedStream.reduce(new ReduceFunction&lt;Integer&gt;() &#123; @Override public Integer reduce(Integer value1, Integer value2) throws Exception &#123; return value1 + value2; &#125;&#125;); FoldKeyedStream→DataStream 具有初始值的被Keys化数据流上的“滚动”折叠。将当前数据元与最后折叠的值组合并发出新值。 折叠函数，当应用于序列（1,2,3,4,5）时，发出序列“start-1”，“start-1-2”，“start-1-2-3”,. .. 1234567DataStream&lt;String&gt; result = keyedStream.fold(&quot;start&quot;, new FoldFunction&lt;Integer, String&gt;() &#123; @Override public String fold(String current, Integer value) &#123; return current + &quot;-&quot; + value; &#125; &#125;); 聚合 KeyedStream→DataStream 在被Keys化数据流上滚动聚合。min和minBy之间的差异是min返回最小值，而minBy返回该字段中具有最小值的数据元(max和maxBy相同)。 12345678910keyedStream.sum(0);keyedStream.sum(&quot;key&quot;);keyedStream.min(0);keyedStream.min(&quot;key&quot;);keyedStream.max(0);keyedStream.max(&quot;key&quot;);keyedStream.minBy(0);keyedStream.minBy(&quot;key&quot;);keyedStream.maxBy(0);keyedStream.maxBy(&quot;key&quot;); Window函数关于Flink的窗口概念，我们会在后面有详细介绍。 WindowKeyedStream→WindowedStream 可以在已经分区的KeyedStream上定义Windows。Windows根据某些特征（例如，在最后5秒内到达的数据）对每个Keys中的数据进行分组。 123dataStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(5))); // Last 5 seconds of data Window ApplyWindowedStream→DataStreamAllWindowedStream→DataStream 将一般函数应用于整个窗口。下面是一个手动求和窗口数据元的函数。 注意：如果您正在使用windowAll转换，则需要使用AllWindowFunction。 12345678910111213141516171819202122232425windowedStream.apply (new WindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Tuple, Window&gt;() &#123; public void apply (Tuple tuple, Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;);// applying an AllWindowFunction on non-keyed window streamallWindowedStream.apply (new AllWindowFunction&lt;Tuple2&lt;String,Integer&gt;, Integer, Window&gt;() &#123; public void apply (Window window, Iterable&lt;Tuple2&lt;String, Integer&gt;&gt; values, Collector&lt;Integer&gt; out) throws Exception &#123; int sum = 0; for (value t: values) &#123; sum += t.f1; &#125; out.collect (new Integer(sum)); &#125;&#125;); Window ReduceWindowedStream→DataStream 将reduce函数应用于窗口并返回reduce后的值。 12345windowedStream.reduce (new ReduceFunction&lt;Tuple2&lt;String,Integer&gt;&gt;() &#123; public Tuple2&lt;String, Integer&gt; reduce(Tuple2&lt;String, Integer&gt; value1, Tuple2&lt;String, Integer&gt; value2) throws Exception &#123; return new Tuple2&lt;String,Integer&gt;(value1.f0, value1.f1 + value2.f1); &#125;&#125;); 提取时间戳 关于Time我们在后面有专门的章节进行介绍 DataStream→DataStream 从记录中提取时间戳，以便使用使用事件时间语义的窗口。 1stream.assignTimestamps (new TimeStampExtractor() &#123;...&#125;); Partition 分区 自定义分区DataStream→DataStream使用用户定义的分区程序为每个数据元选择目标任务。 12dataStream.partitionCustom(partitioner, &quot;someKey&quot;);dataStream.partitionCustom(partitioner, 0); 随机分区DataStream→DataStream根据均匀分布随机分配数据元。12345dataStream.shuffle();``` * Rebalance （循环分区）DataStream→DataStream 分区数据元循环，每个分区创建相等的负载。在存在数据倾斜时用于性能优化。 dataStream.rebalance();1234567891011* rescaleDataStream→DataStream如果上游 算子操作具有并行性2并且下游算子操作具有并行性6，则一个上游 算子操作将分配元件到三个下游算子操作，而另一个上游算子操作将分配到其他三个下游 算子操作。另一方面，如果下游算子操作具有并行性2而上游 算子操作具有并行性6，则三个上游 算子操作将分配到一个下游算子操作，而其他三个上游算子操作将分配到另一个下游算子操作。在不同并行度不是彼此的倍数的情况下，一个或多个下游 算子操作将具有来自上游 算子操作的不同数量的输入。请参阅此图以获取上例中连接模式的可视化：![5bd63a6c99ad06ba3d96d03be3cb25ff.svg+xml](evernotecid://DF961740-2AB0-48AB-AAE7-53BB9D286C7A/appyinxiangcom/12131181/ENResource/p1410) dataStream.rescale();1234* 广播DataStream→DataStream 向每个分区广播数据元。 dataStream.broadcast();`]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>DataStreamAPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink实战(DataSetAPI)]]></title>
    <url>%2F2019%2F07%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E5%AE%9E%E6%88%98(DataSetAPI)%2F</url>
    <content type="text"><![CDATA[编程结构1234567891011121314151617181920public class SocketTextStreamWordCount &#123; public static void main(String[] args) throws Exception &#123; if (args.length != 2)&#123;System.err.println(&quot;USAGE:\nSocketTextStreamWordCount &lt;hostname&gt; &lt;port&gt;&quot;); return; &#125; String hostName = args[0]; Integer port = Integer.parseInt(args[1]); final StreamExecutionEnvironment env = StreamExecutionEnvironment .getExecutionEnvironment(); DataStream&lt;String&gt; text = env.socketTextStream(hostName, port); DataStream&lt;Tuple2&lt;String, Integer&gt;&gt; counts text.flatMap(new LineSplitter()) .keyBy(0) .sum(1); counts.print(); env.execute(&quot;Java WordCount from SocketTextStream Example&quot;); &#125; 上面的SocketTextStreamWordCount是一个典型的Flink程序，他由一下及格部分构成： 获得一个execution environment， 加载/创建初始数据， 指定此数据的转换， 指定放置计算结果的位置， 触发程序执行 DataSet API分类： Source: 数据源创建初始数据集，例如来自文件或Java集合 Transformation: 数据转换将一个或多个DataSet转换为新的DataSet Sink: 将计算结果存储或返回 DataSet Sources基于文件的 readTextFile(path)/ TextInputFormat- 按行读取文件并将其作为字符串返回。 readTextFileWithValue(path)/ TextValueInputFormat- 按行读取文件并将它们作为StringValues返回。StringValues是可变字符串。 readCsvFile(path)/ CsvInputFormat- 解析逗号（或其他字符）分隔字段的文件。返回元组或POJO的DataSet。支持基本java类型及其Value对应作为字段类型。 readFileOfPrimitives(path, Class)/ PrimitiveInputFormat- 解析新行（或其他字符序列）分隔的原始数据类型（如String或）的文件Integer。 readFileOfPrimitives(path, delimiter, Class)/ PrimitiveInputFormat- 解析新行（或其他字符序列）分隔的原始数据类型的文件，例如String或Integer使用给定的分隔符。 readSequenceFile(Key, Value, path)/ SequenceFileInputFormat- 创建一个JobConf并从类型为SequenceFileInputFormat，Key class和Value类的指定路径中读取文件，并将它们作为Tuple2 &lt;Key，Value&gt;返回。 基于集合 fromCollection(Collection) - 从Java Java.util.Collection创建数据集。集合中的所有数据元必须属于同一类型。 fromCollection(Iterator, Class) - 从迭代器创建数据集。该类指定迭代器返回的数据元的数据类型。 fromElements(T ...) - 根据给定的对象序列创建数据集。所有对象必须属于同一类型。 fromParallelCollection(SplittableIterator, Class)- 并行地从迭代器创建数据集。该类指定迭代器返回的数据元的数据类型。 generateSequence(from, to) - 并行生成给定间隔中的数字序列。 通用方法 readFile(inputFormat, path)/ FileInputFormat- 接受文件输入格式。 createInput(inputFormat)/ InputFormat- 接受通用输入格式。 代码示例123456789101112131415161718192021222324252627282930313233ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();// 从本地文件系统读DataSet&lt;String&gt; localLines = env.readTextFile(&quot;file:///path/to/my/textfile&quot;);// 读取HDFS文件DataSet&lt;String&gt; hdfsLines = env.readTextFile(&quot;hdfs://nnHost:nnPort/path/to/my/textfile&quot;);// 读取CSV文件DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; csvInput = env.readCsvFile(&quot;hdfs:///the/CSV/file&quot;).types(Integer.class, String.class, Double.class);// 读取CSV文件中的部分DataSet&lt;Tuple2&lt;String, Double&gt;&gt; csvInput = env.readCsvFile(&quot;hdfs:///the/CSV/file&quot;).includeFields(&quot;10010&quot;).types(String.class, Double.class);// 读取CSV映射为一个java类DataSet&lt;Person&gt;&gt; csvInput = env.readCsvFile(&quot;hdfs:///the/CSV/file&quot;).pojoType(Person.class, &quot;name&quot;, &quot;age&quot;, &quot;zipcode&quot;);// 读取一个指定位置序列化好的文件DataSet&lt;Tuple2&lt;IntWritable, Text&gt;&gt; tuples = env.readSequenceFile(IntWritable.class, Text.class, &quot;hdfs://nnHost:nnPort/path/to/file&quot;);// 从输入字符创建DataSet&lt;String&gt; value = env.fromElements(&quot;Foo&quot;, &quot;bar&quot;, &quot;foobar&quot;, &quot;fubar&quot;);// 创建一个数字序列DataSet&lt;Long&gt; numbers = env.generateSequence(1, 10000000);// 从关系型数据库读取DataSet&lt;Tuple2&lt;String, Integer&gt; dbData =env.createInput(JDBCInputFormat.buildJDBCInputFormat() .setDrivername(&quot;org.apache.derby.jdbc.EmbeddedDriver&quot;) .setDBUrl(&quot;jdbc:derby:memory:persons&quot;).setQuery(&quot;select name, age from persons&quot;).setRowTypeInfo(new RowTypeInfo(BasicTypeInfo.STRING_TYPE_INFO, BasicTypeInfo.INT_TYPE_INFO)).finish()); DataSet Transformation 详细可以参考官网:https://flink.sojb.cn/dev/batch/dataset_transformations.html#filter Map 采用一个数据元并生成一个数据元。123data.map(new MapFunction&lt;String, Integer&gt;() &#123; public Integer map(String value) &#123; return Integer.parseInt(value); &#125;&#125;); FlatMap 采用一个数据元并生成零个，一个或多个数据元。1234567data.flatMap(new FlatMapFunction&lt;String, String&gt;() &#123; public void flatMap(String value, Collector&lt;String&gt; out) &#123; for (String s : value.split(&quot; &quot;)) &#123; out.collect(s); &#125; &#125;&#125;); MapPartition 在单个函数调用中转换并行分区。该函数将分区作为Iterable流来获取，并且可以生成任意数量的结果值。每个分区中的数据元数量取决于并行度和先前的 算子操作。123456789data.mapPartition(new MapPartitionFunction&lt;String, Long&gt;() &#123; public void mapPartition(Iterable&lt;String&gt; values, Collector&lt;Long&gt; out) &#123; long c = 0; for (String s : values) &#123; c++; &#125; out.collect(c); &#125;&#125;); Filter 计算每个数据元的布尔函数，并保存函数返回true的数据元。重要信息：系统假定该函数不会修改应用谓词的数据元。违反此假设可能会导致错误的结果。123data.filter(new FilterFunction&lt;Integer&gt;() &#123; public boolean filter(Integer value) &#123; return value &gt; 1000; &#125;&#125;); Reduce 通过将两个数据元重复组合成一个数据元，将一组数据元组合成一个数据元。Reduce可以应用于完整数据集或分组数据集。123data.reduce(new ReduceFunction&lt;Integer&gt; &#123; public Integer reduce(Integer a, Integer b) &#123; return a + b; &#125;&#125;); 如果将reduce应用于分组数据集，则可以通过提供CombineHintto 来指定运行时执行reduce的组合阶段的方式 setCombineHint。在大多数情况下，基于散列的策略应该更快，特别是如果不同键的数量与输入数据元的数量相比较小（例如1/10）。 ReduceGroup 将一组数据元组合成一个或多个数据元。ReduceGroup可以应用于完整数据集或分组数据集。 123456789data.reduceGroup(new GroupReduceFunction&lt;Integer, Integer&gt; &#123; public void reduce(Iterable&lt;Integer&gt; values, Collector&lt;Integer&gt; out) &#123; int prefixSum = 0; for (Integer i : values) &#123; prefixSum += i; out.collect(prefixSum); &#125; &#125;&#125;); Aggregate 将一组值聚合为单个值。聚合函数可以被认为是内置的reduce函数。聚合可以应用于完整数据集或分组数据集。 12Dataset&lt;Tuple3&lt;Integer, String, Double&gt;&gt; input = // [...]DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; output = input.aggregate(SUM, 0).and(MIN, 2); 您还可以使用简写语法进行最小，最大和总和聚合。 12Dataset&lt;Tuple3&lt;Integer, String, Double&gt;&gt; input = // [...]DataSet&lt;Tuple3&lt;Integer, String, Double&gt;&gt; output = input.sum(0).andMin(2); Distinct 返回数据集的不同数据元。它相对于数据元的所有字段或字段子集从输入DataSet中删除重复条目。1data.distinct(); 使用reduce函数实现Distinct。您可以通过提供CombineHintto 来指定运行时执行reduce的组合阶段的方式 setCombineHint。在大多数情况下，基于散列的策略应该更快，特别是如果不同键的数量与输入数据元的数量相比较小（例如1/10）。 Join 通过创建在其键上相等的所有数据元对来连接两个数据集。可选地使用JoinFunction将数据元对转换为单个数据元，或使用FlatJoinFunction将数据元对转换为任意多个（包括无）数据元。请参阅键部分以了解如何定义连接键。 123result = input1.join(input2) .where(0) // key of the first input (tuple field 0) .equalTo(1); // key of the second input (tuple field 1) 您可以通过Join Hints指定运行时执行连接的方式。提示描述了通过分区或广播进行连接，以及它是使用基于排序还是基于散列的算法。如果未指定提示，系统将尝试估算输入大小，并根据这些估计选择最佳策略。 1234// This executes a join by broadcasting the first data set// using a hash table for the broadcast dataresult = input1.join(input2, JoinHint.BROADCAST_HASH_FIRST) .where(0).equalTo(1); 请注意，连接转换仅适用于等连接。其他连接类型需要使用OuterJoin或CoGroup表示。 OuterJoin 在两个数据集上执行左，右或全外连接。外连接类似于常规（内部）连接，并创建在其键上相等的所有数据元对。此外，如果在另一侧没有找到匹配的Keys，则保存“外部”侧（左侧，右侧或两者都满）的记录。匹配数据元对（或一个数据元和null另一个输入的值）被赋予JoinFunction以将数据元对转换为单个数据元，或者转换为FlatJoinFunction以将数据元对转换为任意多个（包括无）数据元。请参阅键部分以了解如何定义连接键。 1234567891011input1.leftOuterJoin(input2) // rightOuterJoin or fullOuterJoin for right or full outer joins .where(0) // key of the first input (tuple field 0) .equalTo(1) // key of the second input (tuple field 1) .with(new JoinFunction&lt;String, String, String&gt;() &#123; public String join(String v1, String v2) &#123; // NOTE: // - v2 might be null for leftOuterJoin // - v1 might be null for rightOuterJoin // - v1 OR v2 might be null for fullOuterJoin &#125; &#125;); CoGroup reduce 算子操作的二维变体。将一个或多个字段上的每个输入分组，然后关联组。每对组调用转换函数。 12345678data1.coGroup(data2) .where(0) .equalTo(1) .with(new CoGroupFunction&lt;String, String, String&gt;() &#123; public void coGroup(Iterable&lt;String&gt; in1, Iterable&lt;String&gt; in2, Collector&lt;String&gt; out) &#123; out.collect(...); &#125; &#125;); Cross 构建两个输入的笛卡尔积（交叉乘积），创建所有数据元对。可选择使用CrossFunction将数据元对转换为单个数据元 123DataSet&lt;Integer&gt; data1 = // [...]DataSet&lt;String&gt; data2 = // [...]DataSet&lt;Tuple2&lt;Integer, String&gt;&gt; result = data1.cross(data2); 注：交叉是一个潜在的非常计算密集型 算子操作它甚至可以挑战大的计算集群！建议使用crossWithTiny（）和crossWithHuge（）来提示系统的DataSet大小。 Union 生成两个数据集的并集。 123DataSet&lt;String&gt; data1 = // [...]DataSet&lt;String&gt; data2 = // [...]DataSet&lt;String&gt; result = data1.union(data2); Rebalance 均匀地Rebalance 数据集的并行分区以消除数据偏差。只有类似Map的转换可能会遵循Rebalance 转换。 123DataSet&lt;String&gt; in = // [...]DataSet&lt;String&gt; result = in.rebalance() .map(new Mapper()); Hash-Partition 散列分区给定键上的数据集。键可以指定为位置键，表达键和键选择器函数。 123DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]DataSet&lt;Integer&gt; result = in.partitionByHash(0) .mapPartition(new PartitionMapper()); Range-Partition Range-Partition给定键上的数据集。键可以指定为位置键，表达键和键选择器函数。 123DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]DataSet&lt;Integer&gt; result = in.partitionByRange(0) .mapPartition(new PartitionMapper()); Custom Partitioning 手动指定数据分区。注意：此方法仅适用于单个字段键。 12DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]DataSet&lt;Integer&gt; result = in.partitionCustom(Partitioner&lt;K&gt; partitioner, key) Sort Partition 本地按指定顺序对指定字段上的数据集的所有分区进行排序。可以将字段指定为元组位置或字段表达式。通过链接sortPartition（）调用来完成对多个字段的排序。 123DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]DataSet&lt;Integer&gt; result = in.sortPartition(1, Order.ASCENDING) .mapPartition(new PartitionMapper()); First-n 返回数据集的前n个（任意）数据元。First-n可以应用于常规数据集，分组数据集或分组排序数据集。分组键可以指定为键选择器函数或字段位置键。 1234567DataSet&lt;Tuple2&lt;String,Integer&gt;&gt; in = // [...]// regular data setDataSet&lt;Tuple2&lt;String,Integer&gt;&gt; result1 = in.first(3);// grouped data setDataSet&lt;Tuple2&lt;String,Integer&gt;&gt; result2 = in.groupBy(0) .first(3);// grouped-sorted data setDataSet&lt;Tuple2&lt;String,Integer&gt;&gt; result3 = in.groupBy(0) .sortGroup(1, Order.ASCENDING) .first(3); DataSet Sink数据接收器使用DataSet用于存储或返回。使用OutputFormat描述数据接收器算子操作 。Flink带有各种内置输出格式，这些格式封装在DataSet上的算子操作中： writeAsText()/ TextOutputFormat- 按字符串顺序写入数据元。通过调用每个数据元的toString（）方法获得字符串。 writeAsFormattedText()/ TextOutputFormat- 按字符串顺序写数据元。通过为每个数据元调用用户定义的format（）方法来获取字符串。 writeAsCsv(…)/ CsvOutputFormat- 将元组写为逗号分隔值文件。行和字段分隔符是可配置的。每个字段的值来自对象的toString（）方法。 print()/ printToErr()/ print(String msg)/ printToErr(String msg)- 在标准输出/标准错误流上打印每个数据元的toString（）值。可选地，可以提供前缀（msg），其前缀为输出。这有助于区分不同的打印调用。如果并行度大于1，则输出也将与生成输出的任务的标识符一起添加。 write()/ FileOutputFormat- 自定义文件输出的方法和基类。支持自定义对象到字节的转换。 output()/ OutputFormat- 大多数通用输出方法，用于非基于文件的数据接收器（例如将结果存储在数据库中）。 可以将DataSet输入到多个 算子操作。程序可以编写或打印数据集，同时对它们执行其他转换。 示例： 1234567891011121314151617181920212223242526// text dataDataSet&lt;String&gt; textData = // [...]// write DataSet to a file on the local file systemtextData.writeAsText(&quot;file:///my/result/on/localFS&quot;);// write DataSet to a file on a HDFS with a namenode running at nnHost:nnPorttextData.writeAsText(&quot;hdfs://nnHost:nnPort/my/result/on/localFS&quot;);// write DataSet to a file and overwrite the file if it existstextData.writeAsText(&quot;file:///my/result/on/localFS&quot;, WriteMode.OVERWRITE);// tuples as lines with pipe as the separator &quot;a|b|c&quot;DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; values = // [...]values.writeAsCsv(&quot;file:///path/to/the/result/file&quot;, &quot;\n&quot;, &quot;|&quot;);// this writes tuples in the text formatting &quot;(a, b, c)&quot;, rather than as CSV linesvalues.writeAsText(&quot;file:///path/to/the/result/file&quot;);// this writes values as strings using a user-defined TextFormatter objectvalues.writeAsFormattedText(&quot;file:///path/to/the/result/file&quot;, new TextFormatter&lt;Tuple2&lt;Integer, Integer&gt;&gt;() &#123; public String format (Tuple2&lt;Integer, Integer&gt; value) &#123; return value.f1 + &quot; - &quot; + value.f0; &#125; &#125;); 使用自定义输出格式： 1234567891011DataSet&lt;Tuple3&lt;String, Integer, Double&gt;&gt; myResult = [...]// write Tuple DataSet to a relational databasemyResult.output( // build and configure OutputFormat JDBCOutputFormat.buildJDBCOutputFormat() .setDrivername(&quot;org.apache.derby.jdbc.EmbeddedDriver&quot;) .setDBUrl(&quot;jdbc:derby:memory:persons&quot;) .setQuery(&quot;insert into persons (name, age, height) values (?,?,?)&quot;) .finish() ); 序列化器 Flink自带了针对诸如int，long，String等标准类型的序列化器 针对Flink无法实现序列化的数据类型，我们可以交给Avro和Kryo 使用方法：ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); 123使用avro序列化：env.getConfig().enableForceAvro();使用kryo序列化：env.getConfig().enableForceKryo();使用自定义序列化：env.getConfig().addDefaultKryoSerializer(Class&lt;?&gt; type, Class&lt;? extends Serializer&lt;?&gt;&gt; serializerClass) 数据类型 Java Tuple 和 Scala case class Java POJOs：java实体类 Primitive Types默认支持java和scala基本数据类型 General Class Types默认支持大多数java和scala class Hadoop Writables支持hadoop中实现了org.apache.hadoop.Writable的数据类型 Special Types例如scala中的Either Option 和Try]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>DataSetAPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink本地环境搭建&构建第一个Flink应用]]></title>
    <url>%2F2019%2F07%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E6%9C%AC%E5%9C%B0%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%26%E6%9E%84%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AAFlink%E5%BA%94%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本地安装单机版本Flink一般来说，线上都是集群模式，那么单机模式方便我们测试和学习。 环境要求本地机器上需要有 Java 8 和 maven 环境，推荐在linux或者mac上开发Flink应用： 如果有 Java 8 环境，运行下面的命令会输出如下版本信息： -本地环境搭建&amp;构建第一个Flink应用.resources/34F234C6-C9D6-46AB-A864-652BE177B4CA.png) 如果有 maven 环境，运行下面的命令会输出如下版本信息： -本地环境搭建&amp;构建第一个Flink应用.resources/1A1D2049-1042-43E1-BE0B-6D9FAA8224BE.png) 开发工具推荐使用 ItelliJ IDEA。 第一种方式来这里https://flink.apache.org/ 看这里：-本地环境搭建&amp;构建第一个Flink应用.resources/E0A8FC57-9184-4BE8-8D20-BDD91C3C44FD.png) 注意：1An Apache Hadoop installation is not required to use Apache Flink. For users that use Flink without any Hadoop components, we recommend the release without bundled Hadoop libraries. 这是啥意思？这个意思就是说Flink可以不依赖Hadoop环境，如果说单机玩的话，下载一个only版本就行了。 第二种方式(不推荐)123git clone https://github.com/apache/flink.git cd flinkmvn clean package -DskipTests 然后进入编译好的Flink中去执行 bin/start-cluster.sh 其他乱七八糟的安装办法比如 Mac用户可以用brew install apache-flink ,前提是安装过 brew这个mac下的工具. 启动Flink我们先到Flink的目录下来：如下：12$ flink-1.7.1 pwd/Users/wangzhiwu/Downloads/flink-1.7.1 -本地环境搭建&amp;构建第一个Flink应用.resources/BE68C066-BD15-4FAF-B649-82D9B26F255D.png) 执行命令： -本地环境搭建&amp;构建第一个Flink应用.resources/C88AEAF7-42B7-4AD1-A793-3E89EBE751E2.png) 接着就可以进入 web 页面(http://localhost:8081/) 查看 -本地环境搭建&amp;构建第一个Flink应用.resources/DAEECBBB-0FB7-4D4E-B338-B3181C23B6CB.png) 恭喜你，一个单机版的flink就跑起来了。 构建一个应用当然了，我们可以用maven，一顿new，new出来一个过程，这里我们将使用 Flink Maven Archetype 来创建我们的项目结构和一些初始的默认依赖。在你的工作目录下，运行如下命令来创建项目： 123456789mvn archetype:generate \ -DarchetypeGroupId=org.apache.flink \ -DarchetypeArtifactId=flink-quickstart-java \ -DarchetypeVersion=1.7.2 \ -DgroupId=flink-project \ -DartifactId=flink-project \ -Dversion=0.1 \ -Dpackage=myflink \ -DinteractiveMode=false 这样一个工程就构建好了。 还有一个更加牛逼的办法，看这里： 1curl https://flink.apache.org/q/quickstart.sh | bash 直接在命令行执行上面的命令，结果如下图： -本地环境搭建&amp;构建第一个Flink应用.resources/A78DC26C-BD00-44A9-9481-FE67B9BAE9CF.png) 同样可以构建一个Flink工程，而且自带一些demo。 原理是什么？点一下它看看就明白了。https://flink.apache.org/q/quickstart.sh 编写一个入门级的WordCountclass WordCount &#123;12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455 // // Program // public static void main(String[] args) throws Exception &#123; // set up the execution environment final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); // get input data DataSet&lt;String&gt; text = env.fromElements( &quot;To be, or not to be,--that is the question:--&quot;, &quot;Whether &apos;tis nobler in the mind to suffer&quot;, &quot;The slings and arrows of outrageous fortune&quot;, &quot;Or to take arms against a sea of troubles,&quot; ); DataSet&lt;Tuple2&lt;String, Integer&gt;&gt; counts = // split up the lines in pairs (2-tuples) containing: (word,1) text.flatMap(new LineSplitter()) // group by the tuple field &quot;0&quot; and sum up tuple field &quot;1&quot; .groupBy(0) //(i,1) (am,1) (chinese,1) .sum(1); // execute and print result counts.print(); &#125; // // User Functions // /** * Implements the string tokenizer that splits sentences into words as a user-defined * FlatMapFunction. The function takes a line (String) and splits it into * multiple pairs in the form of &quot;(word,1)&quot; (Tuple2&amp;lt;String, Integer&amp;gt;). */ public static final class LineSplitter implements FlatMapFunction&lt;String, Tuple2&lt;String, Integer&gt;&gt; &#123; @Override public void flatMap(String value, Collector&lt;Tuple2&lt;String, Integer&gt;&gt; out) &#123; // normalize and split the line String[] tokens = value.toLowerCase().split(&quot;\\W+&quot;); // emit the pairs for (String token : tokens) &#123; if (token.length() &gt; 0) &#123; out.collect(new Tuple2&lt;String, Integer&gt;(token, 1)); &#125; &#125; &#125; &#125;&#125; 类似的例子，官方也有提供的，可以在这里下载：WordCount官方推荐 运行本地右键运行：-本地环境搭建&amp;构建第一个Flink应用.resources/8F0D8EF2-5C0B-4067-AA87-31D7A0DC16C7.png) 提交到本地单机Flink上 进入工程目录，使用以下命令打包1mvn clean package -Dmaven.test.skip=true 然后，进入 flink 安装目录 bin 下执行以下命令提交程序：1flink run -c org.myorg.laowang.WordCount /Users/wangzhiwu/WorkSpace/quickstart/target/quickstart-0.1.jar 分别制定main方法和jar包的地址。 在刚才的控制台中，可以看到：-本地环境搭建&amp;构建第一个Flink应用.resources/EB619900-BBDE-4E32-9089-0DC867FF9220.png)我们刚才提交过的程序。 flink的log目录下有我们提交过的任务的日志：-本地环境搭建&amp;构建第一个Flink应用.resources/620369FB-ABCA-4184-AA90-C7FEDB114B07.png) 总结一次简单的flink之旅就完成了。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flink</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Flink简介]]></title>
    <url>%2F2019%2F07%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BFlink%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[大纲入门篇：-Flink是什么？.resources/1.png) 放弃篇：-Flink是什么？.resources/A44BE2B6-FBC9-4143-9743-F097B9C0FDD6.png) Flink是什么一句话概括Apache Flink是一个面向分布式数据流处理和批量数据处理的开源计算平台，提供支持流处理和批处理两种类型应用的功能。 前身Apache Flink 的前身是柏林理工大学一个研究性项目， 在 2014 被 Apache 孵化器所接受，然后迅速地成为了Apache Software Foundation的顶级项目之一。 特点现有的开源计算方案，会把流处理和批处理作为两种不同的应用类型：流处理一般需要支持低延迟、Exactly-once保证，而批处理需要支持高吞吐、高效处理。Flink是完全支持流处理，也就是说作为流处理看待时输入数据流是无界的；批处理被作为一种特殊的流处理，只是它的输入数据流被定义为有界的。 Flink组件栈-Flink是什么？.resources/6F963775-B91B-447F-959E-38B4029BE56D.png) Deployment层主要涉及了Flink的部署模式，Flink支持多种部署模式：本地、集群（Standalone/YARN）、云（GCE/EC2） -Flink是什么？.resources/F7406066-68CA-4BE7-9743-7FD65A0D722C.png) Runtime层Runtime层提供了支持Flink计算的全部核心实现，比如：支持分布式Stream处理、JobGraph到ExecutionGraph的映射、调度等等，为上层API层提供基础服务 API层API层主要实现了面向无界Stream的流处理和面向Batch的批处理API，其中面向流处理对应DataStream API，面向批处理对应DataSet API Libaries层 在API层之上构建的满足特定应用的实现计算框架，也分别对应于面向流处理和面向批处理两类 面向流处理支持：CEP（复杂事件处理）、基于SQL-like的操作（基于Table的关系操作） 面向批处理支持：FlinkML（机器学习库）、Gelly（图处理） Flink的优势 支持高吞吐、低延迟、高性能的流处理 支持高度灵活的窗口（Window）操作 支持有状态计算的Exactly-once语义 提供DataStream API和DataSet API -Flink是什么？.resources/3DE5BD22-BFE2-49C4-8DA8-C42EAD1948FB.png) -Flink是什么？.resources/0E6F6341-5EB0-40FD-9953-70C3F0904043.png) Flink基本编程模型 Flink程序的基础构建模块是流(streams) 与 转换(transformations) 每一个数据流起始于一个或多个 source，并终止于一个或多个 sink 下面是一个由Flink程序映射为Streaming Dataflow的示意图: -Flink是什么？.resources/656C0986-42A7-4E76-B3CA-C0372395E451.png) 并行数据流示意图:-Flink是什么？.resources/E6A4AF88-12D9-413A-A318-06A86ABDC1AF.png) Flink基本架构 Flink是基于Master-Slave风格的架构 Flink集群启动时，会启动一个JobManager进程、至少一个TaskManager进程 -Flink是什么？.resources/866EF50B-A9ED-461A-AC13-78BEBBDCCFC9.png) JobManager Flink系统的协调者，它负责接收Flink Job，调度组成Job的多个Task的执行 收集Job的状态信息，并管理Flink集群中从节点TaskManager TaskManager 实际负责执行计算的Worker，在其上执行Flink Job的一组Task TaskManager负责管理其所在节点上的资源信息，如内存、磁盘、网络，在启动的时候将资源的状态向JobManager汇报 Client 用户提交一个Flink程序时，会首先创建一个Client，该Client首先会对用户提交的Flink程序进行预处理，并提交到Flink集群 Client会将用户提交的Flink程序组装一个JobGraph， 并且是以JobGraph的形式提交的 最后本文是例行介绍，熟悉的直接跳过。]]></content>
      <categories>
        <category>Flink</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Flink</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark_Streaming整合Kafka]]></title>
    <url>%2F2019%2F06%2F30%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark_Streaming%E6%95%B4%E5%90%88Kafka%2F</url>
    <content type="text"><![CDATA[Spark Streaming 整合 Kafka一、版本说明二、项目依赖三、整合Kafka3.1 ConsumerRecord3.2 生产者属性3.3 位置策略3.4 订阅方式3.5 提交偏移量四、启动测试 一、版本说明Spark 针对 Kafka 的不同版本，提供了两套整合方案：spark-streaming-kafka-0-8 和 spark-streaming-kafka-0-10，其主要区别如下： spark-streaming-kafka-0-8 spark-streaming-kafka-0-10 Kafka 版本 0.8.2.1 or higher 0.10.0 or higher AP 状态 Deprecated从 Spark 2.3.0 版本开始，Kafka 0.8 支持已被弃用 Stable(稳定版) 语言支持 Scala, Java, Python Scala, Java Receiver DStream Yes No Direct DStream Yes Yes SSL / TLS Support No Yes Offset Commit API(偏移量提交) No Yes Dynamic Topic Subscription(动态主题订阅) No Yes 本文使用的 Kafka 版本为 kafka_2.12-2.2.0，故采用第二种方式进行整合。 二、项目依赖项目采用 Maven 进行构建，主要依赖如下： 123456789101112131415161718&lt;properties&gt; &lt;scala.version&gt;2.12&lt;/scala.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- Spark Streaming--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark Streaming 整合 Kafka 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-kafka-0-10_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 完整源码见本仓库：spark-streaming-kafka 三、整合Kafka通过调用 KafkaUtils 对象的 createDirectStream 方法来创建输入流，完整代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import org.apache.kafka.common.serialization.StringDeserializerimport org.apache.spark.SparkConfimport org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribeimport org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistentimport org.apache.spark.streaming.kafka010._import org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;/** * spark streaming 整合 kafka */object KafkaDirectStream &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf().setAppName("KafkaDirectStream").setMaster("local[2]") val streamingContext = new StreamingContext(sparkConf, Seconds(5)) val kafkaParams = Map[String, Object]( /* * 指定 broker 的地址清单，清单里不需要包含所有的 broker 地址，生产者会从给定的 broker 里查找其他 broker 的信息。 * 不过建议至少提供两个 broker 的信息作为容错。 */ "bootstrap.servers" -&gt; "hadoop001:9092", /*键的序列化器*/ "key.deserializer" -&gt; classOf[StringDeserializer], /*值的序列化器*/ "value.deserializer" -&gt; classOf[StringDeserializer], /*消费者所在分组的 ID*/ "group.id" -&gt; "spark-streaming-group", /* * 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理: * latest: 在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录） * earliest: 在偏移量无效的情况下，消费者将从起始位置读取分区的记录 */ "auto.offset.reset" -&gt; "latest", /*是否自动提交*/ "enable.auto.commit" -&gt; (true: java.lang.Boolean) ) /*可以同时订阅多个主题*/ val topics = Array("spark-streaming-topic") val stream = KafkaUtils.createDirectStream[String, String]( streamingContext, /*位置策略*/ PreferConsistent, /*订阅主题*/ Subscribe[String, String](topics, kafkaParams) ) /*打印输入流*/ stream.map(record =&gt; (record.key, record.value)).print() streamingContext.start() streamingContext.awaitTermination() &#125;&#125; 3.1 ConsumerRecord这里获得的输入流中每一个 Record 实际上是 ConsumerRecord&lt;K, V&gt; 的实例，其包含了 Record 的所有可用信息，源码如下： 12345678910111213141516171819202122232425262728public class ConsumerRecord&lt;K, V&gt; &#123; public static final long NO_TIMESTAMP = RecordBatch.NO_TIMESTAMP; public static final int NULL_SIZE = -1; public static final int NULL_CHECKSUM = -1; /*主题名称*/ private final String topic; /*分区编号*/ private final int partition; /*偏移量*/ private final long offset; /*时间戳*/ private final long timestamp; /*时间戳代表的含义*/ private final TimestampType timestampType; /*键序列化器*/ private final int serializedKeySize; /*值序列化器*/ private final int serializedValueSize; /*值序列化器*/ private final Headers headers; /*键*/ private final K key; /*值*/ private final V value; ..... &#125; 3.2 生产者属性在示例代码中 kafkaParams 封装了 Kafka 消费者的属性，这些属性和 Spark Streaming 无关，是 Kafka 原生 API 中就有定义的。其中服务器地址、键序列化器和值序列化器是必选的，其他配置是可选的。其余可选的配置项如下： 1. fetch.min.byte消费者从服务器获取记录的最小字节数。如果可用的数据量小于设置值，broker 会等待有足够的可用数据时才会把它返回给消费者。 2. fetch.max.wait.msbroker 返回给消费者数据的等待时间。 3. max.partition.fetch.bytes分区返回给消费者的最大字节数。 4. session.timeout.ms消费者在被认为死亡之前可以与服务器断开连接的时间。 5. auto.offset.reset该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理： latest(默认值) ：在偏移量无效的情况下，消费者将从其启动之后生成的最新的记录开始读取数据； earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录。 6. enable.auto.commit是否自动提交偏移量，默认值是 true,为了避免出现重复数据和数据丢失，可以把它设置为 false。 7. client.id客户端 id，服务器用来识别消息的来源。 8. max.poll.records单次调用 poll() 方法能够返回的记录数量。 9. receive.buffer.bytes 和 send.buffer.byte这两个参数分别指定 TCP socket 接收和发送数据包缓冲区的大小，-1 代表使用操作系统的默认值。 3.3 位置策略Spark Streaming 中提供了如下三种位置策略，用于指定 Kafka 主题分区与 Spark 执行程序 Executors 之间的分配关系： PreferConsistent : 它将在所有的 Executors 上均匀分配分区； PreferBrokers : 当 Spark 的 Executor 与 Kafka Broker 在同一机器上时可以选择该选项，它优先将该 Broker 上的首领分区分配给该机器上的 Executor； PreferFixed : 可以指定主题分区与特定主机的映射关系，显示地将分区分配到特定的主机，其构造器如下： 1234567@Experimentaldef PreferFixed(hostMap: collection.Map[TopicPartition, String]): LocationStrategy = new PreferFixed(new ju.HashMap[TopicPartition, String](hostMap.asJava))@Experimentaldef PreferFixed(hostMap: ju.Map[TopicPartition, String]): LocationStrategy = new PreferFixed(hostMap) 3.4 订阅方式Spark Streaming 提供了两种主题订阅方式，分别为 Subscribe 和 SubscribePattern。后者可以使用正则匹配订阅主题的名称。其构造器分别如下： 12345678910111213141516171819/** * @param 需要订阅的主题的集合 * @param Kafka 消费者参数 * @param offsets(可选): 在初始启动时开始的偏移量。如果没有，则将使用保存的偏移量或 auto.offset.reset 属性的值 */def Subscribe[K, V]( topics: ju.Collection[jl.String], kafkaParams: ju.Map[String, Object], offsets: ju.Map[TopicPartition, jl.Long]): ConsumerStrategy[K, V] = &#123; ... &#125;/** * @param 需要订阅的正则 * @param Kafka 消费者参数 * @param offsets(可选): 在初始启动时开始的偏移量。如果没有，则将使用保存的偏移量或 auto.offset.reset 属性的值 */def SubscribePattern[K, V]( pattern: ju.regex.Pattern, kafkaParams: collection.Map[String, Object], offsets: collection.Map[TopicPartition, Long]): ConsumerStrategy[K, V] = &#123; ... &#125; 在示例代码中，我们实际上并没有指定第三个参数 offsets，所以程序默认采用的是配置的 auto.offset.reset 属性的值 latest，即在偏移量无效的情况下，消费者将从其启动之后生成的最新的记录开始读取数据。 3.5 提交偏移量在示例代码中，我们将 enable.auto.commit 设置为 true，代表自动提交。在某些情况下，你可能需要更高的可靠性，如在业务完全处理完成后再提交偏移量，这时候可以使用手动提交。想要进行手动提交，需要调用 Kafka 原生的 API : commitSync: 用于异步提交； commitAsync：用于同步提交。 具体提交方式可以参见：Kafka 消费者详解 四、启动测试4.1 创建主题1. 启动KakfaKafka 的运行依赖于 zookeeper，需要预先启动，可以启动 Kafka 内置的 zookeeper，也可以启动自己安装的： 12345# zookeeper启动命令bin/zkServer.sh start# 内置zookeeper启动命令bin/zookeeper-server-start.sh config/zookeeper.properties 启动单节点 kafka 用于测试： 1# bin/kafka-server-start.sh config/server.properties 2. 创建topic123456789# 创建用于测试主题bin/kafka-topics.sh --create \ --bootstrap-server hadoop001:9092 \ --replication-factor 1 \ --partitions 1 \ --topic spark-streaming-topic# 查看所有主题 bin/kafka-topics.sh --list --bootstrap-server hadoop001:9092 3. 创建生产者这里创建一个 Kafka 生产者，用于发送测试数据： 1bin/kafka-console-producer.sh --broker-list hadoop001:9092 --topic spark-streaming-topic 4.2 本地模式测试这里我直接使用本地模式启动 Spark Streaming 程序。启动后使用生产者发送数据，从控制台查看结果。 从控制台输出中可以看到数据流已经被成功接收，由于采用 kafka-console-producer.sh 发送的数据默认是没有 key 的，所以 key 值为 null。同时从输出中也可以看到在程序中指定的 groupId 和程序自动分配的 clientId。 参考资料 https://spark.apache.org/docs/latest/streaming-kafka-0-10-integration.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>Spark_Streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark_Streaming整合Flume]]></title>
    <url>%2F2019%2F06%2F30%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark_Streaming%E6%95%B4%E5%90%88Flume%2F</url>
    <content type="text"><![CDATA[Spark Streaming 整合 Flume一、简介二、推送式方法2.1 配置日志收集Flume2.2 项目依赖2.3 Spark Streaming接收日志数据2.4 项目打包2.5 启动服务和提交作业2.6 测试2.7 注意事项三、拉取式方法3.1 配置日志收集Flume2.2 新增依赖2.3 Spark Streaming接收日志数据2.4 启动测试 一、简介Apache Flume 是一个分布式，高可用的数据收集系统，可以从不同的数据源收集数据，经过聚合后发送到分布式计算框架或者存储系统中。Spark Straming 提供了以下两种方式用于 Flume 的整合。 二、推送式方法在推送式方法 (Flume-style Push-based Approach) 中，Spark Streaming 程序需要对某台服务器的某个端口进行监听，Flume 通过 avro Sink 将数据源源不断推送到该端口。这里以监听日志文件为例，具体整合方式如下： 2.1 配置日志收集Flume新建配置 netcat-memory-avro.properties，使用 tail 命令监听文件内容变化，然后将新的文件内容通过 avro sink 发送到 hadoop001 这台服务器的 8888 端口： 12345678910111213141516171819202122#指定agent的sources,sinks,channelsa1.sources = s1a1.sinks = k1a1.channels = c1#配置sources属性a1.sources.s1.type = execa1.sources.s1.command = tail -F /tmp/log.txta1.sources.s1.shell = /bin/bash -ca1.sources.s1.channels = c1#配置sinka1.sinks.k1.type = avroa1.sinks.k1.hostname = hadoop001a1.sinks.k1.port = 8888a1.sinks.k1.batch-size = 1a1.sinks.k1.channel = c1#配置channel类型a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 2.2 项目依赖项目采用 Maven 工程进行构建，主要依赖为 spark-streaming 和 spark-streaming-flume。 12345678910111213141516171819&lt;properties&gt; &lt;scala.version&gt;2.11&lt;/scala.version&gt; &lt;spark.version&gt;2.4.0&lt;/spark.version&gt;&lt;/properties&gt;&lt;dependencies&gt; &lt;!-- Spark Streaming--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;$&#123;spark.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;!-- Spark Streaming 整合 Flume 依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming-flume_$&#123;scala.version&#125;&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 2.3 Spark Streaming接收日志数据调用 FlumeUtils 工具类的 createStream 方法，对 hadoop001 的 8888 端口进行监听，获取到流数据并进行打印： 123456789101112131415161718import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.flume.FlumeUtilsobject PushBasedWordCount &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf() val ssc = new StreamingContext(sparkConf, Seconds(5)) // 1.获取输入流 val flumeStream = FlumeUtils.createStream(ssc, "hadoop001", 8888) // 2.打印输入流的数据 flumeStream.map(line =&gt; new String(line.event.getBody.array()).trim).print() ssc.start() ssc.awaitTermination() &#125;&#125; 2.4 项目打包因为 Spark 安装目录下是不含有 spark-streaming-flume 依赖包的，所以在提交到集群运行时候必须提供该依赖包，你可以在提交命令中使用 --jar 指定上传到服务器的该依赖包，或者使用 --packages org.apache.spark:spark-streaming-flume_2.12:2.4.3 指定依赖包的完整名称，这样程序在启动时会先去中央仓库进行下载。 这里我采用的是第三种方式：使用 maven-shade-plugin 插件进行 ALL IN ONE 打包，把所有依赖的 Jar 一并打入最终包中。需要注意的是 spark-streaming 包在 Spark 安装目录的 jars 目录中已经提供，所以不需要打入。插件配置如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;8&lt;/source&gt; &lt;target&gt;8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;!--使用 shade 进行打包--&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;createDependencyReducedPom&gt;true&lt;/createDependencyReducedPom&gt; &lt;filters&gt; &lt;filter&gt; &lt;artifact&gt;*:*&lt;/artifact&gt; &lt;excludes&gt; &lt;exclude&gt;META-INF/*.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.sf&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.DSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.dsa&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.RSA&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.rsa&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.EC&lt;/exclude&gt; &lt;exclude&gt;META-INF/*.ec&lt;/exclude&gt; &lt;exclude&gt;META-INF/MSFTSIG.SF&lt;/exclude&gt; &lt;exclude&gt;META-INF/MSFTSIG.RSA&lt;/exclude&gt; &lt;/excludes&gt; &lt;/filter&gt; &lt;/filters&gt; &lt;artifactSet&gt; &lt;excludes&gt; &lt;exclude&gt;org.apache.spark:spark-streaming_$&#123;scala.version&#125;&lt;/exclude&gt; &lt;exclude&gt;org.scala-lang:scala-library&lt;/exclude&gt; &lt;exclude&gt;org.apache.commons:commons-lang3&lt;/exclude&gt; &lt;/excludes&gt; &lt;/artifactSet&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;shade&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;transformers&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ServicesResourceTransformer"/&gt; &lt;transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer"&gt; &lt;/transformer&gt; &lt;/transformers&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;!--打包.scala 文件需要配置此插件--&gt; &lt;plugin&gt; &lt;groupId&gt;org.scala-tools&lt;/groupId&gt; &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt; &lt;version&gt;2.15.1&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;scala-compile&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;/goals&gt; &lt;configuration&gt; &lt;includes&gt; &lt;include&gt;**/*.scala&lt;/include&gt; &lt;/includes&gt; &lt;/configuration&gt; &lt;/execution&gt; &lt;execution&gt; &lt;id&gt;scala-test-compile&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 本项目完整源码见：spark-streaming-flume 使用 mvn clean package 命令打包后会生产以下两个 Jar 包，提交 非 original 开头的 Jar 即可。 2.5 启动服务和提交作业 启动 Flume 服务： 1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/netcat-memory-avro.properties \--name a1 -Dflume.root.logger=INFO,console 提交 Spark Streaming 作业： 1234spark-submit \--class com.heibaiying.flume.PushBasedWordCount \--master local[4] \/usr/appjar/spark-streaming-flume-1.0.jar 2.6 测试这里使用 echo 命令模拟日志产生的场景，往日志文件中追加数据，然后查看程序的输出： Spark Streaming 程序成功接收到数据并打印输出： 2.7 注意事项1. 启动顺序这里需要注意的，不论你先启动 Spark 程序还是 Flume 程序，由于两者的启动都需要一定的时间，此时先启动的程序会短暂地抛出端口拒绝连接的异常，此时不需要进行任何操作，等待两个程序都启动完成即可。 2. 版本一致最好保证用于本地开发和编译的 Scala 版本和 Spark 的 Scala 版本一致，至少保证大版本一致，如都是 2.11。 三、拉取式方法拉取式方法 (Pull-based Approach using a Custom Sink) 是将数据推送到 SparkSink 接收器中，此时数据会保持缓冲状态，Spark Streaming 定时从接收器中拉取数据。这种方式是基于事务的，即只有在 Spark Streaming 接收和复制数据完成后，才会删除缓存的数据。与第一种方式相比，具有更强的可靠性和容错保证。整合步骤如下： 3.1 配置日志收集Flume新建 Flume 配置文件 netcat-memory-sparkSink.properties，配置和上面基本一致，只是把 a1.sinks.k1.type 的属性修改为 org.apache.spark.streaming.flume.sink.SparkSink，即采用 Spark 接收器。 12345678910111213141516171819202122#指定agent的sources,sinks,channelsa1.sources = s1a1.sinks = k1a1.channels = c1#配置sources属性a1.sources.s1.type = execa1.sources.s1.command = tail -F /tmp/log.txta1.sources.s1.shell = /bin/bash -ca1.sources.s1.channels = c1#配置sinka1.sinks.k1.type = org.apache.spark.streaming.flume.sink.SparkSinka1.sinks.k1.hostname = hadoop001a1.sinks.k1.port = 8888a1.sinks.k1.batch-size = 1a1.sinks.k1.channel = c1#配置channel类型a1.channels.c1.type = memorya1.channels.c1.capacity = 1000a1.channels.c1.transactionCapacity = 100 2.2 新增依赖使用拉取式方法需要额外添加以下两个依赖： 12345678910&lt;dependency&gt; &lt;groupId&gt;org.scala-lang&lt;/groupId&gt; &lt;artifactId&gt;scala-library&lt;/artifactId&gt; &lt;version&gt;2.12.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.5&lt;/version&gt;&lt;/dependency&gt; 注意：添加这两个依赖只是为了本地测试，Spark 的安装目录下已经提供了这两个依赖，所以在最终打包时需要进行排除。 2.3 Spark Streaming接收日志数据这里和上面推送式方法的代码基本相同，只是将调用方法改为 createPollingStream。 123456789101112131415161718import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import org.apache.spark.streaming.flume.FlumeUtilsobject PullBasedWordCount &#123; def main(args: Array[String]): Unit = &#123; val sparkConf = new SparkConf() val ssc = new StreamingContext(sparkConf, Seconds(5)) // 1.获取输入流 val flumeStream = FlumeUtils.createPollingStream(ssc, "hadoop001", 8888) // 2.打印输入流中的数据 flumeStream.map(line =&gt; new String(line.event.getBody.array()).trim).print() ssc.start() ssc.awaitTermination() &#125;&#125; 2.4 启动测试启动和提交作业流程与上面相同，这里给出执行脚本，过程不再赘述。 启动 Flume 进行日志收集： 1234flume-ng agent \--conf conf \--conf-file /usr/app/apache-flume-1.6.0-cdh5.15.2-bin/examples/netcat-memory-sparkSink.properties \--name a1 -Dflume.root.logger=INFO,console 提交 Spark Streaming 作业： 1234spark-submit \--class com.heibaiying.flume.PullBasedWordCount \--master local[4] \/usr/appjar/spark-streaming-flume-1.0.jar 参考资料 streaming-flume-integration 关于大数据应用常用的打包方式可以参见：大数据应用常用打包方式]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>Spark_Streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark_Streaming基本操作]]></title>
    <url>%2F2019%2F06%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark_Streaming%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Spark Streaming 基本操作一、案例引入3.1 StreamingContext3.2 数据源3.3 服务的启动与停止二、Transformation2.1 DStream与RDDs2.2 updateStateByKey2.3 启动测试三、输出操作3.1 输出API3.1 foreachRDD3.3 代码说明3.4 启动测试 一、案例引入这里先引入一个基本的案例来演示流的创建：获取指定端口上的数据并进行词频统计。项目依赖和代码实现如下： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-streaming_2.12&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt;&lt;/dependency&gt; 123456789101112131415161718192021import org.apache.spark.SparkConfimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;object NetworkWordCount &#123; def main(args: Array[String]) &#123; /*指定时间间隔为 5s*/ val sparkConf = new SparkConf().setAppName("NetworkWordCount").setMaster("local[2]") val ssc = new StreamingContext(sparkConf, Seconds(5)) /*创建文本输入流,并进行词频统计*/ val lines = ssc.socketTextStream("hadoop001", 9999) lines.flatMap(_.split(" ")).map(x =&gt; (x, 1)).reduceByKey(_ + _).print() /*启动服务*/ ssc.start() /*等待服务结束*/ ssc.awaitTermination() &#125;&#125; 使用本地模式启动 Spark 程序，然后使用 nc -lk 9999 打开端口并输入测试数据： 123[root@hadoop001 ~]# nc -lk 9999hello world hello spark hive hive hadoopstorm storm flink azkaban 此时控制台输出如下，可以看到已经接收到数据并按行进行了词频统计。 下面针对示例代码进行讲解：### 3.1 StreamingContextSpark Streaming 编程的入口类是 StreamingContext，在创建时候需要指明 sparkConf 和 batchDuration(批次时间)，Spark 流处理本质是将流数据拆分为一个个批次，然后进行微批处理，batchDuration 就是批次拆分的时间间隔。这个时间可以根据业务需求和服务器性能进行指定，如果业务要求低延迟并且服务器性能也允许，则这个时间可以指定得很短。这里需要注意的是：示例代码使用的是本地模式，配置为 local[2]，这里不能配置为 local[1]。这是因为对于流数据的处理，Spark 必须有一个独立的 Executor 来接收数据，然后再由其他的 Executors 来处理，所以为了保证数据能够被处理，至少要有 2 个 Executors。这里我们的程序只有一个数据流，在并行读取多个数据流的时候，也需要保证有足够的 Executors 来接收和处理数据。### 3.2 数据源在示例代码中使用的是 socketTextStream 来创建基于 Socket 的数据流，实际上 Spark 还支持多种数据源，分为以下两类：+ 基本数据源：包括文件系统、Socket 连接等；+ 高级数据源：包括 Kafka，Flume，Kinesis 等。在基本数据源中，Spark 支持监听 HDFS 上指定目录，当有新文件加入时，会获取其文件内容作为输入流。创建方式如下：1234// 对于文本文件，指明监听目录即可streamingContext.textFileStream(dataDirectory)// 对于其他文件，需要指明目录，以及键的类型、值的类型、和输入格式streamingContext.fileStream[KeyClass, ValueClass, InputFormatClass](dataDirectory)被监听的目录可以是具体目录，如 hdfs://host:8040/logs/；也可以使用通配符，如 hdfs://host:8040/logs/2017/*。&gt; 关于高级数据源的整合单独整理至：Spark Streaming 整合 Flume 和 Spark Streaming 整合 Kafka### 3.3 服务的启动与停止在示例代码中，使用 streamingContext.start() 代表启动服务，此时还要使用 streamingContext.awaitTermination() 使服务处于等待和可用的状态，直到发生异常或者手动使用 streamingContext.stop() 进行终止。## 二、Transformation### 2.1 DStream与RDDsDStream 是 Spark Streaming 提供的基本抽象。它表示连续的数据流。在内部，DStream 由一系列连续的 RDD 表示。所以从本质上而言，应用于 DStream 的任何操作都会转换为底层 RDD 上的操作。例如，在示例代码中 flatMap 算子的操作实际上是作用在每个 RDDs 上 (如下图)。因为这个原因，所以 DStream 能够支持 RDD 大部分的transformation算子。 2.2 updateStateByKey除了能够支持 RDD 的算子外，DStream 还有部分独有的transformation算子，这当中比较常用的是 updateStateByKey。文章开头的词频统计程序，只能统计每一次输入文本中单词出现的数量，想要统计所有历史输入中单词出现的数量，可以使用 updateStateByKey 算子。代码如下： 12345678910111213141516171819202122232425262728293031323334353637object NetworkWordCountV2 &#123; def main(args: Array[String]) &#123; /* * 本地测试时最好指定 hadoop 用户名,否则会默认使用本地电脑的用户名, * 此时在 HDFS 上创建目录时可能会抛出权限不足的异常 */ System.setProperty("HADOOP_USER_NAME", "root") val sparkConf = new SparkConf().setAppName("NetworkWordCountV2").setMaster("local[2]") val ssc = new StreamingContext(sparkConf, Seconds(5)) /*必须要设置检查点*/ ssc.checkpoint("hdfs://hadoop001:8020/spark-streaming") val lines = ssc.socketTextStream("hadoop001", 9999) lines.flatMap(_.split(" ")).map(x =&gt; (x, 1)) .updateStateByKey[Int](updateFunction _) //updateStateByKey 算子 .print() ssc.start() ssc.awaitTermination() &#125; /** * 累计求和 * * @param currentValues 当前的数据 * @param preValues 之前的数据 * @return 相加后的数据 */ def updateFunction(currentValues: Seq[Int], preValues: Option[Int]): Option[Int] = &#123; val current = currentValues.sum val pre = preValues.getOrElse(0) Some(current + pre) &#125;&#125; 使用 updateStateByKey 算子，你必须使用 ssc.checkpoint() 设置检查点，这样当使用 updateStateByKey 算子时，它会去检查点中取出上一次保存的信息，并使用自定义的 updateFunction 函数将上一次的数据和本次数据进行相加，然后返回。 2.3 启动测试在监听端口输入如下测试数据： 12345[root@hadoop001 ~]# nc -lk 9999hello world hello spark hive hive hadoopstorm storm flink azkabanhello world hello spark hive hive hadoopstorm storm flink azkaban 此时控制台输出如下，所有输入都被进行了词频累计： 同时在输出日志中还可以看到检查点操作的相关信息： 1234567# 保存检查点信息19/05/27 16:21:05 INFO CheckpointWriter: Saving checkpoint for time 1558945265000 ms to file 'hdfs://hadoop001:8020/spark-streaming/checkpoint-1558945265000'# 删除已经无用的检查点信息19/05/27 16:21:30 INFO CheckpointWriter: Deleting hdfs://hadoop001:8020/spark-streaming/checkpoint-1558945265000 三、输出操作3.1 输出APISpark Streaming 支持以下输出操作： Output Operation Meaning print() 在运行流应用程序的 driver 节点上打印 DStream 中每个批次的前十个元素。用于开发调试。 saveAsTextFiles(prefix, [suffix]) 将 DStream 的内容保存为文本文件。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。 saveAsObjectFiles(prefix, [suffix]) 将 DStream 的内容序列化为 Java 对象，并保存到 SequenceFiles。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。 saveAsHadoopFiles(prefix, [suffix]) 将 DStream 的内容保存为 Hadoop 文件。每个批处理间隔的文件名基于前缀和后缀生成：“prefix-TIME_IN_MS [.suffix]”。 foreachRDD(func) 最通用的输出方式，它将函数 func 应用于从流生成的每个 RDD。此函数应将每个 RDD 中的数据推送到外部系统，例如将 RDD 保存到文件，或通过网络将其写入数据库。 前面的四个 API 都是直接调用即可，下面主要讲解通用的输出方式 foreachRDD(func)，通过该 API 你可以将数据保存到任何你需要的数据源。 3.1 foreachRDD这里我们使用 Redis 作为客户端，对文章开头示例程序进行改变，把每一次词频统计的结果写入到 Redis，并利用 Redis 的 HINCRBY 命令来进行词频统计。这里需要导入 Jedis 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 具体实现代码如下: 12345678910111213141516171819202122232425262728293031323334import org.apache.spark.SparkConfimport org.apache.spark.streaming.dstream.DStreamimport org.apache.spark.streaming.&#123;Seconds, StreamingContext&#125;import redis.clients.jedis.Jedisobject NetworkWordCountToRedis &#123; def main(args: Array[String]) &#123; val sparkConf = new SparkConf().setAppName("NetworkWordCountToRedis").setMaster("local[2]") val ssc = new StreamingContext(sparkConf, Seconds(5)) /*创建文本输入流,并进行词频统计*/ val lines = ssc.socketTextStream("hadoop001", 9999) val pairs: DStream[(String, Int)] = lines.flatMap(_.split(" ")).map(x =&gt; (x, 1)).reduceByKey(_ + _) /*保存数据到 Redis*/ pairs.foreachRDD &#123; rdd =&gt; rdd.foreachPartition &#123; partitionOfRecords =&gt; var jedis: Jedis = null try &#123; jedis = JedisPoolUtil.getConnection partitionOfRecords.foreach(record =&gt; jedis.hincrBy("wordCount", record._1, record._2)) &#125; catch &#123; case ex: Exception =&gt; ex.printStackTrace() &#125; finally &#123; if (jedis != null) jedis.close() &#125; &#125; &#125; ssc.start() ssc.awaitTermination() &#125;&#125; 其中 JedisPoolUtil 的代码如下： 1234567891011121314151617181920212223242526import redis.clients.jedis.Jedis;import redis.clients.jedis.JedisPool;import redis.clients.jedis.JedisPoolConfig;public class JedisPoolUtil &#123; /* 声明为 volatile 防止指令重排序 */ private static volatile JedisPool jedisPool = null; private static final String HOST = "localhost"; private static final int PORT = 6379; /* 双重检查锁实现懒汉式单例 */ public static Jedis getConnection() &#123; if (jedisPool == null) &#123; synchronized (JedisPoolUtil.class) &#123; if (jedisPool == null) &#123; JedisPoolConfig config = new JedisPoolConfig(); config.setMaxTotal(30); config.setMaxIdle(10); jedisPool = new JedisPool(config, HOST, PORT); &#125; &#125; &#125; return jedisPool.getResource(); &#125;&#125; 3.3 代码说明这里将上面保存到 Redis 的代码单独抽取出来，并去除异常判断的部分。精简后的代码如下： 1234567pairs.foreachRDD &#123; rdd =&gt; rdd.foreachPartition &#123; partitionOfRecords =&gt; val jedis = JedisPoolUtil.getConnection partitionOfRecords.foreach(record =&gt; jedis.hincrBy("wordCount", record._1, record._2)) jedis.close() &#125;&#125; 这里可以看到一共使用了三次循环，分别是循环 RDD，循环分区，循环每条记录，上面我们的代码是在循环分区的时候获取连接，也就是为每一个分区获取一个连接。但是这里大家可能会有疑问：为什么不在循环 RDD 的时候，为每一个 RDD 获取一个连接，这样所需要的连接数会更少。实际上这是不可行的，如果按照这种情况进行改写，如下： 1234567pairs.foreachRDD &#123; rdd =&gt; val jedis = JedisPoolUtil.getConnection rdd.foreachPartition &#123; partitionOfRecords =&gt; partitionOfRecords.foreach(record =&gt; jedis.hincrBy("wordCount", record._1, record._2)) &#125; jedis.close()&#125; 此时在执行时候就会抛出 Caused by: java.io.NotSerializableException: redis.clients.jedis.Jedis，这是因为在实际计算时，Spark 会将对 RDD 操作分解为多个 Task，Task 运行在具体的 Worker Node 上。在执行之前，Spark 会对任务进行闭包，之后闭包被序列化并发送给每个 Executor，而 Jedis 显然是不能被序列化的，所以会抛出异常。 第二个需要注意的是 ConnectionPool 最好是一个静态，惰性初始化连接池 。这是因为 Spark 的转换操作本身就是惰性的，且没有数据流时不会触发写出操作，所以出于性能考虑，连接池应该是惰性的，因此上面 JedisPool 在初始化时采用了懒汉式单例进行惰性初始化。 3.4 启动测试在监听端口输入如下测试数据： 12345[root@hadoop001 ~]# nc -lk 9999hello world hello spark hive hive hadoopstorm storm flink azkabanhello world hello spark hive hive hadoopstorm storm flink azkaban 使用 Redis Manager 查看写入结果 (如下图),可以看到与使用 updateStateByKey 算子得到的计算结果相同。 本片文章所有源码见本仓库：spark-streaming-basis 参考资料Spark 官方文档：http://spark.apache.org/docs/latest/streaming-programming-guide.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>Spark_Streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark_Streaming与流处理]]></title>
    <url>%2F2019%2F06%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark_Streaming%E4%B8%8E%E6%B5%81%E5%A4%84%E7%90%86%2F</url>
    <content type="text"><![CDATA[Spark Streaming与流处理一、流处理1.1 静态数据处理1.2 流处理二、Spark Streaming2.1 简介2.2 DStream2.3 Spark &amp; Storm &amp; Flink 一、流处理1.1 静态数据处理在流处理之前，数据通常存储在数据库，文件系统或其他形式的存储系统中。应用程序根据需要查询数据或计算数据。这就是传统的静态数据处理架构。Hadoop 采用 HDFS 进行数据存储，采用 MapReduce 进行数据查询或分析，这就是典型的静态数据处理架构。 1.2 流处理而流处理则是直接对运动中的数据的处理，在接收数据时直接计算数据。 大多数数据都是连续的流：传感器事件，网站上的用户活动，金融交易等等 ，所有这些数据都是随着时间的推移而创建的。 接收和发送数据流并执行应用程序或分析逻辑的系统称为流处理器。流处理器的基本职责是确保数据有效流动，同时具备可扩展性和容错能力，Storm 和 Flink 就是其代表性的实现。 流处理带来了静态数据处理所不具备的众多优点： 应用程序立即对数据做出反应：降低了数据的滞后性，使得数据更具有时效性，更能反映对未来的预期； 流处理可以处理更大的数据量：直接处理数据流，并且只保留数据中有意义的子集，并将其传送到下一个处理单元，逐级过滤数据，降低需要处理的数据量，从而能够承受更大的数据量； 流处理更贴近现实的数据模型：在实际的环境中，一切数据都是持续变化的，要想能够通过过去的数据推断未来的趋势，必须保证数据的不断输入和模型的不断修正，典型的就是金融市场、股票市场，流处理能更好的应对这些数据的连续性的特征和及时性的需求； 流处理分散和分离基础设施：流式处理减少了对大型数据库的需求。相反，每个流处理程序通过流处理框架维护了自己的数据和状态，这使得流处理程序更适合微服务架构。 二、Spark Streaming2.1 简介Spark Streaming 是 Spark 的一个子模块，用于快速构建可扩展，高吞吐量，高容错的流处理程序。具有以下特点： 通过高级 API 构建应用程序，简单易用； 支持多种语言，如 Java，Scala 和 Python； 良好的容错性，Spark Streaming 支持快速从失败中恢复丢失的操作状态； 能够和 Spark 其他模块无缝集成，将流处理与批处理完美结合； Spark Streaming 可以从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，也支持自定义数据源。 2.2 DStreamSpark Streaming 提供称为离散流 (DStream) 的高级抽象，用于表示连续的数据流。 DStream 可以从来自 Kafka，Flume 和 Kinesis 等数据源的输入数据流创建，也可以由其他 DStream 转化而来。在内部，DStream 表示为一系列 RDD。 2.3 Spark &amp; Storm &amp; Flinkstorm 和 Flink 都是真正意义上的流计算框架，但 Spark Streaming 只是将数据流进行极小粒度的拆分，拆分为多个批处理，使得其能够得到接近于流处理的效果，但其本质上还是批处理（或微批处理）。 参考资料 Spark Streaming Programming Guide What is stream processing?]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>Spark_Streaming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之SparkSQL联结操作]]></title>
    <url>%2F2019%2F06%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSparkSQL%E8%81%94%E7%BB%93%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Spark SQL JOIN一、 数据准备二、连接类型2.1 INNER JOIN2.2 FULL OUTER JOIN 2.3 LEFT OUTER JOIN2.4 RIGHT OUTER JOIN2.5 LEFT SEMI JOIN2.6 LEFT ANTI JOIN 2.7 CROSS JOIN2.8 NATURAL JOIN三、连接的执行 一、 数据准备本文主要介绍 Spark SQL 的多表连接，需要预先准备测试数据。分别创建员工和部门的 Datafame，并注册为临时视图，代码如下： 1234567val spark = SparkSession.builder().appName("aggregations").master("local[2]").getOrCreate()val empDF = spark.read.json("/usr/file/json/emp.json")empDF.createOrReplaceTempView("emp")val deptDF = spark.read.json("/usr/file/json/dept.json")deptDF.createOrReplaceTempView("dept") 两表的主要字段如下： 123456789emp 员工表 |-- ENAME: 员工姓名 |-- DEPTNO: 部门编号 |-- EMPNO: 员工编号 |-- HIREDATE: 入职时间 |-- JOB: 职务 |-- MGR: 上级编号 |-- SAL: 薪资 |-- COMM: 奖金 1234dept 部门表 |-- DEPTNO: 部门编号 |-- DNAME: 部门名称 |-- LOC: 部门所在城市 注：emp.json，dept.json 可以在本仓库的resources 目录进行下载。 二、连接类型Spark 中支持多种连接类型： Inner Join : 内连接； Full Outer Join : 全外连接； Left Outer Join : 左外连接； Right Outer Join : 右外连接； Left Semi Join : 左半连接； Left Anti Join : 左反连接； Natural Join : 自然连接； Cross (or Cartesian) Join : 交叉 (或笛卡尔) 连接。 其中内，外连接，笛卡尔积均与普通关系型数据库中的相同，如下图所示： 这里解释一下左半连接和左反连接，这两个连接等价于关系型数据库中的 IN 和 NOT IN 字句： 123456789-- LEFT SEMI JOINSELECT * FROM emp LEFT SEMI JOIN dept ON emp.deptno = dept.deptno-- 等价于如下的 IN 语句SELECT * FROM emp WHERE deptno IN (SELECT deptno FROM dept)-- LEFT ANTI JOINSELECT * FROM emp LEFT ANTI JOIN dept ON emp.deptno = dept.deptno-- 等价于如下的 IN 语句SELECT * FROM emp WHERE deptno NOT IN (SELECT deptno FROM dept) 所有连接类型的示例代码如下： 2.1 INNER JOIN1234567// 1.定义连接表达式val joinExpression = empDF.col("deptno") === deptDF.col("deptno")// 2.连接查询 empDF.join(deptDF,joinExpression).select("ename","dname").show()// 等价 SQL 如下：spark.sql("SELECT ename,dname FROM emp JOIN dept ON emp.deptno = dept.deptno").show() 2.2 FULL OUTER JOIN12empDF.join(deptDF, joinExpression, "outer").show()spark.sql("SELECT * FROM emp FULL OUTER JOIN dept ON emp.deptno = dept.deptno").show() 2.3 LEFT OUTER JOIN12empDF.join(deptDF, joinExpression, "left_outer").show()spark.sql("SELECT * FROM emp LEFT OUTER JOIN dept ON emp.deptno = dept.deptno").show() 2.4 RIGHT OUTER JOIN12empDF.join(deptDF, joinExpression, "right_outer").show()spark.sql("SELECT * FROM emp RIGHT OUTER JOIN dept ON emp.deptno = dept.deptno").show() 2.5 LEFT SEMI JOIN12empDF.join(deptDF, joinExpression, "left_semi").show()spark.sql("SELECT * FROM emp LEFT SEMI JOIN dept ON emp.deptno = dept.deptno").show() 2.6 LEFT ANTI JOIN12empDF.join(deptDF, joinExpression, "left_anti").show()spark.sql("SELECT * FROM emp LEFT ANTI JOIN dept ON emp.deptno = dept.deptno").show() 2.7 CROSS JOIN12empDF.join(deptDF, joinExpression, "cross").show()spark.sql("SELECT * FROM emp CROSS JOIN dept ON emp.deptno = dept.deptno").show() 2.8 NATURAL JOIN自然连接是在两张表中寻找那些数据类型和列名都相同的字段，然后自动地将他们连接起来，并返回所有符合条件的结果。 1spark.sql("SELECT * FROM emp NATURAL JOIN dept").show() 以下是一个自然连接的查询结果，程序自动推断出使用两张表都存在的 dept 列进行连接，其实际等价于： 1spark.sql("SELECT * FROM emp JOIN dept ON emp.deptno = dept.deptno").show() 由于自然连接常常会产生不可预期的结果，所以并不推荐使用。 三、连接的执行在对大表与大表之间进行连接操作时，通常都会触发 Shuffle Join，两表的所有分区节点会进行 All-to-All 的通讯，这种查询通常比较昂贵，会对网络 IO 会造成比较大的负担。 而对于大表和小表的连接操作，Spark 会在一定程度上进行优化，如果小表的数据量小于 Worker Node 的内存空间，Spark 会考虑将小表的数据广播到每一个 Worker Node，在每个工作节点内部执行连接计算，这可以降低网络的 IO，但会加大每个 Worker Node 的 CPU 负担。 是否采用广播方式进行 Join 取决于程序内部对小表的判断，如果想明确使用广播方式进行 Join，则可以在 DataFrame API 中使用 broadcast 方法指定需要广播的小表： 1empDF.join(broadcast(deptDF), joinExpression).show() 参考资料 Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之SparkSQL常用聚合函数]]></title>
    <url>%2F2019%2F06%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSparkSQL%E5%B8%B8%E7%94%A8%E8%81%9A%E5%90%88%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[聚合函数Aggregations一、简单聚合1.1 数据准备1.2 count1.3 countDistinct1.4 approx_count_distinct 1.5 first &amp; last 1.6 min &amp; max1.7 sum &amp; sumDistinct1.8 avg1.9 数学函数1.10 聚合数据到集合二、分组聚合2.1 简单分组2.2 分组聚合三、自定义聚合函数3.1 有类型的自定义函数3.2 无类型的自定义聚合函数 一、简单聚合1.1 数据准备12345678// 需要导入 spark sql 内置的函数包import org.apache.spark.sql.functions._val spark = SparkSession.builder().appName("aggregations").master("local[2]").getOrCreate()val empDF = spark.read.json("/usr/file/json/emp.json")// 注册为临时视图，用于后面演示 SQL 查询empDF.createOrReplaceTempView("emp")empDF.show() 注：emp.json 可以从本仓库的resources 目录下载。 1.2 count12// 计算员工人数empDF.select(count("ename")).show() 1.3 countDistinct12// 计算姓名不重复的员工人数empDF.select(countDistinct("deptno")).show() 1.4 approx_count_distinct通常在使用大型数据集时，你可能关注的只是近似值而不是准确值，这时可以使用 approx_count_distinct 函数，并可以使用第二个参数指定最大允许误差。 1empDF.select(approx_count_distinct ("ename",0.1)).show() 1.5 first &amp; last获取 DataFrame 中指定列的第一个值或者最后一个值。 1empDF.select(first("ename"),last("job")).show() 1.6 min &amp; max获取 DataFrame 中指定列的最小值或者最大值。 1empDF.select(min("sal"),max("sal")).show() 1.7 sum &amp; sumDistinct求和以及求指定列所有不相同的值的和。 12empDF.select(sum("sal")).show()empDF.select(sumDistinct("sal")).show() 1.8 avg内置的求平均数的函数。 1empDF.select(avg("sal")).show() 1.9 数学函数Spark SQL 中还支持多种数学聚合函数，用于通常的数学计算，以下是一些常用的例子： 12345678// 1.计算总体方差、均方差、总体标准差、样本标准差empDF.select(var_pop("sal"), var_samp("sal"), stddev_pop("sal"), stddev_samp("sal")).show()// 2.计算偏度和峰度empDF.select(skewness("sal"), kurtosis("sal")).show()// 3. 计算两列的皮尔逊相关系数、样本协方差、总体协方差。(这里只是演示，员工编号和薪资两列实际上并没有什么关联关系)empDF.select(corr("empno", "sal"), covar_samp("empno", "sal"),covar_pop("empno", "sal")).show() 1.10 聚合数据到集合12345678scala&gt; empDF.agg(collect_set("job"), collect_list("ename")).show()输出：+--------------------+--------------------+| collect_set(job)| collect_list(ename)|+--------------------+--------------------+|[MANAGER, SALESMA...|[SMITH, ALLEN, WA...|+--------------------+--------------------+ 二、分组聚合2.1 简单分组123456789101112131415161718empDF.groupBy("deptno", "job").count().show()//等价 SQLspark.sql("SELECT deptno, job, count(*) FROM emp GROUP BY deptno, job").show()输出：+------+---------+-----+|deptno| job|count|+------+---------+-----+| 10|PRESIDENT| 1|| 30| CLERK| 1|| 10| MANAGER| 1|| 30| MANAGER| 1|| 20| CLERK| 2|| 30| SALESMAN| 4|| 20| ANALYST| 2|| 10| CLERK| 1|| 20| MANAGER| 1|+------+---------+-----+ 2.2 分组聚合1234567891011121314empDF.groupBy("deptno").agg(count("ename").alias("人数"), sum("sal").alias("总工资")).show()// 等价语法empDF.groupBy("deptno").agg("ename"-&gt;"count","sal"-&gt;"sum").show()// 等价 SQLspark.sql("SELECT deptno, count(ename) ,sum(sal) FROM emp GROUP BY deptno").show()输出：+------+----+------+|deptno|人数|总工资|+------+----+------+| 10| 3|8750.0|| 30| 6|9400.0|| 20| 5|9375.0|+------+----+------+ 三、自定义聚合函数Scala 提供了两种自定义聚合函数的方法，分别如下： 有类型的自定义聚合函数，主要适用于 DataSet； 无类型的自定义聚合函数，主要适用于 DataFrame。 以下分别使用两种方式来自定义一个求平均值的聚合函数，这里以计算员工平均工资为例。两种自定义方式分别如下： 3.1 有类型的自定义函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import org.apache.spark.sql.expressions.Aggregatorimport org.apache.spark.sql.&#123;Encoder, Encoders, SparkSession, functions&#125;// 1.定义员工类,对于可能存在 null 值的字段需要使用 Option 进行包装case class Emp(ename: String, comm: scala.Option[Double], deptno: Long, empno: Long, hiredate: String, job: String, mgr: scala.Option[Long], sal: Double)// 2.定义聚合操作的中间输出类型case class SumAndCount(var sum: Double, var count: Long)/* 3.自定义聚合函数 * @IN 聚合操作的输入类型 * @BUF reduction 操作输出值的类型 * @OUT 聚合操作的输出类型 */object MyAverage extends Aggregator[Emp, SumAndCount, Double] &#123; // 4.用于聚合操作的的初始零值 override def zero: SumAndCount = SumAndCount(0, 0) // 5.同一分区中的 reduce 操作 override def reduce(avg: SumAndCount, emp: Emp): SumAndCount = &#123; avg.sum += emp.sal avg.count += 1 avg &#125; // 6.不同分区中的 merge 操作 override def merge(avg1: SumAndCount, avg2: SumAndCount): SumAndCount = &#123; avg1.sum += avg2.sum avg1.count += avg2.count avg1 &#125; // 7.定义最终的输出类型 override def finish(reduction: SumAndCount): Double = reduction.sum / reduction.count // 8.中间类型的编码转换 override def bufferEncoder: Encoder[SumAndCount] = Encoders.product // 9.输出类型的编码转换 override def outputEncoder: Encoder[Double] = Encoders.scalaDouble&#125;object SparkSqlApp &#123; // 测试方法 def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName("Spark-SQL").master("local[2]").getOrCreate() import spark.implicits._ val ds = spark.read.json("file/emp.json").as[Emp] // 10.使用内置 avg() 函数和自定义函数分别进行计算，验证自定义函数是否正确 val myAvg = ds.select(MyAverage.toColumn.name("average_sal")).first() val avg = ds.select(functions.avg(ds.col("sal"))).first().get(0) println("自定义 average 函数 : " + myAvg) println("内置的 average 函数 : " + avg) &#125;&#125; 自定义聚合函数需要实现的方法比较多，这里以绘图的方式来演示其执行流程，以及每个方法的作用： 关于 zero,reduce,merge,finish 方法的作用在上图都有说明，这里解释一下中间类型和输出类型的编码转换，这个写法比较固定，基本上就是两种情况： 自定义类型 Case Class 或者元组就使用 Encoders.product 方法； 基本类型就使用其对应名称的方法，如 scalaByte，scalaFloat，scalaShort 等，示例如下： 12override def bufferEncoder: Encoder[SumAndCount] = Encoders.productoverride def outputEncoder: Encoder[Double] = Encoders.scalaDouble 3.2 无类型的自定义聚合函数理解了有类型的自定义聚合函数后，无类型的定义方式也基本相同，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import org.apache.spark.sql.expressions.&#123;MutableAggregationBuffer, UserDefinedAggregateFunction&#125;import org.apache.spark.sql.types._import org.apache.spark.sql.&#123;Row, SparkSession&#125;object MyAverage extends UserDefinedAggregateFunction &#123; // 1.聚合操作输入参数的类型,字段名称可以自定义 def inputSchema: StructType = StructType(StructField("MyInputColumn", LongType) :: Nil) // 2.聚合操作中间值的类型,字段名称可以自定义 def bufferSchema: StructType = &#123; StructType(StructField("sum", LongType) :: StructField("MyCount", LongType) :: Nil) &#125; // 3.聚合操作输出参数的类型 def dataType: DataType = DoubleType // 4.此函数是否始终在相同输入上返回相同的输出,通常为 true def deterministic: Boolean = true // 5.定义零值 def initialize(buffer: MutableAggregationBuffer): Unit = &#123; buffer(0) = 0L buffer(1) = 0L &#125; // 6.同一分区中的 reduce 操作 def update(buffer: MutableAggregationBuffer, input: Row): Unit = &#123; if (!input.isNullAt(0)) &#123; buffer(0) = buffer.getLong(0) + input.getLong(0) buffer(1) = buffer.getLong(1) + 1 &#125; &#125; // 7.不同分区中的 merge 操作 def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = &#123; buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) &#125; // 8.计算最终的输出值 def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)&#125;object SparkSqlApp &#123; // 测试方法 def main(args: Array[String]): Unit = &#123; val spark = SparkSession.builder().appName("Spark-SQL").master("local[2]").getOrCreate() // 9.注册自定义的聚合函数 spark.udf.register("myAverage", MyAverage) val df = spark.read.json("file/emp.json") df.createOrReplaceTempView("emp") // 10.使用自定义函数和内置函数分别进行计算 val myAvg = spark.sql("SELECT myAverage(sal) as avg_sal FROM emp").first() val avg = spark.sql("SELECT avg(sal) as avg_sal FROM emp").first() println("自定义 average 函数 : " + myAvg) println("内置的 average 函数 : " + avg) &#125;&#125; 参考资料 Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之SparkSQL外部数据源]]></title>
    <url>%2F2019%2F06%2F25%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSparkSQL%E5%A4%96%E9%83%A8%E6%95%B0%E6%8D%AE%E6%BA%90%2F</url>
    <content type="text"><![CDATA[Spark SQL 外部数据源一、简介1.1 多数据源支持1.2 读数据格式1.3 写数据格式二、CSV2.1 读取CSV文件2.2 写入CSV文件2.3 可选配置三、JSON3.1 读取JSON文件3.2 写入JSON文件3.3 可选配置四、Parquet4.1 读取Parquet文件2.2 写入Parquet文件2.3 可选配置五、ORC 5.1 读取ORC文件 4.2 写入ORC文件六、SQL Databases 6.1 读取数据6.2 写入数据七、Text 7.1 读取Text数据7.2 写入Text数据八、数据读写高级特性8.1 并行读8.2 并行写8.3 分区写入8.3 分桶写入8.5 文件大小管理九、可选配置附录9.1 CSV读写可选配置9.2 JSON读写可选配置9.3 数据库读写可选配置 一、简介1.1 多数据源支持Spark 支持以下六个核心数据源，同时 Spark 社区还提供了多达上百种数据源的读取方式，能够满足绝大部分使用场景。 CSV JSON Parquet ORC JDBC/ODBC connections Plain-text files 注：以下所有测试文件均可从本仓库的resources 目录进行下载 1.2 读数据格式所有读取 API 遵循以下调用格式： 12345678910// 格式DataFrameReader.format(...).option("key", "value").schema(...).load()// 示例spark.read.format("csv").option("mode", "FAILFAST") // 读取模式.option("inferSchema", "true") // 是否自动推断 schema.option("path", "path/to/file(s)") // 文件路径.schema(someSchema) // 使用预定义的 schema .load() 读取模式有以下三种可选项： 读模式 描述 permissive 当遇到损坏的记录时，将其所有字段设置为 null，并将所有损坏的记录放在名为 _corruption t_record 的字符串列中 dropMalformed 删除格式不正确的行 failFast 遇到格式不正确的数据时立即失败 1.3 写数据格式123456789// 格式DataFrameWriter.format(...).option(...).partitionBy(...).bucketBy(...).sortBy(...).save()//示例dataframe.write.format("csv").option("mode", "OVERWRITE") //写模式.option("dateFormat", "yyyy-MM-dd") //日期格式.option("path", "path/to/file(s)").save() 写数据模式有以下四种可选项： Scala/Java 描述 SaveMode.ErrorIfExists 如果给定的路径已经存在文件，则抛出异常，这是写数据默认的模式 SaveMode.Append 数据以追加的方式写入 SaveMode.Overwrite 数据以覆盖的方式写入 SaveMode.Ignore 如果给定的路径已经存在文件，则不做任何操作 二、CSVCSV 是一种常见的文本文件格式，其中每一行表示一条记录，记录中的每个字段用逗号分隔。 2.1 读取CSV文件自动推断类型读取读取示例： 123456spark.read.format("csv").option("header", "false") // 文件中的第一行是否为列的名称.option("mode", "FAILFAST") // 是否快速失败.option("inferSchema", "true") // 是否自动推断 schema.load("/usr/file/csv/dept.csv").show() 使用预定义类型： 123456789101112import org.apache.spark.sql.types.&#123;StructField, StructType, StringType,LongType&#125;//预定义数据格式val myManualSchema = new StructType(Array( StructField("deptno", LongType, nullable = false), StructField("dname", StringType,nullable = true), StructField("loc", StringType,nullable = true)))spark.read.format("csv").option("mode", "FAILFAST").schema(myManualSchema).load("/usr/file/csv/dept.csv").show() 2.2 写入CSV文件1df.write.format("csv").mode("overwrite").save("/tmp/csv/dept2") 也可以指定具体的分隔符： 1df.write.format("csv").mode("overwrite").option("sep", "\t").save("/tmp/csv/dept2") 2.3 可选配置为节省主文篇幅，所有读写配置项见文末 9.1 小节。 三、JSON3.1 读取JSON文件1spark.read.format("json").option("mode", "FAILFAST").load("/usr/file/json/dept.json").show(5) 需要注意的是：默认不支持一条数据记录跨越多行 (如下)，可以通过配置 multiLine 为 true 来进行更改，其默认值为 false。 123456789// 默认支持单行&#123;"DEPTNO": 10,"DNAME": "ACCOUNTING","LOC": "NEW YORK"&#125;//默认不支持多行&#123; "DEPTNO": 10, "DNAME": "ACCOUNTING", "LOC": "NEW YORK"&#125; 3.2 写入JSON文件1df.write.format("json").mode("overwrite").save("/tmp/spark/json/dept") 3.3 可选配置为节省主文篇幅，所有读写配置项见文末 9.2 小节。 四、Parquet Parquet 是一个开源的面向列的数据存储，它提供了多种存储优化，允许读取单独的列非整个文件，这不仅节省了存储空间而且提升了读取效率，它是 Spark 是默认的文件格式。 4.1 读取Parquet文件1spark.read.format("parquet").load("/usr/file/parquet/dept.parquet").show(5) 2.2 写入Parquet文件1df.write.format("parquet").mode("overwrite").save("/tmp/spark/parquet/dept") 2.3 可选配置Parquet 文件有着自己的存储规则，因此其可选配置项比较少，常用的有如下两个： 读写操作 配置项 可选值 默认值 描述 Write compression or codec None,uncompressed,bzip2,deflate, gzip,lz4, or snappy None 压缩文件格式 Read mergeSchema true, false 取决于配置项 spark.sql.parquet.mergeSchema 当为真时，Parquet 数据源将所有数据文件收集的 Schema 合并在一起，否则将从摘要文件中选择 Schema，如果没有可用的摘要文件，则从随机数据文件中选择 Schema。 更多可选配置可以参阅官方文档：https://spark.apache.org/docs/latest/sql-data-sources-parquet.html 五、ORCORC 是一种自描述的、类型感知的列文件格式，它针对大型数据的读写进行了优化，也是大数据中常用的文件格式。 5.1 读取ORC文件1spark.read.format("orc").load("/usr/file/orc/dept.orc").show(5) 4.2 写入ORC文件1csvFile.write.format("orc").mode("overwrite").save("/tmp/spark/orc/dept") 六、SQL DatabasesSpark 同样支持与传统的关系型数据库进行数据读写。但是 Spark 程序默认是没有提供数据库驱动的，所以在使用前需要将对应的数据库驱动上传到安装目录下的 jars 目录中。下面示例使用的是 Mysql 数据库，使用前需要将对应的 mysql-connector-java-x.x.x.jar 上传到 jars 目录下。 6.1 读取数据读取全表数据示例如下，这里的 help_keyword 是 mysql 内置的字典表，只有 help_keyword_id 和 name 两个字段。 123456spark.read.format("jdbc").option("driver", "com.mysql.jdbc.Driver") //驱动.option("url", "jdbc:mysql://127.0.0.1:3306/mysql") //数据库地址.option("dbtable", "help_keyword") //表名.option("user", "root").option("password","root").load().show(10) 从查询结果读取数据： 123456789101112131415161718192021222324252627282930313233val pushDownQuery = """(SELECT * FROM help_keyword WHERE help_keyword_id &lt;20) AS help_keywords"""spark.read.format("jdbc").option("url", "jdbc:mysql://127.0.0.1:3306/mysql").option("driver", "com.mysql.jdbc.Driver").option("user", "root").option("password", "root").option("dbtable", pushDownQuery).load().show()//输出+---------------+-----------+|help_keyword_id| name|+---------------+-----------+| 0| &lt;&gt;|| 1| ACTION|| 2| ADD|| 3|AES_DECRYPT|| 4|AES_ENCRYPT|| 5| AFTER|| 6| AGAINST|| 7| AGGREGATE|| 8| ALGORITHM|| 9| ALL|| 10| ALTER|| 11| ANALYSE|| 12| ANALYZE|| 13| AND|| 14| ARCHIVE|| 15| AREA|| 16| AS|| 17| ASBINARY|| 18| ASC|| 19| ASTEXT|+---------------+-----------+ 也可以使用如下的写法进行数据的过滤： 1234567891011121314151617181920212223val props = new java.util.Propertiesprops.setProperty("driver", "com.mysql.jdbc.Driver")props.setProperty("user", "root")props.setProperty("password", "root")val predicates = Array("help_keyword_id &lt; 10 OR name = 'WHEN'") //指定数据过滤条件spark.read.jdbc("jdbc:mysql://127.0.0.1:3306/mysql", "help_keyword", predicates, props).show() //输出：+---------------+-----------+|help_keyword_id| name|+---------------+-----------+| 0| &lt;&gt;|| 1| ACTION|| 2| ADD|| 3|AES_DECRYPT|| 4|AES_ENCRYPT|| 5| AFTER|| 6| AGAINST|| 7| AGGREGATE|| 8| ALGORITHM|| 9| ALL|| 604| WHEN|+---------------+-----------+ 可以使用 numPartitions 指定读取数据的并行度： 1option("numPartitions", 10) 在这里，除了可以指定分区外，还可以设置上界和下界，任何小于下界的值都会被分配在第一个分区中，任何大于上界的值都会被分配在最后一个分区中。 123456val colName = "help_keyword_id" //用于判断上下界的列val lowerBound = 300L //下界val upperBound = 500L //上界val numPartitions = 10 //分区综述val jdbcDf = spark.read.jdbc("jdbc:mysql://127.0.0.1:3306/mysql","help_keyword", colName,lowerBound,upperBound,numPartitions,props) 想要验证分区内容，可以使用 mapPartitionsWithIndex 这个算子，代码如下： 1234567jdbcDf.rdd.mapPartitionsWithIndex((index, iterator) =&gt; &#123; val buffer = new ListBuffer[String] while (iterator.hasNext) &#123; buffer.append(index + "分区:" + iterator.next()) &#125; buffer.toIterator&#125;).foreach(println) 执行结果如下：help_keyword 这张表只有 600 条左右的数据，本来数据应该均匀分布在 10 个分区，但是 0 分区里面却有 319 条数据，这是因为设置了下限，所有小于 300 的数据都会被限制在第一个分区，即 0 分区。同理所有大于 500 的数据被分配在 9 分区，即最后一个分区。 6.2 写入数据1234567val df = spark.read.format("json").load("/usr/file/json/emp.json")df.write.format("jdbc").option("url", "jdbc:mysql://127.0.0.1:3306/mysql").option("user", "root").option("password", "root").option("dbtable", "emp").save() 七、TextText 文件在读写性能方面并没有任何优势，且不能表达明确的数据结构，所以其使用的比较少，读写操作如下： 7.1 读取Text数据1spark.read.textFile("/usr/file/txt/dept.txt").show() 7.2 写入Text数据1df.write.text("/tmp/spark/txt/dept") 八、数据读写高级特性8.1 并行读多个 Executors 不能同时读取同一个文件，但它们可以同时读取不同的文件。这意味着当您从一个包含多个文件的文件夹中读取数据时，这些文件中的每一个都将成为 DataFrame 中的一个分区，并由可用的 Executors 并行读取。 8.2 并行写写入的文件或数据的数量取决于写入数据时 DataFrame 拥有的分区数量。默认情况下，每个数据分区写一个文件。 8.3 分区写入分区和分桶这两个概念和 Hive 中分区表和分桶表是一致的。都是将数据按照一定规则进行拆分存储。需要注意的是 partitionBy 指定的分区和 RDD 中分区不是一个概念：这里的分区表现为输出目录的子目录，数据分别存储在对应的子目录中。 12val df = spark.read.format("json").load("/usr/file/json/emp.json")df.write.mode("overwrite").partitionBy("deptno").save("/tmp/spark/partitions") 输出结果如下：可以看到输出被按照部门编号分为三个子目录，子目录中才是对应的输出文件。 8.3 分桶写入分桶写入就是将数据按照指定的列和桶数进行散列，目前分桶写入只支持保存为表，实际上这就是 Hive 的分桶表。 1234val numberBuckets = 10val columnToBucketBy = "empno"df.write.format("parquet").mode("overwrite").bucketBy(numberBuckets, columnToBucketBy).saveAsTable("bucketedFiles") 8.5 文件大小管理如果写入产生小文件数量过多，这时会产生大量的元数据开销。Spark 和 HDFS 一样，都不能很好的处理这个问题，这被称为“small file problem”。同时数据文件也不能过大，否则在查询时会有不必要的性能开销，因此要把文件大小控制在一个合理的范围内。 在上文我们已经介绍过可以通过分区数量来控制生成文件的数量，从而间接控制文件大小。Spark 2.2 引入了一种新的方法，以更自动化的方式控制文件大小，这就是 maxRecordsPerFile 参数，它允许你通过控制写入文件的记录数来控制文件大小。 12 // Spark 将确保文件最多包含 5000 条记录df.write.option(“maxRecordsPerFile”, 5000) 九、可选配置附录9.1 CSV读写可选配置 读\写操作 配置项 可选值 默认值 描述 Both seq 任意字符 ,(逗号) 分隔符 Both header true, false false 文件中的第一行是否为列的名称。 Read escape 任意字符 \ 转义字符 Read inferSchema true, false false 是否自动推断列类型 Read ignoreLeadingWhiteSpace true, false false 是否跳过值前面的空格 Both ignoreTrailingWhiteSpace true, false false 是否跳过值后面的空格 Both nullValue 任意字符 “” 声明文件中哪个字符表示空值 Both nanValue 任意字符 NaN 声明哪个值表示 NaN 或者缺省值 Both positiveInf 任意字符 Inf 正无穷 Both negativeInf 任意字符 -Inf 负无穷 Both compression or codec None,uncompressed,bzip2, deflate,gzip, lz4, orsnappy none 文件压缩格式 Both dateFormat 任何能转换为 Java 的 SimpleDataFormat 的字符串 yyyy-MM-dd 日期格式 Both timestampFormat 任何能转换为 Java 的 SimpleDataFormat 的字符串 yyyy-MMdd’T’HH:mm:ss.SSSZZ 时间戳格式 Read maxColumns 任意整数 20480 声明文件中的最大列数 Read maxCharsPerColumn 任意整数 1000000 声明一个列中的最大字符数。 Read escapeQuotes true, false true 是否应该转义行中的引号。 Read maxMalformedLogPerPartition 任意整数 10 声明每个分区中最多允许多少条格式错误的数据，超过这个值后格式错误的数据将不会被读取 Write quoteAll true, false false 指定是否应该将所有值都括在引号中，而不只是转义具有引号字符的值。 Read multiLine true, false false 是否允许每条完整记录跨域多行 9.2 JSON读写可选配置 读\写操作 配置项 可选值 默认值 Both compression or codec None,uncompressed,bzip2, deflate,gzip, lz4, orsnappy none Both dateFormat 任何能转换为 Java 的 SimpleDataFormat 的字符串 yyyy-MM-dd Both timestampFormat 任何能转换为 Java 的 SimpleDataFormat 的字符串 yyyy-MMdd’T’HH:mm:ss.SSSZZ Read primitiveAsString true, false false Read allowComments true, false false Read allowUnquotedFieldNames true, false false Read allowSingleQuotes true, false true Read allowNumericLeadingZeros true, false false Read allowBackslashEscapingAnyCharacter true, false false Read columnNameOfCorruptRecord true, false Value of spark.sql.column&amp;NameOf Read multiLine true, false false 9.3 数据库读写可选配置 属性名称 含义 url 数据库地址 dbtable 表名称 driver 数据库驱动 partitionColumn,lowerBound, upperBoun 分区总数，上界，下界 numPartitions 可用于表读写并行性的最大分区数。如果要写的分区数量超过这个限制，那么可以调用 coalesce(numpartition) 重置分区数。 fetchsize 每次往返要获取多少行数据。此选项仅适用于读取数据。 batchsize 每次往返插入多少行数据，这个选项只适用于写入数据。默认值是 1000。 isolationLevel 事务隔离级别：可以是 NONE，READ_COMMITTED, READ_UNCOMMITTED，REPEATABLE_READ 或 SERIALIZABLE，即标准事务隔离级别。默认值是 READ_UNCOMMITTED。这个选项只适用于数据读取。 createTableOptions 写入数据时自定义创建表的相关配置 createTableColumnTypes 写入数据时自定义创建列的列类型 数据库读写更多配置可以参阅官方文档：https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html 参考资料 Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 https://spark.apache.org/docs/latest/sql-data-sources.html]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark_Structured_API的基本使用]]></title>
    <url>%2F2019%2F06%2F25%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark_Structured_API%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Structured API基本使用一、创建DataFrame和Dataset二、Columns列操作三、使用Structured API进行基本查询四、使用Spark SQL进行基本查询 一、创建DataFrame和Dataset1.1 创建DataFrameSpark 中所有功能的入口点是 SparkSession，可以使用 SparkSession.builder() 创建。创建后应用程序就可以从现有 RDD，Hive 表或 Spark 数据源创建 DataFrame。示例如下： 123456val spark = SparkSession.builder().appName("Spark-SQL").master("local[2]").getOrCreate()val df = spark.read.json("/usr/file/json/emp.json")df.show()// 建议在进行 spark SQL 编程前导入下面的隐式转换，因为 DataFrames 和 dataSets 中很多操作都依赖了隐式转换import spark.implicits._ 可以使用 spark-shell 进行测试，需要注意的是 spark-shell 启动后会自动创建一个名为 spark 的 SparkSession，在命令行中可以直接引用即可： 1.2 创建DatasetSpark 支持由内部数据集和外部数据集来创建 DataSet，其创建方式分别如下： 1. 由外部数据集创建12345678910// 1.需要导入隐式转换import spark.implicits._// 2.创建 case class,等价于 Java Beancase class Emp(ename: String, comm: Double, deptno: Long, empno: Long, hiredate: String, job: String, mgr: Long, sal: Double)// 3.由外部数据集创建 Datasetsval ds = spark.read.json("/usr/file/emp.json").as[Emp]ds.show() 2. 由内部数据集创建123456789101112// 1.需要导入隐式转换import spark.implicits._// 2.创建 case class,等价于 Java Beancase class Emp(ename: String, comm: Double, deptno: Long, empno: Long, hiredate: String, job: String, mgr: Long, sal: Double)// 3.由内部数据集创建 Datasetsval caseClassDS = Seq(Emp("ALLEN", 300.0, 30, 7499, "1981-02-20 00:00:00", "SALESMAN", 7698, 1600.0), Emp("JONES", 300.0, 30, 7499, "1981-02-20 00:00:00", "SALESMAN", 7698, 1600.0)) .toDS()caseClassDS.show() 1.3 由RDD创建DataFrameSpark 支持两种方式把 RDD 转换为 DataFrame，分别是使用反射推断和指定 Schema 转换： 1. 使用反射推断123456789101112// 1.导入隐式转换import spark.implicits._// 2.创建部门类case class Dept(deptno: Long, dname: String, loc: String)// 3.创建 RDD 并转换为 dataSetval rddToDS = spark.sparkContext .textFile("/usr/file/dept.txt") .map(_.split("\t")) .map(line =&gt; Dept(line(0).trim.toLong, line(1), line(2))) .toDS() // 如果调用 toDF() 则转换为 dataFrame 2. 以编程方式指定Schema1234567891011121314151617181920import org.apache.spark.sql.Rowimport org.apache.spark.sql.types._// 1.定义每个列的列类型val fields = Array(StructField("deptno", LongType, nullable = true), StructField("dname", StringType, nullable = true), StructField("loc", StringType, nullable = true))// 2.创建 schemaval schema = StructType(fields)// 3.创建 RDDval deptRDD = spark.sparkContext.textFile("/usr/file/dept.txt")val rowRDD = deptRDD.map(_.split("\t")).map(line =&gt; Row(line(0).toLong, line(1), line(2)))// 4.将 RDD 转换为 dataFrameval deptDF = spark.createDataFrame(rowRDD, schema)deptDF.show() 1.4 DataFrames与Datasets互相转换Spark 提供了非常简单的转换方法用于 DataFrame 与 Dataset 间的互相转换，示例如下： 1234567# DataFrames转Datasetsscala&gt; df.as[Emp]res1: org.apache.spark.sql.Dataset[Emp] = [COMM: double, DEPTNO: bigint ... 6 more fields]# Datasets转DataFramesscala&gt; ds.toDF()res2: org.apache.spark.sql.DataFrame = [COMM: double, DEPTNO: bigint ... 6 more fields] 二、Columns列操作2.1 引用列Spark 支持多种方法来构造和引用列，最简单的是使用 col() 或 column() 函数。 123456col("colName")column("colName")// 对于 Scala 语言而言，还可以使用$"myColumn"和'myColumn 这两种语法糖进行引用。df.select($"ename", $"job").show()df.select('ename, 'job).show() 2.2 新增列1234// 基于已有列值新增列df.withColumn("upSal",$"sal"+1000)// 基于固定值新增列df.withColumn("intCol",lit(1000)) 2.3 删除列12// 支持删除多个列df.drop("comm","job").show() 2.4 重命名列1df.withColumnRenamed("comm", "common").show() 需要说明的是新增，删除，重命名列都会产生新的 DataFrame，原来的 DataFrame 不会被改变。 三、使用Structured API进行基本查询1234567891011121314151617// 1.查询员工姓名及工作df.select($"ename", $"job").show()// 2.filter 查询工资大于 2000 的员工信息df.filter($"sal" &gt; 2000).show()// 3.orderBy 按照部门编号降序，工资升序进行查询df.orderBy(desc("deptno"), asc("sal")).show()// 4.limit 查询工资最高的 3 名员工的信息df.orderBy(desc("sal")).limit(3).show()// 5.distinct 查询所有部门编号df.select("deptno").distinct().show()// 6.groupBy 分组统计部门人数df.groupBy("deptno").count().show() 四、使用Spark SQL进行基本查询4.1 Spark SQL基本使用1234567891011121314151617181920// 1.首先需要将 DataFrame 注册为临时视图df.createOrReplaceTempView("emp")// 2.查询员工姓名及工作spark.sql("SELECT ename,job FROM emp").show()// 3.查询工资大于 2000 的员工信息spark.sql("SELECT * FROM emp where sal &gt; 2000").show()// 4.orderBy 按照部门编号降序，工资升序进行查询spark.sql("SELECT * FROM emp ORDER BY deptno DESC,sal ASC").show()// 5.limit 查询工资最高的 3 名员工的信息spark.sql("SELECT * FROM emp ORDER BY sal DESC LIMIT 3").show()// 6.distinct 查询所有部门编号spark.sql("SELECT DISTINCT(deptno) FROM emp").show()// 7.分组统计部门人数spark.sql("SELECT deptno,count(ename) FROM emp group by deptno").show() 4.2 全局临时视图上面使用 createOrReplaceTempView 创建的是会话临时视图，它的生命周期仅限于会话范围，会随会话的结束而结束。 你也可以使用 createGlobalTempView 创建全局临时视图，全局临时视图可以在所有会话之间共享，并直到整个 Spark 应用程序终止后才会消失。全局临时视图被定义在内置的 global_temp 数据库下，需要使用限定名称进行引用，如 SELECT * FROM global_temp.view1。 12345// 注册为全局临时视图df.createGlobalTempView("gemp")// 使用限定名称进行引用spark.sql("SELECT ename,job FROM global_temp.gemp").show() 参考资料Spark SQL, DataFrames and Datasets Guide &gt; Getting Started]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>API使用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之SparkSQL_Dataset和DataFrame简介]]></title>
    <url>%2F2019%2F06%2F25%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSparkSQL_Dataset%E5%92%8CDataFrame%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[DataFrame和Dataset简介一、Spark SQL简介二、DataFrame &amp; DataSet 2.1 DataFrame 2.2 DataFrame 对比 RDDs2.3 DataSet2.4 静态类型与运行时类型安全2.5 Untyped &amp; Typed 三、DataFrame &amp; DataSet &amp; RDDs 总结四、Spark SQL的运行原理4.1 逻辑计划(Logical Plan)4.2 物理计划(Physical Plan) 4.3 执行 一、Spark SQL简介Spark SQL 是 Spark 中的一个子模块，主要用于操作结构化数据。它具有以下特点： 能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询； 支持多种开发语言； 支持多达上百种的外部数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC 等； 支持 HiveQL 语法以及 Hive SerDes 和 UDF，允许你访问现有的 Hive 仓库； 支持标准的 JDBC 和 ODBC 连接； 支持优化器，列式存储和代码生成等特性； 支持扩展并能保证容错。 二、DataFrame &amp; DataSet2.1 DataFrame为了支持结构化数据的处理，Spark SQL 提供了新的数据结构 DataFrame。DataFrame 是一个由具名列组成的数据集。它在概念上等同于关系数据库中的表或 R/Python 语言中的 data frame。 由于 Spark SQL 支持多种语言的开发，所以每种语言都定义了 DataFrame 的抽象，主要如下： 语言 主要抽象 Scala Dataset[T] &amp; DataFrame (Dataset[Row] 的别名) Java Dataset[T] Python DataFrame R DataFrame 2.2 DataFrame 对比 RDDsDataFrame 和 RDDs 最主要的区别在于一个面向的是结构化数据，一个面向的是非结构化数据，它们内部的数据结构如下： DataFrame 内部的有明确 Scheme 结构，即列名、列字段类型都是已知的，这带来的好处是可以减少数据读取以及更好地优化执行计划，从而保证查询效率。 DataFrame 和 RDDs 应该如何选择？ 如果你想使用函数式编程而不是 DataFrame API，则使用 RDDs； 如果你的数据是非结构化的 (比如流媒体或者字符流)，则使用 RDDs， 如果你的数据是结构化的 (如 RDBMS 中的数据) 或者半结构化的 (如日志)，出于性能上的考虑，应优先使用 DataFrame。 2.3 DataSetDataset 也是分布式的数据集合，在 Spark 1.6 版本被引入，它集成了 RDD 和 DataFrame 的优点，具备强类型的特点，同时支持 Lambda 函数，但只能在 Scala 和 Java 语言中使用。在 Spark 2.0 后，为了方便开发者，Spark 将 DataFrame 和 Dataset 的 API 融合到一起，提供了结构化的 API(Structured API)，即用户可以通过一套标准的 API 就能完成对两者的操作。 这里注意一下：DataFrame 被标记为 Untyped API，而 DataSet 被标记为 Typed API，后文会对两者做出解释。 2.4 静态类型与运行时类型安全静态类型 (Static-typing) 与运行时类型安全 (runtime type-safety) 主要表现如下: 在实际使用中，如果你用的是 Spark SQL 的查询语句，则直到运行时你才会发现有语法错误，而如果你用的是 DataFrame 和 Dataset，则在编译时就可以发现错误 (这节省了开发时间和整体代价)。DataFrame 和 Dataset 主要区别在于： 在 DataFrame 中，当你调用了 API 之外的函数，编译器就会报错，但如果你使用了一个不存在的字段名字，编译器依然无法发现。而 Dataset 的 API 都是用 Lambda 函数和 JVM 类型对象表示的，所有不匹配的类型参数在编译时就会被发现。 以上这些最终都被解释成关于类型安全图谱，对应开发中的语法和分析错误。在图谱中，Dataset 最严格，但对于开发者来说效率最高。 上面的描述可能并没有那么直观，下面的给出一个 IDEA 中代码编译的示例： 这里一个可能的疑惑是 DataFrame 明明是有确定的 Scheme 结构 (即列名、列字段类型都是已知的)，但是为什么还是无法对列名进行推断和错误判断，这是因为 DataFrame 是 Untyped 的。 2.5 Untyped &amp; Typed在上面我们介绍过 DataFrame API 被标记为 Untyped API，而 DataSet API 被标记为 Typed API。DataFrame 的 Untyped 是相对于语言或 API 层面而言，它确实有明确的 Scheme 结构，即列名，列类型都是确定的，但这些信息完全由 Spark 来维护，Spark 只会在运行时检查这些类型和指定类型是否一致。这也就是为什么在 Spark 2.0 之后，官方推荐把 DataFrame 看做是 DatSet[Row]，Row 是 Spark 中定义的一个 trait，其子类中封装了列字段的信息。 相对而言，DataSet 是 Typed 的，即强类型。如下面代码，DataSet 的类型由 Case Class(Scala) 或者 Java Bean(Java) 来明确指定的，在这里即每一行数据代表一个 Person，这些信息由 JVM 来保证正确性，所以字段名错误和类型错误在编译的时候就会被 IDE 所发现。 12case class Person(name: String, age: Long)val dataSet: Dataset[Person] = spark.read.json("people.json").as[Person] 三、DataFrame &amp; DataSet &amp; RDDs 总结这里对三者做一下简单的总结： RDDs 适合非结构化数据的处理，而 DataFrame &amp; DataSet 更适合结构化数据和半结构化的处理； DataFrame &amp; DataSet 可以通过统一的 Structured API 进行访问，而 RDDs 则更适合函数式编程的场景； 相比于 DataFrame 而言，DataSet 是强类型的 (Typed)，有着更为严格的静态类型检查； DataSets、DataFrames、SQL 的底层都依赖了 RDDs API，并对外提供结构化的访问接口。 四、Spark SQL的运行原理DataFrame、DataSet 和 Spark SQL 的实际执行流程都是相同的： 进行 DataFrame/Dataset/SQL 编程； 如果是有效的代码，即代码没有编译错误，Spark 会将其转换为一个逻辑计划； Spark 将此逻辑计划转换为物理计划，同时进行代码优化； Spark 然后在集群上执行这个物理计划 (基于 RDD 操作) 。 4.1 逻辑计划(Logical Plan)执行的第一个阶段是将用户代码转换成一个逻辑计划。它首先将用户代码转换成 unresolved logical plan(未解决的逻辑计划)，之所以这个计划是未解决的，是因为尽管您的代码在语法上是正确的，但是它引用的表或列可能不存在。 Spark 使用 analyzer(分析器) 基于 catalog(存储的所有表和 DataFrames 的信息) 进行解析。解析失败则拒绝执行，解析成功则将结果传给 Catalyst 优化器 (Catalyst Optimizer)，优化器是一组规则的集合，用于优化逻辑计划，通过谓词下推等方式进行优化，最终输出优化后的逻辑执行计划。 4.2 物理计划(Physical Plan)得到优化后的逻辑计划后，Spark 就开始了物理计划过程。 它通过生成不同的物理执行策略，并通过成本模型来比较它们，从而选择一个最优的物理计划在集群上面执行的。物理规划的输出结果是一系列的 RDDs 和转换关系 (transformations)。 4.3 执行在选择一个物理计划后，Spark 运行其 RDDs 代码，并在运行时执行进一步的优化，生成本地 Java 字节码，最后将运行结果返回给用户。 参考资料 Matei Zaharia, Bill Chambers . Spark: The Definitive Guide[M] . 2018-02 Spark SQL, DataFrames and Datasets Guide 且谈 Apache Spark 的 API 三剑客：RDD、DataFrame 和 Dataset(译文) A Tale of Three Apache Spark APIs: RDDs vs DataFrames and Datasets(原文)]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>SparkSQL</tag>
        <tag>Dataset</tag>
        <tag>DataFrame</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark集群环境搭建]]></title>
    <url>%2F2019%2F06%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[基于ZooKeeper搭建Spark高可用集群一、集群规划二、前置条件三、Spark集群搭建3.1 下载解压3.2 配置环境变量3.3 集群配置3.4 安装包分发四、启动集群4.1 启动ZooKeeper集群4.2 启动Hadoop集群4.3 启动Spark集群4.4 查看服务五、验证集群高可用六、提交作业 一、集群规划这里搭建一个 3 节点的 Spark 集群，其中三台主机上均部署 Worker 服务。同时为了保证高可用，除了在 hadoop001 上部署主 Master 服务外，还在 hadoop002 和 hadoop003 上分别部署备用的 Master 服务，Master 服务由 Zookeeper 集群进行协调管理，如果主 Master 不可用，则备用 Master 会成为新的主 Master。 二、前置条件搭建 Spark 集群前，需要保证 JDK 环境、Zookeeper 集群和 Hadoop 集群已经搭建，相关步骤可以参阅： Linux 环境下 JDK 安装 Zookeeper 单机环境和集群环境搭建 Hadoop 集群环境搭建 三、Spark集群搭建3.1 下载解压下载所需版本的 Spark，官网下载地址：http://spark.apache.org/downloads.html 下载后进行解压： 1# tar -zxvf spark-2.2.3-bin-hadoop2.6.tgz 3.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export SPARK_HOME=/usr/app/spark-2.2.3-bin-hadoop2.6export PATH=$&#123;SPARK_HOME&#125;/bin:$PATH 使得配置的环境变量立即生效： 1# source /etc/profile 3.3 集群配置进入 ${SPARK_HOME}/conf 目录，拷贝配置样本进行修改： 1. spark-env.sh1cp spark-env.sh.template spark-env.sh 123456# 配置JDK安装位置JAVA_HOME=/usr/java/jdk1.8.0_201# 配置hadoop配置文件的位置HADOOP_CONF_DIR=/usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop# 配置zookeeper地址SPARK_DAEMON_JAVA_OPTS="-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=hadoop001:2181,hadoop002:2181,hadoop003:2181 -Dspark.deploy.zookeeper.dir=/spark" 2. slaves1cp slaves.template slaves 配置所有 Woker 节点的位置： 123hadoop001hadoop002hadoop003 3.4 安装包分发将 Spark 的安装包分发到其他服务器，分发后建议在这两台服务器上也配置一下 Spark 的环境变量。 12scp -r /usr/app/spark-2.4.0-bin-hadoop2.6/ hadoop002:usr/app/scp -r /usr/app/spark-2.4.0-bin-hadoop2.6/ hadoop003:usr/app/ 四、启动集群4.1 启动ZooKeeper集群分别到三台服务器上启动 ZooKeeper 服务： 1zkServer.sh start 4.2 启动Hadoop集群1234# 启动dfs服务start-dfs.sh# 启动yarn服务start-yarn.sh 4.3 启动Spark集群进入 hadoop001 的 ${SPARK_HOME}/sbin 目录下，执行下面命令启动集群。执行命令后，会在 hadoop001 上启动 Maser 服务，会在 slaves 配置文件中配置的所有节点上启动 Worker 服务。 1start-all.sh 分别在 hadoop002 和 hadoop003 上执行下面的命令，启动备用的 Master 服务： 12# $&#123;SPARK_HOME&#125;/sbin 下执行start-master.sh 4.4 查看服务查看 Spark 的 Web-UI 页面，端口为 8080。此时可以看到 hadoop001 上的 Master 节点处于 ALIVE 状态，并有 3 个可用的 Worker 节点。 而 hadoop002 和 hadoop003 上的 Master 节点均处于 STANDBY 状态，没有可用的 Worker 节点。 五、验证集群高可用此时可以使用 kill 命令杀死 hadoop001 上的 Master 进程，此时备用 Master 会中会有一个再次成为 主 Master，我这里是 hadoop002，可以看到 hadoop2 上的 Master 经过 RECOVERING 后成为了新的主 Master，并且获得了全部可以用的 Workers。 Hadoop002 上的 Master 成为主 Master，并获得了全部可以用的 Workers。 此时如果你再在 hadoop001 上使用 start-master.sh 启动 Master 服务，那么其会作为备用 Master 存在。 六、提交作业和单机环境下的提交到 Yarn 上的命令完全一致，这里以 Spark 内置的计算 Pi 的样例程序为例，提交命令如下： 12345678spark-submit \--class org.apache.spark.examples.SparkPi \--master yarn \--deploy-mode client \--executor-memory 1G \--num-executors 10 \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark累加器与广播变量]]></title>
    <url>%2F2019%2F06%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark%E7%B4%AF%E5%8A%A0%E5%99%A8%E4%B8%8E%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F%2F</url>
    <content type="text"><![CDATA[Spark 累加器与广播变量一、简介二、累加器2.1 理解闭包2.2 使用累加器三、广播变量 一、简介在 Spark 中，提供了两种类型的共享变量：累加器 (accumulator) 与广播变量 (broadcast variable)： 累加器：用来对信息进行聚合，主要用于累计计数等场景； 广播变量：主要用于在节点间高效分发大对象。 二、累加器这里先看一个具体的场景，对于正常的累计求和，如果在集群模式中使用下面的代码进行计算，会发现执行结果并非预期： 1234var counter = 0val data = Array(1, 2, 3, 4, 5)sc.parallelize(data).foreach(x =&gt; counter += x) println(counter) counter 最后的结果是 0，导致这个问题的主要原因是闭包。 2.1 理解闭包1. Scala 中闭包的概念 这里先介绍一下 Scala 中关于闭包的概念： 12var more = 10val addMore = (x: Int) =&gt; x + more 如上函数 addMore 中有两个变量 x 和 more: x : 是一个绑定变量 (bound variable)，因为其是该函数的入参，在函数的上下文中有明确的定义； more : 是一个自由变量 (free variable)，因为函数字面量本生并没有给 more 赋予任何含义。 按照定义：在创建函数时，如果需要捕获自由变量，那么包含指向被捕获变量的引用的函数就被称为闭包函数。 2. Spark 中的闭包 在实际计算时，Spark 会将对 RDD 操作分解为 Task，Task 运行在 Worker Node 上。在执行之前，Spark 会对任务进行闭包，如果闭包内涉及到自由变量，则程序会进行拷贝，并将副本变量放在闭包中，之后闭包被序列化并发送给每个执行者。因此，当在 foreach 函数中引用 counter 时，它将不再是 Driver 节点上的 counter，而是闭包中的副本 counter，默认情况下，副本 counter 更新后的值不会回传到 Driver，所以 counter 的最终值仍然为零。 需要注意的是：在 Local 模式下，有可能执行 foreach 的 Worker Node 与 Diver 处在相同的 JVM，并引用相同的原始 counter，这时候更新可能是正确的，但是在集群模式下一定不正确。所以在遇到此类问题时应优先使用累加器。 累加器的原理实际上很简单：就是将每个副本变量的最终值传回 Driver，由 Driver 聚合后得到最终值，并更新原始变量。 2.2 使用累加器SparkContext 中定义了所有创建累加器的方法，需要注意的是：被中横线划掉的累加器方法在 Spark 2.0.0 之后被标识为废弃。 使用示例和执行结果分别如下： 123456val data = Array(1, 2, 3, 4, 5)// 定义累加器val accum = sc.longAccumulator("My Accumulator")sc.parallelize(data).foreach(x =&gt; accum.add(x))// 获取累加器的值accum.value 三、广播变量在上面介绍中闭包的过程中我们说道每个 Task 任务的闭包都会持有自由变量的副本，如果变量很大且 Task 任务很多的情况下，这必然会对网络 IO 造成压力，为了解决这个情况，Spark 提供了广播变量。 广播变量的做法很简单：就是不把副本变量分发到每个 Task 中，而是将其分发到每个 Executor，Executor 中的所有 Task 共享一个副本变量。 1234// 把一个数组定义为一个广播变量val broadcastVar = sc.broadcast(Array(1, 2, 3, 4, 5))// 之后用到该数组时应优先使用广播变量，而不是原值sc.parallelize(broadcastVar.value).map(_ * 10).collect() 参考资料RDD Programming Guide]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>广播变量</tag>
        <tag>累加器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark部署模式与作业提交]]></title>
    <url>%2F2019%2F06%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark%E9%83%A8%E7%BD%B2%E6%A8%A1%E5%BC%8F%E4%B8%8E%E4%BD%9C%E4%B8%9A%E6%8F%90%E4%BA%A4%2F</url>
    <content type="text"><![CDATA[Spark部署模式与作业提交一、作业提交二、Local模式三、Standalone模式三、Spark on Yarn模式 一、作业提交1.1 spark-submitSpark 所有模式均使用 spark-submit 命令提交作业，其格式如下： 12345678./bin/spark-submit \ --class &lt;main-class&gt; \ # 应用程序主入口类 --master &lt;master-url&gt; \ # 集群的 Master Url --deploy-mode &lt;deploy-mode&gt; \ # 部署模式 --conf &lt;key&gt;=&lt;value&gt; \ # 可选配置 ... # other options &lt;application-jar&gt; \ # Jar 包路径 [application-arguments] #传递给主入口类的参数 需要注意的是：在集群环境下，application-jar 必须能被集群中所有节点都能访问，可以是 HDFS 上的路径；也可以是本地文件系统路径，如果是本地文件系统路径，则要求集群中每一个机器节点上的相同路径都存在该 Jar 包。 1.2 deploy-modedeploy-mode 有 cluster 和 client 两个可选参数，默认为 client。这里以 Spark On Yarn 模式对两者进行说明 ： 在 cluster 模式下，Spark Drvier 在应用程序的 Master 进程内运行，该进程由群集上的 YARN 管理，提交作业的客户端可以在启动应用程序后关闭； 在 client 模式下，Spark Drvier 在提交作业的客户端进程中运行，Master 进程仅用于从 YARN 请求资源。 1.3 master-urlmaster-url 的所有可选参数如下表所示： Master URL Meaning local 使用一个线程本地运行 Spark local[K] 使用 K 个 worker 线程本地运行 Spark local[K,F] 使用 K 个 worker 线程本地运行 , 第二个参数为 Task 的失败重试次数 local[*] 使用与 CPU 核心数一样的线程数在本地运行 Spark local[*,F] 使用与 CPU 核心数一样的线程数在本地运行 Spark第二个参数为 Task 的失败重试次数 spark://HOST:PORT 连接至指定的 standalone 集群的 master 节点。端口号默认是 7077。 spark://HOST1:PORT1,HOST2:PORT2 如果 standalone 集群采用 Zookeeper 实现高可用，则必须包含由 zookeeper 设置的所有 master 主机地址。 mesos://HOST:PORT 连接至给定的 Mesos 集群。端口默认是 5050。对于使用了 ZooKeeper 的 Mesos cluster 来说，使用 mesos://zk://... 来指定地址，使用 --deploy-mode cluster 模式来提交。 yarn 连接至一个 YARN 集群，集群由配置的 HADOOP_CONF_DIR 或者 YARN_CONF_DIR 来决定。使用 --deploy-mode 参数来配置 client 或 cluster 模式。 下面主要介绍三种常用部署模式及对应的作业提交方式。 二、Local模式Local 模式下提交作业最为简单，不需要进行任何配置，提交命令如下： 123456# 本地模式提交应用spark-submit \--class org.apache.spark.examples.SparkPi \--master local[2] \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100 # 传给 SparkPi 的参数 spark-examples_2.11-2.4.0.jar 是 Spark 提供的测试用例包，SparkPi 用于计算 Pi 值，执行结果如下： 三、Standalone模式Standalone 是 Spark 提供的一种内置的集群模式，采用内置的资源管理器进行管理。下面按照如图所示演示 1 个 Mater 和 2 个 Worker 节点的集群配置，这里使用两台主机进行演示： hadoop001： 由于只有两台主机，所以 hadoop001 既是 Master 节点，也是 Worker 节点; hadoop002 ： Worker 节点。 3.1 环境配置首先需要保证 Spark 已经解压在两台主机的相同路径上。然后进入 hadoop001 的 ${SPARK_HOME}/conf/ 目录下，拷贝配置样本并进行相关配置： 1# cp spark-env.sh.template spark-env.sh 在 spark-env.sh 中配置 JDK 的目录，完成后将该配置使用 scp 命令分发到 hadoop002 上： 12# JDK安装位置JAVA_HOME=/usr/java/jdk1.8.0_201 3.2 集群配置在 ${SPARK_HOME}/conf/ 目录下，拷贝集群配置样本并进行相关配置： 1# cp slaves.template slaves 指定所有 Worker 节点的主机名： 123# A Spark Worker will be started on each of the machines listed below.hadoop001hadoop002 这里需要注意以下三点： 主机名与 IP 地址的映射必须在 /etc/hosts 文件中已经配置，否则就直接使用 IP 地址； 每个主机名必须独占一行； Spark 的 Master 主机是通过 SSH 访问所有的 Worker 节点，所以需要预先配置免密登录。 3.3 启动使用 start-all.sh 代表启动 Master 和所有 Worker 服务。 1./sbin/start-master.sh 访问 8080 端口，查看 Spark 的 Web-UI 界面,，此时应该显示有两个有效的工作节点： 3.4 提交作业12345678910111213141516171819# 以client模式提交到standalone集群 spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://hadoop001:7077 \--executor-memory 2G \--total-executor-cores 10 \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100# 以cluster模式提交到standalone集群 spark-submit \--class org.apache.spark.examples.SparkPi \--master spark://207.184.161.138:7077 \--deploy-mode cluster \--supervise \ # 配置此参数代表开启监督，如果主应用程序异常退出，则自动重启 Driver--executor-memory 2G \--total-executor-cores 10 \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100 3.5 可选配置在虚拟机上提交作业时经常出现一个的问题是作业无法申请到足够的资源： 12Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources 这时候可以查看 Web UI，我这里是内存空间不足：提交命令中要求作业的 executor-memory 是 2G，但是实际的工作节点的 Memory 只有 1G，这时候你可以修改 --executor-memory，也可以修改 Woker 的 Memory，其默认值为主机所有可用内存值减去 1G。 关于 Master 和 Woker 节点的所有可选配置如下，可以在 spark-env.sh 中进行对应的配置： Environment Variable（环境变量） Meaning（含义） SPARK_MASTER_HOST master 节点地址 SPARK_MASTER_PORT master 节点地址端口（默认：7077） SPARK_MASTER_WEBUI_PORT master 的 web UI 的端口（默认：8080） SPARK_MASTER_OPTS 仅用于 master 的配置属性，格式是 “-Dx=y”（默认：none）,所有属性可以参考官方文档：spark-standalone-mode SPARK_LOCAL_DIRS spark 的临时存储的目录，用于暂存 map 的输出和持久化存储 RDDs。多个目录用逗号分隔 SPARK_WORKER_CORES spark worker 节点可以使用 CPU Cores 的数量。（默认：全部可用） SPARK_WORKER_MEMORY spark worker 节点可以使用的内存数量（默认：全部的内存减去 1GB）； SPARK_WORKER_PORT spark worker 节点的端口（默认： random（随机）） SPARK_WORKER_WEBUI_PORT worker 的 web UI 的 Port（端口）（默认：8081） SPARK_WORKER_DIR worker 运行应用程序的目录，这个目录中包含日志和暂存空间（default：SPARK_HOME/work） SPARK_WORKER_OPTS 仅用于 worker 的配置属性，格式是 “-Dx=y”（默认：none）。所有属性可以参考官方文档：spark-standalone-mode SPARK_DAEMON_MEMORY 分配给 spark master 和 worker 守护进程的内存。（默认： 1G） SPARK_DAEMON_JAVA_OPTS spark master 和 worker 守护进程的 JVM 选项，格式是 “-Dx=y”（默认：none） SPARK_PUBLIC_DNS spark master 和 worker 的公开 DNS 名称。（默认：none） 三、Spark on Yarn模式Spark 支持将作业提交到 Yarn 上运行，此时不需要启动 Master 节点，也不需要启动 Worker 节点。 3.1 配置在 spark-env.sh 中配置 hadoop 的配置目录的位置，可以使用 YARN_CONF_DIR 或 HADOOP_CONF_DIR 进行指定： 123YARN_CONF_DIR=/usr/app/hadoop-2.6.0-cdh5.15.2/etc/hadoop# JDK安装位置JAVA_HOME=/usr/java/jdk1.8.0_201 3.2 启动必须要保证 Hadoop 已经启动，这里包括 YARN 和 HDFS 都需要启动，因为在计算过程中 Spark 会使用 HDFS 存储临时文件，如果 HDFS 没有启动，则会抛出异常。 12# start-yarn.sh# start-dfs.sh 3.3 提交应用12345678910111213141516171819# 以client模式提交到yarn集群 spark-submit \--class org.apache.spark.examples.SparkPi \--master yarn \--deploy-mode client \--executor-memory 2G \--num-executors 10 \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100# 以cluster模式提交到yarn集群 spark-submit \--class org.apache.spark.examples.SparkPi \--master yarn \--deploy-mode cluster \--executor-memory 2G \--num-executors 10 \/usr/app/spark-2.4.0-bin-hadoop2.6/examples/jars/spark-examples_2.11-2.4.0.jar \100]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>作业提交</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark_Transformation和Action算子]]></title>
    <url>%2F2019%2F06%2F19%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark_Transformation%E5%92%8CAction%E7%AE%97%E5%AD%90%2F</url>
    <content type="text"><![CDATA[Transformation 和 Action 常用算子一、Transformation1.1 map1.2 filter1.3 flatMap1.4 mapPartitions1.5 mapPartitionsWithIndex1.6 sample1.7 union1.8 intersection1.9 distinct1.10 groupByKey1.11 reduceByKey1.12 sortBy &amp; sortByKey 1.13 join1.14 cogroup1.15 cartesian1.16 aggregateByKey二、Action2.1 reduce2.2 takeOrdered2.3 countByKey2.4 saveAsTextFile 一、Transformationspark 常用的 Transformation 算子如下表： Transformation 算子 Meaning（含义） map(func) 对原 RDD 中每个元素运用 func 函数，并生成新的 RDD filter(func) 对原 RDD 中每个元素使用func 函数进行过滤，并生成新的 RDD flatMap(func) 与 map 类似，但是每一个输入的 item 被映射成 0 个或多个输出的 items（ func 返回类型需要为 Seq ）。 mapPartitions(func) 与 map 类似，但函数单独在 RDD 的每个分区上运行， func函数的类型为 Iterator\ =&gt; Iterator\ ，其中 T 是 RDD 的类型，即 RDD[T] mapPartitionsWithIndex(func) 与 mapPartitions 类似，但 func 类型为 (Int, Iterator\) =&gt; Iterator\ ，其中第一个参数为分区索引 sample(withReplacement, fraction, seed) 数据采样，有三个可选参数：设置是否放回（withReplacement）、采样的百分比（fraction）、随机数生成器的种子（seed）； union(otherDataset) 合并两个 RDD intersection(otherDataset) 求两个 RDD 的交集 distinct([numTasks])) 去重 groupByKey([numTasks]) 按照 key 值进行分区，即在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, Iterable\) Note: 如果分组是为了在每一个 key 上执行聚合操作（例如，sum 或 average)，此时使用 reduceByKey 或 aggregateByKey 性能会更好Note: 默认情况下，并行度取决于父 RDD 的分区数。可以传入 numTasks 参数进行修改。 reduceByKey(func, [numTasks]) 按照 key 值进行分组，并对分组后的数据执行归约操作。 aggregateByKey(zeroValue,numPartitions)(seqOp, combOp, [numTasks]) 当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 groupByKey 类似，reduce 任务的数量可通过第二个参数进行配置。 sortByKey([ascending], [numTasks]) 按照 key 进行排序，其中的 key 需要实现 Ordered 特质，即可比较 join(otherDataset, [numTasks]) 在一个 (K, V) 和 (K, W) 类型的 dataset 上调用时，返回一个 (K, (V, W)) pairs 的 dataset，等价于内连接操作。如果想要执行外连接，可以使用 leftOuterJoin, rightOuterJoin 和 fullOuterJoin 等算子。 cogroup(otherDataset, [numTasks]) 在一个 (K, V) 对的 dataset 上调用时，返回一个 (K, (Iterable\, Iterable\)) tuples 的 dataset。 cartesian(otherDataset) 在一个 T 和 U 类型的 dataset 上调用时，返回一个 (T, U) 类型的 dataset（即笛卡尔积）。 coalesce(numPartitions) 将 RDD 中的分区数减少为 numPartitions。 repartition(numPartitions) 随机重新调整 RDD 中的数据以创建更多或更少的分区，并在它们之间进行平衡。 repartitionAndSortWithinPartitions(partitioner) 根据给定的 partitioner（分区器）对 RDD 进行重新分区，并对分区中的数据按照 key 值进行排序。这比调用 repartition 然后再 sorting（排序）效率更高，因为它可以将排序过程推送到 shuffle 操作所在的机器。 下面分别给出这些算子的基本使用示例： 1.1 map1234val list = List(1,2,3)sc.parallelize(list).map(_ * 10).foreach(println)// 输出结果： 10 20 30 （这里为了节省篇幅去掉了换行,后文亦同） 1.2 filter1234val list = List(3, 6, 9, 10, 12, 21)sc.parallelize(list).filter(_ &gt;= 10).foreach(println)// 输出： 10 12 21 1.3 flatMapflatMap(func) 与 map 类似，但每一个输入的 item 会被映射成 0 个或多个输出的 items（ func 返回类型需要为 Seq）。 1234val list = List(List(1, 2), List(3), List(), List(4, 5))sc.parallelize(list).flatMap(_.toList).map(_ * 10).foreach(println)// 输出结果 ： 10 20 30 40 50 flatMap 这个算子在日志分析中使用概率非常高，这里进行一下演示：拆分输入的每行数据为单个单词，并赋值为 1，代表出现一次，之后按照单词分组并统计其出现总次数，代码如下： 12345678910val lines = List("spark flume spark", "hadoop flume hive")sc.parallelize(lines).flatMap(line =&gt; line.split(" ")).map(word=&gt;(word,1)).reduceByKey(_+_).foreach(println)// 输出：(spark,2)(hive,1)(hadoop,1)(flume,2) 1.4 mapPartitions与 map 类似，但函数单独在 RDD 的每个分区上运行， func函数的类型为 Iterator&lt;T&gt; =&gt; Iterator&lt;U&gt; (其中 T 是 RDD 的类型)，即输入和输出都必须是可迭代类型。 12345678910val list = List(1, 2, 3, 4, 5, 6)sc.parallelize(list, 3).mapPartitions(iterator =&gt; &#123; val buffer = new ListBuffer[Int] while (iterator.hasNext) &#123; buffer.append(iterator.next() * 100) &#125; buffer.toIterator&#125;).foreach(println)//输出结果100 200 300 400 500 600 1.5 mapPartitionsWithIndex 与 mapPartitions 类似，但 func 类型为 (Int, Iterator&lt;T&gt;) =&gt; Iterator&lt;U&gt; ，其中第一个参数为分区索引。 123456789101112131415val list = List(1, 2, 3, 4, 5, 6)sc.parallelize(list, 3).mapPartitionsWithIndex((index, iterator) =&gt; &#123; val buffer = new ListBuffer[String] while (iterator.hasNext) &#123; buffer.append(index + "分区:" + iterator.next() * 100) &#125; buffer.toIterator&#125;).foreach(println)//输出0 分区:1000 分区:2001 分区:3001 分区:4002 分区:5002 分区:600 1.6 sample 数据采样。有三个可选参数：设置是否放回 (withReplacement)、采样的百分比 (fraction)、随机数生成器的种子 (seed) ： 12val list = List(1, 2, 3, 4, 5, 6)sc.parallelize(list).sample(withReplacement = false, fraction = 0.5).foreach(println) 1.7 union合并两个 RDD： 1234val list1 = List(1, 2, 3)val list2 = List(4, 5, 6)sc.parallelize(list1).union(sc.parallelize(list2)).foreach(println)// 输出: 1 2 3 4 5 6 1.8 intersection求两个 RDD 的交集： 1234val list1 = List(1, 2, 3, 4, 5)val list2 = List(4, 5, 6)sc.parallelize(list1).intersection(sc.parallelize(list2)).foreach(println)// 输出: 4 5 1.9 distinct去重： 123val list = List(1, 2, 2, 4, 4)sc.parallelize(list).distinct().foreach(println)// 输出: 4 1 2 1.10 groupByKey按照键进行分组： 1234567val list = List(("hadoop", 2), ("spark", 3), ("spark", 5), ("storm", 6), ("hadoop", 2))sc.parallelize(list).groupByKey().map(x =&gt; (x._1, x._2.toList)).foreach(println)//输出：(spark,List(3, 5))(hadoop,List(2, 2))(storm,List(6)) 1.11 reduceByKey按照键进行归约操作： 1234567val list = List(("hadoop", 2), ("spark", 3), ("spark", 5), ("storm", 6), ("hadoop", 2))sc.parallelize(list).reduceByKey(_ + _).foreach(println)//输出(spark,8)(hadoop,4)(storm,6) 1.12 sortBy &amp; sortByKey按照键进行排序： 123456val list01 = List((100, "hadoop"), (90, "spark"), (120, "storm"))sc.parallelize(list01).sortByKey(ascending = false).foreach(println)// 输出(120,storm)(90,spark)(100,hadoop) 按照指定元素进行排序： 123456val list02 = List(("hadoop",100), ("spark",90), ("storm",120))sc.parallelize(list02).sortBy(x=&gt;x._2,ascending=false).foreach(println)// 输出(storm,120)(hadoop,100)(spark,90) 1.13 join在一个 (K, V) 和 (K, W) 类型的 Dataset 上调用时，返回一个 (K, (V, W)) 的 Dataset，等价于内连接操作。如果想要执行外连接，可以使用 leftOuterJoin, rightOuterJoin 和 fullOuterJoin 等算子。 12345678val list01 = List((1, "student01"), (2, "student02"), (3, "student03"))val list02 = List((1, "teacher01"), (2, "teacher02"), (3, "teacher03"))sc.parallelize(list01).join(sc.parallelize(list02)).foreach(println)// 输出(1,(student01,teacher01))(3,(student03,teacher03))(2,(student02,teacher02)) 1.14 cogroup在一个 (K, V) 对的 Dataset 上调用时，返回多个类型为 (K, (Iterable\, Iterable\)) 的元组所组成的 Dataset。 123456789val list01 = List((1, "a"),(1, "a"), (2, "b"), (3, "e"))val list02 = List((1, "A"), (2, "B"), (3, "E"))val list03 = List((1, "[ab]"), (2, "[bB]"), (3, "eE"),(3, "eE"))sc.parallelize(list01).cogroup(sc.parallelize(list02),sc.parallelize(list03)).foreach(println)// 输出： 同一个 RDD 中的元素先按照 key 进行分组，然后再对不同 RDD 中的元素按照 key 进行分组(1,(CompactBuffer(a, a),CompactBuffer(A),CompactBuffer([ab])))(3,(CompactBuffer(e),CompactBuffer(E),CompactBuffer(eE, eE)))(2,(CompactBuffer(b),CompactBuffer(B),CompactBuffer([bB]))) 1.15 cartesian计算笛卡尔积： 1234567891011121314val list1 = List("A", "B", "C")val list2 = List(1, 2, 3)sc.parallelize(list1).cartesian(sc.parallelize(list2)).foreach(println)//输出笛卡尔积(A,1)(A,2)(A,3)(B,1)(B,2)(B,3)(C,1)(C,2)(C,3) 1.16 aggregateByKey当调用（K，V）对的数据集时，返回（K，U）对的数据集，其中使用给定的组合函数和 zeroValue 聚合每个键的值。与 groupByKey 类似，reduce 任务的数量可通过第二个参数 numPartitions 进行配置。示例如下： 12345678910// 为了清晰，以下所有参数均使用具名传参val list = List(("hadoop", 3), ("hadoop", 2), ("spark", 4), ("spark", 3), ("storm", 6), ("storm", 8))sc.parallelize(list,numSlices = 2).aggregateByKey(zeroValue = 0,numPartitions = 3)( seqOp = math.max(_, _), combOp = _ + _ ).collect.foreach(println)//输出结果：(hadoop,3)(storm,8)(spark,7) 这里使用了 numSlices = 2 指定 aggregateByKey 父操作 parallelize 的分区数量为 2，其执行流程如下： 基于同样的执行流程，如果 numSlices = 1，则意味着只有输入一个分区，则其最后一步 combOp 相当于是无效的，执行结果为： 123(hadoop,3)(storm,8)(spark,4) 同样的，如果每个单词对一个分区，即 numSlices = 6，此时相当于求和操作，执行结果为： 123(hadoop,5)(storm,14)(spark,7) aggregateByKey(zeroValue = 0,numPartitions = 3) 的第二个参数 numPartitions 决定的是输出 RDD 的分区数量，想要验证这个问题，可以对上面代码进行改写，使用 getNumPartitions 方法获取分区数量： 1234sc.parallelize(list,numSlices = 6).aggregateByKey(zeroValue = 0,numPartitions = 3)( seqOp = math.max(_, _), combOp = _ + _).getNumPartitions 二、ActionSpark 常用的 Action 算子如下： Action（动作） Meaning（含义） reduce(func) 使用函数func执行归约操作 collect() 以一个 array 数组的形式返回 dataset 的所有元素，适用于小结果集。 count() 返回 dataset 中元素的个数。 first() 返回 dataset 中的第一个元素，等价于 take(1)。 take(n) 将数据集中的前 n 个元素作为一个 array 数组返回。 takeSample(withReplacement, num, [seed]) 对一个 dataset 进行随机抽样 takeOrdered(n, [ordering]) 按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 n 个元素。只适用于小结果集，因为所有数据都会被加载到驱动程序的内存中进行排序。 saveAsTextFile(path) 将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。 saveAsSequenceFile(path) 将 dataset 中的元素以 Hadoop SequenceFile 的形式写入到本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。该操作要求 RDD 中的元素需要实现 Hadoop 的 Writable 接口。对于 Scala 语言而言，它可以将 Spark 中的基本数据类型自动隐式转换为对应 Writable 类型。(目前仅支持 Java and Scala) saveAsObjectFile(path) 使用 Java 序列化后存储，可以使用 SparkContext.objectFile() 进行加载。(目前仅支持 Java and Scala) countByKey() 计算每个键出现的次数。 foreach(func) 遍历 RDD 中每个元素，并对其执行fun函数 2.1 reduce使用函数func执行归约操作： 12345 val list = List(1, 2, 3, 4, 5)sc.parallelize(list).reduce((x, y) =&gt; x + y)sc.parallelize(list).reduce(_ + _)// 输出 15 2.2 takeOrdered按自然顺序（natural order）或自定义比较器（custom comparator）排序后返回前 n 个元素。需要注意的是 takeOrdered 使用隐式参数进行隐式转换，以下为其源码。所以在使用自定义排序时，需要继承 Ordering[T] 实现自定义比较器，然后将其作为隐式参数引入。 123def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope &#123; .........&#125; 自定义规则排序： 123456789101112// 继承 Ordering[T],实现自定义比较器，按照 value 值的长度进行排序class CustomOrdering extends Ordering[(Int, String)] &#123; override def compare(x: (Int, String), y: (Int, String)): Int = if (x._2.length &gt; y._2.length) 1 else -1&#125;val list = List((1, "hadoop"), (1, "storm"), (1, "azkaban"), (1, "hive"))// 引入隐式默认值implicit val implicitOrdering = new CustomOrderingsc.parallelize(list).takeOrdered(5)// 输出： Array((1,hive), (1,storm), (1,hadoop), (1,azkaban) 2.3 countByKey计算每个键出现的次数： 1234val list = List(("hadoop", 10), ("hadoop", 10), ("storm", 3), ("storm", 3), ("azkaban", 1))sc.parallelize(list).countByKey()// 输出： Map(hadoop -&gt; 2, storm -&gt; 2, azkaban -&gt; 1) 2.4 saveAsTextFile将 dataset 中的元素以文本文件的形式写入本地文件系统、HDFS 或其它 Hadoop 支持的文件系统中。Spark 将对每个元素调用 toString 方法，将元素转换为文本文件中的一行记录。 12val list = List(("hadoop", 10), ("hadoop", 10), ("storm", 3), ("storm", 3), ("azkaban", 1))sc.parallelize(list).saveAsTextFile("/usr/file/temp") 参考资料RDD Programming Guide]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>算子</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark_RDD]]></title>
    <url>%2F2019%2F06%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark_RDD%2F</url>
    <content type="text"><![CDATA[弹性式数据集RDDs一、RDD简介二、创建RDD2.1 由现有集合创建2.2 引用外部存储系统中的数据集2.3 textFile &amp; wholeTextFiles三、操作RDD四、缓存RDD4.1 缓存级别4.2 使用缓存4.3 移除缓存五、理解shuffle5.1 shuffle介绍5.2 Shuffle的影响5.3 导致Shuffle的操作五、宽依赖和窄依赖六、DAG的生成 一、RDD简介RDD 全称为 Resilient Distributed Datasets，是 Spark 最基本的数据抽象，它是只读的、分区记录的集合，支持并行操作，可以由外部数据集或其他 RDD 转换而来，它具有以下特性： 一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数； RDD 拥有一个用于计算分区的函数 compute； RDD 会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算； Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)； 一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。 RDD[T] 抽象类的部分相关代码如下： 1234567891011121314// 由子类实现以计算给定分区def compute(split: Partition, context: TaskContext): Iterator[T]// 获取所有分区protected def getPartitions: Array[Partition]// 获取所有依赖关系protected def getDependencies: Seq[Dependency[_]] = deps// 获取优先位置列表protected def getPreferredLocations(split: Partition): Seq[String] = Nil// 分区器 由子类重写以指定它们的分区方式@transient val partitioner: Option[Partitioner] = None 二、创建RDDRDD 有两种创建方式，分别介绍如下： 2.1 由现有集合创建这里使用 spark-shell 进行测试，启动命令如下： 1spark-shell --master local[4] 启动 spark-shell 后，程序会自动创建应用上下文，相当于执行了下面的 Scala 语句： 12val conf = new SparkConf().setAppName("Spark shell").setMaster("local[4]")val sc = new SparkContext(conf) 由现有集合创建 RDD，你可以在创建时指定其分区个数，如果没有指定，则采用程序所分配到的 CPU 的核心数： 1234567val data = Array(1, 2, 3, 4, 5)// 由现有集合创建 RDD,默认分区数为程序所分配到的 CPU 的核心数val dataRDD = sc.parallelize(data) // 查看分区数dataRDD.getNumPartitions// 明确指定分区数val dataRDD = sc.parallelize(data,2) 执行结果如下： ### 2.2 引用外部存储系统中的数据集引用外部存储系统中的数据集，例如本地文件系统，HDFS，HBase 或支持 Hadoop InputFormat 的任何数据源。123val fileRDD = sc.textFile("/usr/file/emp.txt")// 获取第一行文本fileRDD.take(1)使用外部存储系统时需要注意以下两点：+ 如果在集群环境下从本地文件系统读取数据，则要求该文件必须在集群中所有机器上都存在，且路径相同；+ 支持目录路径，支持压缩文件，支持使用通配符。### 2.3 textFile &amp; wholeTextFiles两者都可以用来读取外部文件，但是返回格式是不同的：+ textFile：其返回格式是 RDD[String] ，返回的是就是文件内容，RDD 中每一个元素对应一行数据；+ wholeTextFiles：其返回格式是 RDD[(String, String)]，元组中第一个参数是文件路径，第二个参数是文件内容；+ 两者都提供第二个参数来控制最小分区数；+ 从 HDFS 上读取文件时，Spark 会为每个块创建一个分区。12def textFile(path: String,minPartitions: Int = defaultMinPartitions): RDD[String] = withScope &#123;...&#125;def wholeTextFiles(path: String,minPartitions: Int = defaultMinPartitions): RDD[(String, String)]=&#123;..&#125;## 三、操作RDDRDD 支持两种类型的操作：transformations（转换，从现有数据集创建新数据集）和 actions（在数据集上运行计算后将值返回到驱动程序）。RDD 中的所有转换操作都是惰性的，它们只是记住这些转换操作，但不会立即执行，只有遇到 action 操作后才会真正的进行计算，这类似于函数式编程中的惰性求值。1234val list = List(1, 2, 3)// map 是一个 transformations 操作，而 foreach 是一个 actions 操作sc.parallelize(list).map(_ * 10).foreach(println)// 输出： 10 20 30## 四、缓存RDD### 4.1 缓存级别Spark 速度非常快的一个原因是 RDD 支持缓存。成功缓存后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。Spark 支持多种缓存级别 ：| Storage Level（存储级别） | Meaning（含义） || ———————————————- | ———————————————————— || MEMORY_ONLY | 默认的缓存级别，将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，则部分分区数据将不再缓存。 || MEMORY_AND_DISK | 将 RDD 以反序列化的 Java 对象的形式存储 JVM 中。如果内存空间不够，将未缓存的分区数据存储到磁盘，在需要使用这些分区时从磁盘读取。 || MEMORY_ONLY_SER | 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式比反序列化对象节省存储空间，但在读取时会增加 CPU 的计算负担。仅支持 Java 和 Scala 。 || MEMORY_AND_DISK_SER | 类似于 MEMORY_ONLY_SER，但是溢出的分区数据会存储到磁盘，而不是在用到它们时重新计算。仅支持 Java 和 Scala。 || DISK_ONLY | 只在磁盘上缓存 RDD || MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc | 与上面的对应级别功能相同，但是会为每个分区在集群中的两个节点上建立副本。 || OFF_HEAP | 与 MEMORY_ONLY_SER 类似，但将数据存储在堆外内存中。这需要启用堆外内存。 |&gt; 启动堆外内存需要配置两个参数：&gt;&gt; + spark.memory.offHeap.enabled ：是否开启堆外内存，默认值为 false，需要设置为 true；&gt; + spark.memory.offHeap.size : 堆外内存空间的大小，默认值为 0，需要设置为正值。### 4.2 使用缓存缓存数据的方法有两个：persist 和 cache 。cache 内部调用的也是 persist，它是 persist 的特殊化形式，等价于 persist(StorageLevel.MEMORY_ONLY)。示例如下：123// 所有存储级别均定义在 StorageLevel 对象中fileRDD.persist(StorageLevel.MEMORY_AND_DISK)fileRDD.cache()### 4.3 移除缓存Spark 会自动监视每个节点上的缓存使用情况，并按照最近最少使用（LRU）的规则删除旧数据分区。当然，你也可以使用 RDD.unpersist() 方法进行手动删除。## 五、理解shuffle### 5.1 shuffle介绍在 Spark 中，一个任务对应一个分区，通常不会跨分区操作数据。但如果遇到 reduceByKey 等操作，Spark 必须从所有分区读取数据，并查找所有键的所有值，然后汇总在一起以计算每个键的最终结果 ，这称为 Shuffle。 5.2 Shuffle的影响Shuffle 是一项昂贵的操作，因为它通常会跨节点操作数据，这会涉及磁盘 I/O，网络 I/O，和数据序列化。某些 Shuffle 操作还会消耗大量的堆内存，因为它们使用堆内存来临时存储需要网络传输的数据。Shuffle 还会在磁盘上生成大量中间文件，从 Spark 1.3 开始，这些文件将被保留，直到相应的 RDD 不再使用并进行垃圾回收，这样做是为了避免在计算时重复创建 Shuffle 文件。如果应用程序长期保留对这些 RDD 的引用，则垃圾回收可能在很长一段时间后才会发生，这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间，通常可以使用 spark.local.dir 参数来指定这些临时文件的存储目录。 5.3 导致Shuffle的操作由于 Shuffle 操作对性能的影响比较大，所以需要特别注意使用，以下操作都会导致 Shuffle： 涉及到重新分区操作： 如 repartition 和 coalesce； 所有涉及到 ByKey 的操作：如 groupByKey 和 reduceByKey，但 countByKey 除外； 联结操作：如 cogroup 和 join。 五、宽依赖和窄依赖RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型： 窄依赖 (narrow dependency)：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖； 宽依赖 (wide dependency)：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。 如下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区： 区分这两种依赖是非常有用的： 首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）对父分区数据进行计算，例如先执行 map 操作，然后执行 filter 操作。而宽依赖则需要计算好所有父分区的数据，然后再在节点之间进行 Shuffle，这与 MapReduce 类似。 窄依赖能够更有效地进行数据恢复，因为只需重新对丢失分区的父分区进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据进行计算并再次 Shuffle。 六、DAG的生成RDD(s) 及其之间的依赖关系组成了 DAG(有向无环图)，DAG 定义了这些 RDD(s) 之间的 Lineage(血统) 关系，通过血统关系，如果一个 RDD 的部分或者全部计算结果丢失了，也可以重新进行计算。那么 Spark 是如何根据 DAG 来生成计算任务呢？主要是根据依赖关系的不同将 DAG 划分为不同的计算阶段 (Stage)： 对于窄依赖，由于分区的依赖关系是确定的，其转换操作可以在同一个线程执行，所以可以划分到同一个执行阶段； 对于宽依赖，由于 Shuffle 的存在，只能在父 RDD(s) 被 Shuffle 处理完成后，才能开始接下来的计算，因此遇到宽依赖就需要重新划分阶段。 参考资料 张安站 . Spark 技术内幕：深入解析 Spark 内核架构设计与实现原理[M] . 机械工业出版社 . 2015-09-01 RDD Programming Guide RDD：基于内存的集群计算容错抽象]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark开发环境搭建]]></title>
    <url>%2F2019%2F06%2F16%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Spark开发环境搭建一、安装Spark二、词频统计案例三、Scala开发环境配置 一、安装Spark1.1 下载并解压官方下载地址：http://spark.apache.org/downloads.html ，选择 Spark 版本和对应的 Hadoop 版本后再下载： 解压安装包： 1# tar -zxvf spark-2.2.3-bin-hadoop2.6.tgz 1.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export SPARK_HOME=/usr/app/spark-2.2.3-bin-hadoop2.6export PATH=$&#123;SPARK_HOME&#125;/bin:$PATH 使得配置的环境变量立即生效： 1# source /etc/profile 1.3 Local模式Local 模式是最简单的一种运行方式，它采用单节点多线程方式运行，不用部署，开箱即用，适合日常测试开发。 12# 启动spark-shellspark-shell --master local[2] local：只启动一个工作线程； local[k]：启动 k 个工作线程； local[*]：启动跟 cpu 数目相同的工作线程数。 进入 spark-shell 后，程序已经自动创建好了上下文 SparkContext，等效于执行了下面的 Scala 代码： 12val conf = new SparkConf().setAppName("Spark shell").setMaster("local[2]")val sc = new SparkContext(conf) 二、词频统计案例安装完成后可以先做一个简单的词频统计例子，感受 spark 的魅力。准备一个词频统计的文件样本 wc.txt，内容如下： 123hadoop,spark,hadoopspark,flink,flink,sparkhadoop,hadoop 在 scala 交互式命令行中执行如下 Scala 语句： 123val file = spark.sparkContext.textFile("file:///usr/app/wc.txt")val wordCounts = file.flatMap(line =&gt; line.split(",")).map((word =&gt; (word, 1))).reduceByKey(_ + _)wordCounts.collect 执行过程如下，可以看到已经输出了词频统计的结果： 同时还可以通过 Web UI 查看作业的执行情况，访问端口为 4040： 三、Scala开发环境配置Spark 是基于 Scala 语言进行开发的，分别提供了基于 Scala、Java、Python 语言的 API，如果你想使用 Scala 语言进行开发，则需要搭建 Scala 语言的开发环境。 3.1 前置条件Scala 的运行依赖于 JDK，所以需要你本机有安装对应版本的 JDK，最新的 Scala 2.12.x 需要 JDK 1.8+。 3.2 安装Scala插件IDEA 默认不支持 Scala 语言的开发，需要通过插件进行扩展。打开 IDEA，依次点击 File =&gt; settings=&gt; plugins 选项卡，搜索 Scala 插件 (如下图)。找到插件后进行安装，并重启 IDEA 使得安装生效。 3.3 创建Scala项目在 IDEA 中依次点击 File =&gt; New =&gt; Project 选项卡，然后选择创建 Scala—IDEA 工程： 3.4 下载Scala SDK1. 方式一此时看到 Scala SDK 为空，依次点击 Create =&gt; Download ，选择所需的版本后，点击 OK 按钮进行下载，下载完成点击 Finish 进入工程。 2. 方式二方式一是 Scala 官方安装指南里使用的方式，但下载速度通常比较慢，且这种安装下并没有直接提供 Scala 命令行工具。所以个人推荐到官网下载安装包进行安装，下载地址：https://www.scala-lang.org/download/ 这里我的系统是 Windows，下载 msi 版本的安装包后，一直点击下一步进行安装，安装完成后会自动配置好环境变量。 由于安装时已经自动配置好环境变量，所以 IDEA 会自动选择对应版本的 SDK。 3.5 创建Hello World在工程 src 目录上右击 New =&gt; Scala class 创建 Hello.scala。输入代码如下，完成后点击运行按钮，成功运行则代表搭建成功。 3.6 切换Scala版本在日常的开发中，由于对应软件（如 Spark）的版本切换，可能导致需要切换 Scala 的版本，则可以在 Project Structures 中的 Global Libraries 选项卡中进行切换。 3.7 可能出现的问题在 IDEA 中有时候重新打开项目后，右击并不会出现新建 scala 文件的选项，或者在编写时没有 Scala 语法提示，此时可以先删除 Global Libraries 中配置好的 SDK，之后再重新添加： 另外在 IDEA 中以本地模式运行 Spark 项目是不需要在本机搭建 Spark 和 Hadoop 环境的。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Spark简介]]></title>
    <url>%2F2019%2F06%2F16%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BSpark%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Spark简介一、简介二、特点三、集群架构四、核心组件3.1 Spark SQL3.2 Spark Streaming3.3 MLlib3.4 Graphx 一、简介Spark 于 2009 年诞生于加州大学伯克利分校 AMPLab，2013 年被捐赠给 Apache 软件基金会，2014 年 2 月成为 Apache 的顶级项目。相对于 MapReduce 的批处理计算，Spark 可以带来上百倍的性能提升，因此它成为继 MapReduce 之后，最为广泛使用的分布式计算框架。 二、特点Apache Spark 具有以下特点： 使用先进的 DAG 调度程序，查询优化器和物理执行引擎，以实现性能上的保证； 多语言支持，目前支持的有 Java，Scala，Python 和 R； 提供了 80 多个高级 API，可以轻松地构建应用程序； 支持批处理，流处理和复杂的业务分析； 丰富的类库支持：包括 SQL，MLlib，GraphX 和 Spark Streaming 等库，并且可以将它们无缝地进行组合； 丰富的部署模式：支持本地模式和自带的集群模式，也支持在 Hadoop，Mesos，Kubernetes 上运行； 多数据源支持：支持访问 HDFS，Alluxio，Cassandra，HBase，Hive 以及数百个其他数据源中的数据。 三、集群架构 Term（术语） Meaning（含义） Application Spark 应用程序，由集群上的一个 Driver 节点和多个 Executor 节点组成。 Driver program 主运用程序，该进程运行应用的 main() 方法并且创建 SparkContext Cluster manager 集群资源管理器（例如，Standlone Manager，Mesos，YARN） Worker node 执行计算任务的工作节点 Executor 位于工作节点上的应用进程，负责执行计算任务并且将输出数据保存到内存或者磁盘中 Task 被发送到 Executor 中的工作单元 执行过程： 用户程序创建 SparkContext 后，它会连接到集群资源管理器，集群资源管理器会为用户程序分配计算资源，并启动 Executor； Dirver 将计算程序划分为不同的执行阶段和多个 Task，之后将 Task 发送给 Executor； Executor 负责执行 Task，并将执行状态汇报给 Driver，同时也会将当前节点资源的使用情况汇报给集群资源管理器。 四、核心组件Spark 基于 Spark Core 扩展了四个核心组件，分别用于满足不同领域的计算需求。 3.1 Spark SQLSpark SQL 主要用于结构化数据的处理。其具有以下特点： 能够将 SQL 查询与 Spark 程序无缝混合，允许您使用 SQL 或 DataFrame API 对结构化数据进行查询； 支持多种数据源，包括 Hive，Avro，Parquet，ORC，JSON 和 JDBC； 支持 HiveQL 语法以及用户自定义函数 (UDF)，允许你访问现有的 Hive 仓库； 支持标准的 JDBC 和 ODBC 连接； 支持优化器，列式存储和代码生成等特性，以提高查询效率。 3.2 Spark StreamingSpark Streaming 主要用于快速构建可扩展，高吞吐量，高容错的流处理程序。支持从 HDFS，Flume，Kafka，Twitter 和 ZeroMQ 读取数据，并进行处理。 Spark Streaming 的本质是微批处理，它将数据流进行极小粒度的拆分，拆分为多个批处理，从而达到接近于流处理的效果。 3.3 MLlibMLlib 是 Spark 的机器学习库。其设计目标是使得机器学习变得简单且可扩展。它提供了以下工具： 常见的机器学习算法：如分类，回归，聚类和协同过滤； 特征化：特征提取，转换，降维和选择； 管道：用于构建，评估和调整 ML 管道的工具； 持久性：保存和加载算法，模型，管道数据； 实用工具：线性代数，统计，数据处理等。 3.4 GraphxGraphX 是 Spark 中用于图形计算和图形并行计算的新组件。在高层次上，GraphX 通过引入一个新的图形抽象来扩展 RDD(一种具有附加到每个顶点和边缘的属性的定向多重图形)。为了支持图计算，GraphX 提供了一组基本运算符（如： subgraph，joinVertices 和 aggregateMessages）以及优化后的 Pregel API。此外，GraphX 还包括越来越多的图形算法和构建器，以简化图形分析任务。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hive数据查询详解]]></title>
    <url>%2F2019%2F06%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHive%E6%95%B0%E6%8D%AE%E6%9F%A5%E8%AF%A2%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Hive数据查询详解一、数据准备二、单表查询2.1 SELECT2.2 WHERE2.3 DISTINCT2.4 分区查询2.5 LIMIT2.6 GROUP BY2.7 ORDER AND SORT2.8 HAVING2.9 DISTRIBUTE BY2.10 CLUSTER BY三、多表联结查询3.1 INNER JOIN3.2 LEFT OUTER JOIN 3.3 RIGHT OUTER JOIN3.4 FULL OUTER JOIN 3.5 LEFT SEMI JOIN3.6 JOIN四、JOIN优化4.1 STREAMTABLE4.2 MAPJOIN五、SELECT的其他用途六、本地模式 一、数据准备为了演示查询操作，这里需要预先创建三张表，并加载测试数据。 数据文件 emp.txt 和 dept.txt 可以从本仓库的resources 目录下载。 1.1 员工表1234567891011121314 -- 建表语句 CREATE TABLE emp( empno INT, -- 员工表编号 ename STRING, -- 员工姓名 job STRING, -- 职位类型 mgr INT, hiredate TIMESTAMP, --雇佣日期 sal DECIMAL(7,2), --工资 comm DECIMAL(7,2), deptno INT) --部门编号 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"; --加载数据LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp; 1.2 部门表12345678910-- 建表语句CREATE TABLE dept( deptno INT, --部门编号 dname STRING, --部门名称 loc STRING --部门所在的城市)ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t";--加载数据LOAD DATA LOCAL INPATH "/usr/file/dept.txt" OVERWRITE INTO TABLE dept; 1.3 分区表这里需要额外创建一张分区表，主要是为了演示分区查询： 123456789101112131415161718CREATE EXTERNAL TABLE emp_ptn( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t";--加载数据LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=20)LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=30)LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=40)LOAD DATA LOCAL INPATH "/usr/file/emp.txt" OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=50) 二、单表查询2.1 SELECT12-- 查询表中全部数据SELECT * FROM emp; 2.2 WHERE12-- 查询 10 号部门中员工编号大于 7782 的员工信息 SELECT * FROM emp WHERE empno &gt; 7782 AND deptno = 10; 2.3 DISTINCTHive 支持使用 DISTINCT 关键字去重。 12-- 查询所有工作类型SELECT DISTINCT job FROM emp; 2.4 分区查询分区查询 (Partition Based Queries)，可以指定某个分区或者分区范围。 123-- 查询分区表中部门编号在[20,40]之间的员工SELECT emp_ptn.* FROM emp_ptnWHERE emp_ptn.deptno &gt;= 20 AND emp_ptn.deptno &lt;= 40; 2.5 LIMIT12-- 查询薪资最高的 5 名员工SELECT * FROM emp ORDER BY sal DESC LIMIT 5; 2.6 GROUP BYHive 支持使用 GROUP BY 进行分组聚合操作。 1234set hive.map.aggr=true;-- 查询各个部门薪酬综合SELECT deptno,SUM(sal) FROM emp GROUP BY deptno; hive.map.aggr 控制程序如何进行聚合。默认值为 false。如果设置为 true，Hive 会在 map 阶段就执行一次聚合。这可以提高聚合效率，但需要消耗更多内存。 2.7 ORDER AND SORT可以使用 ORDER BY 或者 Sort BY 对查询结果进行排序，排序字段可以是整型也可以是字符串：如果是整型，则按照大小排序；如果是字符串，则按照字典序排序。ORDER BY 和 SORT BY 的区别如下： 使用 ORDER BY 时会有一个 Reducer 对全部查询结果进行排序，可以保证数据的全局有序性； 使用 SORT BY 时只会在每个 Reducer 中进行排序，这可以保证每个 Reducer 的输出数据是有序的，但不能保证全局有序。 由于 ORDER BY 的时间可能很长，如果你设置了严格模式 (hive.mapred.mode = strict)，则其后面必须再跟一个 limit 子句。 注 ：hive.mapred.mode 默认值是 nonstrict ，也就是非严格模式。 12-- 查询员工工资，结果按照部门升序，按照工资降序排列SELECT empno, deptno, sal FROM emp ORDER BY deptno ASC, sal DESC; 2.8 HAVING可以使用 HAVING 对分组数据进行过滤。 12-- 查询工资总和大于 9000 的所有部门SELECT deptno,SUM(sal) FROM emp GROUP BY deptno HAVING SUM(sal)&gt;9000; 2.9 DISTRIBUTE BY默认情况下，MapReduce 程序会对 Map 输出结果的 Key 值进行散列，并均匀分发到所有 Reducer 上。如果想要把具有相同 Key 值的数据分发到同一个 Reducer 进行处理，这就需要使用 DISTRIBUTE BY 字句。 需要注意的是，DISTRIBUTE BY 虽然能保证具有相同 Key 值的数据分发到同一个 Reducer，但是不能保证数据在 Reducer 上是有序的。情况如下： 把以下 5 个数据发送到两个 Reducer 上进行处理： 12345k1k2k4k3k1 Reducer1 得到如下乱序数据： 123k1k2k1 Reducer2 得到数据如下： 12k4k3 如果想让 Reducer 上的数据时有序的，可以结合 SORT BY 使用 (示例如下)，或者使用下面我们将要介绍的 CLUSTER BY。 12-- 将数据按照部门分发到对应的 Reducer 上处理SELECT empno, deptno, sal FROM emp DISTRIBUTE BY deptno SORT BY deptno ASC; 2.10 CLUSTER BY如果 SORT BY 和 DISTRIBUTE BY 指定的是相同字段，且 SORT BY 排序规则是 ASC，此时可以使用 CLUSTER BY 进行替换，同时 CLUSTER BY 可以保证数据在全局是有序的。 1SELECT empno, deptno, sal FROM emp CLUSTER BY deptno ; 三、多表联结查询Hive 支持内连接，外连接，左外连接，右外连接，笛卡尔连接，这和传统数据库中的概念是一致的，可以参见下图。 需要特别强调：JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定，否则就会先做笛卡尔积，再过滤，这会导致你得不到预期的结果 (下面的演示会有说明)。 3.1 INNER JOIN12345678-- 查询员工编号为 7369 的员工的详细信息SELECT e.*,d.* FROM emp e JOIN dept dON e.deptno = d.deptno WHERE empno=7369;--如果是三表或者更多表连接，语法如下SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1) 3.2 LEFT OUTER JOINLEFT OUTER JOIN 和 LEFT JOIN 是等价的。 1234-- 左连接SELECT e.*,d.*FROM emp e LEFT OUTER JOIN dept dON e.deptno = d.deptno; 3.3 RIGHT OUTER JOIN1234--右连接SELECT e.*,d.*FROM emp e RIGHT OUTER JOIN dept dON e.deptno = d.deptno; 执行右连接后，由于 40 号部门下没有任何员工，所以此时员工信息为 NULL。这个查询可以很好的复述上面提到的——JOIN 语句的关联条件必须用 ON 指定，不能用 WHERE 指定。你可以把 ON 改成 WHERE，你会发现无论如何都查不出 40 号部门这条数据，因为笛卡尔运算不会有 (NULL, 40) 这种情况。 3.4 FULL OUTER JOIN123SELECT e.*,d.*FROM emp e FULL OUTER JOIN dept dON e.deptno = d.deptno; 3.5 LEFT SEMI JOINLEFT SEMI JOIN （左半连接）是 IN/EXISTS 子查询的一种更高效的实现。 JOIN 子句中右边的表只能在 ON 子句中设置过滤条件; 查询结果只包含左边表的数据，所以只能 SELECT 左表中的列。 12345678-- 查询在纽约办公的所有员工信息SELECT emp.*FROM emp LEFT SEMI JOIN dept ON emp.deptno = dept.deptno AND dept.loc="NEW YORK";--上面的语句就等价于SELECT emp.* FROM empWHERE emp.deptno IN (SELECT deptno FROM dept WHERE loc="NEW YORK"); 3.6 JOIN笛卡尔积连接，这个连接日常的开发中可能很少遇到，且性能消耗比较大，基于这个原因，如果在严格模式下 (hive.mapred.mode = strict)，Hive 会阻止用户执行此操作。 1SELECT * FROM emp JOIN dept; 四、JOIN优化4.1 STREAMTABLE在多表进行联结的时候，如果每个 ON 字句都使用到共同的列（如下面的 b.key），此时 Hive 会进行优化，将多表 JOIN 在同一个 map / reduce 作业上进行。同时假定查询的最后一个表（如下面的 c 表）是最大的一个表，在对每行记录进行 JOIN 操作时，它将尝试将其他的表缓存起来，然后扫描最后那个表进行计算。因此用户需要保证查询的表的大小从左到右是依次增加的。 1`SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key) JOIN c ON (c.key = b.key)` 然后，用户并非需要总是把最大的表放在查询语句的最后面，Hive 提供了 /*+ STREAMTABLE() */ 标志，用于标识最大的表，示例如下： 1234SELECT /*+ STREAMTABLE(d) */ e.*,d.* FROM emp e JOIN dept dON e.deptno = d.deptnoWHERE job='CLERK'; 4.2 MAPJOIN如果所有表中只有一张表是小表，那么 Hive 把这张小表加载到内存中。这时候程序会在 map 阶段直接拿另外一个表的数据和内存中表数据做匹配，由于在 map 就进行了 JOIN 操作，从而可以省略 reduce 过程，这样效率可以提升很多。Hive 中提供了 /*+ MAPJOIN() */ 来标记小表，示例如下： 1234SELECT /*+ MAPJOIN(d) */ e.*,d.* FROM emp e JOIN dept dON e.deptno = d.deptnoWHERE job='CLERK'; 五、SELECT的其他用途查看当前数据库： 1SELECT current_database() 六、本地模式在上面演示的语句中，大多数都会触发 MapReduce, 少部分不会触发，比如 select * from emp limit 5 就不会触发 MR，此时 Hive 只是简单的读取数据文件中的内容，然后格式化后进行输出。在需要执行 MapReduce 的查询中，你会发现执行时间可能会很长，这时候你可以选择开启本地模式。 12--本地模式默认关闭，需要手动开启此功能SET hive.exec.mode.local.auto=true; 启用后，Hive 将分析查询中每个 map-reduce 作业的大小，如果满足以下条件，则可以在本地运行它： 作业的总输入大小低于：hive.exec.mode.local.auto.inputbytes.max（默认为 128MB）； map-tasks 的总数小于：hive.exec.mode.local.auto.tasks.max（默认为 4）； 所需的 reduce 任务总数为 1 或 0。 因为我们测试的数据集很小，所以你再次去执行上面涉及 MR 操作的查询，你会发现速度会有显著的提升。 参考资料 LanguageManual Select LanguageManual Joins LanguageManual GroupBy LanguageManual SortBy]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>数据查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hive常用DML操作]]></title>
    <url>%2F2019%2F06%2F11%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHive%E5%B8%B8%E7%94%A8DML%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Hive 常用DML操作一、加载文件数据到表二、查询结果插入到表三、使用SQL语句插入值四、更新和删除数据五、查询结果写出到文件系统 一、加载文件数据到表1.1 语法12LOAD DATA [LOCAL] INPATH 'filepath' [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)] LOCAL 关键字代表从本地文件系统加载文件，省略则代表从 HDFS 上加载文件： 从本地文件系统加载文件时， filepath 可以是绝对路径也可以是相对路径 (建议使用绝对路径)； 从 HDFS 加载文件时候，filepath 为文件完整的 URL 地址：如 hdfs://namenode:port/user/hive/project/ data1 filepath 可以是文件路径 (在这种情况下 Hive 会将文件移动到表中)，也可以目录路径 (在这种情况下，Hive 会将该目录中的所有文件移动到表中)； 如果使用 OVERWRITE 关键字，则将删除目标表（或分区）的内容，使用新的数据填充；不使用此关键字，则数据以追加的方式加入； 加载的目标可以是表或分区。如果是分区表，则必须指定加载数据的分区； 加载文件的格式必须与建表时使用 STORED AS 指定的存储格式相同。 使用建议： 不论是本地路径还是 URL 都建议使用完整的。虽然可以使用不完整的 URL 地址，此时 Hive 将使用 hadoop 中的 fs.default.name 配置来推断地址，但是为避免不必要的错误，建议使用完整的本地路径或 URL 地址； 加载对象是分区表时建议显示指定分区。在 Hive 3.0 之后，内部将加载 (LOAD) 重写为 INSERT AS SELECT，此时如果不指定分区，INSERT AS SELECT 将假设最后一组列是分区列，如果该列不是表定义的分区，它将抛出错误。为避免错误，还是建议显示指定分区。 1.2 示例新建分区表： 1234567891011CREATE TABLE emp_ptn( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"; 从 HDFS 上加载数据到分区表： 1LOAD DATA INPATH "hdfs://hadoop001:8020/mydir/emp.txt" OVERWRITE INTO TABLE emp_ptn PARTITION (deptno=20); emp.txt 文件可在本仓库的 resources 目录中下载 加载后表中数据如下,分区列 deptno 全部赋值成 20： 二、查询结果插入到表2.1 语法12345INSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1 FROM from_statement;INSERT INTO TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...)] select_statement1 FROM from_statement; Hive 0.13.0 开始，建表时可以通过使用 TBLPROPERTIES（“immutable”=“true”）来创建不可变表 (immutable table) ，如果不可以变表中存在数据，则 INSERT INTO 失败。（注：INSERT OVERWRITE 的语句不受 immutable 属性的影响）; 可以对表或分区执行插入操作。如果表已分区，则必须通过指定所有分区列的值来指定表的特定分区； 从 Hive 1.1.0 开始，TABLE 关键字是可选的； 从 Hive 1.2.0 开始 ，可以采用 INSERT INTO tablename(z，x，c1) 指明插入列； 可以将 SELECT 语句的查询结果插入多个表（或分区），称为多表插入。语法如下： 12345FROM from_statementINSERT OVERWRITE TABLE tablename1 [PARTITION (partcol1=val1, partcol2=val2 ...) [IF NOT EXISTS]] select_statement1[INSERT OVERWRITE TABLE tablename2 [PARTITION ... [IF NOT EXISTS]] select_statement2][INSERT INTO TABLE tablename2 [PARTITION ...] select_statement2] ...; 2.2 动态插入分区12345INSERT OVERWRITE TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement;INSERT INTO TABLE tablename PARTITION (partcol1[=val1], partcol2[=val2] ...) select_statement FROM from_statement; 在向分区表插入数据时候，分区列名是必须的，但是列值是可选的。如果给出了分区列值，我们将其称为静态分区，否则它是动态分区。动态分区列必须在 SELECT 语句的列中最后指定，并且与它们在 PARTITION() 子句中出现的顺序相同。 注意：Hive 0.9.0 之前的版本动态分区插入是默认禁用的，而 0.9.0 之后的版本则默认启用。以下是动态分区的相关配置： 配置 默认值 说明 hive.exec.dynamic.partition true 需要设置为 true 才能启用动态分区插入 hive.exec.dynamic.partition.mode strict 在严格模式 (strict) 下，用户必须至少指定一个静态分区，以防用户意外覆盖所有分区，在非严格模式下，允许所有分区都是动态的 hive.exec.max.dynamic.partitions.pernode 100 允许在每个 mapper/reducer 节点中创建的最大动态分区数 hive.exec.max.dynamic.partitions 1000 允许总共创建的最大动态分区数 hive.exec.max.created.files 100000 作业中所有 mapper/reducer 创建的 HDFS 文件的最大数量 hive.error.on.empty.partition false 如果动态分区插入生成空结果，是否抛出异常 2.3 示例 新建 emp 表，作为查询对象表 12345678910111213CREATE TABLE emp( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"; -- 加载数据到 emp 表中 这里直接从本地加载load data local inpath "/usr/file/emp.txt" into table emp; ​ 完成后 emp 表中数据如下： 为清晰演示，先清空 emp_ptn 表中加载的数据： 1TRUNCATE TABLE emp_ptn; 静态分区演示：从 emp 表中查询部门编号为 20 的员工数据，并插入 emp_ptn 表中，语句如下： 12INSERT OVERWRITE TABLE emp_ptn PARTITION (deptno=20) SELECT empno,ename,job,mgr,hiredate,sal,comm FROM emp WHERE deptno=20; ​ 完成后 emp_ptn 表中数据如下： 接着演示动态分区： 123456-- 由于我们只有一个分区，且还是动态分区，所以需要关闭严格默认。因为在严格模式下，用户必须至少指定一个静态分区set hive.exec.dynamic.partition.mode=nonstrict;-- 动态分区 此时查询语句的最后一列为动态分区列，即 deptnoINSERT OVERWRITE TABLE emp_ptn PARTITION (deptno) SELECT empno,ename,job,mgr,hiredate,sal,comm,deptno FROM emp WHERE deptno=30; ​ 完成后 emp_ptn 表中数据如下： 三、使用SQL语句插入值12INSERT INTO TABLE tablename [PARTITION (partcol1[=val1], partcol2[=val2] ...)] VALUES ( value [, value ...] ) 使用时必须为表中的每个列都提供值。不支持只向部分列插入值（可以为缺省值的列提供空值来消除这个弊端）； 如果目标表表支持 ACID 及其事务管理器，则插入后自动提交； 不支持支持复杂类型 (array, map, struct, union) 的插入。 四、更新和删除数据4.1 语法更新和删除的语法比较简单，和关系型数据库一致。需要注意的是这两个操作都只能在支持 ACID 的表，也就是事务表上才能执行。 12345-- 更新UPDATE tablename SET column = value [, column = value ...] [WHERE expression]--删除DELETE FROM tablename [WHERE expression] 4.2 示例1. 修改配置 首先需要更改 hive-site.xml，添加如下配置，开启事务支持，配置完成后需要重启 Hive 服务。 123456789101112131415161718192021222324&lt;property&gt; &lt;name&gt;hive.support.concurrency&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.enforce.bucketing&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.dynamic.partition.mode&lt;/name&gt; &lt;value&gt;nonstrict&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.txn.manager&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hive.ql.lockmgr.DbTxnManager&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.compactor.initiator.on&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.in.test&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 2. 创建测试表 创建用于测试的事务表，建表时候指定属性 transactional = true 则代表该表是事务表。需要注意的是，按照官方文档 的说明，目前 Hive 中的事务表有以下限制： 必须是 buckets Table; 仅支持 ORC 文件格式； 不支持 LOAD DATA …语句。 123456CREATE TABLE emp_ts( empno int, ename String)CLUSTERED BY (empno) INTO 2 BUCKETS STORED AS ORCTBLPROPERTIES ("transactional"="true"); 3. 插入测试数据 1INSERT INTO TABLE emp_ts VALUES (1,"ming"),(2,"hong"); 插入数据依靠的是 MapReduce 作业，执行成功后数据如下： 4. 测试更新和删除 12345--更新数据UPDATE emp_ts SET ename = "lan" WHERE empno=1;--删除数据DELETE FROM emp_ts WHERE empno=2; 更新和删除数据依靠的也是 MapReduce 作业，执行成功后数据如下： 五、查询结果写出到文件系统5.1 语法123INSERT OVERWRITE [LOCAL] DIRECTORY directory1 [ROW FORMAT row_format] [STORED AS file_format] SELECT ... FROM ... OVERWRITE 关键字表示输出文件存在时，先删除后再重新写入； 和 Load 语句一样，建议无论是本地路径还是 URL 地址都使用完整的； 写入文件系统的数据被序列化为文本，其中列默认由^A 分隔，行由换行符分隔。如果列不是基本类型，则将其序列化为 JSON 格式。其中行分隔符不允许自定义，但列分隔符可以自定义，如下： 1234567-- 定义列分隔符为'\t' insert overwrite local directory './test-04' row format delimited FIELDS TERMINATED BY '\t'COLLECTION ITEMS TERMINATED BY ','MAP KEYS TERMINATED BY ':'select * from src; 5.2 示例这里我们将上面创建的 emp_ptn 表导出到本地文件系统，语句如下： 1234INSERT OVERWRITE LOCAL DIRECTORY '/usr/file/ouput'ROW FORMAT DELIMITEDFIELDS TERMINATED BY '\t'SELECT * FROM emp_ptn; 导出结果如下： 参考资料 Hive Transactions Hive Data Manipulation Language]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>DML操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hive视图和索引]]></title>
    <url>%2F2019%2F06%2F11%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHive%E8%A7%86%E5%9B%BE%E5%92%8C%E7%B4%A2%E5%BC%95%2F</url>
    <content type="text"><![CDATA[Hive 视图和索引一、视图二、索引三、索引案例四、索引的缺陷 一、视图1.1 简介Hive 中的视图和 RDBMS 中视图的概念一致，都是一组数据的逻辑表示，本质上就是一条 SELECT 语句的结果集。视图是纯粹的逻辑对象，没有关联的存储 (Hive 3.0.0 引入的物化视图除外)，当查询引用视图时，Hive 可以将视图的定义与查询结合起来，例如将查询中的过滤器推送到视图中。 1.2 创建视图12345CREATE VIEW [IF NOT EXISTS] [db_name.]view_name -- 视图名称 [(column_name [COMMENT column_comment], ...) ] --列名 [COMMENT view_comment] --视图注释 [TBLPROPERTIES (property_name = property_value, ...)] --额外信息 AS SELECT ...; 在 Hive 中可以使用 CREATE VIEW 创建视图，如果已存在具有相同名称的表或视图，则会抛出异常，建议使用 IF NOT EXISTS 预做判断。在使用视图时候需要注意以下事项： 视图是只读的，不能用作 LOAD / INSERT / ALTER 的目标； 在创建视图时候视图就已经固定，对基表的后续更改（如添加列）将不会反映在视图； 删除基表并不会删除视图，需要手动删除视图； 视图可能包含 ORDER BY 和 LIMIT 子句。如果引用视图的查询语句也包含这类子句，其执行优先级低于视图对应字句。例如，视图 custom_view 指定 LIMIT 5，查询语句为 select * from custom_view LIMIT 10，此时结果最多返回 5 行。 创建视图时，如果未提供列名，则将从 SELECT 语句中自动派生列名； 创建视图时，如果 SELECT 语句中包含其他表达式，例如 x + y，则列名称将以_C0，_C1 等形式生成； 1CREATE VIEW IF NOT EXISTS custom_view AS SELECT empno, empno+deptno , 1+2 FROM emp; 1.3 查看视图123456-- 查看所有视图： 没有单独查看视图列表的语句，只能使用 show tablesshow tables;-- 查看某个视图desc view_name;-- 查看某个视图详细信息desc formatted view_name; 1.4 删除视图1DROP VIEW [IF EXISTS] [db_name.]view_name; 删除视图时，如果被删除的视图被其他视图所引用，这时候程序不会发出警告，但是引用该视图其他视图已经失效，需要进行重建或者删除。 1.5 修改视图1ALTER VIEW [db_name.]view_name AS select_statement; 被更改的视图必须存在，且视图不能具有分区，如果视图具有分区，则修改失败。 1.6 修改视图属性语法： 1234ALTER VIEW [db_name.]view_name SET TBLPROPERTIES table_properties; table_properties: : (property_name = property_value, property_name = property_value, ...) 示例： 1ALTER VIEW custom_view SET TBLPROPERTIES ('create'='heibaiying','date'='2019-05-05'); 二、索引2.1 简介Hive 在 0.7.0 引入了索引的功能，索引的设计目标是提高表某些列的查询速度。如果没有索引，带有谓词的查询（如’WHERE table1.column = 10’）会加载整个表或分区并处理所有行。但是如果 column 存在索引，则只需要加载和处理文件的一部分。 2.2 索引原理在指定列上建立索引，会产生一张索引表（表结构如下），里面的字段包括：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。在查询涉及到索引字段时，首先到索引表查找索引列值对应的 HDFS 文件路径及偏移量，这样就避免了全表扫描。 1234567+--------------+----------------+----------+--+| col_name | data_type | comment |+--------------+----------------+----------+--+| empno | int | 建立索引的列 | | _bucketname | string | HDFS 文件路径 || _offsets | array&lt;bigint&gt; | 偏移量 |+--------------+----------------+----------+--+ 2.3 创建索引12345678910111213CREATE INDEX index_name --索引名称 ON TABLE base_table_name (col_name, ...) --建立索引的列 AS index_type --索引类型 [WITH DEFERRED REBUILD] --重建索引 [IDXPROPERTIES (property_name=property_value, ...)] --索引额外属性 [IN TABLE index_table_name] --索引表的名字 [ [ ROW FORMAT ...] STORED AS ... | STORED BY ... ] --索引表行分隔符 、 存储格式 [LOCATION hdfs_path] --索引表存储位置 [TBLPROPERTIES (...)] --索引表表属性 [COMMENT "index comment"]; --索引注释 2.4 查看索引12--显示表上所有列的索引SHOW FORMATTED INDEX ON table_name; 2.4 删除索引删除索引会删除对应的索引表。 1DROP INDEX [IF EXISTS] index_name ON table_name; 如果存在索引的表被删除了，其对应的索引和索引表都会被删除。如果被索引表的某个分区被删除了，那么分区对应的分区索引也会被删除。 2.5 重建索引1ALTER INDEX index_name ON table_name [PARTITION partition_spec] REBUILD; 重建索引。如果指定了 PARTITION，则仅重建该分区的索引。 三、索引案例3.1 创建索引在 emp 表上针对 empno 字段创建名为 emp_index,索引数据存储在 emp_index_table 索引表中 1234create index emp_index on table emp(empno) as 'org.apache.hadoop.hive.ql.index.compact.CompactIndexHandler' with deferred rebuild in table emp_index_table ; 此时索引表中是没有数据的，需要重建索引才会有索引的数据。 3.2 重建索引1alter index emp_index on emp rebuild; Hive 会启动 MapReduce 作业去建立索引，建立好后查看索引表数据如下。三个表字段分别代表：索引列的值、该值对应的 HDFS 文件路径、该值在文件中的偏移量。 3.3 自动使用索引默认情况下，虽然建立了索引，但是 Hive 在查询时候是不会自动去使用索引的，需要开启相关配置。开启配置后，涉及到索引列的查询就会使用索引功能去优化查询。 123SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;SET hive.optimize.index.filter=true;SET hive.optimize.index.filter.compact.minsize=0; 3.4 查看索引1SHOW INDEX ON emp; 四、索引的缺陷索引表最主要的一个缺陷在于：索引表无法自动 rebuild，这也就意味着如果表中有数据新增或删除，则必须手动 rebuild，重新执行 MapReduce 作业，生成索引表数据。 同时按照官方文档 的说明，Hive 会从 3.0 开始移除索引功能，主要基于以下两个原因： 具有自动重写的物化视图 (Materialized View) 可以产生与索引相似的效果（Hive 2.3.0 增加了对物化视图的支持，在 3.0 之后正式引入）。 使用列式存储文件格式（Parquet，ORC）进行存储时，这些格式支持选择性扫描，可以跳过不需要的文件或块。 ORC 内置的索引功能可以参阅这篇文章：Hive 性能优化之 ORC 索引–Row Group Index vs Bloom Filter Index 参考资料 Create/Drop/Alter View Materialized views Hive 索引 Overview of Hive Indexes]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>视图索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hive分区表和分桶表]]></title>
    <url>%2F2019%2F06%2F11%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHive%E5%88%86%E5%8C%BA%E8%A1%A8%E5%92%8C%E5%88%86%E6%A1%B6%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[Hive分区表和分桶表一、分区表二、分桶表三、分区表和分桶表结合使用 一、分区表1.1 概念Hive 中的表对应为 HDFS 上的指定目录，在查询数据时候，默认会对全表进行扫描，这样时间和性能的消耗都非常大。 分区为 HDFS 上表目录的子目录，数据按照分区存储在子目录中。如果查询的 where 字句的中包含分区条件，则直接从该分区去查找，而不是扫描整个表目录，合理的分区设计可以极大提高查询速度和性能。 这里说明一下分区表并 Hive 独有的概念，实际上这个概念非常常见。比如在我们常用的 Oracle 数据库中，当表中的数据量不断增大，查询数据的速度就会下降，这时也可以对表进行分区。表进行分区后，逻辑上表仍然是一张完整的表，只是将表中的数据存放到多个表空间（物理文件上），这样查询数据时，就不必要每次都扫描整张表，从而提升查询性能。 1.2 使用场景通常，在管理大规模数据集的时候都需要进行分区，比如将日志文件按天进行分区，从而保证数据细粒度的划分，使得查询性能得到提升。 1.3 创建分区表在 Hive 中可以使用 PARTITIONED BY 子句创建分区表。表可以包含一个或多个分区列，程序会为分区列中的每个不同值组合创建单独的数据目录。下面的我们创建一张雇员表作为测试： 123456789101112CREATE EXTERNAL TABLE emp_partition( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t" LOCATION '/hive/emp_partition'; 1.4 加载数据到分区表加载数据到分区表时候必须要指定数据所处的分区： 1234# 加载部门编号为20的数据到表中LOAD DATA LOCAL INPATH "/usr/file/emp20.txt" OVERWRITE INTO TABLE emp_partition PARTITION (deptno=20)# 加载部门编号为30的数据到表中LOAD DATA LOCAL INPATH "/usr/file/emp30.txt" OVERWRITE INTO TABLE emp_partition PARTITION (deptno=30) 1.5 查看分区目录这时候我们直接查看表目录，可以看到表目录下存在两个子目录，分别是 deptno=20 和 deptno=30,这就是分区目录，分区目录下才是我们加载的数据文件。 1# hadoop fs -ls hdfs://hadoop001:8020/hive/emp_partition/ 这时候当你的查询语句的 where 包含 deptno=20，则就去对应的分区目录下进行查找，而不用扫描全表。 二、分桶表1.1 简介分区提供了一个隔离数据和优化查询的可行方案，但是并非所有的数据集都可以形成合理的分区，分区的数量也不是越多越好，过多的分区条件可能会导致很多分区上没有数据。同时 Hive 会限制动态分区可以创建的最大分区数，用来避免过多分区文件对文件系统产生负担。鉴于以上原因，Hive 还提供了一种更加细粒度的数据拆分方案：分桶表 (bucket Table)。 分桶表会将指定列的值进行哈希散列，并对 bucket（桶数量）取余，然后存储到对应的 bucket（桶）中。 1.2 理解分桶表单从概念上理解分桶表可能会比较晦涩，其实和分区一样，分桶这个概念同样不是 Hive 独有的，对于 Java 开发人员而言，这可能是一个每天都会用到的概念，因为 Hive 中的分桶概念和 Java 数据结构中的 HashMap 的分桶概念是一致的。 当调用 HashMap 的 put() 方法存储数据时，程序会先对 key 值调用 hashCode() 方法计算出 hashcode，然后对数组长度取模计算出 index，最后将数据存储在数组 index 位置的链表上，链表达到一定阈值后会转换为红黑树 (JDK1.8+)。下图为 HashMap 的数据结构图： 图片引用自：HashMap vs. Hashtable 1.3 创建分桶表在 Hive 中，我们可以通过 CLUSTERED BY 指定分桶列，并通过 SORTED BY 指定桶中数据的排序参考列。下面为分桶表建表语句示例： 123456789101112CREATE EXTERNAL TABLE emp_bucket( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) CLUSTERED BY(empno) SORTED BY(empno ASC) INTO 4 BUCKETS --按照员工编号散列到四个 bucket 中 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t" LOCATION '/hive/emp_bucket'; 1.4 加载数据到分桶表这里直接使用 Load 语句向分桶表加载数据，数据时可以加载成功的，但是数据并不会分桶。 这是由于分桶的实质是对指定字段做了 hash 散列然后存放到对应文件中，这意味着向分桶表中插入数据是必然要通过 MapReduce，且 Reducer 的数量必须等于分桶的数量。由于以上原因，分桶表的数据通常只能使用 CTAS(CREATE TABLE AS SELECT) 方式插入，因为 CTAS 操作会触发 MapReduce。加载数据步骤如下： 1. 设置强制分桶1set hive.enforce.bucketing = true; --Hive 2.x 不需要这一步 在 Hive 0.x and 1.x 版本，必须使用设置 hive.enforce.bucketing = true，表示强制分桶，允许程序根据表结构自动选择正确数量的 Reducer 和 cluster by column 来进行分桶。 2. CTAS导入数据1INSERT INTO TABLE emp_bucket SELECT * FROM emp; --这里的 emp 表就是一张普通的雇员表 可以从执行日志看到 CTAS 触发 MapReduce 操作，且 Reducer 数量和建表时候指定 bucket 数量一致： 1.5 查看分桶文件bucket(桶) 本质上就是表目录下的具体文件： 三、分区表和分桶表结合使用分区表和分桶表的本质都是将数据按照不同粒度进行拆分，从而使得在查询时候不必扫描全表，只需要扫描对应的分区或分桶，从而提升查询效率。两者可以结合起来使用，从而保证表数据在不同粒度上都能得到合理的拆分。下面是 Hive 官方给出的示例： 12345678910111213CREATE TABLE page_view_bucketed( viewTime INT, userid BIGINT, page_url STRING, referrer_url STRING, ip STRING ) PARTITIONED BY(dt STRING) CLUSTERED BY(userid) SORTED BY(viewTime) INTO 32 BUCKETS ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' STORED AS SEQUENCEFILE; 此时导入数据时需要指定分区： 123INSERT OVERWRITE page_view_bucketedPARTITION (dt='2009-02-25')SELECT * FROM page_view WHERE dt='2009-02-25'; 参考资料 LanguageManual DDL BucketedTables]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>分区分桶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hive常用DDL操作]]></title>
    <url>%2F2019%2F06%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHive%E5%B8%B8%E7%94%A8DDL%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Hive常用DDL操作一、Database1.1 查看数据列表1.2 使用数据库1.3 新建数据库1.4 查看数据库信息1.5 删除数据库二、创建表2.1 建表语法2.2 内部表2.3 外部表2.4 分区表2.5 分桶表2.6 倾斜表2.7 临时表2.8 CTAS创建表2.9 复制表结构2.10 加载数据到表三、修改表3.1 重命名表3.2 修改列3.3 新增列四、清空表/删除表4.1 清空表4.2 删除表五、其他命令5.1 Describe5.2 Show 一、Database1.1 查看数据列表1show databases; 1.2 使用数据库1USE database_name; 1.3 新建数据库语法： 1234CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name --DATABASE|SCHEMA 是等价的 [COMMENT database_comment] --数据库注释 [LOCATION hdfs_path] --存储在 HDFS 上的位置 [WITH DBPROPERTIES (property_name=property_value, ...)]; --指定额外属性 示例： 123CREATE DATABASE IF NOT EXISTS hive_test COMMENT 'hive database for test' WITH DBPROPERTIES ('create'='heibaiying'); 1.4 查看数据库信息语法： 1DESC DATABASE [EXTENDED] db_name; --EXTENDED 表示是否显示额外属性 示例： 1DESC DATABASE EXTENDED hive_test; 1.5 删除数据库语法： 1DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE]; 默认行为是 RESTRICT，如果数据库中存在表则删除失败。要想删除库及其中的表，可以使用 CASCADE 级联删除。 示例： 1DROP DATABASE IF EXISTS hive_test CASCADE; 二、创建表2.1 建表语法1234567891011121314151617181920CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name --表名 [(col_name data_type [COMMENT col_comment], ... [constraint_specification])] --列名 列数据类型 [COMMENT table_comment] --表描述 [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)] --分区表分区规则 [ CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS ] --分桶表分桶规则 [SKEWED BY (col_name, col_name, ...) ON ((col_value, col_value, ...), (col_value, col_value, ...), ...) [STORED AS DIRECTORIES] ] --指定倾斜列和值 [ [ROW FORMAT row_format] [STORED AS file_format] | STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] ] -- 指定行分隔符、存储文件格式或采用自定义存储格式 [LOCATION hdfs_path] -- 指定表的存储位置 [TBLPROPERTIES (property_name=property_value, ...)] --指定表的属性 [AS select_statement]; --从查询结果创建表 2.2 内部表12345678910CREATE TABLE emp( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"; 2.3 外部表1234567891011CREATE EXTERNAL TABLE emp_external( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t" LOCATION '/hive/emp_external'; 使用 desc format emp_external 命令可以查看表的详细信息如下： 2.4 分区表123456789101112CREATE EXTERNAL TABLE emp_partition( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) PARTITIONED BY (deptno INT) -- 按照部门编号进行分区 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t" LOCATION '/hive/emp_partition'; 2.5 分桶表123456789101112CREATE EXTERNAL TABLE emp_bucket( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2), deptno INT) CLUSTERED BY(empno) SORTED BY(empno ASC) INTO 4 BUCKETS --按照员工编号散列到四个 bucket 中 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t" LOCATION '/hive/emp_bucket'; 2.6 倾斜表通过指定一个或者多个列经常出现的值（严重偏斜），Hive 会自动将涉及到这些值的数据拆分为单独的文件。在查询时，如果涉及到倾斜值，它就直接从独立文件中获取数据，而不是扫描所有文件，这使得性能得到提升。 123456789101112CREATE EXTERNAL TABLE emp_skewed( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) SKEWED BY (empno) ON (66,88,100) --指定 empno 的倾斜值 66,88,100 ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t" LOCATION '/hive/emp_skewed'; 2.7 临时表临时表仅对当前 session 可见，临时表的数据将存储在用户的暂存目录中，并在会话结束后删除。如果临时表与永久表表名相同，则对该表名的任何引用都将解析为临时表，而不是永久表。临时表还具有以下两个限制： 不支持分区列； 不支持创建索引。 12345678910CREATE TEMPORARY TABLE emp_temp( empno INT, ename STRING, job STRING, mgr INT, hiredate TIMESTAMP, sal DECIMAL(7,2), comm DECIMAL(7,2) ) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"; 2.8 CTAS创建表支持从查询语句的结果创建表： 1CREATE TABLE emp_copy AS SELECT * FROM emp WHERE deptno='20'; 2.9 复制表结构语法： 123CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name --创建表表名 LIKE existing_table_or_view_name --被复制表的表名 [LOCATION hdfs_path]; --存储位置 示例： 1CREATE TEMPORARY EXTERNAL TABLE IF NOT EXISTS emp_co LIKE emp 2.10 加载数据到表加载数据到表中属于 DML 操作，这里为了方便大家测试，先简单介绍一下加载本地数据到表中： 12-- 加载数据到 emp 表中load data local inpath "/usr/file/emp.txt" into table emp; 其中 emp.txt 的内容如下，你可以直接复制使用，也可以到本仓库的resources 目录下载： 12345678910111213147369 SMITH CLERK 7902 1980-12-17 00:00:00 800.00 207499 ALLEN SALESMAN 7698 1981-02-20 00:00:00 1600.00 300.00 307521 WARD SALESMAN 7698 1981-02-22 00:00:00 1250.00 500.00 307566 JONES MANAGER 7839 1981-04-02 00:00:00 2975.00 207654 MARTIN SALESMAN 7698 1981-09-28 00:00:00 1250.00 1400.00 307698 BLAKE MANAGER 7839 1981-05-01 00:00:00 2850.00 307782 CLARK MANAGER 7839 1981-06-09 00:00:00 2450.00 107788 SCOTT ANALYST 7566 1987-04-19 00:00:00 1500.00 207839 KING PRESIDENT 1981-11-17 00:00:00 5000.00 107844 TURNER SALESMAN 7698 1981-09-08 00:00:00 1500.00 0.00 307876 ADAMS CLERK 7788 1987-05-23 00:00:00 1100.00 207900 JAMES CLERK 7698 1981-12-03 00:00:00 950.00 307902 FORD ANALYST 7566 1981-12-03 00:00:00 3000.00 207934 MILLER CLERK 7782 1982-01-23 00:00:00 1300.00 10 加载后可查询表中数据： 三、修改表3.1 重命名表语法： 1ALTER TABLE table_name RENAME TO new_table_name; 示例： 1ALTER TABLE emp_temp RENAME TO new_emp; --把 emp_temp 表重命名为 new_emp 3.2 修改列语法： 12ALTER TABLE table_name [PARTITION partition_spec] CHANGE [COLUMN] col_old_name col_new_name column_type [COMMENT col_comment] [FIRST|AFTER column_name] [CASCADE|RESTRICT]; 示例： 12345678-- 修改字段名和类型ALTER TABLE emp_temp CHANGE empno empno_new INT; -- 修改字段 sal 的名称 并将其放置到 empno 字段后ALTER TABLE emp_temp CHANGE sal sal_new decimal(7,2) AFTER ename;-- 为字段增加注释ALTER TABLE emp_temp CHANGE mgr mgr_new INT COMMENT 'this is column mgr'; 3.3 新增列示例： 1ALTER TABLE emp_temp ADD COLUMNS (address STRING COMMENT 'home address'); 四、清空表/删除表4.1 清空表语法： 12-- 清空整个表或表指定分区中的数据TRUNCATE TABLE table_name [PARTITION (partition_column = partition_col_value, ...)]; 目前只有内部表才能执行 TRUNCATE 操作，外部表执行时会抛出异常 Cannot truncate non-managed table XXXX。 示例： 1TRUNCATE TABLE emp_mgt_ptn PARTITION (deptno=20); 4.2 删除表语法： 1DROP TABLE [IF EXISTS] table_name [PURGE]; 内部表：不仅会删除表的元数据，同时会删除 HDFS 上的数据； 外部表：只会删除表的元数据，不会删除 HDFS 上的数据； 删除视图引用的表时，不会给出警告（但视图已经无效了，必须由用户删除或重新创建）。 五、其他命令5.1 Describe查看数据库： 1DESCRIBE|Desc DATABASE [EXTENDED] db_name; --EXTENDED 是否显示额外属性 查看表： 1DESCRIBE|Desc [EXTENDED|FORMATTED] table_name --FORMATTED 以友好的展现方式查看表详情 5.2 Show1. 查看数据库列表 12345-- 语法SHOW (DATABASES|SCHEMAS) [LIKE 'identifier_with_wildcards'];-- 示例：SHOW DATABASES like 'hive*'; LIKE 子句允许使用正则表达式进行过滤，但是 SHOW 语句当中的 LIKE 子句只支持 *（通配符）和 |（条件或）两个符号。例如 employees，emp *，emp * | * ees，所有这些都将匹配名为 employees 的数据库。 2. 查看表的列表 12345-- 语法SHOW TABLES [IN database_name] ['identifier_with_wildcards'];-- 示例SHOW TABLES IN default; 3. 查看视图列表 1SHOW VIEWS [IN/FROM database_name] [LIKE 'pattern_with_wildcards']; --仅支持 Hive 2.2.0 + 4. 查看表的分区列表 1SHOW PARTITIONS table_name; 5. 查看表/视图的创建语句 1SHOW CREATE TABLE ([db_name.]table_name|view_name); 参考资料LanguageManual DDL]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>DDL操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之HiveCLI和Beeline命令行的基本使用]]></title>
    <url>%2F2019%2F06%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHiveCLI%E5%92%8CBeeline%E5%91%BD%E4%BB%A4%E8%A1%8C%E7%9A%84%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Hive CLI和Beeline命令行的基本使用一、Hive CLI1.1 Help1.2 交互式命令行1.3 执行SQL命令1.4 执行SQL脚本1.5 配置Hive变量1.6 配置文件启动1.7 用户自定义变量二、Beeline 2.1 HiveServer22.1 Beeline2.3 常用参数三、Hive配置3.1 配置文件3.2 hiveconf3.3 set3.4 配置优先级3.5 配置参数 一、Hive CLI1.1 Help使用 hive -H 或者 hive --help 命令可以查看所有命令的帮助，显示如下： 12345678910111213usage: hive -d,--define &lt;key=value&gt; Variable subsitution to apply to hive commands. e.g. -d A=B or --define A=B --定义用户自定义变量 --database &lt;databasename&gt; Specify the database to use -- 指定使用的数据库 -e &lt;quoted-query-string&gt; SQL from command line -- 执行指定的 SQL -f &lt;filename&gt; SQL from files --执行 SQL 脚本 -H,--help Print help information -- 打印帮助信息 --hiveconf &lt;property=value&gt; Use value for given property --自定义配置 --hivevar &lt;key=value&gt; Variable subsitution to apply to hive --自定义变量 commands. e.g. --hivevar A=B -i &lt;filename&gt; Initialization SQL file --在进入交互模式之前运行初始化脚本 -S,--silent Silent mode in interactive shell --静默模式 -v,--verbose Verbose mode (echo executed SQL to the console) --详细模式 1.2 交互式命令行直接使用 Hive 命令，不加任何参数，即可进入交互式命令行。 1.3 执行SQL命令在不进入交互式命令行的情况下，可以使用 hive -e 执行 SQL 命令。 1hive -e 'select * from emp'; 1.4 执行SQL脚本用于执行的 sql 脚本可以在本地文件系统，也可以在 HDFS 上。 12345# 本地文件系统hive -f /usr/file/simple.sql;# HDFS文件系统hive -f hdfs://hadoop001:8020/tmp/simple.sql; 其中 simple.sql 内容如下： 1select * from emp; 1.5 配置Hive变量可以使用 --hiveconf 设置 Hive 运行时的变量。 123hive -e 'select * from emp' \--hiveconf hive.exec.scratchdir=/tmp/hive_scratch \--hiveconf mapred.reduce.tasks=4; hive.exec.scratchdir：指定 HDFS 上目录位置，用于存储不同 map/reduce 阶段的执行计划和这些阶段的中间输出结果。 1.6 配置文件启动使用 -i 可以在进入交互模式之前运行初始化脚本，相当于指定配置文件启动。 1hive -i /usr/file/hive-init.conf; 其中 hive-init.conf 的内容如下： 1set hive.exec.mode.local.auto = true; hive.exec.mode.local.auto 默认值为 false，这里设置为 true ，代表开启本地模式。 1.7 用户自定义变量--define &lt;key=value&gt; 和 --hivevar &lt;key=value&gt; 在功能上是等价的，都是用来实现自定义变量，这里给出一个示例: 定义变量： 1hive --define n=ename --hiveconf --hivevar j=job; 在查询中引用自定义变量： 1234567# 以下两条语句等价hive &gt; select $&#123;n&#125; from emp;hive &gt; select $&#123;hivevar:n&#125; from emp;# 以下两条语句等价hive &gt; select $&#123;j&#125; from emp;hive &gt; select $&#123;hivevar:j&#125; from emp; 结果如下： 二、Beeline2.1 HiveServer2Hive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，所以产生了 HiveServer2。 HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务器。 HiveServer2 拥有自己的 CLI(Beeline)，Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于 HiveServer2 是 Hive 开发维护的重点 (Hive0.15 后就不再支持 hiveserver)，所以 Hive CLI 已经不推荐使用了，官方更加推荐使用 Beeline。 2.1 BeelineBeeline 拥有更多可使用参数，可以使用 beeline --help 查看，完整参数如下： 1234567891011121314151617181920212223242526272829303132333435363738394041Usage: java org.apache.hive.cli.beeline.BeeLine -u &lt;database url&gt; the JDBC URL to connect to -r reconnect to last saved connect url (in conjunction with !save) -n &lt;username&gt; the username to connect as -p &lt;password&gt; the password to connect as -d &lt;driver class&gt; the driver class to use -i &lt;init file&gt; script file for initialization -e &lt;query&gt; query that should be executed -f &lt;exec file&gt; script file that should be executed -w (or) --password-file &lt;password file&gt; the password file to read password from --hiveconf property=value Use value for given property --hivevar name=value hive variable name and value This is Hive specific settings in which variables can be set at session level and referenced in Hive commands or queries. --property-file=&lt;property-file&gt; the file to read connection properties (url, driver, user, password) from --color=[true/false] control whether color is used for display --showHeader=[true/false] show column names in query results --headerInterval=ROWS; the interval between which heades are displayed --fastConnect=[true/false] skip building table/column list for tab-completion --autoCommit=[true/false] enable/disable automatic transaction commit --verbose=[true/false] show verbose error messages and debug info --showWarnings=[true/false] display connection warnings --showNestedErrs=[true/false] display nested errors --numberFormat=[pattern] format numbers using DecimalFormat pattern --force=[true/false] continue running script even after errors --maxWidth=MAXWIDTH the maximum width of the terminal --maxColumnWidth=MAXCOLWIDTH the maximum width to use when displaying columns --silent=[true/false] be more silent --autosave=[true/false] automatically save preferences --outputformat=[table/vertical/csv2/tsv2/dsv/csv/tsv] format mode for result display --incrementalBufferRows=NUMROWS the number of rows to buffer when printing rows on stdout, defaults to 1000; only applicable if --incremental=true and --outputformat=table --truncateTable=[true/false] truncate table column when it exceeds length --delimiterForDSV=DELIMITER specify the delimiter for delimiter-separated values output format (default: |) --isolation=LEVEL set the transaction isolation level --nullemptystring=[true/false] set to true to get historic behavior of printing null as empty string --maxHistoryRows=MAXHISTORYROWS The maximum number of rows to store beeline history. --convertBinaryArrayToString=[true/false] display binary column data as string or as byte array --help display this message 2.3 常用参数在 Hive CLI 中支持的参数，Beeline 都支持，常用的参数如下。更多参数说明可以参见官方文档 Beeline Command Options 参数 说明 -u \ 数据库地址 -n \ 用户名 -p \ 密码 -d \ 驱动 (可选) -e \ 执行 SQL 命令 -f \ 执行 SQL 脚本 -i (or)–init \ 在进入交互模式之前运行初始化脚本 –property-file \ 指定配置文件 –hiveconf property=value 指定配置属性 –hivevar name=value 用户自定义属性，在会话级别有效 示例： 使用用户名和密码连接 Hive 1$ beeline -u jdbc:hive2://localhost:10000 -n username -p password ​ 三、Hive配置可以通过三种方式对 Hive 的相关属性进行配置，分别介绍如下： 3.1 配置文件方式一为使用配置文件，使用配置文件指定的配置是永久有效的。Hive 有以下三个可选的配置文件： hive-site.xml ：Hive 的主要配置文件； hivemetastore-site.xml： 关于元数据的配置； hiveserver2-site.xml：关于 HiveServer2 的配置。 示例如下,在 hive-site.xml 配置 hive.exec.scratchdir： 12345&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;/tmp/mydir&lt;/value&gt; &lt;description&gt;Scratch space for Hive jobs&lt;/description&gt; &lt;/property&gt; 3.2 hiveconf方式二为在启动命令行 (Hive CLI / Beeline) 的时候使用 --hiveconf 指定配置，这种方式指定的配置作用于整个 Session。 1hive --hiveconf hive.exec.scratchdir=/tmp/mydir 3.3 set方式三为在交互式环境下 (Hive CLI / Beeline)，使用 set 命令指定。这种设置的作用范围也是 Session 级别的，配置对于执行该命令后的所有命令生效。set 兼具设置参数和查看参数的功能。如下： 123456780: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir=/tmp/mydir;No rows affected (0.025 seconds)0: jdbc:hive2://hadoop001:10000&gt; set hive.exec.scratchdir;+----------------------------------+--+| set |+----------------------------------+--+| hive.exec.scratchdir=/tmp/mydir |+----------------------------------+--+ 3.4 配置优先级配置的优先顺序如下 (由低到高)：hive-site.xml - &gt;hivemetastore-site.xml- &gt; hiveserver2-site.xml - &gt;-- hiveconf- &gt; set 3.5 配置参数Hive 可选的配置参数非常多，在用到时查阅官方文档即可AdminManual Configuration 参考资料 HiveServer2 Clients LanguageManual Cli AdminManual Configuration]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>基本命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Linux环境下Hive的安装部署]]></title>
    <url>%2F2019%2F06%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BLinux%E7%8E%AF%E5%A2%83%E4%B8%8BHive%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Linux环境下Hive的安装一、安装Hive1.1 下载并解压1.2 配置环境变量1.3 修改配置1.4 拷贝数据库驱动1.5 初始化元数据库1.6 启动二、HiveServer2/beeline2.1 修改Hadoop配置2.2 启动hiveserver22.3 使用beeline 一、安装Hive1.1 下载并解压下载所需版本的 Hive，这里我下载版本为 cdh5.15.2。下载地址：http://archive.cloudera.com/cdh5/cdh/5/ 12# 下载后进行解压 tar -zxvf hive-1.1.0-cdh5.15.2.tar.gz 1.2 配置环境变量1# vim /etc/profile 添加环境变量： 12export HIVE_HOME=/usr/app/hive-1.1.0-cdh5.15.2export PATH=$HIVE_HOME/bin:$PATH 使得配置的环境变量立即生效： 1# source /etc/profile 1.3 修改配置1. hive-env.sh 进入安装目录下的 conf/ 目录，拷贝 Hive 的环境配置模板 flume-env.sh.template 1cp hive-env.sh.template hive-env.sh 修改 hive-env.sh，指定 Hadoop 的安装路径： 1HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2 2. hive-site.xml 新建 hive-site.xml 文件，内容如下，主要是配置存放元数据的 MySQL 的地址、驱动、用户名和密码等信息： 12345678910111213141516171819202122232425&lt;?xml version="1.0"?&gt;&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://hadoop001:3306/hadoop_hive?createDatabaseIfNotExist=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 1.4 拷贝数据库驱动将 MySQL 驱动包拷贝到 Hive 安装目录的 lib 目录下, MySQL 驱动的下载地址为：https://dev.mysql.com/downloads/connector/j/ , 在本仓库的resources 目录下我也上传了一份，有需要的可以自行下载。 1.5 初始化元数据库 当使用的 hive 是 1.x 版本时，可以不进行初始化操作，Hive 会在第一次启动的时候会自动进行初始化，但不会生成所有的元数据信息表，只会初始化必要的一部分，在之后的使用中用到其余表时会自动创建； 当使用的 hive 是 2.x 版本时，必须手动初始化元数据库。初始化命令： 12# schematool 命令在安装目录的 bin 目录下，由于上面已经配置过环境变量，在任意位置执行即可schematool -dbType mysql -initSchema 这里我使用的是 CDH 的 hive-1.1.0-cdh5.15.2.tar.gz，对应 Hive 1.1.0 版本，可以跳过这一步。 1.6 启动由于已经将 Hive 的 bin 目录配置到环境变量，直接使用以下命令启动，成功进入交互式命令行后执行 show databases 命令，无异常则代表搭建成功。 1# hive 在 Mysql 中也能看到 Hive 创建的库和存放元数据信息的表 二、HiveServer2/beelineHive 内置了 HiveServer 和 HiveServer2 服务，两者都允许客户端使用多种编程语言进行连接，但是 HiveServer 不能处理多个客户端的并发请求，因此产生了 HiveServer2。HiveServer2（HS2）允许远程客户端可以使用各种编程语言向 Hive 提交请求并检索结果，支持多客户端并发访问和身份验证。HS2 是由多个服务组成的单个进程，其包括基于 Thrift 的 Hive 服务（TCP 或 HTTP）和用于 Web UI 的 Jetty Web 服务。 HiveServer2 拥有自己的 CLI 工具——Beeline。Beeline 是一个基于 SQLLine 的 JDBC 客户端。由于目前 HiveServer2 是 Hive 开发维护的重点，所以官方更加推荐使用 Beeline 而不是 Hive CLI。以下主要讲解 Beeline 的配置方式。 2.1 修改Hadoop配置修改 hadoop 集群的 core-site.xml 配置文件，增加如下配置，指定 hadoop 的 root 用户可以代理本机上所有的用户。 12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 之所以要配置这一步，是因为 hadoop 2.0 以后引入了安全伪装机制，使得 hadoop 不允许上层系统（如 hive）直接将实际用户传递到 hadoop 层，而应该将实际用户传递给一个超级代理，由该代理在 hadoop 上执行操作，以避免任意客户端随意操作 hadoop。如果不配置这一步，在之后的连接中可能会抛出 AuthorizationException 异常。 关于 Hadoop 的用户代理机制，可以参考：hadoop 的用户代理机制 或 Superusers Acting On Behalf Of Other Users 2.2 启动hiveserver2由于上面已经配置过环境变量，这里直接启动即可： 1# nohup hiveserver2 &amp; 2.3 使用beeline可以使用以下命令进入 beeline 交互式命令行，出现 Connected 则代表连接成功。 1# beeline -u jdbc:hive2://hadoop001:10000 -n root]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>环境部署</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hive简介及核心概念]]></title>
    <url>%2F2019%2F06%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHive%E7%AE%80%E4%BB%8B%E5%8F%8A%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[Hive简介及核心概念一、简介二、Hive的体系架构三、数据类型3.1 基本数据类型3.2 隐式转换3.3 复杂类型四、内容格式五、存储格式六、内部表和外部表 一、简介Hive 是一个构建在 Hadoop 之上的数据仓库，它可以将结构化的数据文件映射成表，并提供类 SQL 查询功能，用于查询的 SQL 语句会被转化为 MapReduce 作业，然后提交到 Hadoop 上运行。 特点： 简单、容易上手 (提供了类似 sql 的查询语言 hql)，使得精通 sql 但是不了解 Java 编程的人也能很好地进行大数据分析； 灵活性高，可以自定义用户函数 (UDF) 和存储格式； 为超大的数据集设计的计算和存储能力，集群扩展容易; 统一的元数据管理，可与 presto／impala／sparksql 等共享数据； 执行延迟高，不适合做数据的实时处理，但适合做海量数据的离线处理。 二、Hive的体系架构 2.1 command-line shell &amp; thrift/jdbc可以用 command-line shell 和 thrift／jdbc 两种方式来操作数据： command-line shell：通过 hive 命令行的的方式来操作数据； thrift／jdbc：通过 thrift 协议按照标准的 JDBC 的方式操作数据。 2.2 Metastore在 Hive 中，表名、表结构、字段名、字段类型、表的分隔符等统一被称为元数据。所有的元数据默认存储在 Hive 内置的 derby 数据库中，但由于 derby 只能有一个实例，也就是说不能有多个命令行客户端同时访问，所以在实际生产环境中，通常使用 MySQL 代替 derby。 Hive 进行的是统一的元数据管理，就是说你在 Hive 上创建了一张表，然后在 presto／impala／sparksql 中都是可以直接使用的，它们会从 Metastore 中获取统一的元数据信息，同样的你在 presto／impala／sparksql 中创建一张表，在 Hive 中也可以直接使用。 2.3 HQL的执行流程Hive 在执行一条 HQL 的时候，会经过以下步骤： 语法解析：Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象 语法树 AST Tree； 语义解析：遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock； 生成逻辑执行计划：遍历 QueryBlock，翻译为执行操作树 OperatorTree； 优化逻辑执行计划：逻辑层优化器进行 OperatorTree 变换，合并不必要的 ReduceSinkOperator，减少 shuffle 数据量； 生成物理执行计划：遍历 OperatorTree，翻译为 MapReduce 任务； 优化物理执行计划：物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。 关于 Hive SQL 的详细执行流程可以参考美团技术团队的文章：Hive SQL 的编译过程 三、数据类型3.1 基本数据类型Hive 表中的列支持以下基本数据类型： 大类 类型 Integers（整型） TINYINT—1 字节的有符号整数 SMALLINT—2 字节的有符号整数 INT—4 字节的有符号整数 BIGINT—8 字节的有符号整数 Boolean（布尔型） BOOLEAN—TRUE/FALSE Floating point numbers（浮点型） FLOAT— 单精度浮点型 DOUBLE—双精度浮点型 Fixed point numbers（定点数） DECIMAL—用户自定义精度定点数，比如 DECIMAL(7,2) String types（字符串） STRING—指定字符集的字符序列 VARCHAR—具有最大长度限制的字符序列 CHAR—固定长度的字符序列 Date and time types（日期时间类型） TIMESTAMP — 时间戳 TIMESTAMP WITH LOCAL TIME ZONE — 时间戳，纳秒精度 DATE—日期类型 Binary types（二进制类型） BINARY—字节序列 TIMESTAMP 和 TIMESTAMP WITH LOCAL TIME ZONE 的区别如下： TIMESTAMP WITH LOCAL TIME ZONE：用户提交时间给数据库时，会被转换成数据库所在的时区来保存。查询时则按照查询客户端的不同，转换为查询客户端所在时区的时间。 TIMESTAMP ：提交什么时间就保存什么时间，查询时也不做任何转换。 3.2 隐式转换Hive 中基本数据类型遵循以下的层次结构，按照这个层次结构，子类型到祖先类型允许隐式转换。例如 INT 类型的数据允许隐式转换为 BIGINT 类型。额外注意的是：按照类型层次结构允许将 STRING 类型隐式转换为 DOUBLE 类型。 3.3 复杂类型 类型 描述 示例 STRUCT 类似于对象，是字段的集合，字段的类型可以不同，可以使用 名称.字段名 方式进行访问 STRUCT (‘xiaoming’, 12 , ‘2018-12-12’) MAP 键值对的集合，可以使用 名称[key] 的方式访问对应的值 map(‘a’, 1, ‘b’, 2) ARRAY 数组是一组具有相同类型和名称的变量的集合，可以使用 名称[index] 访问对应的值 ARRAY(‘a’, ‘b’, ‘c’, ‘d’) 3.4 示例如下给出一个基本数据类型和复杂数据类型的使用示例： 1234567CREATE TABLE students( name STRING, -- 姓名 age INT, -- 年龄 subject ARRAY&lt;STRING&gt;, --学科 score MAP&lt;STRING,FLOAT&gt;, --各个学科考试成绩 address STRUCT&lt;houseNumber:int, street:STRING, city:STRING, province：STRING&gt; --家庭居住地址) ROW FORMAT DELIMITED FIELDS TERMINATED BY "\t"; 四、内容格式当数据存储在文本文件中，必须按照一定格式区别行和列，如使用逗号作为分隔符的 CSV 文件 (Comma-Separated Values) 或者使用制表符作为分隔值的 TSV 文件 (Tab-Separated Values)。但此时也存在一个缺点，就是正常的文件内容中也可能出现逗号或者制表符。 所以 Hive 默认使用了几个平时很少出现的字符，这些字符一般不会作为内容出现在文件中。Hive 默认的行和列分隔符如下表所示。 分隔符 描述 \n 对于文本文件来说，每行是一条记录，所以可以使用换行符来分割记录 ^A (Ctrl+A) 分割字段 (列)，在 CREATE TABLE 语句中也可以使用八进制编码 \001 来表示 ^B 用于分割 ARRAY 或者 STRUCT 中的元素，或者用于 MAP 中键值对之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 \002 表示 ^C 用于 MAP 中键和值之间的分割，在 CREATE TABLE 语句中也可以使用八进制编码 \003 表示 使用示例如下： 123456CREATE TABLE page_view(viewTime INT, userid BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' STORED AS SEQUENCEFILE; 五、存储格式5.1 支持的存储格式Hive 会在 HDFS 为每个数据库上创建一个目录，数据库中的表是该目录的子目录，表中的数据会以文件的形式存储在对应的表目录下。Hive 支持以下几种文件存储格式： 格式 说明 TextFile 存储为纯文本文件。 这是 Hive 默认的文件存储格式。这种存储方式数据不做压缩，磁盘开销大，数据解析开销大。 SequenceFile SequenceFile 是 Hadoop API 提供的一种二进制文件，它将数据以&lt;key,value&gt;的形式序列化到文件中。这种二进制文件内部使用 Hadoop 的标准的 Writable 接口实现序列化和反序列化。它与 Hadoop API 中的 MapFile 是互相兼容的。Hive 中的 SequenceFile 继承自 Hadoop API 的 SequenceFile，不过它的 key 为空，使用 value 存放实际的值，这样是为了避免 MR 在运行 map 阶段进行额外的排序操作。 RCFile RCFile 文件格式是 FaceBook 开源的一种 Hive 的文件存储格式，首先将表分为几个行组，对每个行组内的数据按列存储，每一列的数据都是分开存储。 ORC Files ORC 是在一定程度上扩展了 RCFile，是对 RCFile 的优化。 Avro Files Avro 是一个数据序列化系统，设计用于支持大批量数据交换的应用。它的主要特点有：支持二进制序列化方式，可以便捷，快速地处理大量数据；动态语言友好，Avro 提供的机制使动态语言可以方便地处理 Avro 数据。 Parquet Parquet 是基于 Dremel 的数据模型和算法实现的，面向分析型业务的列式存储格式。它通过按列进行高效压缩和特殊的编码技术，从而在降低存储空间的同时提高了 IO 效率。 以上压缩格式中 ORC 和 Parquet 的综合性能突出，使用较为广泛，推荐使用这两种格式。 5.2 指定存储格式通常在创建表的时候使用 STORED AS 参数指定： 123456CREATE TABLE page_view(viewTime INT, userid BIGINT) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\001' COLLECTION ITEMS TERMINATED BY '\002' MAP KEYS TERMINATED BY '\003' STORED AS SEQUENCEFILE; 各个存储文件类型指定方式如下： STORED AS TEXTFILE STORED AS SEQUENCEFILE STORED AS ORC STORED AS PARQUET STORED AS AVRO STORED AS RCFILE 六、内部表和外部表内部表又叫做管理表 (Managed/Internal Table)，创建表时不做任何指定，默认创建的就是内部表。想要创建外部表 (External Table)，则需要使用 External 进行修饰。 内部表和外部表主要区别如下： 内部表 外部表 数据存储位置 内部表数据存储的位置由 hive.metastore.warehouse.dir 参数指定，默认情况下表的数据存储在 HDFS 的 /user/hive/warehouse/数据库名.db/表名/ 目录下 外部表数据的存储位置创建表时由 Location 参数指定； 导入数据 在导入数据到内部表，内部表将数据移动到自己的数据仓库目录下，数据的生命周期由 Hive 来进行管理 外部表不会将数据移动到自己的数据仓库目录下，只是在元数据中存储了数据的位置 删除表 删除元数据（metadata）和文件 只删除元数据（metadata） 参考资料 Hive Getting Started Hive SQL 的编译过程 LanguageManual DDL LanguageManual Types Managed vs. External Tables]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>核心概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之MapReduce编程模型和计算框架架构原理]]></title>
    <url>%2F2019%2F06%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BMapReduce%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B%E5%92%8C%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%E6%9E%B6%E6%9E%84%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[Hadoop解决大规模数据分布式计算的方案是MapReduce。MapReduce既是一个编程模型，又是一个计算框架。也就是说，开发人员必须基于MapReduce编程模型进行编程开发，然后将程序通过MapReduce计算框架分发到Hadoop集群中运行。我们先看一下作为编程模型的MapReduce。 MapReduce编程模型MapReduce是一种非常简单又非常强大的编程模型。 简单在于其编程模型只包含map和reduce两个过程，map的主要输入是一对值，经过map计算后输出一对值；然后将相同key合并，形成；再将这个输入reduce，经过计算输出零个或多个对。 但是MapReduce同时又是非常强大的，不管是关系代数运算（SQL计算），还是矩阵运算（图计算），大数据领域几乎所有的计算需求都可以通过MapReduce编程来实现。 我们以WordCount程序为例。WordCount主要解决文本处理中的词频统计问题，就是统计文本中每一个单词出现的次数。如果只是统计一篇文章的词频，几十K到几M的数据，那么写一个程序，将数据读入内存，建一个Hash表记录每个词出现的次数就可以了，如下图。 但是如果想统计全世界互联网所有网页（数万亿计）的词频数（这正是google这样的搜索引擎典型需求），你不可能写一个程序把全世界的网页都读入内存，这时候就需要用MapReduce编程来解决。 WordCount的MapReduce程序如下。 12345678910111213141516171819202122232425262728293031323334public class WordCount &#123;public static class TokenizerMapperextends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123;private final static IntWritable one = new IntWritable(1);private Text word = new Text();public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123;StringTokenizer itr = new StringTokenizer(value.toString());while (itr.hasMoreTokens()) &#123;word.set(itr.nextToken());context.write(word, one);&#125;&#125;&#125;public static class IntSumReducerextends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; &#123;private IntWritable result = new IntWritable();public void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) throws IOException, InterruptedException &#123;int sum = 0;for (IntWritable val : values) &#123;sum += val.get();&#125;result.set(sum);context.write(key, result);&#125;&#125;&#125; 其核心是一个map函数，一个reduce函数。 map函数的输入主要是一个对，在这个例子里，value是要统计的所有文本中的一行数据，key在这里不重要，我们忽略。1public void map(Object key, Text value, Context context) map函数的计算过程就是，将这行文本中的单词提取出来，针对每个单词输出一个这样的对。 MapReduce计算框架会将这些收集起来，将相同的word放在一起，形成&lt;word , &lt;1,1,1,1,1,1,1…..&gt;&gt;这样的数据，然后将其输入给reduce函数。1public void reduce(Text key, Iterable&lt;IntWritable&gt; values,Context context) 这里的reduce的输入参数values就是由很多个1组成的集合，而key就是具体的单词word。 reduce函数的计算过程就是，将这个集合里的1求和，再将单词（word）和这个和（sum）组成一个()输出。每一个输出就是一个单词和它的词频统计总和。 假设有两个block的文本数据需要进行词频统计，MapReduce计算过程如下图。 MapReduce计算过程一个map函数可以针对一部分数据进行运算，这样就可以将一个大数据切分成很多块（这也正是HDFS所做的），MapReduce计算框架为每个块分配一个map函数去计算，从而实现大数据的分布式计算。 上面提到MapReduce编程模型将大数据计算过程切分为map和reduce两个阶段，在map阶段为每个数据块分配一个map计算任务，然后将所有map输出的key进行合并，相同的key及其对应的value发送给同一个reduce任务去处理。 这个过程有两个关键问题需要处理 如何为每个数据块分配一个map计算任务，代码是如何发送数据块所在服务器的，发送过去是如何启动的，启动以后又如何知道自己需要计算的数据在文件什么位置（数据块id是什么） 处于不同服务器的map输出的 ，如何把相同的key聚合在一起发送给reduce任务 这两个关键问题正好对应文章中“MapReduce计算过程”一图中两处“MapReduce框架处理”。 我们先看下MapReduce是如何启动处理一个大数据计算应用作业的。 MapReduce作业启动和运行机制我们以Hadoop1为例，MapReduce运行过程涉及以下几类关键进程： 大数据应用进程：启动用户MapReduce程序的主入口，主要指定Map和Reduce类、输入输出文件路径等，并提交作业给Hadoop集群。 JobTracker进程：根据要处理的输入数据量启动相应数量的map和reduce进程任务，并管理整个作业生命周期的任务调度和监控。JobTracker进程在整个Hadoop集群全局唯一。 TaskTracker进程：负责启动和管理map进程以及reduce进程。因为需要每个数据块都有对应的map函数，TaskTracker进程通常和HDFS的DataNode进程启动在同一个服务器，也就是说，Hadoop集群中绝大多数服务器同时运行DataNode进程和TaskTacker进程。 如下图所示。 具体作业启动和计算过程如下： 应用进程将用户作业jar包存储在HDFS中，将来这些jar包会分发给Hadoop集群中的服务器执行MapReduce计算。 应用程序提交job作业给JobTracker。 JobTacker根据作业调度策略创建JobInProcess树，每个作业都会有一个自己的JobInProcess树。 JobInProcess根据输入数据分片数目（通常情况就是数据块的数目）和设置的reduce数目创建相应数量的TaskInProcess。 TaskTracker进程和JobTracker进程进行定时通信。 如果TaskTracker有空闲的计算资源（空闲CPU核），JobTracker就会给他分配任务。分配任务的时候会根据TaskTracker的服务器名字匹配在同一台机器上的数据块计算任务给它，使启动的计算任务正好处理本机上的数据。 TaskRunner收到任务后根据任务类型（map还是reduce），任务参数（作业jar包路径，输入数据文件路径，要处理的数据在文件中的起始位置和偏移量，数据块多个备份的DataNode主机名等）启动相应的map或者reduce进程。 map或者reduce程序启动后，检查本地是否有要执行任务的jar包文件，如果没有，就去HDFS上下载，然后加载map或者reduce代码开始执行。 如果是map进程，从HDFS读取数据（通常要读取的数据块正好存储在本机）。如果是reduce进程，将结果数据写出到HDFS。 通过以上过程，MapReduce可以将大数据作业计算任务分布在整个Hadoop集群中运行，每个map计算任务要处理的数据通常都能从本地磁盘上读取到。而用户要做的仅仅是编写一个map函数和一个reduce函数就可以了，根本不用关心这两个函数是如何被分布启动到集群上的，数据块又是如何分配给计算任务的。这一切都由MapReduce计算框架完成。 MapReduce数据合并与连接机制在WordCount例子中，要统计相同单词在所有输入数据中出现的次数，而一个map只能处理一部分数据，一个热门单词几乎会出现在所有的map中，这些单词必须要合并到一起进行统计才能得到正确的结果。 事实上，几乎所有的大数据计算场景都需要处理数据关联的问题，简单如WordCount只要对key进行合并就可以了，复杂如数据库的join操作，需要对两种类型（或者更多类型）的数据根据key进行连接。 MapReduce计算框架处理数据合并与连接的操作就在map输出与reduce输入之间，这个过程有个专门的词汇来描述，叫做shuffle。 MapReduce shuffle过程每个map任务的计算结果都会写入到本地文件系统，等map任务快要计算完成的时候，MapReduce计算框架会启动shuffle过程，在map端调用一个Partitioner接口，对map产生的每个进行reduce分区选择，然后通过http通信发送给对应的reduce进程。这样不管map位于哪个服务器节点，相同的key一定会被发送给相同的reduce进程。reduce端对收到的进行排序和合并，相同的key放在一起，组成一个传递给reduce执行。 MapReduce框架缺省的Partitioner用key的哈希值对reduce任务数量取模，相同的key一定会落在相同的reduce任务id上，实现上，这样的Partitioner代码只需要一行，如下所示。 1234/** Use &#123;@link Object#hashCode()&#125; to partition. */ public int getPartition(K2 key, V2 value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125; shuffle是大数据计算过程中发生奇迹的地方，不管是MapReduce还是Spark，只要是大数据批处理计算，一定会有shuffle过程，让数据关联起来，数据的内在关系和价值才会呈现出来。不理解shuffle，就会在map和reduce编程中产生困惑，不知道该如何正确设计map的输出和reduce的输入。shuffle也是整个MapReduce过程中最难最消耗性能的地方，在MapReduce早期代码中，一半代码都是关于shuffle处理的。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
        <tag>编程模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hadoop极简入门]]></title>
    <url>%2F2019%2F06%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHadoop%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[其实Hadoop诞生至今已经十多年了，网络上也充斥着关于Hadoop相关知识的的海量资源。但是，有时还是会使刚刚接触大数据领域的童鞋分不清hadoop、hdfs、Yarn和MapReduce等等技术词汇。 Hadoop是ASF(Apache软件基金会)开源的，根据Google开源的三篇大数据论文设计的，一个能够允许大量数据在计算机集群中，通过使用简单的编程模型进行分布式处理的框架。其设计的规模可从单一的服务器到数千台服务器，每一个均可提供局部运算和存储功能。Hadoop并不依赖昂贵的硬件以支持高可用性。Hadoop可以检测并处理应用层上的错误，并可以把错误转移到其他服务器上(让它错误，我在用别的服务器顶上就可以了)，所以Hadoop提供一个基于计算机集群的、高效性的服务。 经过十年的发展，Hadoop这个名词的本身也在不断进化者，目前我们提到Hadoop大多是指大数据的生态圈，这个生态圈包括众多的软件技术(e.g. HBase、Hive和Spark等等)。 有如Spring框架有着最基础的几个模块Context、Bean和Core，其他的模块和项目都是基于这些基础模块构建。Hadoop与之一样，也有最基础的几个模块。 Common: 支持其他模块的公用工具包。 HDFS: 一个可高吞吐访问应用数据的分布式文件系统。 YARN: 一个管理集群服务器资源和任务调度的框架。 MapReduce: 基于Yarn对大数据集进行并行计算的系统。 其它的，像HBase、Hive等等不过在这几个基础模块上的高级抽象。另外Hadoop也不是目前大数据的唯一解决方案，像Amazon的大数据技术方案等等。 CommonCommon模块是Hadoop最为基础的模块，他为其他模块提供了像操作文件系统、I/O、序列化和远程方法调用等最为基础的实现。如果想深入的了解Hadoop的具体实现，可以阅读一下Common的源码。 HDFSHDFS是“Hadoop Distributed File System”的首字母缩写，是一种设计运行在一般硬件条件（不需要一定是服务器级别的设备，但更好的设备能发挥更大的作用）下的分布式文件系统. 他和现有的其他分布式文件系统(e.g. RAID)有很多相似的地方。和其他分布式文件系统的不同之处是HDFS设计为运行在低成本的硬件上(e.g. 普通的PC机)，且提供高可靠性的服务器. HDFS设计满足大数据量，大吞吐量的应用情况。 为了更好的理解分布式文件系统，我们先从文件讲起。 文件文件这个词，恐怕只要是现代人都不会陌生。但是在不同行业中，文件有着不同的意义。在计算机科学领域，文件是什么呢？文件是可以在目录中看的见的图标么？当然不是。文件在存储设备时，是个N长的字节序列。而在一个计算机使用者的角度而言，文件是对所有I/O设备的抽象。每个I/O设备都可以视为文件，包括磁盘、键盘和网络等。文件这个简单而精致的概念其内涵是十分丰富的，它向应用程序提供了一个统一的视角，来看待系统中可能含有的各式各样的I/O设备。 文件系统那么一台计算机上肯定不止一个文件，成千上万的文件怎么管理呢？因此需要我们需要一种对文件进行管理的东西，即文件系统。文件系统是一种在计算机上存储和组织数据的方法，它使得对其访问和查找变得容易，文件系统使用文件和树形目录的抽象逻辑概念代替了硬盘和光盘等物理设备使用数据块的概念，用户使用文件系统来保存数据而不必关心数据实际保存在硬盘的地址为多少的数据块上，只需要记住这个文件的所属目录和文件名。在写入新数据之前，用户不必关心硬盘上的那个块地址没有被使用，硬盘上的存储空间管理(分配和释放)功能由文件系统自动完成，用户只需要记住数据被写入到了哪个文件中即可。 分布式文件系统相对于单机的文件系统而言，分布式文件系统(Distributed file system)。是一种允许文件通过网络在多台主机上分享的文件系统，可让多计算机上的多用户分享文件和存储空间。 在这样的文件系统中，客户端并非直接访问底层的数据存储区块和磁盘。而是通过网络，基于单机文件系统并借由特定的通信协议的帮助，来实现对于文件系统的读写。 分布式文件系统需要拥有的最基本的能力是通过畅通网络I/O来实现数据的复制与容错。也就是说，一方面一个文件是分为多个数据块分布在多个设备中。另一方面，数据块有多个副本分布在不同的设备上。即使有一小部分的设备出现离线和宕机等情况，整体来说文件系统仍然可以持续运作而不会有数据损失。 注意:分布式文件系统和分布式数据存储的界线是模糊的，但一般来说，分布式文件系统是被设计用在局域网，比较强调的是传统文件系统概念的延伸，并通过软件方法来达成容错的目的。而分布式数据存储，则是泛指应用分布式运算技术的文件和数据库等提供数据存储服务的系统。 HDFSHDFS正是Hadoop中负责分布式文件系统的。HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的命名空间以及文件的访问控制。集群中的Datanode一般是一个设备上部署一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的命名空间，用户能够以文件的形式在上面存储数据。实际上，一个文件会被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的命名空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode设备的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。为了保证文件系统的高可靠，往往需要另一个Standby的Namenode在Actived Namenode出现问题后，立刻接管文件系统。 网络上有很多关于hdfs的安装配置手册，本文就不再复述。只提供一个以前项目中应用过的部署架构仅供大家参考。 这个高可用的HDFS架构是由3台zookeeper设备、2台域名服务(DNS)和时间服务(NTP)设备、2台Namenode设备(如果必要Standby可以更多)、一个共享存储设备(NFS)和N个DataNode组成。 Zookeeper负责接受NameNode的心跳，当Actived namenode不向zookeeper报告心跳时，Standby Namenode的监控进程会收到这个消息，从而激活Standby NameNode并接管Active NameNode的工作。 NFS负责为2个NameNode存储EditLog文件，(NameNode 在执行 HDFS 客户端提交的创建文件或者移动文件这样的写操作时，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像，最终再刷新到磁盘。 EditLog 只是在数据恢复的时候起作用。记录在 EditLog 之中的每一个操作又称为一个事务，每个事务有一个整数形式的事务 id 作为编号。EditLog 会被切割为很多段，每一段称为一个 Segment)当发生NameNode切换的情况时，Standby NameNode接管后，会根据EditLog中把未完成的写操作继续下去并开使向EditLog写入新的写操作记录。(此外，hadoop还提供了另一种QJM的EditLog方案) DNS&amp;NTP分布负责整个系统的(包括客户端)域名服务和时间服务。这个在集群部署中是非常有必要的两个存在。首先说一下DNS的必要性，一、Hadoop是极力提倡用机器名作为在HDFS环境中的标识。二、当然可以在/etc/hosts文件中去标明机器名和IP的映射关系，可是请想想如果在一个数千台设备的集群中添加一个设备时，负责系统维护的伙伴会不会骂集群的设计者呢？其次是NTP的必要性，在刚刚开始接触Hadoop集群时我遇到的大概90%的问题是由于各个设备时间不一致导致的。各个设备的时间同步是数据一致性和管理一致性的一个基本保障。 MapReduceMapReduce是一个使用简单的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。 一个MapReduce 作业(job)通常会把输入的数据集切分为若干独立的数据块，由 map任务(task)以完全并行的方式处理它们。框架会对map的输出先进行排序， 然后把结果输入给reduce任务。通常作业的输入和输出都会被存储在文件系统中。 整个框架负责任务的调度和监控，以及重新执行已经失败的任务。 通常，MapReduce框架和HDFS是运行在一相同的设备集群上的，也就是说，计算设备和存储设备通常在一起。这种配置允许框架在那些已经存好数据的设备上高效地调度任务，这可以使整个集群的网络带宽被非常高效地利用。 MapReduce框架由一个单独的master JobTracker 和每个集群设备一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。 用户编写的MapReduce应用程序应该指明输入/输出的文件位置(路径)，并通过实现合适的接口或抽象类提供map和reduce函数。再加上其他作业的参数，就构成了作业配置(job configuration)。然后，job client提交作业(jar包/可执行程序等)和配置信息给JobTracker，后者负责分发这些软件和配置信息给slave、调度任务并监控它们的执行，同时提供状态和诊断信息给job-client。 在抽象的层面上MapReduce是由两个函数Map和Reduce组成的。简单来说，一个Map函数就是对一些独立元素组成的概念上的列表的每一个元素进行指定的操作。事实上，每个元素都是被独立操作的，而原始列表没有被更改，因为这里创建了一个新的列表来保存操作结果。这就是说，Map操作是可以高度并行的。而Reduce函数指的是对Map函数的结果（中间经过洗牌的过程，会把map的结果进行分组）分组后多个列表的元素进行适当的归并。 注意:虽然Hadoop框架是用JavaTM实现的，但MapReduce应用程序则不一定要用 Java来写 。至少Scala是可以写的哟。 附上Scala实现的计算词频的Scala源码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.io.IOExceptionimport java.util.StringTokenizerimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.fs.Pathimport org.apache.hadoop.io.&#123;IntWritable, Text&#125;import org.apache.hadoop.mapreduce.lib.input.FileInputFormatimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormatimport org.apache.hadoop.mapreduce.&#123;Job, Mapper, Reducer&#125;import scala.collection.JavaConversionsobject WordCount &#123;def main(args: Array[String]): Unit = &#123;val job = new Job(new Configuration(), &quot;WordCount&quot;)job.setJarByClass(classOf[WordMapper]);job.setMapperClass(classOf[WordMapper]);job.setCombinerClass(classOf[WordReducer]);job.setReducerClass(classOf[WordReducer]);job.setOutputKeyClass(classOf[Text]);job.setOutputValueClass(classOf[IntWritable]);job.setNumReduceTasks(1)FileInputFormat.addInputPath(job, new Path(args(0)));FileOutputFormat.setOutputPath(job, new Path(args(1)));System.exit(job.waitForCompletion(true) match &#123; case true =&gt; 0case false =&gt; 1&#125;);&#125;&#125;class WordMapper extends Mapper[Object, Text, Text, IntWritable] &#123;val one = new IntWritable(1)@throws[IOException]@throws[InterruptedException]override def map(key: Object, value: Text, context: Mapper[Object, Text, Text, IntWritable]#Context) = &#123;val stringTokenizer = new StringTokenizer(value.toString());while (stringTokenizer.hasMoreTokens()) &#123;context.write(new Text(stringTokenizer.nextToken()), one);&#125;&#125;&#125;class WordReducer extends Reducer[Text, IntWritable, Text, IntWritable] &#123;@throws[IOException]@throws[InterruptedException]override def reduce(key: Text, values: java.lang.Iterable[IntWritable], context: Reducer[Text, IntWritable, Text, IntWritable]#Context) = &#123;import JavaConversions.iterableAsScalaIterablecontext.write(key, new IntWritable(values.map(x=&gt;x.get()).reduce(_+_)));&#125;&#125; YarnYARN(Yet Another Resource Negotiator)是Hadoop的设备资源管理器，它是一个通用资源管理系统，MapReduce和其他上层应用提供统一的资源管理和调度，它为集群在利用率、资源统一管理和数据共享等方面提供了巨大的帮助。 Yarn由ResourceManager、NodeManager、ApplicationMaster和Containe四个概念构成。 ResourceManager是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成:调度器(Scheduler)和应用程序管理器(Applications Manager)。调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的MapReduce程序。应用程序管理器负责管理整个系统中所有MapReduce程序，包括提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。 用户提交的每个MapReduce程序均包含一个ApplicationMaster，主要功能包括：与ResourceManager调度器协商以获取资源(用Container表示)；将得到的任务进一步分配给内部的任务(资源的二次分配)；与NodeManager通信以启动/停止任务；监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。 NodeManager是每个设备上的资源和任务管理器，一方面，它会定时地向ResourceManager汇报本设备上的资源使用情况和各个Container的运行状态；另一方面，它接收并处理来自ApplicationMaster的Container启动/停止等各种请求。 Container是YARN中的资源抽象，它封装了某个设备上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示。 结语本文走马观花的介绍了Hadoop相关内容。文章的主要目的是给大家一个对大数据的分布式解决方案的感官印象，为后面的大数据相关文章提供一个基础的理解。最后要强调的是，思考大数据方向的问题是一定要记住分布式的概念，因为你的数据并不在一个设备中甚至不再一个集群中，而且计算也是分布的。所以在设计大数据应用程序时，要花时间思考程序和算法在单机应用和分布式应用所产生的不同(e.g. 加权平均值)。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之基于Zookeeper搭建Hadoop高可用集群]]></title>
    <url>%2F2019%2F06%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%9F%BA%E4%BA%8EZookeeper%E6%90%AD%E5%BB%BAHadoop%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%2F</url>
    <content type="text"><![CDATA[基于ZooKeeper搭建Hadoop高可用集群一、高可用简介二、集群规划三、前置条件四、集群配置五、启动集群六、查看集群七、集群的二次启动 一、高可用简介Hadoop 高可用 (High Availability) 分为 HDFS 高可用和 YARN 高可用，两者的实现基本类似，但 HDFS NameNode 对数据存储及其一致性的要求比 YARN ResourceManger 高得多，所以它的实现也更加复杂，故下面先进行讲解： 1.1 高可用整体架构HDFS 高可用架构如下： 图片引用自：https://www.edureka.co/blog/how-to-set-up-hadoop-cluster-with-hdfs-high-availability/ HDFS 高可用架构主要由以下组件所构成： Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。 主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。 Zookeeper 集群：为主备切换控制器提供主备选举支持。 共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和 NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。 DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。 1.2 基于 QJM 的共享存储系统的数据同步机制分析目前 Hadoop 支持使用 Quorum Journal Manager (QJM) 或 Network File System (NFS) 作为共享的存储系统，这里以 QJM 集群为例进行说明：Active NameNode 首先把 EditLog 提交到 JournalNode 集群，然后 Standby NameNode 再从 JournalNode 集群定时同步 EditLog，当 Active NameNode 宕机后， Standby NameNode 在确认元数据完全同步之后就可以对外提供服务。 需要说明的是向 JournalNode 集群写入 EditLog 是遵循 “过半写入则成功” 的策略，所以你至少要有 3 个 JournalNode 节点，当然你也可以继续增加节点数量，但是应该保证节点总数是奇数。同时如果有 2N+1 台 JournalNode，那么根据过半写的原则，最多可以容忍有 N 台 JournalNode 节点挂掉。 1.3 NameNode 主备切换NameNode 实现主备切换的流程下图所示： 1. HealthMonitor 初始化完成之后会启动内部的线程来定时调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法，对 NameNode 的健康状态进行检测。2. HealthMonitor 如果检测到 NameNode 的健康状态发生变化，会回调 ZKFailoverController 注册的相应方法进行处理。3. 如果 ZKFailoverController 判断需要进行主备切换，会首先使用 ActiveStandbyElector 来进行自动的主备选举。4. ActiveStandbyElector 与 Zookeeper 进行交互完成自动的主备选举。5. ActiveStandbyElector 在主备选举完成后，会回调 ZKFailoverController 的相应方法来通知当前的 NameNode 成为主 NameNode 或备 NameNode。6. ZKFailoverController 调用对应 NameNode 的 HAServiceProtocol RPC 接口的方法将 NameNode 转换为 Active 状态或 Standby 状态。### 1.4 YARN高可用YARN ResourceManager 的高可用与 HDFS NameNode 的高可用类似，但是 ResourceManager 不像 NameNode ，没有那么多的元数据信息需要维护，所以它的状态信息可以直接写到 Zookeeper 上，并依赖 Zookeeper 来进行主备选举。 二、集群规划按照高可用的设计目标：需要保证至少有两个 NameNode (一主一备) 和 两个 ResourceManager (一主一备) ，同时为满足“过半写入则成功”的原则，需要至少要有 3 个 JournalNode 节点。这里使用三台主机进行搭建，集群规划如下： 三、前置条件 所有服务器都安装有 JDK，安装步骤可以参见：Linux 下 JDK 的安装； 搭建好 ZooKeeper 集群，搭建步骤可以参见：Zookeeper 单机环境和集群环境搭建 所有服务器之间都配置好 SSH 免密登录。 四、集群配置4.1 下载并解压下载 Hadoop。这里我下载的是 CDH 版本 Hadoop，下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 1# tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz 4.2 配置环境变量编辑 profile 文件： 1# vim /etc/profile 增加如下配置： 12export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2export PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH 执行 source 命令，使得配置立即生效： 1# source /etc/profile 4.3 修改配置进入 ${HADOOP_HOME}/etc/hadoop 目录下，修改配置文件。各个配置文件内容如下： 1. hadoop-env.sh12# 指定JDK的安装位置export JAVA_HOME=/usr/java/jdk1.8.0_201/ 2. core-site.xml12345678910111213141516171819202122&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定 namenode 的 hdfs 协议文件系统的通信地址 --&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 指定 hadoop 集群存储临时文件的目录 --&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- ZooKeeper 集群的地址 --&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop002:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- ZKFC 连接到 ZooKeeper 超时时长 --&gt; &lt;name&gt;ha.zookeeper.session-timeout.ms&lt;/name&gt; &lt;value&gt;10000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. hdfs-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182&lt;configuration&gt; &lt;property&gt; &lt;!-- 指定 HDFS 副本的数量 --&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- namenode 节点数据（即元数据）的存放位置，可以指定多个目录实现容错，多个目录用逗号分隔 --&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/namenode/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- datanode 节点数据（即数据块）的存放位置 --&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/datanode/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 集群服务的逻辑名称 --&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- NameNode ID 列表--&gt; &lt;name&gt;dfs.ha.namenodes.mycluster&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- nn1 的 RPC 通信地址 --&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- nn2 的 RPC 通信地址 --&gt; &lt;name&gt;dfs.namenode.rpc-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop002:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- nn1 的 http 通信地址 --&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn1&lt;/name&gt; &lt;value&gt;hadoop001:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- nn2 的 http 通信地址 --&gt; &lt;name&gt;dfs.namenode.http-address.mycluster.nn2&lt;/name&gt; &lt;value&gt;hadoop002:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- NameNode 元数据在 JournalNode 上的共享存储目录 --&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://hadoop001:8485;hadoop002:8485;hadoop003:8485/mycluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- Journal Edit Files 的存储目录 --&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/journalnode/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 配置隔离机制，确保在任何给定时间只有一个 NameNode 处于活动状态 --&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 使用 sshfence 机制时需要 ssh 免密登录 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- SSH 超时时间 --&gt; &lt;name&gt;dfs.ha.fencing.ssh.connect-timeout&lt;/name&gt; &lt;value&gt;30000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 访问代理类，用于确定当前处于 Active 状态的 NameNode --&gt; &lt;name&gt;dfs.client.failover.proxy.provider.mycluster&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 开启故障自动转移 --&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4. yarn-site.xml12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667&lt;configuration&gt; &lt;property&gt; &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 是否启用日志聚合 (可选) --&gt; &lt;name&gt;yarn.log-aggregation-enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 聚合日志的保存时间 (可选) --&gt; &lt;name&gt;yarn.log-aggregation.retain-seconds&lt;/name&gt; &lt;value&gt;86400&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 启用 RM HA --&gt; &lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM 集群标识 --&gt; &lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt; &lt;value&gt;my-yarn-cluster&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM 的逻辑 ID 列表 --&gt; &lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt; &lt;value&gt;rm1,rm2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM1 的服务地址 --&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt; &lt;value&gt;hadoop002&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM2 的服务地址 --&gt; &lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt; &lt;value&gt;hadoop003&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM1 Web 应用程序的地址 --&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm1&lt;/name&gt; &lt;value&gt;hadoop002:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- RM2 Web 应用程序的地址 --&gt; &lt;name&gt;yarn.resourcemanager.webapp.address.rm2&lt;/name&gt; &lt;value&gt;hadoop003:8088&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- ZooKeeper 集群的地址 --&gt; &lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt; &lt;value&gt;hadoop001:2181,hadoop002:2181,hadoop003:2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 启用自动恢复 --&gt; &lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!-- 用于进行持久化存储的类 --&gt; &lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. mapred-site.xml1234567&lt;configuration&gt; &lt;property&gt; &lt;!--指定 mapreduce 作业运行在 yarn 上--&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. slaves配置所有从属节点的主机名或 IP 地址，每行一个。所有从属节点上的 DataNode 服务和 NodeManager 服务都会被启动。 123hadoop001hadoop002hadoop003 4.4 分发程序将 Hadoop 安装包分发到其他两台服务器，分发后建议在这两台服务器上也配置一下 Hadoop 的环境变量。 1234# 将安装包分发到hadoop002scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/ hadoop002:/usr/app/# 将安装包分发到hadoop003scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/ hadoop003:/usr/app/ 五、启动集群5.1 启动ZooKeeper分别到三台服务器上启动 ZooKeeper 服务： 1zkServer.sh start 5.2 启动Journalnode分别到三台服务器的的 ${HADOOP_HOME}/sbin 目录下，启动 journalnode 进程： 1hadoop-daemon.sh start journalnode 5.3 初始化NameNode在 hadop001 上执行 NameNode 初始化命令： 1hdfs namenode -format 执行初始化命令后，需要将 NameNode 元数据目录的内容，复制到其他未格式化的 NameNode 上。元数据存储目录就是我们在 hdfs-site.xml 中使用 dfs.namenode.name.dir 属性指定的目录。这里我们需要将其复制到 hadoop002 上： 1scp -r /home/hadoop/namenode/data hadoop002:/home/hadoop/namenode/ 5.4 初始化HA状态在任意一台 NameNode 上使用以下命令来初始化 ZooKeeper 中的 HA 状态： 1hdfs zkfc -formatZK 5.5 启动HDFS进入到 hadoop001 的 ${HADOOP_HOME}/sbin 目录下，启动 HDFS。此时 hadoop001 和 hadoop002 上的 NameNode 服务，和三台服务器上的 DataNode 服务都会被启动： 1start-dfs.sh 5.6 启动YARN进入到 hadoop002 的 ${HADOOP_HOME}/sbin 目录下，启动 YARN。此时 hadoop002 上的 ResourceManager 服务，和三台服务器上的 NodeManager 服务都会被启动： 1start-yarn.sh 需要注意的是，这个时候 hadoop003 上的 ResourceManager 服务通常是没有启动的，需要手动启动： 1yarn-daemon.sh start resourcemanager 六、查看集群6.1 查看进程成功启动后，每台服务器上的进程应该如下： 12345678910111213141516171819202122232425[root@hadoop001 sbin]# jps4512 DFSZKFailoverController3714 JournalNode4114 NameNode3668 QuorumPeerMain5012 DataNode4639 NodeManager[root@hadoop002 sbin]# jps4499 ResourceManager4595 NodeManager3465 QuorumPeerMain3705 NameNode3915 DFSZKFailoverController5211 DataNode3533 JournalNode[root@hadoop003 sbin]# jps3491 JournalNode3942 NodeManager4102 ResourceManager4201 DataNode3435 QuorumPeerMain 6.2 查看Web UIHDFS 和 YARN 的端口号分别为 50070 和 8080，界面应该如下： 此时 hadoop001 上的 NameNode 处于可用状态： 而 hadoop002 上的 NameNode 则处于备用状态： hadoop002 上的 ResourceManager 处于可用状态： hadoop003 上的 ResourceManager 则处于备用状态： 同时界面上也有 Journal Manager 的相关信息： 七、集群的二次启动上面的集群初次启动涉及到一些必要初始化操作，所以过程略显繁琐。但是集群一旦搭建好后，想要再次启用它是比较方便的，步骤如下（首选需要确保 ZooKeeper 集群已经启动）： 在 hadoop001 启动 HDFS，此时会启动所有与 HDFS 高可用相关的服务，包括 NameNode，DataNode 和 JournalNode： 1start-dfs.sh 在 hadoop002 启动 YARN： 1start-yarn.sh 这个时候 hadoop003 上的 ResourceManager 服务通常还是没有启动的，需要手动启动： 1yarn-daemon.sh start resourcemanager 参考资料以上搭建步骤主要参考自官方文档： HDFS High Availability Using the Quorum Journal Manager ResourceManager High Availability 关于 Hadoop 高可用原理的详细分析，推荐阅读： Hadoop NameNode 高可用 (High Availability) 实现解析]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之HDFS-Java-API]]></title>
    <url>%2F2019%2F06%2F06%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS-Java-API%2F</url>
    <content type="text"><![CDATA[HDFS Java API一、 简介二、API的使用2.1 FileSystem2.2 创建目录2.3 创建指定权限的目录2.4 创建文件，并写入内容2.5 判断文件是否存在2.6 查看文件内容2.7 文件重命名2.8 删除目录或文件2.9 上传文件到HDFS2.10 上传大文件并显示上传进度2.11 从HDFS上下载文件2.12 查看指定目录下所有文件的信息2.13 递归查看指定目录下所有文件的信息2.14 查看文件的块信息 一、 简介想要使用 HDFS API，需要导入依赖 hadoop-client。如果是 CDH 版本的 Hadoop，还需要额外指明其仓库地址： 12345678910111213141516171819202122232425262728293031323334353637383940414243&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;project xmlns="http://maven.apache.org/POM/4.0.0" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.heibaiying&lt;/groupId&gt; &lt;artifactId&gt;hdfs-java-api&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;hadoop.version&gt;2.6.0-cdh5.15.2&lt;/hadoop.version&gt; &lt;/properties&gt; &lt;!---配置 CDH 仓库地址--&gt; &lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt; &lt;/repositories&gt; &lt;dependencies&gt; &lt;!--Hadoop-client--&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 二、API的使用2.1 FileSystemFileSystem 是所有 HDFS 操作的主入口。由于之后的每个单元测试都需要用到它，这里使用 @Before 注解进行标注。 12345678910111213141516171819202122232425private static final String HDFS_PATH = "hdfs://192.168.0.106:8020";private static final String HDFS_USER = "root";private static FileSystem fileSystem;@Beforepublic void prepare() &#123; try &#123; Configuration configuration = new Configuration(); // 这里我启动的是单节点的 Hadoop,所以副本系数设置为 1,默认值为 3 configuration.set("dfs.replication", "1"); fileSystem = FileSystem.get(new URI(HDFS_PATH), configuration, HDFS_USER); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (URISyntaxException e) &#123; e.printStackTrace(); &#125;&#125;@Afterpublic void destroy() &#123; fileSystem = null;&#125; 2.2 创建目录支持递归创建目录： 1234@Testpublic void mkDir() throws Exception &#123; fileSystem.mkdirs(new Path("/hdfs-api/test0/"));&#125; 2.3 创建指定权限的目录FsPermission(FsAction u, FsAction g, FsAction o) 的三个参数分别对应：创建者权限，同组其他用户权限，其他用户权限，权限值定义在 FsAction 枚举类中。 12345@Testpublic void mkDirWithPermission() throws Exception &#123; fileSystem.mkdirs(new Path("/hdfs-api/test1/"), new FsPermission(FsAction.READ_WRITE, FsAction.READ, FsAction.READ));&#125; 2.4 创建文件，并写入内容123456789101112@Testpublic void create() throws Exception &#123; // 如果文件存在，默认会覆盖, 可以通过第二个参数进行控制。第三个参数可以控制使用缓冲区的大小 FSDataOutputStream out = fileSystem.create(new Path("/hdfs-api/test/a.txt"), true, 4096); out.write("hello hadoop!".getBytes()); out.write("hello spark!".getBytes()); out.write("hello flink!".getBytes()); // 强制将缓冲区中内容刷出 out.flush(); out.close();&#125; 2.5 判断文件是否存在12345@Testpublic void exist() throws Exception &#123; boolean exists = fileSystem.exists(new Path("/hdfs-api/test/a.txt")); System.out.println(exists);&#125; 2.6 查看文件内容查看小文本文件的内容，直接转换成字符串后输出： 123456@Testpublic void readToString() throws Exception &#123; FSDataInputStream inputStream = fileSystem.open(new Path("/hdfs-api/test/a.txt")); String context = inputStreamToString(inputStream, "utf-8"); System.out.println(context);&#125; inputStreamToString 是一个自定义方法，代码如下： 1234567891011121314151617181920212223/** * 把输入流转换为指定编码的字符 * * @param inputStream 输入流 * @param encode 指定编码类型 */private static String inputStreamToString(InputStream inputStream, String encode) &#123; try &#123; if (encode == null || ("".equals(encode))) &#123; encode = "utf-8"; &#125; BufferedReader reader = new BufferedReader(new InputStreamReader(inputStream, encode)); StringBuilder builder = new StringBuilder(); String str = ""; while ((str = reader.readLine()) != null) &#123; builder.append(str).append("\n"); &#125; return builder.toString(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null;&#125; 2.7 文件重命名1234567@Testpublic void rename() throws Exception &#123; Path oldPath = new Path("/hdfs-api/test/a.txt"); Path newPath = new Path("/hdfs-api/test/b.txt"); boolean result = fileSystem.rename(oldPath, newPath); System.out.println(result);&#125; 2.8 删除目录或文件123456789public void delete() throws Exception &#123; /* * 第二个参数代表是否递归删除 * + 如果 path 是一个目录且递归删除为 true, 则删除该目录及其中所有文件; * + 如果 path 是一个目录但递归删除为 false,则会则抛出异常。 */ boolean result = fileSystem.delete(new Path("/hdfs-api/test/b.txt"), true); System.out.println(result);&#125; 2.9 上传文件到HDFS1234567@Testpublic void copyFromLocalFile() throws Exception &#123; // 如果指定的是目录，则会把目录及其中的文件都复制到指定目录下 Path src = new Path("D:\\BigData-Notes\\notes\\installation"); Path dst = new Path("/hdfs-api/test/"); fileSystem.copyFromLocalFile(src, dst);&#125; 2.10 上传大文件并显示上传进度123456789101112131415161718192021@Test public void copyFromLocalBigFile() throws Exception &#123; File file = new File("D:\\kafka.tgz"); final float fileSize = file.length(); InputStream in = new BufferedInputStream(new FileInputStream(file)); FSDataOutputStream out = fileSystem.create(new Path("/hdfs-api/test/kafka5.tgz"), new Progressable() &#123; long fileCount = 0; public void progress() &#123; fileCount++; // progress 方法每上传大约 64KB 的数据后就会被调用一次 System.out.println("上传进度：" + (fileCount * 64 * 1024 / fileSize) * 100 + " %"); &#125; &#125;); IOUtils.copyBytes(in, out, 4096); &#125; 2.11 从HDFS上下载文件12345678910111213@Testpublic void copyToLocalFile() throws Exception &#123; Path src = new Path("/hdfs-api/test/kafka.tgz"); Path dst = new Path("D:\\app\\"); /* * 第一个参数控制下载完成后是否删除源文件,默认是 true,即删除; * 最后一个参数表示是否将 RawLocalFileSystem 用作本地文件系统; * RawLocalFileSystem 默认为 false,通常情况下可以不设置, * 但如果你在执行时候抛出 NullPointerException 异常,则代表你的文件系统与程序可能存在不兼容的情况 (window 下常见), * 此时可以将 RawLocalFileSystem 设置为 true */ fileSystem.copyToLocalFile(false, src, dst, true);&#125; 2.12 查看指定目录下所有文件的信息1234567public void listFiles() throws Exception &#123; FileStatus[] statuses = fileSystem.listStatus(new Path("/hdfs-api")); for (FileStatus fileStatus : statuses) &#123; //fileStatus 的 toString 方法被重写过，直接打印可以看到所有信息 System.out.println(fileStatus.toString()); &#125;&#125; FileStatus 中包含了文件的基本信息，比如文件路径，是否是文件夹，修改时间，访问时间，所有者，所属组，文件权限，是否是符号链接等，输出内容示例如下： 12345678910FileStatus&#123;path=hdfs://192.168.0.106:8020/hdfs-api/test; isDirectory=true; modification_time=1556680796191; access_time=0; owner=root; group=supergroup; permission=rwxr-xr-x; isSymlink=false&#125; 2.13 递归查看指定目录下所有文件的信息1234567@Testpublic void listFilesRecursive() throws Exception &#123; RemoteIterator&lt;LocatedFileStatus&gt; files = fileSystem.listFiles(new Path("/hbase"), true); while (files.hasNext()) &#123; System.out.println(files.next()); &#125;&#125; 和上面输出类似，只是多了文本大小，副本系数，块大小信息。 1234567891011LocatedFileStatus&#123;path=hdfs://192.168.0.106:8020/hbase/hbase.version; isDirectory=false; length=7; replication=1; blocksize=134217728; modification_time=1554129052916; access_time=1554902661455; owner=root; group=supergroup;permission=rw-r--r--; isSymlink=false&#125; 2.14 查看文件的块信息123456789@Testpublic void getFileBlockLocations() throws Exception &#123; FileStatus fileStatus = fileSystem.getFileStatus(new Path("/hdfs-api/test/kafka.tgz")); BlockLocation[] blocks = fileSystem.getFileBlockLocations(fileStatus, 0, fileStatus.getLen()); for (BlockLocation block : blocks) &#123; System.out.println(block); &#125;&#125; 块输出信息有三个值，分别是文件的起始偏移量 (offset)，文件大小 (length)，块所在的主机名 (hosts)。 10,57028557,hadoop001 这里我上传的文件只有 57M(小于 128M)，且程序中设置了副本系数为 1，所有只有一个块信息。 以上所有测试用例下载地址：HDFS Java API]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之HDFS常用Shell命令]]></title>
    <url>%2F2019%2F06%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHDFS%E5%B8%B8%E7%94%A8Shell%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[HDFS 常用 shell 命令1. 显示当前目录结构 123456# 显示当前目录结构hadoop fs -ls &lt;path&gt;# 递归显示当前目录结构hadoop fs -ls -R &lt;path&gt;# 显示根目录下内容hadoop fs -ls / 2. 创建目录 1234# 创建目录hadoop fs -mkdir &lt;path&gt; # 递归创建目录hadoop fs -mkdir -p &lt;path&gt; 3. 删除操作 1234# 删除文件hadoop fs -rm &lt;path&gt;# 递归删除目录和文件hadoop fs -rm -R &lt;path&gt; 4. 从本地加载文件到 HDFS 123# 二选一执行即可hadoop fs -put [localsrc] [dst] hadoop fs - copyFromLocal [localsrc] [dst] 5. 从 HDFS 导出文件到本地 123# 二选一执行即可hadoop fs -get [dst] [localsrc] hadoop fs -copyToLocal [dst] [localsrc] 6. 查看文件内容 123# 二选一执行即可hadoop fs -text &lt;path&gt; hadoop fs -cat &lt;path&gt; 7. 显示文件的最后一千字节 123hadoop fs -tail &lt;path&gt; # 和Linux下一样，会持续监听文件内容变化 并显示文件的最后一千字节hadoop fs -tail -f &lt;path&gt; 8. 拷贝文件 1hadoop fs -cp [src] [dst] 9. 移动文件 1hadoop fs -mv [src] [dst] 10. 统计当前目录下各文件大小 默认单位字节 -s : 显示所有文件大小总和， -h : 将以更友好的方式显示文件大小（例如 64.0m 而不是 67108864）1hadoop fs -du &lt;path&gt; 11. 合并下载多个文件 -nl 在每个文件的末尾添加换行符（LF） -skip-empty-file 跳过空文件 123hadoop fs -getmerge# 示例 将HDFS上的hbase-policy.xml和hbase-site.xml文件合并后下载到本地的/usr/test.xmlhadoop fs -getmerge -nl /test/hbase-policy.xml /test/hbase-site.xml /usr/test.xml 12. 统计文件系统的可用空间信息 1hadoop fs -df -h / 13. 更改文件复制因子1hadoop fs -setrep [-R] [-w] &lt;numReplicas&gt; &lt;path&gt; 更改文件的复制因子。如果 path 是目录，则更改其下所有文件的复制因子 -w : 请求命令是否等待复制完成 12# 示例hadoop fs -setrep -w 3 /user/hadoop/dir1 14. 权限控制1234567# 权限控制和Linux上使用方式一致# 变更文件或目录的所属群组。 用户必须是文件的所有者或超级用户。hadoop fs -chgrp [-R] GROUP URI [URI ...]# 修改文件或目录的访问权限 用户必须是文件的所有者或超级用户。hadoop fs -chmod [-R] &lt;MODE[,MODE]... | OCTALMODE&gt; URI [URI ...]# 修改文件的拥有者 用户必须是超级用户。hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] 15. 文件检测1hadoop fs -test - [defsz] URI 可选选项： -d：如果路径是目录，返回 0。 -e：如果路径存在，则返回 0。 -f：如果路径是文件，则返回 0。 -s：如果路径不为空，则返回 0。 -r：如果路径存在且授予读权限，则返回 0。 -w：如果路径存在且授予写入权限，则返回 0。 -z：如果文件长度为零，则返回 0。 12# 示例hadoop fs -test -e filename]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hadoop集群环境搭建]]></title>
    <url>%2F2019%2F06%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHadoop%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Hadoop集群环境搭建一、集群规划二、前置条件三、配置免密登录3.1 生成密匙3.2 免密登录3.3 验证免密登录四、集群搭建3.1 下载并解压3.2 配置环境变量3.3 修改配置3.4 分发程序3.5 初始化3.6 启动集群3.7 查看集群五、提交服务到集群 一、集群规划这里搭建一个 3 节点的 Hadoop 集群，其中三台主机均部署 DataNode 和 NodeManager 服务，但只有 hadoop001 上部署 NameNode 和 ResourceManager 服务。 二、前置条件Hadoop 的运行依赖 JDK，需要预先安装。其安装步骤单独整理至： Linux 下 JDK 的安装 三、配置免密登录3.1 生成密匙在每台主机上使用 ssh-keygen 命令生成公钥私钥对： 1ssh-keygen 3.2 免密登录将 hadoop001 的公钥写到本机和远程机器的 ~/ .ssh/authorized_key 文件中： 123ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop003 3.3 验证免密登录12ssh hadoop002ssh hadoop003 四、集群搭建3.1 下载并解压下载 Hadoop。这里我下载的是 CDH 版本 Hadoop，下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 1# tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz 3.2 配置环境变量编辑 profile 文件： 1# vim /etc/profile 增加如下配置： 12export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2export PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH 执行 source 命令，使得配置立即生效： 1# source /etc/profile 3.3 修改配置进入 ${HADOOP_HOME}/etc/hadoop 目录下，修改配置文件。各个配置文件内容如下： 1. hadoop-env.sh12# 指定JDK的安装位置export JAVA_HOME=/usr/java/jdk1.8.0_201/ 2. core-site.xml123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!--指定 namenode 的 hdfs 协议文件系统的通信地址--&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--指定 hadoop 集群存储临时文件的目录--&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. hdfs-site.xml12345678910&lt;property&gt; &lt;!--namenode 节点数据（即元数据）的存放位置，可以指定多个目录实现容错，多个目录用逗号分隔--&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/namenode/data&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;!--datanode 节点数据（即数据块）的存放位置--&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/datanode/data&lt;/value&gt;&lt;/property&gt; 4. yarn-site.xml123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--resourcemanager 的主机名--&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;hadoop001&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. mapred-site.xml1234567&lt;configuration&gt; &lt;property&gt; &lt;!--指定 mapreduce 作业运行在 yarn 上--&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 5. slaves配置所有从属节点的主机名或 IP 地址，每行一个。所有从属节点上的 DataNode 服务和 NodeManager 服务都会被启动。 123hadoop001hadoop002hadoop003 3.4 分发程序将 Hadoop 安装包分发到其他两台服务器，分发后建议在这两台服务器上也配置一下 Hadoop 的环境变量。 1234# 将安装包分发到hadoop002scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/ hadoop002:/usr/app/# 将安装包分发到hadoop003scp -r /usr/app/hadoop-2.6.0-cdh5.15.2/ hadoop003:/usr/app/ 3.5 初始化在 Hadoop001 上执行 namenode 初始化命令： 1hdfs namenode -format 3.6 启动集群进入到 Hadoop001 的 ${HADOOP_HOME}/sbin 目录下，启动 Hadoop。此时 hadoop002 和 hadoop003 上的相关服务也会被启动： 1234# 启动dfs服务start-dfs.sh# 启动yarn服务start-yarn.sh 3.7 查看集群在每台服务器上使用 jps 命令查看服务进程，或直接进入 Web-UI 界面进行查看，端口为 50070。可以看到此时有三个可用的 Datanode： 点击 Live Nodes 进入，可以看到每个 DataNode 的详细情况： 接着可以查看 Yarn 的情况，端口号为 8088 ： 五、提交服务到集群提交作业到集群的方式和单机环境完全一致，这里以提交 Hadoop 内置的计算 Pi 的示例程序为例，在任何一个节点上执行都可以，命令如下： 1hadoop jar /usr/app/hadoop-2.6.0-cdh5.15.2/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>集群环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hadoop单机环境搭建]]></title>
    <url>%2F2019%2F06%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHadoop%E5%8D%95%E6%9C%BA%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Hadoop单机版环境搭建一、前置条件二、配置 SSH 免密登录三、Hadoop(HDFS)环境搭建四、Hadoop(YARN)环境搭建 一、前置条件Hadoop 的运行依赖 JDK，需要预先安装，安装步骤见： Linux 下 JDK 的安装 二、配置免密登录Hadoop 组件之间需要基于 SSH 进行通讯。 2.1 配置映射配置 ip 地址和主机名映射： 123vim /etc/hosts# 文件末尾增加192.168.43.202 hadoop001 2.2 生成公私钥执行下面命令行生成公匙和私匙： 1ssh-keygen -t rsa 3.3 授权进入 ~/.ssh 目录下，查看生成的公匙和私匙，并将公匙写入到授权文件： 1234[root@@hadoop001 sbin]# cd ~/.ssh[root@@hadoop001 .ssh]# ll-rw-------. 1 root root 1675 3 月 15 09:48 id_rsa-rw-r--r--. 1 root root 388 3 月 15 09:48 id_rsa.pub 123# 写入公匙到授权文件[root@hadoop001 .ssh]# cat id_rsa.pub &gt;&gt; authorized_keys[root@hadoop001 .ssh]# chmod 600 authorized_keys 三、Hadoop(HDFS)环境搭建3.1 下载并解压下载 Hadoop 安装包，这里我下载的是 CDH 版本的，下载地址为：http://archive.cloudera.com/cdh5/cdh/5/ 12# 解压tar -zvxf hadoop-2.6.0-cdh5.15.2.tar.gz 3.2 配置环境变量1# vi /etc/profile 配置环境变量： 12export HADOOP_HOME=/usr/app/hadoop-2.6.0-cdh5.15.2export PATH=$&#123;HADOOP_HOME&#125;/bin:$PATH 执行 source 命令，使得配置的环境变量立即生效： 1# source /etc/profile 3.3 修改Hadoop配置进入 ${HADOOP_HOME}/etc/hadoop/ 目录下，修改以下配置： 1. hadoop-env.sh12# JDK安装路径export JAVA_HOME=/usr/java/jdk1.8.0_201/ 2. core-site.xml123456789101112&lt;configuration&gt; &lt;property&gt; &lt;!--指定 namenode 的 hdfs 协议文件系统的通信地址--&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://hadoop001:8020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--指定 hadoop 存储临时文件的目录--&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hadoop/tmp&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 3. hdfs-site.xml指定副本系数和临时文件存储位置： 1234567&lt;configuration&gt; &lt;property&gt; &lt;!--由于我们这里搭建是单机版本，所以指定 dfs 的副本系数为 1--&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4. slaves配置所有从属节点的主机名或 IP 地址，由于是单机版本，所以指定本机即可： 1hadoop001 3.4 关闭防火墙不关闭防火墙可能导致无法访问 Hadoop 的 Web UI 界面： 1234# 查看防火墙状态sudo firewall-cmd --state# 关闭防火墙:sudo systemctl stop firewalld.service 3.5 初始化第一次启动 Hadoop 时需要进行初始化，进入 ${HADOOP_HOME}/bin/ 目录下，执行以下命令： 1[root@hadoop001 bin]# ./hdfs namenode -format 3.6 启动HDFS进入 ${HADOOP_HOME}/sbin/ 目录下，启动 HDFS： 1[root@hadoop001 sbin]# ./start-dfs.sh 3.7 验证是否启动成功方式一：执行 jps 查看 NameNode 和 DataNode 服务是否已经启动： 1234[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps9137 DataNode9026 NameNode9390 SecondaryNameNode 方式二：查看 Web UI 界面，端口为 50070： 四、Hadoop(YARN)环境搭建4.1 修改配置进入 ${HADOOP_HOME}/etc/hadoop/ 目录下，修改以下配置： 1. mapred-site.xml12# 如果没有mapred-site.xml，则拷贝一份样例文件后再修改cp mapred-site.xml.template mapred-site.xml 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 2. yarn-site.xml1234567&lt;configuration&gt; &lt;property&gt; &lt;!--配置 NodeManager 上运行的附属服务。需要配置成 mapreduce_shuffle 后才可以在 Yarn 上运行 MapReduce 程序。--&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 4.2 启动服务进入 ${HADOOP_HOME}/sbin/ 目录下，启动 YARN： 1./start-yarn.sh 4.3 验证是否启动成功方式一：执行 jps 命令查看 NodeManager 和 ResourceManager 服务是否已经启动： 123456[root@hadoop001 hadoop-2.6.0-cdh5.15.2]# jps9137 DataNode9026 NameNode12294 NodeManager12185 ResourceManager9390 SecondaryNameNode 方式二：查看 Web UI 界面，端口号为 8088：]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>单机环境搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hadoop-YARN简介]]></title>
    <url>%2F2019%2F06%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHadoop-YARN%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[集群资源管理器——YARN一、hadoop yarn 简介二、YARN架构1. ResourceManager2. NodeManager3. ApplicationMaster 4. Contain三、YARN工作原理简述四、YARN工作原理详述五、提交作业到YARN上运行 一、hadoop yarn 简介Apache YARN (Yet Another Resource Negotiator) 是 hadoop 2.0 引入的集群资源管理系统。用户可以将各种服务框架部署在 YARN 上，由 YARN 进行统一地管理和资源分配。 二、YARN架构 1. ResourceManagerResourceManager 通常在独立的机器上以后台进程的形式运行，它是整个集群资源的主要协调者和管理者。ResourceManager 负责给用户提交的所有应用程序分配资源，它根据应用程序优先级、队列容量、ACLs、数据位置等信息，做出决策，然后以共享的、安全的、多租户的方式制定分配策略，调度集群资源。 2. NodeManagerNodeManager 是 YARN 集群中的每个具体节点的管理者。主要负责该节点内所有容器的生命周期的管理，监视资源和跟踪节点健康。具体如下： 启动时向 ResourceManager 注册并定时发送心跳消息，等待 ResourceManager 的指令； 维护 Container 的生命周期，监控 Container 的资源使用情况； 管理任务运行时的相关依赖，根据 ApplicationMaster 的需要，在启动 Container 之前将需要的程序及其依赖拷贝到本地。 3. ApplicationMaster在用户提交一个应用程序时，YARN 会启动一个轻量级的进程 ApplicationMaster。ApplicationMaster 负责协调来自 ResourceManager 的资源，并通过 NodeManager 监视容器内资源的使用情况，同时还负责任务的监控与容错。具体如下： 根据应用的运行状态来决定动态计算资源需求； 向 ResourceManager 申请资源，监控申请的资源的使用情况； 跟踪任务状态和进度，报告资源的使用情况和应用的进度信息； 负责任务的容错。 4. ContainContainer 是 YARN 中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。当 AM 向 RM 申请资源时，RM 为 AM 返回的资源是用 Container 表示的。YARN 会为每个任务分配一个 Container，该任务只能使用该 Container 中描述的资源。ApplicationMaster 可在 Container 内运行任何类型的任务。例如，MapReduce ApplicationMaster 请求一个容器来启动 map 或 reduce 任务，而 Giraph ApplicationMaster 请求一个容器来运行 Giraph 任务。 三、YARN工作原理简述 Client 提交作业到 YARN 上； Resource Manager 选择一个 Node Manager，启动一个 Container 并运行 Application Master 实例； Application Master 根据实际需要向 Resource Manager 请求更多的 Container 资源（如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务）； Application Master 通过获取到的 Container 资源执行分布式计算。 四、YARN工作原理详述 1. 作业提交client 调用 job.waitForCompletion 方法，向整个集群提交 MapReduce 作业 (第 1 步) 。新的作业 ID(应用 ID) 由资源管理器分配 (第 2 步)。作业的 client 核实作业的输出, 计算输入的 split, 将作业的资源 (包括 Jar 包，配置文件, split 信息) 拷贝给 HDFS(第 3 步)。 最后, 通过调用资源管理器的 submitApplication() 来提交作业 (第 4 步)。 2. 作业初始化当资源管理器收到 submitApplciation() 的请求时, 就将该请求发给调度器 (scheduler), 调度器分配 container, 然后资源管理器在该 container 内启动应用管理器进程, 由节点管理器监控 (第 5 步)。 MapReduce 作业的应用管理器是一个主类为 MRAppMaster 的 Java 应用，其通过创造一些 bookkeeping 对象来监控作业的进度, 得到任务的进度和完成报告 (第 6 步)。然后其通过分布式文件系统得到由客户端计算好的输入 split(第 7 步)，然后为每个输入 split 创建一个 map 任务, 根据 mapreduce.job.reduces 创建 reduce 任务对象。 3. 任务分配如果作业很小, 应用管理器会选择在其自己的 JVM 中运行任务。 如果不是小作业, 那么应用管理器向资源管理器请求 container 来运行所有的 map 和 reduce 任务 (第 8 步)。这些请求是通过心跳来传输的, 包括每个 map 任务的数据位置，比如存放输入 split 的主机名和机架 (rack)，调度器利用这些信息来调度任务，尽量将任务分配给存储数据的节点, 或者分配给和存放输入 split 的节点相同机架的节点。 4. 任务运行当一个任务由资源管理器的调度器分配给一个 container 后，应用管理器通过联系节点管理器来启动 container(第 9 步)。任务由一个主类为 YarnChild 的 Java 应用执行， 在运行任务之前首先本地化任务需要的资源，比如作业配置，JAR 文件, 以及分布式缓存的所有文件 (第 10 步。 最后, 运行 map 或 reduce 任务 (第 11 步)。 YarnChild 运行在一个专用的 JVM 中, 但是 YARN 不支持 JVM 重用。 5. 进度和状态更新YARN 中的任务将其进度和状态 (包括 counter) 返回给应用管理器, 客户端每秒 (通 mapreduce.client.progressmonitor.pollinterval 设置) 向应用管理器请求进度更新, 展示给用户。 6. 作业完成除了向应用管理器请求作业进度外, 客户端每 5 分钟都会通过调用 waitForCompletion() 来检查作业是否完成，时间间隔可以通过 mapreduce.client.completion.pollinterval 来设置。作业完成之后, 应用管理器和 container 会清理工作状态， OutputCommiter 的作业清理方法也会被调用。作业的信息会被作业历史服务器存储以备之后用户核查。 五、提交作业到YARN上运行这里以提交 Hadoop Examples 中计算 Pi 的 MApReduce 程序为例，相关 Jar 包在 Hadoop 安装目录的 share/hadoop/mapreduce 目录下： 12# 提交格式: hadoop jar jar包路径 主类名称 主类参数# hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.2.jar pi 3 3 参考资料 初步掌握 Yarn 的架构及原理 Apache Hadoop 2.9.2 &gt; Apache Hadoop YARN]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hadoop-MapReduce简介]]></title>
    <url>%2F2019%2F06%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHadoop-MapReduce%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[分布式计算框架——MapReduce一、MapReduce概述二、MapReduce编程模型简述三、combiner &amp; partitioner四、MapReduce词频统计案例4.1 项目简介4.2 项目依赖4.3 WordCountMapper4.4 WordCountReducer4.4 WordCountApp4.5 提交到服务器运行五、词频统计案例进阶之Combiner六、词频统计案例进阶之Partitioner 一、MapReduce概述Hadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。 MapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 map 以并行的方式处理，框架对 map 的输出进行排序，然后输入到 reduce 中。MapReduce 框架专门用于 &lt;key，value&gt; 键值对处理，它将作业的输入视为一组 &lt;key，value&gt; 对，并生成一组 &lt;key，value&gt; 对作为输出。输出和输出的 key 和 value 都必须实现Writable 接口。 1(input) &lt;k1, v1&gt; -&gt; map -&gt; &lt;k2, v2&gt; -&gt; combine -&gt; &lt;k2, v2&gt; -&gt; reduce -&gt; &lt;k3, v3&gt; (output) 二、MapReduce编程模型简述这里以词频统计为例进行说明，MapReduce 处理的流程如下： input : 读取文本文件； splitting : 将文件按照行进行拆分，此时得到的 K1 行数，V1 表示对应行的文本内容； mapping : 并行将每一行按照空格进行拆分，拆分得到的 List(K2,V2)，其中 K2 代表每一个单词，由于是做词频统计，所以 V2 的值为 1，代表出现 1 次； shuffling：由于 Mapping 操作可能是在不同的机器上并行处理的，所以需要通过 shuffling 将相同 key 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 K2 为每一个单词，List(V2) 为可迭代集合，V2 就是 Mapping 中的 V2； Reducing : 这里的案例是统计单词出现的总次数，所以 Reducing 对 List(V2) 进行归约求和操作，最终输出。 MapReduce 编程模型中 splitting 和 shuffing 操作都是由框架实现的，需要我们自己编程实现的只有 mapping 和 reducing，这也就是 MapReduce 这个称呼的来源。 三、combiner &amp; partitioner 3.1 InputFormat &amp; RecordReadersInputFormat 将输出文件拆分为多个 InputSplit，并由 RecordReaders 将 InputSplit 转换为标准的&lt;key，value&gt;键值对，作为 map 的输出。这一步的意义在于只有先进行逻辑拆分并转为标准的键值对格式后，才能为多个 map 提供输入，以便进行并行处理。 3.2 Combinercombiner 是 map 运算后的可选操作，它实际上是一个本地化的 reduce 操作，它主要是在 map 计算出中间文件后做一个简单的合并重复 key 值的操作。这里以词频统计为例： map 在遇到一个 hadoop 的单词时就会记录为 1，但是这篇文章里 hadoop 可能会出现 n 多次，那么 map 输出文件冗余就会很多，因此在 reduce 计算前对相同的 key 做一个合并操作，那么需要传输的数据量就会减少，传输效率就可以得到提升。 但并非所有场景都适合使用 combiner，使用它的原则是 combiner 的输出不会影响到 reduce 计算的最终输入，例如：求总数，最大值，最小值时都可以使用 combiner，但是做平均值计算则不能使用 combiner。 不使用 combiner 的情况： 使用 combiner 的情况： 可以看到使用 combiner 的时候，需要传输到 reducer 中的数据由 12keys，降低到 10keys。降低的幅度取决于你 keys 的重复率，下文词频统计案例会演示用 combiner 降低数百倍的传输量。 3.3 Partitionerpartitioner 可以理解成分类器，将 map 的输出按照 key 值的不同分别分给对应的 reducer，支持自定义实现，下文案例会给出演示。 四、MapReduce词频统计案例4.1 项目简介这里给出一个经典的词频统计的案例：统计如下样本数据中每个单词出现的次数。 12345678910111213Spark HBaseHive Flink Storm Hadoop HBase SparkFlinkHBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase HiveHadoop Spark HBase StormHBase Hadoop Hive FlinkHBase Flink Hive StormHive Flink HadoopHBase Hive 为方便大家开发，我在项目源码中放置了一个工具类 WordCountDataUtils，用于模拟产生词频统计的样本，生成的文件支持输出到本地或者直接写到 HDFS 上。 项目完整源码下载地址：hadoop-word-count 4.2 项目依赖想要进行 MapReduce 编程，需要导入 hadoop-client 依赖： 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;$&#123;hadoop.version&#125;&lt;/version&gt;&lt;/dependency&gt; 4.3 WordCountMapper将每行数据按照指定分隔符进行拆分。这里需要注意在 MapReduce 中必须使用 Hadoop 定义的类型，因为 Hadoop 预定义的类型都是可序列化，可比较的，所有类型均实现了 WritableComparable 接口。 123456789101112public class WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String[] words = value.toString().split("\t"); for (String word : words) &#123; context.write(new Text(word), new IntWritable(1)); &#125; &#125;&#125; WordCountMapper 对应下图的 Mapping 操作： WordCountMapper 继承自 Mappe 类，这是一个泛型类，定义如下： 12345WordCountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt;public class Mapper&lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT&gt; &#123; ......&#125; KEYIN : mapping 输入 key 的类型，即每行的偏移量 (每行第一个字符在整个文本中的位置)，Long 类型，对应 Hadoop 中的 LongWritable 类型； VALUEIN : mapping 输入 value 的类型，即每行数据；String 类型，对应 Hadoop 中 Text 类型； KEYOUT ：mapping 输出的 key 的类型，即每个单词；String 类型，对应 Hadoop 中 Text 类型； VALUEOUT：mapping 输出 value 的类型，即每个单词出现的次数；这里用 int 类型，对应 IntWritable 类型。 4.4 WordCountReducer在 Reduce 中进行单词出现次数的统计： 123456789101112public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int count = 0; for (IntWritable value : values) &#123; count += value.get(); &#125; context.write(key, new IntWritable(count)); &#125;&#125; 如下图，shuffling 的输出是 reduce 的输入。这里的 key 是每个单词，values 是一个可迭代的数据类型，类似 (1,1,1,...)。 4.4 WordCountApp组装 MapReduce 作业，并提交到服务器运行，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 组装作业 并提交到集群运行 */public class WordCountApp &#123; // 这里为了直观显示参数 使用了硬编码，实际开发中可以通过外部传参 private static final String HDFS_URL = "hdfs://192.168.0.107:8020"; private static final String HADOOP_USER_NAME = "root"; public static void main(String[] args) throws Exception &#123; // 文件输入路径和输出路径由外部传参指定 if (args.length &lt; 2) &#123; System.out.println("Input and output paths are necessary!"); return; &#125; // 需要指明 hadoop 用户名，否则在 HDFS 上创建目录时可能会抛出权限不足的异常 System.setProperty("HADOOP_USER_NAME", HADOOP_USER_NAME); Configuration configuration = new Configuration(); // 指明 HDFS 的地址 configuration.set("fs.defaultFS", HDFS_URL); // 创建一个 Job Job job = Job.getInstance(configuration); // 设置运行的主类 job.setJarByClass(WordCountApp.class); // 设置 Mapper 和 Reducer job.setMapperClass(WordCountMapper.class); job.setReducerClass(WordCountReducer.class); // 设置 Mapper 输出 key 和 value 的类型 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 设置 Reducer 输出 key 和 value 的类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 如果输出目录已经存在，则必须先删除，否则重复运行程序时会抛出异常 FileSystem fileSystem = FileSystem.get(new URI(HDFS_URL), configuration, HADOOP_USER_NAME); Path outputPath = new Path(args[1]); if (fileSystem.exists(outputPath)) &#123; fileSystem.delete(outputPath, true); &#125; // 设置作业输入文件和输出文件的路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, outputPath); // 将作业提交到群集并等待它完成，参数设置为 true 代表打印显示对应的进度 boolean result = job.waitForCompletion(true); // 关闭之前创建的 fileSystem fileSystem.close(); // 根据作业结果,终止当前运行的 Java 虚拟机,退出程序 System.exit(result ? 0 : -1); &#125;&#125; 需要注意的是：如果不设置 Mapper 操作的输出类型，则程序默认它和 Reducer 操作输出的类型相同。 4.5 提交到服务器运行在实际开发中，可以在本机配置 hadoop 开发环境，直接在 IDE 中启动进行测试。这里主要介绍一下打包提交到服务器运行。由于本项目没有使用除 Hadoop 外的第三方依赖，直接打包即可： 1# mvn clean package 使用以下命令提交作业： 123hadoop jar /usr/appjar/hadoop-word-count-1.0.jar \com.heibaiying.WordCountApp \/wordcount/input.txt /wordcount/output/WordCountApp 作业完成后查看 HDFS 上生成目录： 12345# 查看目录hadoop fs -ls /wordcount/output/WordCountApp# 查看统计结果hadoop fs -cat /wordcount/output/WordCountApp/part-r-00000 五、词频统计案例进阶之Combiner5.1 代码实现想要使用 combiner 功能只要在组装作业时，添加下面一行代码即可： 12// 设置 Combinerjob.setCombinerClass(WordCountReducer.class); 5.2 执行结果加入 combiner 后统计结果是不会有变化的，但是可以从打印的日志看出 combiner 的效果： 没有加入 combiner 的打印日志： 加入 combiner 后的打印日志如下： 这里我们只有一个输入文件并且小于 128M，所以只有一个 Map 进行处理。可以看到经过 combiner 后，records 由 3519 降低为 6(样本中单词种类就只有 6 种)，在这个用例中 combiner 就能极大地降低需要传输的数据量。 六、词频统计案例进阶之Partitioner6.1 默认的Partitioner这里假设有个需求：将不同单词的统计结果输出到不同文件。这种需求实际上比较常见，比如统计产品的销量时，需要将结果按照产品种类进行拆分。要实现这个功能，就需要用到自定义 Partitioner。 这里先介绍下 MapReduce 默认的分类规则：在构建 job 时候，如果不指定，默认的使用的是 HashPartitioner：对 key 值进行哈希散列并对 numReduceTasks 取余。其实现如下： 12345678public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; &#123; public int getPartition(K key, V value, int numReduceTasks) &#123; return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; &#125;&#125; 6.2 自定义Partitioner这里我们继承 Partitioner 自定义分类规则，这里按照单词进行分类： 123456public class CustomPartitioner extends Partitioner&lt;Text, IntWritable&gt; &#123; public int getPartition(Text text, IntWritable intWritable, int numPartitions) &#123; return WordCountDataUtils.WORD_LIST.indexOf(text.toString()); &#125;&#125; 在构建 job 时候指定使用我们自己的分类规则，并设置 reduce 的个数： 1234// 设置自定义分区规则job.setPartitionerClass(CustomPartitioner.class);// 设置 reduce 个数job.setNumReduceTasks(WordCountDataUtils.WORD_LIST.size()); 6.3 执行结果执行结果如下，分别生成 6 个文件，每个文件中为对应单词的统计结果： 参考资料 分布式计算框架 MapReduce Apache Hadoop 2.9.2 &gt; MapReduce Tutorial MapReduce - Combiners]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Hadoop-HDFS简介]]></title>
    <url>%2F2019%2F06%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHadoop-HDFS%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Hadoop分布式文件系统——HDFS一、介绍二、HDFS 设计原理2.1 HDFS 架构2.2 文件系统命名空间2.3 数据复制2.4 数据复制的实现原理2.5 副本的选择2.6 架构的稳定性1. 心跳机制和重新复制2. 数据的完整性3.元数据的磁盘故障4.支持快照三、HDFS 的特点3.1 高容错3.2 高吞吐量3.3 大文件支持3.3 简单一致性模型3.4 跨平台移植性附：图解HDFS存储原理1. HDFS写数据原理2. HDFS读数据原理3. HDFS故障类型和其检测方法 一、介绍HDFS （Hadoop Distributed File System）是 Hadoop 下的分布式文件系统，具有高容错、高吞吐量等特性，可以部署在低成本的硬件上。 二、HDFS 设计原理 2.1 HDFS 架构HDFS 遵循主/从架构，由单个 NameNode(NN) 和多个 DataNode(DN) 组成： NameNode : 负责执行有关 文件系统命名空间 的操作，例如打开，关闭、重命名文件和目录等。它同时还负责集群元数据的存储，记录着文件中各个数据块的位置信息。 DataNode：负责提供来自文件系统客户端的读写请求，执行块的创建，删除等操作。 2.2 文件系统命名空间HDFS 的 文件系统命名空间 的层次结构与大多数文件系统类似 (如 Linux)， 支持目录和文件的创建、移动、删除和重命名等操作，支持配置用户和访问权限，但不支持硬链接和软连接。NameNode 负责维护文件系统名称空间，记录对名称空间或其属性的任何更改。 2.3 数据复制由于 Hadoop 被设计运行在廉价的机器上，这意味着硬件是不可靠的，为了保证容错性，HDFS 提供了数据复制机制。HDFS 将每一个文件存储为一系列块，每个块由多个副本来保证容错，块的大小和复制因子可以自行配置（默认情况下，块大小是 128M，默认复制因子是 3）。 2.4 数据复制的实现原理大型的 HDFS 实例在通常分布在多个机架的多台服务器上，不同机架上的两台服务器之间通过交换机进行通讯。在大多数情况下，同一机架中的服务器间的网络带宽大于不同机架中的服务器之间的带宽。因此 HDFS 采用机架感知副本放置策略，对于常见情况，当复制因子为 3 时，HDFS 的放置策略是： 在写入程序位于 datanode 上时，就优先将写入文件的一个副本放置在该 datanode 上，否则放在随机 datanode 上。之后在另一个远程机架上的任意一个节点上放置另一个副本，并在该机架上的另一个节点上放置最后一个副本。此策略可以减少机架间的写入流量，从而提高写入性能。 如果复制因子大于 3，则随机确定第 4 个和之后副本的放置位置，同时保持每个机架的副本数量低于上限，上限值通常为 （复制系数 - 1）/机架数量 + 2，需要注意的是不允许同一个 dataNode 上具有同一个块的多个副本。 2.5 副本的选择为了最大限度地减少带宽消耗和读取延迟，HDFS 在执行读取请求时，优先读取距离读取器最近的副本。如果在与读取器节点相同的机架上存在副本，则优先选择该副本。如果 HDFS 群集跨越多个数据中心，则优先选择本地数据中心上的副本。 2.6 架构的稳定性1. 心跳机制和重新复制每个 DataNode 定期向 NameNode 发送心跳消息，如果超过指定时间没有收到心跳消息，则将 DataNode 标记为死亡。NameNode 不会将任何新的 IO 请求转发给标记为死亡的 DataNode，也不会再使用这些 DataNode 上的数据。 由于数据不再可用，可能会导致某些块的复制因子小于其指定值，NameNode 会跟踪这些块，并在必要的时候进行重新复制。 2. 数据的完整性由于存储设备故障等原因，存储在 DataNode 上的数据块也会发生损坏。为了避免读取到已经损坏的数据而导致错误，HDFS 提供了数据完整性校验机制来保证数据的完整性，具体操作如下： 当客户端创建 HDFS 文件时，它会计算文件的每个块的 校验和，并将 校验和 存储在同一 HDFS 命名空间下的单独的隐藏文件中。当客户端检索文件内容时，它会验证从每个 DataNode 接收的数据是否与存储在关联校验和文件中的 校验和 匹配。如果匹配失败，则证明数据已经损坏，此时客户端会选择从其他 DataNode 获取该块的其他可用副本。 3.元数据的磁盘故障FsImage 和 EditLog 是 HDFS 的核心数据，这些数据的意外丢失可能会导致整个 HDFS 服务不可用。为了避免这个问题，可以配置 NameNode 使其支持 FsImage 和 EditLog 多副本同步，这样 FsImage 或 EditLog 的任何改变都会引起每个副本 FsImage 和 EditLog 的同步更新。 4.支持快照快照支持在特定时刻存储数据副本，在数据意外损坏时，可以通过回滚操作恢复到健康的数据状态。 三、HDFS 的特点3.1 高容错由于 HDFS 采用数据的多副本方案，所以部分硬件的损坏不会导致全部数据的丢失。 3.2 高吞吐量HDFS 设计的重点是支持高吞吐量的数据访问，而不是低延迟的数据访问。 3.3 大文件支持HDFS 适合于大文件的存储，文档的大小应该是是 GB 到 TB 级别的。 3.3 简单一致性模型HDFS 更适合于一次写入多次读取 (write-once-read-many) 的访问模型。支持将内容追加到文件末尾，但不支持数据的随机访问，不能从文件任意位置新增数据。 3.4 跨平台移植性HDFS 具有良好的跨平台移植性，这使得其他大数据计算框架都将其作为数据持久化存储的首选方案。 附：图解HDFS存储原理 说明：以下图片引用自博客：翻译经典 HDFS 原理讲解漫画 1. HDFS写数据原理 2. HDFS读数据原理 3. HDFS故障类型和其检测方法 第二部分：读写故障的处理 第三部分：DataNode 故障处理 副本布局策略： 参考资料 Apache Hadoop 2.9.2 &gt; HDFS Architecture Tom White . hadoop 权威指南 [M] . 清华大学出版社 . 2017. 翻译经典 HDFS 原理讲解漫画]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Linux基础和命令]]></title>
    <url>%2F2019%2F06%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BLinux%E5%9F%BA%E7%A1%80%E5%92%8C%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[Linux命令我是小白，我从来没玩过Linux,请点这里：1http://www.runoob.com/linux/Linux-intro.html 推荐的一个Git仓库我有些基础，推荐一个快速查询命令的手册，请点这里：1https://github.com/jaywcjlove/linux-command 必须学会的命令1.man和page12345678910111213141516171819201.内部命令：echo查看内部命令帮助：help echo 或者 man echo2.外部命令：ls查看外部命令帮助：ls --help 或者 man ls 或者 info ls3.man文档的类型(1~9)man 7 manman 5 passwd4.快捷键：ctrl + c：停止进程ctrl + l：清屏ctrl + r：搜索历史命令ctrl + q：退出5.善于用tab键 2.常用1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283说明：安装linux时，创建一个hadoop用户，然后使用root用户登陆系统1.进入到用户根目录cd ~ 或 cd2.查看当前所在目录pwd3.进入到hadoop用户根目录cd ~hadoop4.返回到原来目录cd -5.返回到上一级目录cd ..6.查看hadoop用户根目录下的所有文件ls -la7.在根目录下创建一个hadoop的文件夹mkdir /hadoop8.在/hadoop目录下创建src和WebRoot两个文件夹分别创建：mkdir /hadoop/src mkdir /hadoop/WebRoot同时创建：mkdir /hadoop/&#123;src,WebRoot&#125;进入到/hadoop目录，在该目录下创建.classpath和README文件分别创建：touch .classpath touch README同时创建：touch &#123;.classpath,README&#125;查看/hadoop目录下面的所有文件ls -la在/hadoop目录下面创建一个test.txt文件,同时写入内容&quot;this is test&quot;echo &quot;this is test&quot; &gt; test.txt查看一下test.txt的内容cat test.txtmore test.txtless test.txt向README文件追加写入&quot;please read me first&quot;echo &quot;please read me first&quot; &gt;&gt; README将test.txt的内容追加到README文件中cat test.txt &gt;&gt; README拷贝/hadoop目录下的所有文件到/hadoop-bakcp -r /hadoop /hadoop-bak进入到/hadoop-bak目录，将test.txt移动到src目录下，并修改文件名为Student.javamv test.txt src/Student.java在src目录下创建一个struts.xml&gt; struts.xml删除所有的xml类型的文件rm -rf *.xml删除/hadoop-bak目录和下面的所有文件rm -rf /hadoop-bak返回到/hadoop目录，查看一下README文件有多单词，多少个少行wc -w READMEwc -l README返回到根目录，将/hadoop目录先打包，再用gzip压缩分步完成：tar -cvf hadoop.tar hadoop gzip hadoop.tar一步完成：tar -zcvf hadoop.tar.gz hadoop 将其解压缩，再取消打包分步完成：gzip -d hadoop.tar.gz 或 gunzip hadoop.tar.gz一步完成：tar -zxvf hadoop.tar.gz将/hadoop目录先打包，同时用bzip2压缩，并保存到/tmp目录下tar -jcvf /tmp/hadoop.tar.bz2 hadoop将/tmp/hadoop.tar.bz2解压到/usr目录下面tar -jxvf hadoop.tar.bz2 -C /usr/ 文件命令123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869701.进入到用户根目录cd ~ 或者 cdcd ~hadoop回到原来路径cd -2.查看文件详情stat a.txt3.移动mv a.txt /ect/改名mv b.txt a.txt移动并改名mv a.txt ../b.txt4拷贝并改名cp a.txt /etc/b.txt5.vi撤销修改ctrl + u (undo)恢复ctrl + r (redo)6.名令设置别名(重启后无效)alias ll=&quot;ls -l&quot;取消unalias ll7.如果想让别名重启后仍然有效需要修改vi ~/.bashrc8.添加用户useradd hadooppasswd hadoop9创建多个文件touch a.txt b.txttouch /home/&#123;a.txt,b.txt&#125;10.将一个文件的内容复制到里另一个文件中cat a.txt &gt; b.txt追加内容cat a.txt &gt;&gt; b.txt 11.将a.txt 与b.txt设为其拥有者和其所属同一个组者可写入，但其他以外的人则不可写入:chmod ug+w,o-w a.txt b.txtchmod a=wx c.txt12.将当前目录下的所有文件与子目录皆设为任何人可读取:chmod -R a+r *13.将a.txt的用户拥有者设为users,组的拥有者设为jessie:chown users:jessie a.txt14.将当前目录下的所有文件与子目录的用户的使用者为lamport,组拥有者皆设为users，chown -R lamport:users *15.将所有的java语言程式拷贝至finished子目录中:cp *.java finished16.将目前目录及其子目录下所有扩展名是java的文件列出来。find -name &quot;*.java&quot;查找当前目录下扩展名是java 的文件find -name *.java17.删除当前目录下扩展名是java的文件rm -f *.java 3.系统命令1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253541.查看主机名hostname2.修改主机名(重启后无效)hostname hadoop3.修改主机名(重启后永久生效)vi /ect/sysconfig/network4.修改IP(重启后无效)ifconfig eth0 192.168.12.225.修改IP(重启后永久生效)vi /etc/sysconfig/network-scripts/ifcfg-eth06.查看系统信息uname -auname -r7.查看ID命令id -uid -g8.日期datedate +%Y-%m-%ddate +%Tdate +%Y-%m-%d&quot; &quot;%T9.日历cal 201210.查看文件信息file filename11.挂载硬盘mountumount加载windows共享mount -t cifs //192.168.1.100/tools /mnt12.查看文件大小du -hdu -ah13.查看分区df -h14.sshssh hadoop@192.168.1.115.关机shutdown -h now /init 0shutdown -r now /reboot 4.用户和组 12345678910111213141516171819202122232425262728293031323334添加一个tom用户，设置它属于users组，并添加注释信息分步完成：useradd tom usermod -g users tom usermod -c &quot;hr tom&quot; tom一步完成：useradd -g users -c &quot;hr tom&quot; tom设置tom用户的密码passwd tom修改tom用户的登陆名为tomcatusermod -l tomcat tom将tomcat添加到sys和root组中usermod -G sys,root tomcat查看tomcat的组信息groups tomcat添加一个jerry用户并设置密码useradd jerrypasswd jerry添加一个交america的组groupadd america将jerry添加到america组中usermod -g america jerry将tomcat用户从root组和sys组删除gpasswd -d tomcat rootgpasswd -d tomcat sys将america组名修改为amgroupmod -n am america 权限 1234567891011121314创建a.txt和b.txt文件，将他们设为其拥有者和所在组可写入，但其他以外的人则不可写入:chmod ug+w,o-w a.txt b.txt创建c.txt文件所有人都可以写和执行chmod a=wx c.txt 或chmod 666 c.txt将/hadoop目录下的所有文件与子目录皆设为任何人可读取chmod -R a+r /hadoop将/hadoop目录下的所有文件与子目录的拥有者设为root，用户拥有组为userschown -R root:users /hadoop将当前目录下的所有文件与子目录的用户皆设为hadoop，组设为userschown -R hadoop:users * 6.目录属性 12345678910111.查看文件夹属性ls -ld test2.文件夹的rwx--x:可以cd进去r-x:可以cd进去并ls-wx:可以cd进去并touch，rm自己的文件，并且可以vi其他用户的文件-wt:可以cd进去并touch，rm自己的文件ls -ld /tmpdrwxrwxrwt的权限值是1777(sticky) 7.软件安装 1234567891011121314151617181920212223241.安装JDK *添加执行权限 chmod u+x jdk-6u45-linux-i586.bin *解压 ./jdk-6u45-linux-i586.bin *在/usr目录下创建java目录 mkdir /usr/java *将/soft目录下的解压的jdk1.6.0_45剪切到/usr/java目录下 mv jdk1.6.0_45/ /usr/java/ *添加环境变量 vim /etc/profile *在/etc/profile文件最后添加 export JAVA_HOME=/usr/java/jdk1.6.0_45 export CLASSPATH=$JAVA_HOME/lib export PATH=$PATH:$JAVA_HOME/bin *更新配置 source /etc/profile 2.安装tomcat tar -zxvf /soft/apache-tomcat-7.0.47.tar.gz -C /programs/ cd /programs/apache-tomcat-7.0.47/bin/ ./startup.sh 3.安装eclipse 8.vim1234567891011121314151617181920212223242526272829303132333435ia/Ao/Or + ?替换0:文件当前行的开头$:文件当前行的末尾G:文件的最后一行开头1 + G到第一行 9 + G到第九行 = :9dd:删除一行3dd：删除3行yy:复制一行3yy:复制3行p:粘贴u:undoctrl + r:redo&quot;a剪切板a&quot;b剪切板b&quot;ap粘贴剪切板a的内容每次进入vi就有行号vi ~/.vimrcset nu:w a.txt另存为:w &gt;&gt; a.txt内容追加到a.txt:e!恢复到最初状态:1,$s/hadoop/root/g 将第一行到追后一行的hadoop替换为root:1,$s/hadoop/root/c 将第一行到追后一行的hadoop替换为root(有提示) 9.查找 123456789101112131415161718192021222324252627282930313233341.查找可执行的命令：which ls2.查找可执行的命令和帮助的位置：whereis ls3.查找文件(需要更新库:updatedb)locate hadoop.txt4.从某个文件夹开始查找find / -name &quot;hadooop*&quot;find / -name &quot;hadooop*&quot; -ls5.查找并删除find / -name &quot;hadooop*&quot; -ok rm &#123;&#125; \;find / -name &quot;hadooop*&quot; -exec rm &#123;&#125; \;6.查找用户为hadoop的文件find /usr -user hadoop -ls7.查找用户为hadoop并且(-a)拥有组为root的文件find /usr -user hadoop -a -group root -ls8.查找用户为hadoop或者(-o)拥有组为root并且是文件夹类型的文件find /usr -user hadoop -o -group root -a -type d9.查找权限为777的文件find / -perm -777 -type d -ls10.显示命令历史history11.grepgrep hadoop /etc/password 10.打包与压缩 123456789101112131415161718192021222324252627282930313233343536373839401.gzip压缩gzip a.txt2.解压gunzip a.txt.gzgzip -d a.txt.gz3.bzip2压缩bzip2 a4.解压bunzip2 a.bz2bzip2 -d a.bz25.将当前目录的文件打包tar -cvf bak.tar .将/etc/password追加文件到bak.tar中(r)tar -rvf bak.tar /etc/password6.解压tar -xvf bak.tar7.打包并压缩gziptar -zcvf a.tar.gz8.解压缩tar -zxvf a.tar.gz解压到/usr/下tar -zxvf a.tar.gz -C /usr9.查看压缩包内容tar -ztvf a.tar.gzzip/unzip10.打包并压缩成bz2tar -jcvf a.tar.bz211.解压bz2tar -jxvf a.tar.bz2 11.正则 12345678910111213141516171819202122232425262728293031323334353637383940414243441.cut截取以:分割保留第七段grep hadoop /etc/passwd | cut -d: -f72.排序du | sort -n 3.查询不包含hadoop的grep -v hadoop /etc/passwd4.正则表达包含hadoopgrep &apos;hadoop&apos; /etc/passwd5.正则表达(点代表任意一个字符)grep &apos;h.*p&apos; /etc/passwd6.正则表达以hadoop开头grep &apos;^hadoop&apos; /etc/passwd7.正则表达以hadoop结尾grep &apos;hadoop$&apos; /etc/passwd规则：. : 任意一个字符a* : 任意多个a(零个或多个a)a? : 零个或一个aa+ : 一个或多个a.* : 任意多个任意字符\. : 转义.\&lt;h.*p\&gt; ：以h开头，p结尾的一个单词o\&#123;2\&#125; : o重复两次grep &apos;^i.\&#123;18\&#125;n$&apos; /usr/share/dict/words查找不是以#开头的行grep -v &apos;^#&apos; a.txt | grep -v &apos;^$&apos; 以h或r开头的grep &apos;^[hr]&apos; /etc/passwd不是以h和r开头的grep &apos;^[^hr]&apos; /etc/passwd不是以h到r开头的grep &apos;^[^h-r]&apos; /etc/passwd 12.输入输出重定向及管道 1234567891011121314151617181920212223242526271.新建一个文件touch a.txt&gt; b.txt2.错误重定向:2&gt;find /etc -name zhaoxing.txt 2&gt; error.txt3.将正确或错误的信息都输入到log.txt中find /etc -name passwd &gt; /tmp/log.txt 2&gt;&amp;1 find /etc -name passwd &amp;&gt; /tmp/log.txt4.追加&gt;&gt;5.将小写转为大写（输入重定向）tr &quot;a-z&quot; &quot;A-Z&quot; &lt; /etc/passwd6.自动创建文件cat &gt; log.txt &lt;&lt; EXIT&gt; ccc&gt; ddd&gt; EXI7.查看/etc下的文件有多少个？ls -l /etc/ | grep &apos;^d&apos; | wc -l8.查看/etc下的文件有多少个，并将文件详情输入到result.txt中ls -l /etc/ | grep &apos;^d&apos; | tee result.txt | wc -l 13.进程控制 123456789101112131415161718192021222324252627281.查看用户最近登录情况lastlastlog2.查看硬盘使用情况df3.查看文件大小du4.查看内存使用情况free5.查看文件系统/proc6.查看日志ls /var/log/7.查看系统报错日志tail /var/log/messages8.查看进程top9.结束进程kill 1234kill -9 4333]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Linux</tag>
        <tag>基础命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析9-ChannelHandler实例之MessageToByteEncoder]]></title>
    <url>%2F2019%2F05%2F31%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%909-ChannelHandler%E5%AE%9E%E4%BE%8B%E4%B9%8BMessageToByteEncoder%2F</url>
    <content type="text"><![CDATA[MessageToByteEncoder框架可见用户使用POJO对象编码为字节数据存储到ByteBuf。用户只需定义自己的编码方法encode()即可。首先看类签名：12public abstract class MessageToByteEncoder&lt;I&gt; extends ChannelOutboundHandlerAdapter 可知该类只处理出站事件，切确的说是write事件。 该类有两个成员变量，preferDirect表示是否使用内核的DirectedByteBuf，默认为true。TypeParameterMatcher用于检测泛型参数是否是期待的类型，比如说，如果需要编码String类的POJO对象，Matcher会确保write()传入的参数Object的实际切确类型为String。直接分析write()的处理：1234567891011121314151617181920212223242526272829303132333435public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; ByteBuf buf = null; try &#123; if (acceptOutboundMessage(msg)) &#123; I cast = (I) msg; // 分配一个输出缓冲区 buf = allocateBuffer(ctx, cast, preferDirect); try &#123; encode(ctx, cast, buf); // 用户定义的编码方法 &#125; finally &#123; ReferenceCountUtil.release(cast); &#125; if (buf.isReadable()) &#123; ctx.write(buf, promise); // 确实写入了数据 &#125; else &#123; // 没有需要写的数据，也有可能是用户编码错误 buf.release(); ctx.write(Unpooled.EMPTY_BUFFER, promise); &#125; buf = null; &#125; else &#123; ctx.write(msg, promise); &#125; &#125; catch (EncoderException e) &#123; throw e; &#125; catch (Throwable e) &#123; throw new EncoderException(e); &#125; finally &#123; if (buf != null) &#123; buf.release(); &#125; &#125;&#125; 编码框架简单明了，再列出allocateBuffer()方法的代码：12345678protected ByteBuf allocateBuffer(ChannelHandlerContext ctx, I msg, boolean preferDirect) throws Exception &#123; if (preferDirect) &#123; return ctx.alloc().ioBuffer(); // 内核直接缓存 &#125; else &#123; return ctx.alloc().heapBuffer(); // JAVA队缓存 &#125;&#125; 总的来说，编码的复杂度大大小于解码的复杂度，这是因为编码不需考虑TCP粘包。编解码的处理还有一个常用的类MessageToMessageCodec用于POJO对象之间的转换。如果有兴趣，可下载源码查看。至此，编解码框架已分析完毕。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Netty</tag>
        <tag>ChannelHandler</tag>
        <tag>MessageToByteEncoder</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析8-ChannelHandler实例之CodecHandler]]></title>
    <url>%2F2019%2F05%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%908-ChannelHandler%E5%AE%9E%E4%BE%8B%E4%B9%8BCodecHandler%2F</url>
    <content type="text"><![CDATA[编解码处理器作为Netty编程时必备的ChannelHandler，每个应用都必不可少。Netty作为网络应用框架，在网络上的各个应用之间不断进行数据交互。而网络数据交换的基本单位是字节，所以需要将本应用的POJO对象编码为字节数据发送到其他应用，或者将收到的其他应用的字节数据解码为本应用可使用的POJO对象。这一部分，又和JAVA中的序列化和反序列化对应。幸运的是，有很多其他的开源工具（protobuf，thrift，json，xml等等）可方便的处理POJO对象的序列化，可参见这个链接。在互联网中，Netty使用TCP/UDP协议传输数据。由于Netty基于异步事件处理以及TCP的一些特性，使得TCP数据包会发生粘包现象。 想象这样的情况，客户端与服务端建立连接后，连接发送了两条消息： +------+ +------+ | MSG1 | | MSG2 | +------+ +------+ 在互联网上传输数据时，连续发送的两条消息，在服务端极有可能被合并为一条： +------------+ | MSG1 MSG2 | +------------+ 这还不是最坏的情况，由于路由器的拆包和重组，可能收到这样的两个数据包： +----+ +---------+ +-------+ +-----+ | MS | | G1MSG2 | 或者 | MSG1M | | SG2 | +----+ +---------+ +-------+ +-----+ 而服务端要正确的识别出这样的两条消息，就需要编码器的正确工作。为了正确的识别出消息，业界有以下几种做法： 使用定界符分割消息，一个特例是使用换行符分隔每条消息。使用定长的消息。在消息的某些字段指明消息长度。 明白了这些，进入正题，分析Netty的编码框架ByteToMessageDecoder。 ByteToMessageDecoder在分析之前，需要说明一点：ByteToMessage容易引起误解，解码结果Message会被认为是JAVA对象POJO，但实际解码结果是消息帧。也就是说该解码器处理TCP的粘包现象，将网络发送的字节流解码为具有确定含义的消息帧，之后的解码器再将消息帧解码为实际的POJO对象。明白了这点，再次回顾两条消息发送的最坏情况，可知要正确取得两条消息，需要一个内存区域存储消息，当收到MS时继续等待第二个包G1MSG2到达再进行解码操作。在ByteToMessageDecoder中，这个内存区域被抽象为Cumulator，直译累积器，可自动扩容累积字节数据，Netty将其定义为一个接口： 123public interface Cumulator &#123; ByteBuf cumulate(ByteBufAllocator alloc, ByteBuf cumulation, ByteBuf in);&#125; 其中，两个ByteBuf参数cumulation指已经累积的字节数据，in表示该次channelRead()读取到的新数据。返回ByteBuf为累积数据后的新累积区（必要时候自动扩容）。自动扩容的代码如下： 12345678910static ByteBuf expandCumulation(ByteBufAllocator alloc, ByteBuf cumulation, int newReadBytes) &#123; ByteBuf oldCumulation = cumulation; // 扩容后新的缓冲区 cumulation = alloc.buffer(oldCumulation.readableBytes() + readable); cumulation.writeBytes(oldCumulation); // 旧的缓冲区释放 oldCumulation.release(); return cumulation; &#125; 自动扩容的方法简单粗暴，直接使用大容量的Bytebuf替换旧的ByteBuf。Netty定义了两个累积器，一个为MERGE_CUMULATOR： 1234567891011121314151617public static final Cumulator MERGE_CUMULATOR = new Cumulator() &#123; @Override public ByteBuf cumulate(ByteBufAllocator alloc, ByteBuf cumulation, ByteBuf in) &#123; ByteBuf buffer; // 1.累积区容量不够容纳数据 // 2.用户使用了slice().retain()或duplicate().retain()使refCnt增加 if (cumulation.writerIndex() &gt; cumulation.maxCapacity() - in.readableBytes() || cumulation.refCnt() &gt; 1) &#123; buffer = expandCumulation(alloc, cumulation, in.readableBytes()); &#125; else &#123; buffer = cumulation; &#125; buffer.writeBytes(in); in.release(); return buffer; &#125; &#125;; 可知，两种情况下会扩容： 累积区容量不够容纳新读入的数据 用户使用了slice().retain()或duplicate().retain()使refCnt增加并且大于1，此时扩容返回一个新的累积区ByteBuf，方便用户对老的累积区ByteBuf进行后续处理。 另一个累积器为COMPOSITE_CUMULATOR： 12345678910111213141516171819202122public static final Cumulator COMPOSITE_CUMULATOR = new Cumulator() &#123; @Override public ByteBuf cumulate(ByteBufAllocator alloc, ByteBuf cumulation, ByteBuf in) &#123; ByteBuf buffer; if (cumulation.refCnt() &gt; 1) &#123; buffer = expandCumulation(alloc, cumulation, in.readableBytes()); buffer.writeBytes(in); in.release(); &#125; else &#123; CompositeByteBuf composite; if (cumulation instanceof CompositeByteBuf) &#123; composite = (CompositeByteBuf) cumulation; &#125; else &#123; composite = alloc.compositeBuffer(Integer.MAX_VALUE); composite.addComponent(true, cumulation); &#125; composite.addComponent(true, in); buffer = composite; &#125; return buffer; &#125; &#125;; 这个累积器只在第二种情况refCnt&gt;1时扩容，除此之外处理和MERGE_CUMULATOR一致，不同的是当cumulation不是CompositeByteBuf时会创建新的同类CompositeByteBuf，这样最后返回的ByteBuf必定是CompositeByteBuf。使用这个累积器后，当容量不够时并不会进行内存复制，只会讲新读入的in加到CompositeByteBuf中。需要注意的是：此种情况下虽然不需内存复制，却要求用户维护复杂的索引，在某些使用中可能慢于MERGE_CUMULATOR。故Netty默认使用MERGE_CUMULATOR累积器。累积器分析完毕，步入正题ByteToMessageDecoder，首先看类签名： 12public abstract class ByteToMessageDecoder extends ChannelInboundHandlerAdapter 该类是一个抽象类，其中的抽象方法只有一个decode()： 12protected abstract void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception; 用户使用了该解码框架后，只需实现该方法就可定义自己的解码器。参数in表示累积器已累积的数据，out表示本次可从累积数据解码出的结果列表，结果可为POJO对象或者ByteBuf等等Object。关注一下成员变量，以便更好的分析： 123456789ByteBuf cumulation; // 累积区private Cumulator cumulator = MERGE_CUMULATOR; // 累积器// 设置为true后每个channelRead事件只解码出一个结果private boolean singleDecode; // 某些特殊协议使用private boolean decodeWasNull; // 解码结果为空private boolean first; // 是否首个消息// 累积区不丢弃字节的最大次数，16次后开始丢弃private int discardAfterReads = 16;private int numReads; // 累积区不丢弃字节的channelRead次数 下面，直接进入channelRead()事件处理： 12345678910111213141516171819202122232425262728293031323334353637383940414243public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; // 只对ByteBuf处理即只对字节数据进行处理 if (msg instanceof ByteBuf) &#123; // 解码结果列表 CodecOutputList out = CodecOutputList.newInstance(); try &#123; ByteBuf data = (ByteBuf) msg; first = cumulation == null; // 累积区为空表示首次解码 if (first) &#123; // 首次解码直接使用读入的ByteBuf作为累积区 cumulation = data; &#125; else &#123; // 非首次需要进行字节数据累积 cumulation = cumulator.cumulate(ctx.alloc(), cumulation, data); &#125; callDecode(ctx, cumulation, out); // 解码操作 &#125; catch (DecoderException e) &#123; throw e; &#125; catch (Throwable t) &#123; throw new DecoderException(t); &#125; finally &#123; if (cumulation != null &amp;&amp; !cumulation.isReadable()) &#123; // 此时累积区不再有字节数据，已被处理完毕 numReads = 0; cumulation.release(); cumulation = null; &#125; else if (++ numReads &gt;= discardAfterReads) &#123; // 连续discardAfterReads次后 // 累积区还有字节数据，此时丢弃一部分数据 numReads = 0; discardSomeReadBytes(); // 丢弃一些已读字节 &#125; int size = out.size(); // 本次没有解码出数据，此时size=0 decodeWasNull = !out.insertSinceRecycled(); fireChannelRead(ctx, out, size); // 触发事件 out.recycle(); // 回收解码结果 &#125; &#125; else &#123; ctx.fireChannelRead(msg); &#125; &#125; 解码结果列表CodecOutputList是Netty定制的一个特殊列表，该列表在线程中被缓存，可循环使用来存储解码结果，减少不必要的列表实例创建，从而提升性能。由于解码结果需要频繁存储，普通的ArrayList难以满足该需求，故定制化了一个特殊列表，由此可见Netty对优化的极致追求。注意finally块的第一个if情况满足时，即累积区的数据已被读取完毕，请考虑释放累积区的必要性。想象这样的情况，当一条消息被解码完毕后，如果客户端长时间不发送消息，那么，服务端保存该条消息的累积区将一直占据服务端内存浪费资源，所以必须释放该累积区。第二个if情况满足时，即累积区的数据一直在channelRead读取数据进行累积和解码，直到达到了discardAfterReads次（默认16），此时累积区依然还有数据。在这样的情况下，Netty主动丢弃一些字节，这是为了防止该累积区占用大量内存甚至耗尽内存引发OOM。处理完这些情况后，最后统一触发ChannelRead事件，将解码出的数据传递给下一个处理器。注意：当out=0时，统一到一起被处理了。再看细节的discardSomeReadBytes()和fireChannelRead()： 123456789101112protected final void discardSomeReadBytes() &#123; if (cumulation != null &amp;&amp; !first &amp;&amp; cumulation.refCnt() == 1) &#123; cumulation.discardSomeReadBytes(); &#125; &#125; static void fireChannelRead(ChannelHandlerContext ctx, CodecOutputList msgs, int numElements) &#123; for (int i = 0; i &lt; numElements; i ++) &#123; ctx.fireChannelRead(msgs.getUnsafe(i)); &#125; &#125; 代码比较简单，只需注意discardSomeReadBytes中，累积区的refCnt() == 1时才丢弃数据是因为：如果用户使用了slice().retain()和duplicate().retain()使refCnt&gt;1，表明该累积区还在被用户使用，丢弃数据可能导致用户的困惑，所以须确定用户不再使用该累积区的已读数据，此时才丢弃。下面分析解码核心方法callDecode()： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455protected void callDecode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) &#123; try &#123; while (in.isReadable()) &#123; int outSize = out.size(); if (outSize &gt; 0) &#123; // 解码出消息就立即处理，防止消息等待 fireChannelRead(ctx, out, outSize); out.clear(); // 用户主动删除该Handler，继续操作in是不安全的 if (ctx.isRemoved()) &#123; break; &#125; outSize = 0; &#125; int oldInputLength = in.readableBytes(); decode(ctx, in, out); // 子类需要实现的具体解码步骤 // 用户主动删除该Handler，继续操作in是不安全的 if (ctx.isRemoved()) &#123; break; &#125; // 此时outSize都==0（这的代码容易产生误解 应该直接使用0） if (outSize == out.size()) &#123; if (oldInputLength == in.readableBytes()) &#123; // 没有解码出消息，且没读取任何in数据 break; &#125; else &#123; // 读取了一部份数据但没有解码出消息 // 说明需要更多的数据，故继续 continue; &#125; &#125; // 运行到这里outSize&gt;0 说明已经解码出消息 if (oldInputLength == in.readableBytes()) &#123; // 解码出消息但是in的读索引不变，用户的decode方法有Bug throw new DecoderException( &quot;did not read anything but decoded a message.&quot;); &#125; // 用户设定一个channelRead事件只解码一次 if (isSingleDecode()) &#123; break; &#125; &#125; &#125; catch (DecoderException e) &#123; throw e; &#125; catch (Throwable cause) &#123; throw new DecoderException(cause); &#125; &#125; 循环中的第一个if分支，检查解码结果，如果已经解码出消息则立即将消息传播到下一个处理器进行处理，这样可使消息得到及时处理。在调用decode()方法的前后，都检查该Handler是否被用户从ChannelPipeline中删除，如果删除则跳出解码步骤不对输入缓冲区in进行操作，因为继续操作in已经不安全。解码完成后，对in解码前后的读索引进行了检查，防止用户的错误使用，如果用户错误使用将抛出异常。至此，核心的解码框架已经分析完毕，再看最后的一些边角处理。首先是channelReadComplete()读事件完成后的处理： 123456789101112public void channelReadComplete(ChannelHandlerContext ctx) throws Exception &#123; numReads = 0; // 连续读次数置0 discardSomeReadBytes(); // 丢弃已读数据，节约内存 if (decodeWasNull) &#123; // 没有解码出结果，则期待更多数据读入 decodeWasNull = false; if (!ctx.channel().config().isAutoRead()) &#123; ctx.read(); &#125; &#125; ctx.fireChannelReadComplete(); &#125; 如果channelRead()中没有解码出消息，极有可能是数据不够，由此调用ctx.read()期待读入更多的数据。如果设置了自动读取，将会在HeadHandler中调用ctx.read()；没有设置自动读取，则需要此处显式调用。最后再看Handler从ChannelPipelien中移除的处理handlerRemoved(): 1234567891011121314151617181920public final void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; ByteBuf buf = cumulation; if (buf != null) &#123; cumulation = null; // 释放累积区，GC回收 int readable = buf.readableBytes(); if (readable &gt; 0) &#123; ByteBuf bytes = buf.readBytes(readable); buf.release(); // 解码器已被删除故不再解码，只将数据传播到下一个Handler ctx.fireChannelRead(bytes); &#125; else &#123; buf.release(); &#125; numReads = 0; // 置0，有可能被再次添加 ctx.fireChannelReadComplete(); &#125; handlerRemoved0(ctx); // 用户可进行的自定义处理 &#125; 当解码器被删除时，如果还有没被解码的数据，则将数据传播到下一个处理器处理，防止丢失数据。此外，当连接不再有效触发channelInactive事件或者触发ChannelInputShutdownEvent时，则会调用callDecode()解码，如果解码出消息，传播到下一个处理器。这部分的代码不再列出。至此，ByteToMessageDecoder解码框架已分析完毕，下面，我们选用具体的实例进行分析。 LineBasedFrameDecoder基于行分隔的解码器LineBasedFrameDecoder是一个特殊的分隔符解码器，该解码器使用的分隔符为：windows的\r\n和类linux的\n。首先看该类定义的成员变量：1234567891011// 最大帧长度，超过此长度将抛出异常TooLongFrameExceptionprivate final int maxLength;// 是否快速失败，true-检测到帧长度过长立即抛出异常不在读取整个帧// false-检测到帧长度过长依然读完整个帧再抛出异常private final boolean failFast;// 是否略过分隔符，true-解码结果不含分隔符private final boolean stripDelimiter;// 超过最大帧长度是否丢弃字节private boolean discarding;private int discardedBytes; // 丢弃的字节数 其中，前三个变量可由用户根据实际情况配置，后两个变量解码时使用。该子类覆盖的解码方法如下： 1234567protected final void decode(ChannelHandlerContext ctx, ByteBuf in, List&lt;Object&gt; out) throws Exception &#123; Object decoded = decode(ctx, in); if (decoded != null) &#123; out.add(decoded); &#125; &#125; 其中又定义了decode(ctx, in)解码出单个消息帧，事实上这也是其他编码子类使用的方法。decode(ctx, in)方法处理很绕弯，只给出伪代码： 12345678910111213141516171819202122232425262728protected Object decode(ChannelHandlerContext ctx, ByteBuf buffer) throws Exception &#123; final int eol = findEndOfLine(buffer); if (!discarding) &#123; if (eol &gt;= 0) &#123; // 此时已找到换行符 if(!checkMaxLength()) &#123; return getFrame().retain(); &#125; // 超过最大长度抛出异常 &#125; else &#123; if (checkMaxLength()) &#123; // 设置true表示下一次解码需要丢弃字节 discarding = true; if (failFast) &#123; // 抛出异常 &#125; &#125; &#125; &#125; else &#123; if (eol &gt;= 0) &#123; // 丢弃换行符以及之前的字节 buffer.readerIndex(eol + delimLength); &#125; else &#123; // 丢弃收到的所有字节 buffer.readerIndex(buffer.writerIndex()); &#125; &#125; &#125; 该方法需要结合解码框架的while循环反复理解，每个if情况都是一次while循环，而变量discarding就成为控制每次解码流程的状态量，注意其中的状态转移。(想法：使用状态机实现，则流程更清晰) DelimiterBasedFrameDecoder该解码器是更通用的分隔符解码器，可支持多个分隔符，每个分隔符可为一个或多个字符。如果定义了多个分隔符，并且可解码出多个消息帧，则选择产生最小帧长的结果。例如，使用行分隔符\r\n和\n分隔： +--------------+ | ABC\nDEF\r\n | +--------------+ 可有两种结果： +-----+-----+ +----------+ | ABC | DEF | (√) 和 | ABC\nDEF | (×) +-----+-----+ +----------+ 该编码器可配置的变量与LineBasedFrameDecoder类似，只是多了一个ByteBuf[] delimiters用于配置具体的分隔符。Netty在Delimiters类中定义了两种默认的分隔符，分别是NULL分隔符和行分隔符： 1234567891011public static ByteBuf[] nulDelimiter() &#123; return new ByteBuf[] &#123; Unpooled.wrappedBuffer(new byte[] &#123; 0 &#125;) &#125;; &#125; public static ByteBuf[] lineDelimiter() &#123; return new ByteBuf[] &#123; Unpooled.wrappedBuffer(new byte[] &#123; &apos;\r&apos;, &apos;\n&apos; &#125;), Unpooled.wrappedBuffer(new byte[] &#123; &apos;\n&apos; &#125;), &#125;; &#125; FixedLengthFrameDecoder该解码器十分简单，按照固定长度frameLength解码出消息帧。如下的数据帧解码为固定长度3的消息帧示例如下： +---+----+------+----+ +-----+-----+-----+ | A | BC | DEFG | HI | -&gt; | ABC | DEF | GHI | +---+----+------+----+ +-----+-----+-----+ 其中的解码方法也十分简单： 1234567protected Object decode(ChannelHandlerContext ctx, ByteBuf in) throws Exception &#123; if (in.readableBytes() &lt; frameLength) &#123; return null; &#125; else &#123; return in.readSlice(frameLength).retain(); &#125; &#125; LengthFieldBasedFrameDecoder基于长度字段的消息帧解码器，该解码器可根据数据包中的长度字段动态的解码出消息帧。一个推荐的二进制传输协议可设计为如下格式： +----------+------+----------+------+ | 头部长度 | 头部 | 数据长度 | 数据 | +----------+------+----------+------+ 这样的协议可满足大多数场景使用，但不幸的是：很多情况下并不可以设计新的协议，往往要在老旧的协议上传输数据。由此，Netty将该解码器设计的十分通用，只要有类似的长度字段便能正确解码出消息帧。当然前提是：正确使用解码器。没有什么是完美的，由于该解码器十分通用，所以有大量的配置变量： 1234567private final ByteOrder byteOrder;private final int maxFrameLength;private final boolean failFast;private final int lengthFieldOffset;private final int lengthFieldLength;private final int lengthAdjustment;private final int initialBytesToStrip; 变量byteOrder表示长度字段的字节序：大端或小端，默认为大端。如果对字节序有疑问，请查阅其他资料，不再赘述。maxFrameLength和failFast与其他解码器相同，控制最大帧长度和快速失败抛异常，注意：该解码器failFast默认为true。接下来将重点介绍其它四个变量： lengthFieldOffset表示长度字段偏移量即在一个数据包中长度字段的具体下标位置。标准情况，该长度字段为数据部分长度。 lengthFieldLength表示长度字段的具体字节数，如一个int占4字节。该解码器支持的字节数有：1，2，3，4和8，其他则会抛出异常。另外，还需要注意的是：长度字段的结果为无符号数。 lengthAdjustment是一个长度调节量，当数据包的长度字段不是数据部分长度而是总长度时，可将此值设定为头部长度，便能正确解码出包含整个数据包的结果消息帧。注意：某些情况下，该值可设定为负数。 initialBytesToStrip表示需要略过的字节数，如果我们只关心数据部分而不关心头部，可将此值设定为头部长度从而丢弃头部。下面我们使用具体的例子来说明： 需求1：如下待解码数据包，正确解码为消息帧，其中长度字段在最前面的2字节，数据部分为12字节的字符串”HELLO, WORLD”，长度字段0x000C=12 表示数据部分长度，数据包总长度则为14字节。 解码前(14 bytes) 解码后(14 bytes) +--------+----------------+ +--------+----------------+ | Length | Actual Content |-----&gt;| Length | Actual Content | | 0x000C | &quot;HELLO, WORLD&quot; | | 0x000C | &quot;HELLO, WORLD&quot; | +--------+----------------+ +--------+----------------+ 正确配置（只列出四个值中不为0的值）： 1lengthFieldLength = 2; 需求2：需求1的数据包不变，消息帧中去除长度字段。 解码前(14 bytes) 解码后(12 bytes) +--------+----------------+ +----------------+ | Length | Actual Content |-----&gt;| Actual Content | | 0x000C | &quot;HELLO, WORLD&quot; | | &quot;HELLO, WORLD&quot; | +--------+----------------+ +----------------+ 正确配置：12lengthFieldLength = 2; initialBytesToStrip = 2; 需求3：需求1数据包中长度字段表示数据包总长度。 解码前(14 bytes) 解码后(14 bytes) +--------+----------------+ +--------+----------------+ | Length | Actual Content |-----&gt;| Length | Actual Content | | 0x000E | &quot;HELLO, WORLD&quot; | | 0x000E | &quot;HELLO, WORLD&quot; | +--------+----------------+ +--------+----------------+ 正确配置： 12lengthFieldLength = 2;lengthAdjustment = -2; // 调整长度字段的2字节 需求4：综合难度，数据包有两个头部HDR1和HDR2，长度字段以及数据部分组成，其中长度字段值表示数据包总长度。结果消息帧需要第二个头部HDR2和数据部分。请先给出答案再与标准答案比较，结果正确说明你已完全掌握了该解码器的使用。 解码前 (16 bytes) 解码后 (13 bytes) +------+--------+------+----------------+ +------+----------------+ | HDR1 | Length | HDR2 | Actual Content |-----&gt;| HDR2 | Actual Content | | 0xCA | 0x0010 | 0xFE | &quot;HELLO, WORLD&quot; | | 0xFE | &quot;HELLO, WORLD&quot; | +------+--------+------+----------------+ +------+----------------+ 正确配置： 1234lengthFieldOffset = 1; lengthFieldLength = 2; lengthAdjustment = -3; initialBytesToStrip = 3; 本解码器的解码过程总体上较为复杂，由于解码的代码是在while循环里面，decode方法return或者抛出异常时可看做一次循环结束，直到in中数据被解析完或者in的readerIndex读索引不再增加才会从while循环跳出。使用状态的思路理解，每个return或者抛出异常看为一个状态： 状态1：丢弃过长帧状态，可能是用户设置了错误的帧长度或者实际帧过长。 123456789if (discardingTooLongFrame) &#123; long bytesToDiscard = this.bytesToDiscard; int localBytesToDiscard = (int) Math.min(bytesToDiscard, in.readableBytes()); in.skipBytes(localBytesToDiscard); // 丢弃实际的字节数 bytesToDiscard -= localBytesToDiscard; this.bytesToDiscard = bytesToDiscard; failIfNecessary(false); &#125; 变量localBytesToDiscard取得实际需要丢弃的字节数，由于过长帧有两种情况：a.用户设置了错误的长度字段，此时in中并没有如此多的字节；b.in中确实有如此长度的帧，这个帧确实超过了设定的最大长度。bytesToDiscard的计算是为了failIfNecessary()确定异常的抛出，其值为0表示当次丢弃状态已经丢弃了in中的所有数据，可以对新读入in的数据进行处理；否则，还处于异常状态。 1234567891011121314151617private void failIfNecessary(boolean firstDetectionOfTooLongFrame) &#123; if (bytesToDiscard == 0) &#123; long tooLongFrameLength = this.tooLongFrameLength; this.tooLongFrameLength = 0; // 由于已经丢弃所有数据，关闭丢弃模式 discardingTooLongFrame = false; // 已经丢弃了所有字节，当非快速失败模式抛异常 if (!failFast || firstDetectionOfTooLongFrame) &#123; fail(tooLongFrameLength); &#125; &#125; else &#123; if (failFast &amp;&amp; firstDetectionOfTooLongFrame) &#123; // 帧长度异常，快速失败模式检测到即抛异常 fail(tooLongFrameLength); &#125; &#125; &#125; 可见，首次检测到帧长度是一种特殊情况，在之后的一个状态进行分析。请注意该状态并不是都抛异常，还有可能进入状态2。 状态2：in中数据不足够组成消息帧，此时直接返回null等待更多数据到达。 123if (in.readableBytes() &lt; lengthFieldEndOffset) &#123; return null;&#125; 状态3：帧长度错误检测，检测长度字段为负值得帧以及加入调整长度后总长小于长度字段的帧，均抛出异常。 12345678910111213int actualLengthFieldOffset = in.readerIndex() + lengthFieldOffset; // 该方法取出长度字段的值，不再深入分析 long frameLength = getUnadjustedFrameLength(in, actualLengthFieldOffset, lengthFieldLength, byteOrder); if (frameLength &lt; 0) &#123; in.skipBytes(lengthFieldEndOffset); throw new CorruptedFrameException(&quot;...&quot;); &#125; frameLength += lengthAdjustment + lengthFieldEndOffset; if (frameLength &lt; lengthFieldEndOffset) &#123; in.skipBytes(lengthFieldEndOffset); throw new CorruptedFrameException(&quot;...&quot;); 状态4：帧过长，由前述可知：可能是用户设置了错误的帧长度或者实际帧过长 1234567891011121314if (frameLength &gt; maxFrameLength) &#123; long discard = frameLength - in.readableBytes(); tooLongFrameLength = frameLength; if (discard &lt; 0) &#123; in.skipBytes((int) frameLength); &#125; else &#123; discardingTooLongFrame = true; bytesToDiscard = discard; in.skipBytes(in.readableBytes()); &#125; failIfNecessary(true); return null; &#125; 变量discard&lt;0表示当前收到的数据足以确定是实际的帧过长，所以直接丢弃过长的帧长度；&gt;0表示当前in中的数据并不足以确定是用户设置了错误的帧长度，还是正确帧的后续数据字节还没有到达，但无论何种情况，将丢弃状态discardingTooLongFrame标记设置为true，之后后续数据字节进入状态1处理。==0时，在failIfNecessary(true)无论如何都将抛出异常，&gt;&lt;0时，只有设置快速失败才会抛出异常。还需注意一点：failIfNecessary()的参数firstDetectionOfTooLongFrame的首次是指正确解析数据后发生的第一次发生的帧过长，可知会有很多首次。 状态5：正确解码出消息帧。 1234567891011121314151617int frameLengthInt = (int) frameLength; if (in.readableBytes() &lt; frameLengthInt) &#123; return null; // 到达的数据还达不到帧长 &#125; if (initialBytesToStrip &gt; frameLengthInt) &#123; in.skipBytes(frameLengthInt); // 跳过字节数错误 throw new CorruptedFrameException(&quot;...&quot;); &#125; in.skipBytes(initialBytesToStrip); // 正确解码出数据帧 int readerIndex = in.readerIndex(); int actualFrameLength = frameLengthInt - initialBytesToStrip; ByteBuf frame = in.slice(readerIndex, actualFrameLength).retain(); in.readerIndex(readerIndex + actualFrameLength); return frame; 代码中混合了两个简单状态，到达的数据还达不到帧长和用户设置的忽略字节数错误。由于较为简单，故合并到一起。至此解码框架分析完毕。可见，要正确的写出基于长度字段的解码器还是较为复杂的，如果开发时确有需求，特别要注意状态的转移。下面介绍较为简单的编码框架。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Netty</tag>
        <tag>ChannelHandler</tag>
        <tag>CodecHandler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析7-ChannelHandler实例之TimeoutHandler]]></title>
    <url>%2F2019%2F05%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%907-ChannelHandler%E5%AE%9E%E4%BE%8B%E4%B9%8BTimeoutHandler%2F</url>
    <content type="text"><![CDATA[TimeoutHandler在开发TCP服务时，一个常见的需求便是使用心跳保活客户端。而Netty自带的三个超时处理器IdleStateHandler，ReadTimeoutHandler和WriteTimeoutHandler可完美满足此需求。其中IdleStateHandler可处理读超时（客户端长时间没有发送数据给服务端）、写超时（服务端长时间没有发送数据到客户端）和读写超时（客户端与服务端长时间无数据交互）三种情况。这三种情况的枚举为： 12345public enum IdleState &#123; READER_IDLE, // 读超时 WRITER_IDLE, // 写超时 ALL_IDLE // 数据交互超时 &#125; 以IdleStateHandler的读超时事件为例进行分析，首先看类签名： 1public class IdleStateHandler extends ChannelDuplexHandler 注意到此Handler没有Sharable注解，这是因为每个连接的超时时间是特有的即每个连接有独立的状态，所以不能标注Sharable注解。继承自ChannelDuplexHandler是因为既要处理读超时又要处理写超时。该类的一个典型构造方法如下： 12345public IdleStateHandler(int readerIdleTimeSeconds, int writerIdleTimeSeconds, int allIdleTimeSeconds) &#123; this(readerIdleTimeSeconds, writerIdleTimeSeconds, allIdleTimeSeconds, TimeUnit.SECONDS);&#125; 分别设定各个超时事件的时间阈值。以读超时事件为例，有以下相关的字段： 123456789101112// 用户配置的读超时时间 private final long readerIdleTimeNanos; // 判定超时的调度任务Future private ScheduledFuture&lt;?&gt; readerIdleTimeout; // 最近一次读取数据的时间 private long lastReadTime; // 是否第一次读超时事件 private boolean firstReaderIdleEvent = true; // 状态，0 - 无关， 1 - 初始化完成 2 - 已被销毁 private byte state; // 是否正在读取 private boolean reading; 首先看初始化方法initialize()： 12345678910111213private void initialize(ChannelHandlerContext ctx) &#123; switch (state) &#123; case 1: // 初始化进行中或者已完成 case 2: // 销毁进行中或者已完成 return; &#125; state = 1; lastReadTime = ticksInNanos(); if (readerIdleTimeNanos &gt; 0) &#123; readerIdleTimeout = schedule(ctx, new ReaderIdleTimeoutTask(ctx), readerIdleTimeNanos, TimeUnit.NANOSECONDS); &#125; 初始化的工作较为简单，设定最近一次读取时间lastReadTime为当前系统时间，然后在用户设置的读超时时间readerIdleTimeNanos截止时，执行一个ReaderIdleTimeoutTask进行检测。其中使用的方法很简洁，如下： 12345678 long ticksInNanos() &#123; return System.nanoTime();&#125;ScheduledFuture&lt;?&gt; schedule(ChannelHandlerContext ctx, Runnable task, long delay, TimeUnit unit) &#123; return ctx.executor().schedule(task, delay, unit);&#125; 然后，分析销毁方法destroy()： 12345678private void destroy() &#123; state = 2; // 这里结合initialize对比理解 if (readerIdleTimeout != null) &#123; // 取消调度任务，并置null readerIdleTimeout.cancel(false); readerIdleTimeout = null; &#125; &#125; 可知销毁的处理也很简单，分析完初始化和销毁，再看这两个方法被调用的地方，initialize()在三个方法中被调用： 123456789101112131415161718public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; if (ctx.channel().isActive() &amp;&amp; ctx.channel().isRegistered()) &#123; initialize(ctx); &#125; &#125; public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; if (ctx.channel().isActive()) &#123; initialize(ctx); &#125; super.channelRegistered(ctx); &#125; public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; initialize(ctx); super.channelActive(ctx); &#125; 当客户端与服务端成功建立连接后，Channel被激活，此时channelActive的初始化被调用；如果Channel被激活后，动态添加此Handler，则handlerAdded的初始化被调用；如果Channel被激活，用户主动切换Channel的执行线程Executor，则channelRegistered的初始化被调用。这一部分较难理解，请仔细体会。destroy()则有两处调用： 12345678public void channelInactive(ChannelHandlerContext ctx) throws Exception &#123; destroy(); super.channelInactive(ctx); &#125; public void handlerRemoved(ChannelHandlerContext ctx) throws Exception &#123; destroy(); &#125; 即该Handler被动态删除时，handlerRemoved的销毁被执行；Channel失效时，channelInactive的销毁被执行。分析完这些，在分析核心的调度任务ReaderIdleTimeoutTask： 12345678910111213141516171819202122232425262728293031323334353637383940414243private final class ReaderIdleTimeoutTask implements Runnable &#123; private final ChannelHandlerContext ctx; ReaderIdleTimeoutTask(ChannelHandlerContext ctx) &#123; this.ctx = ctx; &#125; @Override protected void run() &#123; if (!ctx.channel().isOpen()) &#123; // Channel不再有效 return; &#125; long nextDelay = readerIdleTimeNanos; if (!reading) &#123; // nextDelay&lt;=0 说明在设置的超时时间内没有读取数据 nextDelay -= ticksInNanos() - lastReadTime; &#125; // 隐含正在读取时，nextDelay = readerIdleTimeNanos &gt; 0 if (nextDelay &lt;= 0) &#123; // 超时时间已到，则再次调度该任务本身 readerIdleTimeout = schedule(ctx, this, readerIdleTimeNanos, TimeUnit.NANOSECONDS); boolean first = firstReaderIdleEvent; firstReaderIdleEvent = false; try &#123; IdleStateEvent event = newIdleStateEvent(IdleState.READER_IDLE, first); channelIdle(ctx, event); // 模板方法处理 &#125; catch (Throwable t) &#123; ctx.fireExceptionCaught(t); &#125; &#125; else &#123; // 注意此处的nextDelay值，会跟随lastReadTime刷新 readerIdleTimeout = schedule(ctx, this, nextDelay, TimeUnit.NANOSECONDS); &#125; &#125; &#125; 这个读超时检测任务执行的过程中又递归调用了它本身进行下一次调度，请仔细品味该种使用方法。再列出channelIdle()的代码： 1234protected void channelIdle(ChannelHandlerContext ctx, IdleStateEvent evt) throws Exception &#123; ctx.fireUserEventTriggered(evt); &#125; 本例中，该方法将写超时事件作为用户事件传播到下一个Handler，用户需要在某个Handler中拦截该事件进行处理。该方法标记为protect说明子类通常可覆盖，ReadTimeoutHandler子类即定义了自己的处理： 1234567891011121314@Override protected final void channelIdle(ChannelHandlerContext ctx, IdleStateEvent evt) throws Exception &#123; assert evt.state() == IdleState.READER_IDLE; readTimedOut(ctx); &#125; protected void readTimedOut(ChannelHandlerContext ctx) throws Exception &#123; if (!closed) &#123; ctx.fireExceptionCaught(ReadTimeoutException.INSTANCE); ctx.close(); closed = true; &#125; &#125; 可知在ReadTimeoutHandler中，如果发生读超时事件，将会关闭该Channel。当进行心跳处理时，使用IdleStateHandler较为麻烦，一个简便的方法是：直接继承ReadTimeoutHandler然后覆盖readTimedOut()进行用户所需的超时处理。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Netty</tag>
        <tag>ChannelHandler</tag>
        <tag>TimeoutHandler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析6-ChannelHandler实例之LoggingHandler]]></title>
    <url>%2F2019%2F05%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%906-ChannelHandler%E5%AE%9E%E4%BE%8B%E4%B9%8BLoggingHandler%2F</url>
    <content type="text"><![CDATA[LoggingHandler日志处理器LoggingHandler是使用Netty进行开发时的好帮手，它可以对入站\出站事件进行日志记录，从而方便我们进行问题排查。首先看类签名：12@Sharablepublic class LoggingHandler extends ChannelDuplexHandler 注解Sharable说明LoggingHandler没有状态相关变量，所有Channel可以使用一个实例。继承自ChannelDuplexHandler表示对入站出站事件都进行日志记录。最佳实践：使用static修饰LoggingHandler实例，并在生产环境删除LoggingHandler。该类的成员变量如下： 123456789// 实际使用的日志处理，slf4j、log4j等 protected final InternalLogger logger; // 日志框架使用的日志级别 protected final InternalLogLevel internalLevel; // Netty使用的日志级别 private final LogLevel level; // 默认级别为Debug private static final LogLevel DEFAULT_LEVEL = LogLevel.DEBUG; 看完成员变量，在移目构造方法，LoggingHandler的构造方法较多，一个典型的如下： 12345678910public LoggingHandler(LogLevel level) &#123; if (level == null) &#123; throw new NullPointerException(&quot;level&quot;); &#125; // 获得实际的日志框架 logger = InternalLoggerFactory.getInstance(getClass()); // 设置日志级别 this.level = level; internalLevel = level.toInternalLevel(); &#125; 在构造方法中获取用户实际使用的日志框架，如slf4j、log4j等，并日志设置记录级别。其他的构造方法也类似，不在赘述。记录出站、入站事件的过程类似，我们以ChannelRead()为例分析，代码如下： 1234567891011121314151617181920public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; logMessage(ctx, &quot;RECEIVED&quot;, msg); // 记录日志 ctx.fireChannelRead(msg); // 传播事件 &#125; private void logMessage(ChannelHandlerContext ctx, String eventName, Object msg) &#123; if (logger.isEnabled(internalLevel)) &#123; logger.log(internalLevel, format(ctx, formatMessage(eventName, msg))); &#125; &#125; protected String formatMessage(String eventName, Object msg) &#123; if (msg instanceof ByteBuf) &#123; return formatByteBuf(eventName, (ByteBuf) msg); &#125; else if (msg instanceof ByteBufHolder) &#123; return formatByteBufHolder(eventName, (ByteBufHolder) msg); &#125; else &#123; return formatNonByteBuf(eventName, msg); &#125; &#125; 其中的代码都简单明了，主要分析formatByteBuf()方法： 1234567891011121314151617protected String formatByteBuf(String eventName, ByteBuf msg) &#123; int length = msg.readableBytes(); if (length == 0) &#123; StringBuilder buf = new StringBuilder(eventName.length() + 4); buf.append(eventName).append(&quot;: 0B&quot;); return buf.toString(); &#125; else &#123; int rows = length / 16 + (length % 15 == 0? 0 : 1) + 4; StringBuilder buf = new StringBuilder(eventName.length() + 2 + 10 + 1 + 2 + rows * 80); buf.append(eventName) .append(&quot;: &quot;).append(length).append(&apos;B&apos;).append(NEWLINE); appendPrettyHexDump(buf, msg); return buf.toString(); &#125; 其中的数字计算，容易让人失去耐心，使用逆向思维，放上结果反推：]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Netty</tag>
        <tag>ChannelHandler</tag>
        <tag>LoggingHandler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析5-ChannelHandler]]></title>
    <url>%2F2019%2F05%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%905-ChannelHandler%2F</url>
    <content type="text"><![CDATA[ChannelHandler并不处理事件，而由其子类代为处理：ChannelInboundHandler拦截和处理入站事件，ChannelOutboundHandler拦截和处理出站事件。ChannelHandler和ChannelHandlerContext通过组合或继承的方式关联到一起成对使用。事件通过ChannelHandlerContext主动调用如fireXXX()和write(msg)等方法，将事件传播到下一个处理器。注意：入站事件在ChannelPipeline双向链表中由头到尾正向传播，出站事件则方向相反。 当客户端连接到服务器时，Netty新建一个ChannelPipeline处理其中的事件，而一个ChannelPipeline中含有若干ChannelHandler。如果每个客户端连接都新建一个ChannelHandler实例，当有大量客户端时，服务器将保存大量的ChannelHandler实例。为此，Netty提供了Sharable注解，如果一个ChannelHandler状态无关，那么可将其标注为Sharable，如此，服务器只需保存一个实例就能处理所有客户端的事件。 核心类图上图是ChannelHandler的核心类类图，其继承层次清晰，我们逐一分析。 1.ChannelHandlerChannaleHandler 作为最顶层的接口，并不处理入站和出站事件，所以接口中只包含最基本的方法：123456// Handler本身被添加到ChannelPipeline时调用 void handlerAdded(ChannelHandlerContext ctx) throws Exception; // Handler本身被从ChannelPipeline中删除时调用 void handlerRemoved(ChannelHandlerContext ctx) throws Exception; // 发生异常时调用 void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception; 其中也定义了Sharable标记注解：1234567@Inherited @Documented @Target(ElementType.TYPE) @Retention(RetentionPolicy.RUNTIME) @interface Sharable &#123; // no value &#125; 作为ChannelHandler的默认实现，ChannelHandlerAdapter有个重要的方法isSharable()，代码如下： 12345678910111213public boolean isSharable() &#123; Class&lt;?&gt; clazz = getClass(); // 每个线程一个缓存 Map&lt;Class&lt;?&gt;, Boolean&gt; cache = InternalThreadLocalMap.get().handlerSharableCache(); Boolean sharable = cache.get(clazz); if (sharable == null) &#123; // Handler是否存在Sharable注解 sharable = clazz.isAnnotationPresent(Sharable.class); cache.put(clazz, sharable); &#125; return sharable; &#125; 这里引入了优化的线程局部变量InternalThreadLocalMap，将在以后分析，此处可简单理解为线程变量ThreadLocal，即每个线程都有一份ChannelHandler是否Sharable的缓存。这样可以减少线程间的竞争，提升性能。 2.ChannelInboundHandlerChannelInboundHandler处理入站事件，以及用户自定义事件：1234// 类似的入站事件void channeXXX(ChannelHandlerContext ctx) throws Exception;// 用户自定义事件void userEventTriggered(ChannelHandlerContext ctx, Object evt) throws Exception; ChannelInboundHandlerAdapter作为ChannelInboundHandler的实现，默认将入站事件自动传播到下一个入站处理器。其中的代码高度一致，如下：123public void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ctx.fireChannelRead(msg);&#125; 3.ChannelOutboundHandlerChannelOutboundHandler处理出站事件：12// 类似的出站事件 void read(ChannelHandlerContext ctx) throws Exception; 同理，ChannelOutboundHandlerAdapter作为ChannelOutboundHandler的事件，默认将出站事件传播到下一个出站处理器：1234@Overridepublic void read(ChannelHandlerContext ctx) throws Exception &#123; ctx.read();&#125; 4.ChannelDuplexHandlerChannelDuplexHandler则同时实现了ChannelInboundHandler和ChannelOutboundHandler接口。如果一个所需的ChannelHandler既要处理入站事件又要处理出站事件，推荐继承此类。至此，ChannelHandler的核心类已分析完毕，接下来将分析一些Netty自带的Handler。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Netty</tag>
        <tag>ChannelHandler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析4-Handler综述]]></title>
    <url>%2F2019%2F05%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%904-Handler%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Netty中的Handler简介Handler在Netty中，占据着非常重要的地位。Handler与Servlet中的filter很像，通过Handler可以完成通讯报文的解码编码、拦截指定的报文、 统一对日志错误进行处理、统一对请求进行计数、控制Handler执行与否。一句话，没有它做不到的只有你想不到的 Netty中的所有handler都实现自ChannelHandler接口。按照输入输出来分，分为ChannelInboundHandler、ChannelOutboundHandler两大类 ChannelInboundHandler对从客户端发往服务器的报文进行处理，一般用来执行解码、读取客户端数据、进行业务处理等；ChannelOutboundHandler 对从服务器发往客户端的报文进行处理，一般用来进行编码、发送报文到客户端 Netty中可以注册多个handler。ChannelInboundHandler按照注册的先后顺序执行；ChannelOutboundHandler按照注册的先后顺序逆序执行。 ChannelPipeline中的事件不会自动流动，而我们一般需求事件自动流动，Netty提供了两个Adapter：ChannelInboundHandlerAdapter和ChannelOutboundHandlerAdapter来满足这种需求。其中的实现类似如下： 12345678910// inboud事件默认处理过程public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; ctx.fireChannelRegistered(); // 事件传播到下一个Handler&#125;// outboud事件默认处理过程public void bind(ChannelHandlerContext ctx, SocketAddress localAddress, ChannelPromise promise) throws Exception &#123; ctx.bind(localAddress, promise); // 事件传播到下一个Handler&#125; 在Adapter中，事件默认自动传播到下一个Handler，这样带来的另一个好处是：用户的Handler类可以继承Adapter且覆盖自己感兴趣的事件实现，其他事件使用默认实现，不用再实现ChannelIn/outboudHandler接口中所有方法，提高效率。我们常常遇到这样的需求：在一个业务逻辑处理器中，需要写数据库、进行网络连接等耗时业务。Netty的原则是不阻塞I/O线程，所以需指定Handler执行的线程池，可使用如下代码： 12345678static final EventExecutorGroup group = new DefaultEventExecutorGroup(16); ... ChannelPipeline pipeline = ch.pipeline(); // 简单非阻塞业务，可以使用I/O线程执行 pipeline.addLast(&quot;decoder&quot;, new MyProtocolDecoder()); pipeline.addLast(&quot;encoder&quot;, new MyProtocolEncoder()); // 复杂耗时业务，使用新的线程池 pipeline.addLast(group, &quot;handler&quot;, new MyBusinessLogicHandler()); ChannelHandler中有一个Sharable注解，使用该注解后多个ChannelPipeline中的Handler对象实例只有一个，从而减少Handler对象实例的创建。代码示例如下： 12345678 public class DataServerInitializer extends ChannelInitializer&lt;Channel&gt; &#123; private static final DataServerHandler SHARED = new DataServerHandler(); @Override public void initChannel(Channel channel) &#123; channel.pipeline().addLast(&quot;handler&quot;, SHARED); &#125;&#125; Sharable注解的使用是有限制的，多个ChannelPipeline只有一个实例，所以该Handler要求无状态。上述示例中，DataServerHandler的事件处理方法中，不能使用或改变本身的私有变量，因为ChannelHandler是非线程安全的，使用私有变量会造成线程竞争而产生错误结果。 ChannelHandlerContextContext指上下文关系，ChannelHandler的Context指的是ChannleHandler之间的关系以及ChannelHandler与ChannelPipeline之间的关系。ChannelPipeline中的事件传播主要依赖于ChannelHandlerContext实现，由于ChannelHandlerContext中有ChannelHandler之间的关系，所以能得到ChannelHandler的后继节点，从而将事件传播到下一个ChannelHandler。 ChannelHandlerContext继承自AttributeMap，所以提供了attr()方法设置和删除一些状态属性值，用户可将业务逻辑中所需使用的状态属性值存入到Context中。此外，Channel也继承自AttributeMap，也有attr()方法，在Netty4.0中，这两个attr()方法并不等效，这会给用户程序员带来困惑并且增加内存开销，所以Netty4.1中将channel.attr()==ctx.attr()。在使用Netty4.0时，建议只使用channel.attr()防止引起不必要的困惑。 一个Channel对应一个ChannelPipeline，一个ChannelHandlerContext对应一个ChannelHandler，但一个ChannelHandler可以对应多个ChannelHandlerContext。当一个ChannelHandler使用Sharable注解修饰且添加同一个实例对象到不用的Channel时，只有一个ChannelHandler实例对象，但每个Channel中都有一个ChannelHandlerContext对象实例与之对应。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Netty</tag>
        <tag>Handler</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析3-Pipeline]]></title>
    <url>%2F2019%2F05%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%903-Pipeline%2F</url>
    <content type="text"><![CDATA[Channel实现概览在Netty里，Channel是通讯的载体，而ChannelHandler负责Channel中的逻辑处理。 那么ChannelPipeline是什么呢？我觉得可以理解为ChannelHandler的容器：一个Channel包含一个ChannelPipeline，所有ChannelHandler都会注册到ChannelPipeline中，并按顺序组织起来。 在Netty中，ChannelEvent是数据或者状态的载体，例如传输的数据对应MessageEvent，状态的改变对应ChannelStateEvent。当对Channel进行操作时，会产生一个ChannelEvent，并发送到ChannelPipeline。ChannelPipeline会选择一个ChannelHandler进行处理。这个ChannelHandler处理之后，可能会产生新的ChannelEvent，并流转到下一个ChannelHandler。 例如，一个数据最开始是一个MessageEvent，它附带了一个未解码的原始二进制消息ChannelBuffer，然后某个Handler将其解码成了一个数据对象，并生成了一个新的MessageEvent，并传递给下一步进行处理。 到了这里，可以看到，其实Channel的核心流程位于ChannelPipeline中。于是我们进入ChannelPipeline的深层梦境里，来看看它具体的实现。 ChannelPipeline的主流程Netty的ChannelPipeline包含两条线路：Upstream和Downstream。Upstream对应上行，接收到的消息、被动的状态改变，都属于Upstream。Downstream则对应下行，发送的消息、主动的状态改变，都属于Downstream。ChannelPipeline接口包含了两个重要的方法:sendUpstream(ChannelEvent e)和sendDownstream(ChannelEvent e)，就分别对应了Upstream和Downstream。 对应的，ChannelPipeline里包含的ChannelHandler也包含两类：ChannelUpstreamHandler和ChannelDownstreamHandler。每条线路的Handler是互相独立的。它们都很简单的只包含一个方法：ChannelUpstreamHandler.handleUpstream和ChannelDownstreamHandler.handleDownstream。 Netty官方的javadoc里有一张图(ChannelPipeline接口里)，非常形象的说明了这个机制(我对原图进行了一点修改，加上了ChannelSink，因为我觉得这部分对理解代码流程会有些帮助)： 什么叫ChannelSink呢？ChannelSink包含一个重要方法ChannelSink.eventSunk，可以接受任意ChannelEvent。”sink”的意思是”下沉”，那么”ChannelSink”好像可以理解为”Channel下沉的地方”？实际上，它的作用确实是这样，也可以换个说法：”处于末尾的万能Handler”。最初读到这里，也有些困惑，这么理解之后，就感觉简单许多。只有Downstream包含ChannelSink，这里会做一些建立连接、绑定端口等重要操作。为什么UploadStream没有ChannelSink呢？我只能认为，一方面，不符合”sink”的意义，另一方面，也没有什么处理好做的吧！ 这里有个值得注意的地方：在一条“流”里，一个ChannelEvent并不会主动的”流”经所有的Handler，而是由上一个Handler显式的调用ChannelPipeline.sendUp(Down)stream产生，并交给下一个Handler处理。也就是说，每个Handler接收到一个ChannelEvent，并处理结束后，如果需要继续处理，那么它需要调用sendUp(Down)stream新发起一个事件。如果它不再发起事件，那么处理就到此结束，即使它后面仍然有Handler没有执行。这个机制可以保证最大的灵活性，当然对Handler的先后顺序也有了更严格的要求。 顺便说一句，在Netty 3.x里，这个机制会导致大量的ChannelEvent对象创建，因此Netty 4.x版本对此进行了改进。twitter的finagle框架实践中，就提到从Netty 3.x升级到Netty 4.x，可以大大降低GC开销。有兴趣的可以看看这篇文章：https://blog.twitter.com/2013/netty-4-at-twitter-reduced-gc-overhead 下面我们从代码层面来对这里面发生的事情进行深入分析，这部分涉及到一些细节，需要打开项目源码，对照来看，会比较有收获。 深入ChannelPipeline内部DefaultChannelPipeline的内部结构ChannelPipeline的主要的实现代码在DefaultChannelPipeline类里。列一下DefaultChannelPipeline的主要字段： 123456789public class DefaultChannelPipeline implements ChannelPipeline &#123; private volatile Channel channel; private volatile ChannelSink sink; private volatile DefaultChannelHandlerContext head; private volatile DefaultChannelHandlerContext tail; private final Map&lt;String, DefaultChannelHandlerContext&gt; name2ctx = new HashMap&lt;String, DefaultChannelHandlerContext&gt;(4);&#125; 这里需要介绍一下ChannelHandlerContext这个接口。顾名思义，ChannelHandlerContext保存了Netty与Handler相关的的上下文信息。而咱们这里的DefaultChannelHandlerContext，则是对ChannelHandler的一个包装。一个DefaultChannelHandlerContext内部，除了包含一个ChannelHandler，还保存了”next”和”prev”两个指针，从而形成一个双向链表。 因此，在DefaultChannelPipeline中，我们看到的是对DefaultChannelHandlerContext的引用，而不是对ChannelHandler的直接引用。这里包含”head”和”tail”两个引用，分别指向链表的头和尾。而name2ctx则是一个按名字索引DefaultChannelHandlerContext用户的一个map，主要在按照名称删除或者添加ChannelHandler时使用。 sendUpstream和sendDownstream前面提到了，ChannelPipeline接口的两个重要的方法：sendUpstream(ChannelEvent e)和sendDownstream(ChannelEvent e)。所有事件的发起都是基于这两个方法进行的。Channels类有一系列fireChannelBound之类的fireXXXX方法，其实都是对这两个方法的facade包装。 下面来看一下这两个方法的实现。先看sendUpstream(对代码做了一些简化，保留主逻辑)： 123456789101112131415public void sendUpstream(ChannelEvent e) &#123; DefaultChannelHandlerContext head = getActualUpstreamContext(this.head); head.getHandler().handleUpstream(head, e);&#125;private DefaultChannelHandlerContext getActualUpstreamContext(DefaultChannelHandlerContext ctx) &#123; DefaultChannelHandlerContext realCtx = ctx; while (!realCtx.canHandleUpstream()) &#123; realCtx = realCtx.next; if (realCtx == null) &#123; return null; &#125; &#125; return realCtx;&#125; 这里最终调用了ChannelUpstreamHandler.handleUpstream来处理这个ChannelEvent。有意思的是，这里我们看不到任何”将Handler向后移一位”的操作，但是我们总不能每次都用同一个Handler来进行处理啊？实际上，我们更为常用的是ChannelHandlerContext.handleUpstream方法(实现是DefaultChannelHandlerContext.sendUpstream方法)： 1234public void sendUpstream(ChannelEvent e) &#123; DefaultChannelHandlerContext next = getActualUpstreamContext(this.next); DefaultChannelPipeline.this.sendUpstream(next, e);&#125; 可以看到，这里最终仍然调用了ChannelPipeline.sendUpstream方法，但是它会将Handler指针后移。 我们接下来看看DefaultChannelHandlerContext.sendDownstream: 123456789101112public void sendDownstream(ChannelEvent e) &#123; DefaultChannelHandlerContext prev = getActualDownstreamContext(this.prev); if (prev == null) &#123; try &#123; getSink().eventSunk(DefaultChannelPipeline.this, e); &#125; catch (Throwable t) &#123; notifyHandlerException(e, t); &#125; &#125; else &#123; DefaultChannelPipeline.this.sendDownstream(prev, e); &#125;&#125; 与sendUpstream好像不大相同哦？这里有两点：一是到达末尾时，就如梦境二所说，会调用ChannelSink进行处理；二是这里指针是往前移的，所以我们知道了： UpstreamHandler是从前往后执行的，DownstreamHandler是从后往前执行的。在ChannelPipeline里添加时需要注意顺序了！ DefaultChannelPipeline里还有些机制，像添加/删除/替换Handler，以及ChannelPipelineFactory等，比较好理解，就不细说了。 回到现实：Pipeline解决的问题好了，深入分析完代码，有点头晕了，我们回到最开始的地方，来想一想，Netty的Pipeline机制解决了什么问题？ 我认为至少有两点： 一是提供了ChannelHandler的编程模型，基于ChannelHandler开发业务逻辑，基本不需要关心网络通讯方面的事情，专注于编码/解码/逻辑处理就可以了。Handler也是比较方便的开发模式，在很多框架中都有用到。 二是实现了所谓的”Universal Asynchronous API”。这也是Netty官方标榜的一个功能。用过OIO和NIO的都知道，这两套API风格相差极大，要从一个迁移到另一个成本是很大的。即使是NIO，异步和同步编程差距也很大。而Netty屏蔽了OIO和NIO的API差异，通过Channel提供对外接口，并通过ChannelPipeline将其连接起来，因此替换起来非常简单。 理清了ChannelPipeline的主流程，我们对Channel部分的大致结构算是弄清楚了。可是到了这里，我们依然对一个连接具体怎么处理没有什么概念，下篇文章，我们会分析一下，在Netty中，捷径如何处理连接的建立、数据的传输这些事情。 参考资料： Sink http://en.wikipedia.org/wiki/Sink_(computing))]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Netty</tag>
        <tag>Pipeline</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析2-Reactor]]></title>
    <url>%2F2019%2F05%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%902-Reactor%2F</url>
    <content type="text"><![CDATA[一：Netty、NIO、多线程？理清NIO与Netty的关系之前，我们必须先要来看看Reactor模式。Netty是一个典型的多线程的Reactor模式的使用，理解了这部分，在宏观上理解Netty的NIO及多线程部分就不会有什么困难了。 二：Reactor1、Reactor的由来Reactor是一种广泛应用在服务器端开发的设计模式。Reactor中文大多译为“反应堆”，我当初接触这个概念的时候，就感觉很厉害，是不是它的原理就跟“核反应”差不多？后来才知道其实没有什么关系，从Reactor的兄弟“Proactor”（多译为前摄器）就能看得出来，这两个词的中文翻译其实都不是太好，不够形象。实际上，Reactor模式又有别名“Dispatcher”或者“Notifier”，我觉得这两个都更加能表明它的本质。 那么，Reactor模式究竟是个什么东西呢？这要从事件驱动的开发方式说起。我们知道，对于应用服务器，一个主要规律就是，CPU的处理速度是要远远快于IO速度的，如果CPU为了IO操作（例如从Socket读取一段数据）而阻塞显然是不划算的。好一点的方法是分为多进程或者线程去进行处理，但是这样会带来一些进程切换的开销，试想一个进程一个数据读了500ms，期间进程切换到它3次，但是CPU却什么都不能干，就这么切换走了，是不是也不划算？ 这时先驱们找到了事件驱动，或者叫回调的方式，来完成这件事情。这种方式就是，应用业务向一个中间人注册一个回调（event handler），当IO就绪后，就这个中间人产生一个事件，并通知此handler进行处理。这种回调的方式，也体现了“好莱坞原则”（Hollywood principle）-“Don’t call us, we’ll call you”，在我们熟悉的IoC中也有用到。看来软件开发真是互通的！ 好了，我们现在来看Reactor模式。在前面事件驱动的例子里有个问题：我们如何知道IO就绪这个事件，谁来充当这个中间人？Reactor模式的答案是：由一个不断等待和循环的单独进程（线程）来做这件事，它接受所有handler的注册，并负责先操作系统查询IO是否就绪，在就绪后就调用指定handler进行处理，这个角色的名字就叫做Reactor。 2、Reactor与NIOJava中的NIO可以很好的和Reactor模式结合。关于NIO中的Reactor模式，我想没有什么资料能比Doug Lea大神（不知道Doug Lea？看看JDK集合包和并发包的作者吧）在《Scalable IO in Java》解释的更简洁和全面了。NIO中Reactor的核心是Selector，我写了一个简单的Reactor示例，这里我贴一个核心的Reactor的循环（这种循环结构又叫做EventLoop），剩余代码在learning-src目录下。 12345678910111213public void run() &#123; try &#123; while (!Thread.interrupted()) &#123; selector.select(); Set selected = selector.selectedKeys(); Iterator it = selected.iterator(); while (it.hasNext()) dispatch((SelectionKey) (it.next())); selected.clear(); &#125; &#125; catch (IOException ex) &#123; /* ... */ &#125;&#125; 3、与Reactor相关的其他概念前面提到了Proactor模式，这又是什么呢？简单来说，Reactor模式里，操作系统只负责通知IO就绪，具体的IO操作（例如读写）仍然是要在业务进程里阻塞的去做的，而Proactor模式则更进一步，由操作系统将IO操作执行好（例如读取，会将数据直接读到内存buffer中），而handler只负责处理自己的逻辑，真正做到了IO与程序处理异步执行。所以我们一般又说Reactor是同步IO，Proactor是异步IO。 关于阻塞和非阻塞、异步和非异步，以及UNIX底层的机制，大家可以看看这篇文章IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇），以及陶辉（《深入理解nginx》的作者）《高性能网络编程》的系列。 三：由Reactor出发来理解Netty1、多线程下的Reactor讲了一堆Reactor，我们回到Netty。在《Scalable IO in Java》中讲到了一种多线程下的Reactor模式。在这个模式里，mainReactor只有一个，负责响应client的连接请求，并建立连接，它使用一个NIO Selector；subReactor可以有一个或者多个，每个subReactor都会在一个独立线程中执行，并且维护一个独立的NIO Selector。 这样的好处很明显，因为subReactor也会执行一些比较耗时的IO操作，例如消息的读写，使用多个线程去执行，则更加有利于发挥CPU的运算能力，减少IO等待时间。 2、Netty中的Reactor与NIO好了，了解了多线程下的Reactor模式，我们来看看Netty吧（以下部分主要针对NIO，OIO部分更加简单一点，不重复介绍了）。Netty里对应mainReactor的角色叫做“Boss”，而对应subReactor的角色叫做”Worker”。Boss负责分配请求，Worker负责执行，好像也很贴切！以TCP的Server端为例，这两个对应的实现类分别为NioServerBoss和NioWorker（Server和Client的Worker没有区别，因为建立连接之后，双方就是对等的进行传输了）。 Netty 3.7中Reactor的EventLoop在AbstractNioSelector.run()中，它实现了Runnable接口。这个类是Netty NIO部分的核心。它的逻辑非常复杂，其中还包括一些对JDK Bug的处理（例如rebuildSelector），刚开始读的时候不需要深入那么细节。我精简了大部分代码，保留主干如下： 12345678910111213141516171819202122232425262728293031323334353637383940abstract class AbstractNioSelector implements NioSelector &#123; //NIO Selector protected volatile Selector selector; //内部任务队列 private final Queue&lt;Runnable&gt; taskQueue = new ConcurrentLinkedQueue&lt;Runnable&gt;(); //selector循环 public void run() &#123; for (;;) &#123; try &#123; //处理内部任务队列 processTaskQueue(); //处理selector事件对应逻辑 process(selector); &#125; catch (Throwable t) &#123; try &#123; Thread.sleep(1000); &#125; catch (InterruptedException e) &#123; // Ignore. &#125; &#125; &#125; &#125; private void processTaskQueue() &#123; for (;;) &#123; final Runnable task = taskQueue.poll(); if (task == null) &#123; break; &#125; task.run(); &#125; &#125; protected abstract void process(Selector selector) throws IOException;&#125; 其中process是主要的处理事件的逻辑，例如在AbstractNioWorker中，处理逻辑如下： 12345678910111213141516171819202122232425262728protected void process(Selector selector) throws IOException &#123; Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); if (selectedKeys.isEmpty()) &#123; return; &#125; for (Iterator&lt;SelectionKey&gt; i = selectedKeys.iterator(); i.hasNext();) &#123; SelectionKey k = i.next(); i.remove(); try &#123; int readyOps = k.readyOps(); if ((readyOps &amp; SelectionKey.OP_READ) != 0 || readyOps == 0) &#123; if (!read(k)) &#123; // Connection already closed - no need to handle write. continue; &#125; &#125; if ((readyOps &amp; SelectionKey.OP_WRITE) != 0) &#123; writeFromSelectorLoop(k); &#125; &#125; catch (CancelledKeyException e) &#123; close(k); &#125; if (cleanUpCancelledKeys()) &#123; break; // break the loop to avoid ConcurrentModificationException &#125; &#125;&#125; 这不就是第二部分提到的selector经典用法了么？ 在Netty 4.0之后，作者觉得NioSelector这个叫法，以及区分NioBoss和NioWorker的做法稍微繁琐了点，干脆就将这些合并成了NioEventLoop，从此这两个角色就不做区分了。我倒是觉得新版本的会更优雅一点。 3、Netty中的多线程下面我们来看Netty的多线程部分。一旦对应的Boss或者Worker启动，就会分配给它们一个线程去一直执行。对应的概念为BossPool和WorkerPool。对于每个NioServerSocketChannel，Boss的Reactor有一个线程，而Worker的线程数由Worker线程池大小决定，但是默认最大不会超过CPU核数*2，当然，这个参数可以通过NioServerSocketChannelFactory构造函数的参数来设置。 12345public NioServerSocketChannelFactory( Executor bossExecutor, Executor workerExecutor, int workerCount) &#123; this(bossExecutor, 1, workerExecutor, workerCount);&#125; 最后我们比较关心一个问题，我们之前ChannlePipeline中的ChannleHandler是在哪个线程执行的呢？答案是在Worker线程里执行的，并且会阻塞Worker的EventLoop。例如，在NioWorker中，读取消息完毕之后，会触发MessageReceived事件，这会使得Pipeline中的handler都得到执行。 12345678910protected boolean read(SelectionKey k) &#123; .... if (readBytes &gt; 0) &#123; // Fire the event. fireMessageReceived(channel, buffer); &#125; return true;&#125; 可以看到，对于处理事件较长的业务，并不太适合直接放到ChannelHandler中执行。那么怎么处理呢？我们在Handler部分会进行介绍。 参考资料： Scalable IO in Java http://gee.cs.oswego.edu/dl/cpjslides/nio.pdf Netty5.0架构剖析和源码解读 http://vdisk.weibo.com/s/C9LV9iVqH13rW/1391437855 Reactor pattern http://en.wikipedia.org/wiki/Reactor_pattern Reactor - An Object Behavioral Pattern for Demultiplexing and Dispatching Handles for Synchronous Events http://www.cs.wustl.edu/~schmidt/PDF/reactor-siemens.pdf 高性能网络编程6–reactor反应堆与定时器管理 http://blog.csdn.net/russell_tao/article/details/17452997 IO - 同步，异步，阻塞，非阻塞 （亡羊补牢篇）http://blog.csdn.net/historyasamirror/article/details/5778378 题图来自：http://www.worldindustrialreporter.com/france-gives-green-light-to-tokamak-fusion-reactor/]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Netty</tag>
        <tag>Reactor</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析1-Buffer]]></title>
    <url>%2F2019%2F05%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%901-Buffer%2F</url>
    <content type="text"><![CDATA[What：buffer简介buffer中文名又叫缓冲区，按照维基百科的解释，是”在数据传输时，在内存里开辟的一块临时保存数据的区域”。它其实是一种化同步为异步的机制，可以解决数据传输的速率不对等以及不稳定的问题。 根据这个定义，我们可以知道涉及I/O(特别是I/O写)的地方，基本会有Buffer了。就Java来说，我们非常熟悉的Old I/O–InputStream&amp;OutputStream系列API，基本都是在内部使用到了buffer。Java课程老师就教过，必须调用OutputStream.flush()，才能保证数据写入生效！ 而NIO中则直接将buffer这个概念封装成了对象，其中最常用的大概是ByteBuffer了。于是使用方式变为了：将数据写入Buffer，flip()一下，然后将数据读出来。于是，buffer的概念更加深入人心了！ Netty中的buffer也不例外。不同的是，Netty的buffer专为网络通讯而生，所以它又叫ChannelBuffer(好吧其实没有什么因果关系…)。我们下面就来讲讲Netty中得buffer。当然，关于Netty，我们必须讲讲它的所谓”Zero-Copy-Capable”机制。 TCP/IP协议与bufferTCP/IP协议是目前的主流网络协议。它是一个多层协议，最下层是物理层，最上层是应用层(HTTP协议等)，而做Java应用开发，一般只接触TCP以上，即传输层和应用层的内容。这也是Netty的主要应用场景。 TCP报文有个比较大的特点，就是它传输的时候，会先把应用层的数据项拆开成字节，然后按照自己的传输需要，选择合适数量的字节进行传输。什么叫”自己的传输需要”？首先TCP包有最大长度限制，那么太大的数据项肯定是要拆开的。其次因为TCP以及下层协议会附加一些协议头信息，如果数据项太小，那么可能报文大部分都是没有价值的头信息，这样传输是很不划算的。因此有了收集一定数量的小数据，并打包传输的Nagle算法(这个东东在HTTP协议里会很讨厌，Netty里可以用setOption(“tcpNoDelay”, true)关掉它)。 这么说可能太学院派了一点，我们举个例子吧： 发送时，我们这样分3次写入(‘|’表示两个buffer的分隔): +-----+-----+-----+ | ABC | DEF | GHI | +-----+-----+-----+ 接收时，可能变成了这样: +----+-------+---+---+ | AB | CDEFG | H | I | +----+-------+---+---+ 很好懂吧？可是，说了这么多，跟buffer有个什么关系呢？别急，我们来看下面一部分。 Buffer中的分层思想我们先回到之前的messageReceived方法： 123456public void messageReceived( ChannelHandlerContext ctx, MessageEvent e) &#123; // Send back the received message to the remote peer. transferredBytes.addAndGet(((ChannelBuffer) e.getMessage()).readableBytes()); e.getChannel().write(e.getMessage());&#125; 这里MessageEvent.getMessage()默认的返回值是一个ChannelBuffer。我们知道，业务中需要的”Message”，其实是一条应用层级别的完整消息，而一般的buffer工作在传输层，与”Message”是不能对应上的。那么这个ChannelBuffer是什么呢？ 来一个官方给的图，我想这个答案就很明显了： 这里可以看到，TCP层HTTP报文被分成了两个ChannelBuffer，这两个Buffer对我们上层的逻辑(HTTP处理)是没有意义的。但是两个ChannelBuffer被组合起来，就成为了一个有意义的HTTP报文，这个报文对应的ChannelBuffer，才是能称之为”Message”的东西。这里用到了一个词”Virtual Buffer”，也就是所谓的”Zero-Copy-Capable Byte Buffer”了。顿时觉得豁然开朗了有没有！ 我这里总结一下，如果说NIO的Buffer和Netty的ChannelBuffer最大的区别的话，就是前者仅仅是传输上的Buffer，而后者其实是传输Buffer和抽象后的逻辑Buffer的结合。延伸开来说，NIO仅仅是一个网络传输框架，而Netty是一个网络应用框架，包括网络以及应用的分层结构。 当然，在Netty里，默认使用ChannelBuffer表示”Message”，不失为一个比较实用的方法，但是MessageEvent.getMessage()是可以存放一个POJO的，这样子抽象程度又高了一些，这个我们在以后讲到ChannelPipeline的时候会说到。 Netty中的ChannelBuffer及实现好了，终于来到了代码实现部分。之所以啰嗦了这么多，因为我觉得，关于”Zero-Copy-Capable Rich Byte Buffer”，理解为什么需要它，比理解它是怎么实现的，可能要更重要一点。 我想可能很多朋友跟我一样，喜欢”顺藤摸瓜”式读代码–找到一个入口，然后顺着查看它的调用，直到理解清楚。很幸运，ChannelBuffers(注意有s!)就是这样一根”藤”，它是所有ChannelBuffer实现类的入口，它提供了很多静态的工具方法来创建不同的Buffer，靠“顺藤摸瓜”式读代码方式，大致能把各种ChannelBuffer的实现类摸个遍。先列一下ChannelBuffer相关类图。 此外还有WrappedChannelBuffer系列也是继承自AbstractChannelBuffer，图放到了后面。 ChannelBuffer中的readerIndex和writerIndex开始以为Netty的ChannelBuffer是对NIO ByteBuffer的一个封装，其实不是的，它是把ByteBuffer重新实现了一遍。 以最常用的HeapChannelBuffer为例，其底层也是一个byte[]，与ByteBuffer不同的是，它是可以同时进行读和写的，而不需要使用flip()进行读写切换。ChannelBuffer读写的核心代码在AbstactChannelBuffer里，这里通过readerIndex和writerIndex两个整数，分别指向当前读的位置和当前写的位置，并且，readerIndex总是小于writerIndex的。贴两段代码，让大家能看的更明白一点： 12345678910111213141516171819public void writeByte(int value) &#123; setByte(writerIndex ++, value);&#125;public byte readByte() &#123; if (readerIndex == writerIndex) &#123; throw new IndexOutOfBoundsException("Readable byte limit exceeded: " + readerIndex); &#125; return getByte(readerIndex ++);&#125;public int writableBytes() &#123; return capacity() - writerIndex;&#125;public int readableBytes() &#123; return writerIndex - readerIndex;&#125; 我倒是觉得这样的方式非常自然，比单指针与flip()要更加好理解一些。AbstactChannelBuffer还有两个相应的mark指针markedReaderIndex和markedWriterIndex，跟NIO的原理是一样的，这里不再赘述了。 字节序Endianness与HeapChannelBuffer在创建Buffer时，我们注意到了这样一个方法：public static ChannelBuffer buffer(ByteOrder endianness, int capacity);，其中ByteOrder是什么意思呢？ 这里有个很基础的概念：字节序(ByteOrder/Endianness)。它规定了多余一个字节的数字(int啊long什么的)，如何在内存中表示。BIG_ENDIAN(大端序)表示高位在前，整型数12会被存储为0 0 0 12四字节，而LITTLE_ENDIAN则正好相反。可能搞C/C++的程序员对这个会比较熟悉，而Javaer则比较陌生一点，因为Java已经把内存给管理好了。但是在网络编程方面，根据协议的不同，不同的字节序也可能会被用到。目前大部分协议还是采用大端序，可参考RFC1700。 了解了这些知识，我们也很容易就知道为什么会有BigEndianHeapChannelBuffer和LittleEndianHeapChannelBuffer了！ DynamicChannelBufferDynamicChannelBuffer是一个很方便的Buffer，之所以叫Dynamic是因为它的长度会根据内容的长度来扩充，你可以像使用ArrayList一样，无须关心其容量。实现自动扩容的核心在于ensureWritableBytes方法，算法很简单：在写入前做容量检查，容量不够时，新建一个容量x2的buffer，跟ArrayList的扩容是相同的。贴一段代码吧(为了代码易懂，这里我删掉了一些边界检查，只保留主逻辑)： 1234567891011121314151617181920public void writeByte(int value) &#123; ensureWritableBytes(1); super.writeByte(value);&#125;public void ensureWritableBytes(int minWritableBytes) &#123; if (minWritableBytes &lt;= writableBytes()) &#123; return; &#125; int newCapacity = capacity(); int minNewCapacity = writerIndex() + minWritableBytes; while (newCapacity &lt; minNewCapacity) &#123; newCapacity &lt;&lt;= 1; &#125; ChannelBuffer newBuffer = factory().getBuffer(order(), newCapacity); newBuffer.writeBytes(buffer, 0, writerIndex()); buffer = newBuffer;&#125; CompositeChannelBufferCompositeChannelBuffer是由多个ChannelBuffer组合而成的，可以看做一个整体进行读写。这里有一个技巧：CompositeChannelBuffer并不会开辟新的内存并直接复制所有ChannelBuffer内容，而是直接保存了所有ChannelBuffer的引用，并在子ChannelBuffer里进行读写，从而实现了”Zero-Copy-Capable”了。来段简略版的代码吧： 123456789101112131415161718192021public class CompositeChannelBuffer&#123; //components保存所有内部ChannelBuffer private ChannelBuffer[] components; //indices记录在整个CompositeChannelBuffer中，每个components的起始位置 private int[] indices; //缓存上一次读写的componentId private int lastAccessedComponentId; public byte getByte(int index) &#123; //通过indices中记录的位置索引到对应第几个子Buffer int componentId = componentId(index); return components[componentId].getByte(index - indices[componentId]); &#125; public void setByte(int index, int value) &#123; int componentId = componentId(index); components[componentId].setByte(index - indices[componentId], value); &#125;&#125; 查找componentId的算法再次不作介绍了，大家自己实现起来也不会太难。值得一提的是，基于ChannelBuffer连续读写的特性，使用了顺序查找(而不是二分查找)，并且用lastAccessedComponentId来进行缓存。 ByteBufferBackedChannelBuffer前面说ChannelBuffer是自己的实现的，其实只说对了一半。ByteBufferBackedChannelBuffer就是封装了NIO ByteBuffer的类，用于实现堆外内存的Buffer(使用NIO的DirectByteBuffer)。当然，其实它也可以放其他的ByteBuffer的实现类。代码实现就不说了，也没啥可说的。 WrappedChannelBuffer WrappedChannelBuffer都是几个对已有ChannelBuffer进行包装，完成特定功能的类。代码不贴了，实现都比较简单，列一下功能吧。 可以看到，关于实现方面，Netty 3.7的buffer相关内容还是比较简单的，也没有太多费脑细胞的地方。 而Netty 4.0之后就不同了。4.0，ChannelBuffer改名ByteBuf，成了单独项目buffer，并且为了性能优化，加入了BufferPool之类的机制，已经变得比较复杂了(本质倒没怎么变)。性能优化是个很复杂的事情，研究源码时，建议先避开这些东西，除非你对算法情有独钟。举个例子，Netty4.0里为了优化，将Map换成了Java 8里6000行的ConcurrentHashMapV8，你们感受一下… 参考资料： TCP/IP协议 http://zh.wikipedia.org/zh-cn/TCP/IP%E5%8D%8F%E8%AE%AE Data_buffer http://en.wikipedia.org/wiki/Data_buffer Endianness http://en.wikipedia.org/wiki/Endianness]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Buffer</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty源码解析-概述篇]]></title>
    <url>%2F2019%2F05%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90-%E6%A6%82%E8%BF%B0%E7%AF%87%2F</url>
    <content type="text"><![CDATA[概述Netty是什么大概用Netty的，无论新手还是老手，都知道它是一个“网络通讯框架”。所谓框架，基本上都是一个作用：基于底层API，提供更便捷的编程模型。那么”通讯框架”到底做了什么事情呢？回答这个问题并不太容易，我们不妨反过来看看，不使用netty，直接基于NIO编写网络程序，你需要做什么(以Server端TCP连接为例，这里我们使用Reactor模型)： 监听端口，建立Socket连接 建立线程，处理内容 读取Socket内容，并对协议进行解析 进行逻辑处理 回写响应内容 如果是多次交互的应用(SMTP、FTP)，则需要保持连接多进行几次交互 关闭连接 建立线程是一个比较耗时的操作，同时维护线程本身也有一些开销，所以我们会需要多线程机制，幸好JDK已经有很方便的多线程框架了，这里我们不需要花很多心思。 此外，因为TCP连接的特性，我们还要使用连接池来进行管理： 建立TCP连接是比较耗时的操作，对于频繁的通讯，保持连接效果更好 对于并发请求，可能需要建立多个连接 维护多个连接后，每次通讯，需要选择某一可用连接 连接超时和关闭机制 想想就觉得很复杂了！实际上，基于NIO直接实现这部分东西，即使是老手也容易出现错误，而使用Netty之后，你只需要关注逻辑处理部分就可以了。 体验Netty这里我们引用Netty的example包里的一个例子，一个简单的EchoServer，它接受客户端输入，并将输入原样返回。其主要代码如下： 1234567891011121314151617public void run() &#123; // Configure the server. ServerBootstrap bootstrap = new ServerBootstrap( new NioServerSocketChannelFactory( Executors.newCachedThreadPool(), Executors.newCachedThreadPool())); // Set up the pipeline factory. bootstrap.setPipelineFactory(new ChannelPipelineFactory() &#123; public ChannelPipeline getPipeline() throws Exception &#123; return Channels.pipeline(new EchoServerHandler()); &#125; &#125;); // Bind and start to accept incoming connections. bootstrap.bind(new InetSocketAddress(port));&#125; 这里EchoServerHandler是其业务逻辑的实现者，大致代码如下： 123456789public class EchoServerHandler extends SimpleChannelUpstreamHandler &#123; @Override public void messageReceived( ChannelHandlerContext ctx, MessageEvent e) &#123; // Send back the received message to the remote peer. e.getChannel().write(e.getMessage()); &#125;&#125; 还是挺简单的，不是吗？ Netty背后的事件驱动机制完成了以上一段代码，我们算是与Netty进行了第一次亲密接触。如果想深入学习呢？ 阅读源码是了解一个开源工具非常好的手段，但是Java世界的框架大多追求大而全，功能完备，如果逐个阅读，难免迷失方向，Netty也并不例外。相反，抓住几个重点对象，理解其领域概念及设计思想，从而理清其脉络，相当于打通了任督二脉，以后的阅读就不再困难了。 理解Netty的关键点在哪呢？我觉得，除了NIO的相关知识，另一个就是事件驱动的设计思想。什么叫事件驱动？我们回头看看EchoServerHandler的代码，其中的参数：public void messageReceived(ChannelHandlerContext ctx, MessageEvent e)，MessageEvent就是一个事件。这个事件携带了一些信息，例如这里e.getMessage()就是消息的内容，而EchoServerHandler则描述了处理这种事件的方式。一旦某个事件触发，相应的Handler则会被调用，并进行处理。这种事件机制在UI编程里广泛应用，而Netty则将其应用到了网络编程领域。 在Netty里，所有事件都来自ChannelEvent接口，这些事件涵盖监听端口、建立连接、读写数据等网络通讯的各个阶段。而事件的处理者就是ChannelHandler，这样，不但是业务逻辑，连网络通讯流程中底层的处理，都可以通过实现ChannelHandler来完成了。事实上，Netty内部的连接处理、协议编解码、超时等机制，都是通过handler完成的。当博主弄明白其中的奥妙时，不得不佩服这种设计！ 下图描述了Netty进行事件处理的流程。Channel是连接的通道，是ChannelEvent的产生者，而ChannelPipeline可以理解为ChannelHandler的集合。 开启Netty源码之门理解了Netty的事件驱动机制，我们现在可以来研究Netty的各个模块了。Netty的包结构如下： org └── jboss └── netty ├── bootstrap 配置并启动服务的类 ├── buffer 缓冲相关类，对NIO Buffer做了一些封装 ├── channel 核心部分，处理连接 ├── container 连接其他容器的代码 ├── example 使用示例 ├── handler 基于handler的扩展部分，实现协议编解码等附加功能 ├── logging 日志 └── util 工具类 在这里面，channel和handler两部分比较复杂。我们不妨与Netty官方的结构图对照一下，来了解其功能。 具体的解释可以看这里：http://netty.io/3.7/guide/#architecture。图中可以看到，除了之前说到的事件驱动机制之外，Netty的核心功能还包括两部分： Zero-Copy-Capable Rich Byte Buffer 零拷贝的Buffer。为什么叫零拷贝？因为在数据传输时，最终处理的数据会需要对单个传输层的报文，进行组合或者拆分。NIO原生的ByteBuffer无法做到这件事，而Netty通过提供Composite(组合)和Slice(切分)两种Buffer来实现零拷贝。这部分代码在org.jboss.netty.buffer包中。 这里需要额外注意，不要和操作系统级别的Zero-Copy混淆了, 操作系统中的零拷贝主要是用户空间和内核空间之间的数据拷贝, NIO中通过DirectBuffer做了实现. Universal Communication API 统一的通讯API。这个是针对Java的Old I/O和New I/O，使用了不同的API而言。Netty则提供了统一的API(org.jboss.netty.channel.Channel)来封装这两种I/O模型。这部分代码在org.jboss.netty.channel包中。 此外，Protocol Support功能通过handler机制实现。 接下来的文章，我们会根据模块，详细的对Netty源码进行分析。 参考资料： Netty 3.7 User Guide http://netty.io/3.7/guide/ What is Netty? http://ayedo.github.io/netty/2013/06/19/what-is-netty.html]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>概述</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Netty简单介绍]]></title>
    <url>%2F2019%2F05%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BNetty%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[1.BIO、NIO和AIO的区别？ BIO：一个连接一个线程，客户端有连接请求时服务器端就需要启动一个线程进行处理。线程开销大。 伪异步IO：将请求连接放入线程池，一对多，但线程还是很宝贵的资源。 NIO：一个请求一个线程，但客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。 AIO：一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理， BIO是面向流的，NIO是面向缓冲区的；BIO的各种流是阻塞的。而NIO是非阻塞的；BIO的Stream是单向的，而NIO的channel是双向的。 NIO的特点：事件驱动模型、单线程处理多任务、非阻塞I/O，I/O读写不再阻塞，而是返回0、基于block的传输比基于流的传输更高效、更高级的IO函数zero-copy、IO多路复用大大提高了Java网络应用的可伸缩性和实用性。基于Reactor线程模型。 在Reactor模式中，事件分发器等待某个事件或者可应用或个操作的状态发生，事件分发器就把这个事件传给事先注册的事件处理函数或者回调函数，由后者来做实际的读写操作。如在Reactor中实现读：注册读就绪事件和相应的事件处理器、事件分发器等待事件、事件到来，激活分发器，分发器调用事件对应的处理器、事件处理器完成实际的读操作，处理读到的数据，注册新的事件，然后返还控制权。 2.NIO的组成？Buffer：与Channel进行交互，数据是从Channel读入缓冲区，从缓冲区写入Channel中的 flip方法 ： 反转此缓冲区，将position给limit，然后将position置为0，其实就是切换读写模式clear方法 ：清除此缓冲区，将position置为0，把capacity的值给limit。rewind方法 ： 重绕此缓冲区，将position置为0DirectByteBuffer可减少一次系统空间到用户空间的拷贝。但Buffer创建和销毁的成本更高，不可控，通常会用内存池来提高性能。直接缓冲区主要分配给那些易受基础系统的本机I/O 操作影响的大型、持久的缓冲区。如果数据量比较小的中小应用情况下，可以考虑使用heapBuffer，由JVM进行管理。 Channel：表示 IO 源与目标打开的连接，是双向的，但不能直接访问数据，只能与Buffer 进行交互。通过源码可知，FileChannel的read方法和write方法都导致数据复制了两次！ Selector可使一个单独的线程管理多个Channel，open方法可创建Selector，register方法向多路复用器器注册通道，可以监听的事件类型：读、写、连接、accept。注册事件后会产生一个SelectionKey：它表示SelectableChannel 和Selector 之间的注册关系，wakeup方法：使尚未返回的第一个选择操作立即返回，唤醒的原因是：注册了新的channel或者事件；channel关闭，取消注册；优先级更高的事件触发（如定时器事件），希望及时处理。 Selector在Linux的实现类是EPollSelectorImpl，委托给EPollArrayWrapper实现，其中三个native方法是对epoll的封装，而EPollSelectorImpl. implRegister方法，通过调用epoll_ctl向epoll实例中注册事件，还将注册的文件描述符(fd)与SelectionKey的对应关系添加到fdToKey中，这个map维护了文件描述符与SelectionKey的映射。 fdToKey有时会变得非常大，因为注册到Selector上的Channel非常多（百万连接）；过期或失效的Channel没有及时关闭。fdToKey总是串行读取的，而读取是在select方法中进行的，该方法是非线程安全的。 Pipe：两个线程之间的单向数据连接，数据会被写到sink通道，从source通道读取 NIO的服务端建立过程：Selector.open()：打开一个Selector；ServerSocketChannel.open()：创建服务端的Channel；bind()：绑定到某个端口上。并配置非阻塞模式；register()：注册Channel和关注的事件到Selector上；select()轮询拿到已经就绪的事件 3.Netty的特点？一个高性能、异步事件驱动的NIO框架，它提供了对TCP、UDP和文件传输的支持使用更高效的socket底层，对epoll空轮询引起的cpu占用飙升在内部进行了处理，避免了直接使用NIO的陷阱，简化了NIO的处理方式。采用多种decoder/encoder 支持，对TCP粘包/分包进行自动化处理可使用接受/处理线程池，提高连接效率，对重连、心跳检测的简单支持可配置IO线程数、TCP参数， TCP接收和发送缓冲区使用直接内存代替堆内存，通过内存池的方式循环利用ByteBuf通过引用计数器及时申请释放不再引用的对象，降低了GC频率使用单线程串行化的方式，高效的Reactor线程模型大量使用了volitale、使用了CAS和原子类、线程安全类的使用、读写锁的使用 4.Netty的线程模型？Netty通过Reactor模型基于多路复用器接收并处理用户请求，内部实现了两个线程池，boss线程池和work线程池，其中boss线程池的线程负责处理请求的accept事件，当接收到accept事件的请求时，把对应的socket封装到一个NioSocketChannel中，并交给work线程池，其中work线程池负责请求的read和write事件，由对应的Handler处理。 单线程模型：所有I/O操作都由一个线程完成，即多路复用、事件分发和处理都是在一个Reactor线程上完成的。既要接收客户端的连接请求,向服务端发起连接，又要发送/读取请求或应答/响应消息。一个NIO 线程同时处理成百上千的链路，性能上无法支撑，速度慢，若线程进入死循环，整个程序不可用，对于高负载、大并发的应用场景不合适。 多线程模型：有一个NIO 线程（Acceptor） 只负责监听服务端，接收客户端的TCP 连接请求；NIO 线程池负责网络IO 的操作，即消息的读取、解码、编码和发送；1 个NIO 线程可以同时处理N 条链路，但是1 个链路只对应1 个NIO 线程，这是为了防止发生并发操作问题。但在并发百万客户端连接或需要安全认证时，一个Acceptor 线程可能会存在性能不足问题。 主从多线程模型：Acceptor 线程用于绑定监听端口，接收客户端连接，将SocketChannel 从主线程池的Reactor 线程的多路复用器上移除，重新注册到Sub 线程池的线程上，用于处理I/O 的读写等操作，从而保证mainReactor只负责接入认证、握手等操作； 5.TCP 粘包/拆包的原因及解决方法？TCP是以流的方式来处理数据，一个完整的包可能会被TCP拆分成多个包进行发送，也可能把小的封装成一个大的数据包发送。 TCP粘包/分包的原因： 应用程序写入的字节大小大于套接字发送缓冲区的大小，会发生拆包现象，而应用程序写入数据小于套接字缓冲区大小，网卡将应用多次写入的数据发送到网络上，这将会发生粘包现象； 进行MSS大小的TCP分段，当TCP报文长度-TCP头部长度&gt;MSS的时候将发生拆包 以太网帧的payload（净荷）大于MTU（1500字节）进行ip分片。 解决方法 消息定长：FixedLengthFrameDecoder类 包尾增加特殊字符分割：行分隔符类：LineBasedFrameDecoder或自定义分隔符类 ：DelimiterBasedFrameDecoder 将消息分为消息头和消息体：LengthFieldBasedFrameDecoder类。分为有头部的拆包与粘包、长度字段在前且有头部的拆包与粘包、多扩展头部的拆包与粘包。 6.了解哪几种序列化协议？ 序列化（编码）是将对象序列化为二进制形式（字节数组），主要用于网络传输、数据持久化等；而反序列化（解码）则是将从网络、磁盘等读取的字节数组还原成原始对象，主要用于网络传输对象的解码，以便完成远程调用。 影响序列化性能的关键因素：序列化后的码流大小（网络带宽的占用）、序列化的性能（CPU资源占用）；是否支持跨语言（异构系统的对接和开发语言切换）。 Java默认提供的序列化：无法跨语言、序列化后的码流太大、序列化的性能差 XML，优点：人机可读性好，可指定元素或特性的名称。缺点：序列化数据只包含数据本身以及类的结构，不包括类型标识和程序集信息；只能序列化公共属性和字段；不能序列化方法；文件庞大，文件格式复杂，传输占带宽。适用场景：当做配置文件存储数据，实时数据转换。 JSON，是一种轻量级的数据交换格式，优点：兼容性高、数据格式比较简单，易于读写、序列化后数据较小，可扩展性好，兼容性好、与XML相比，其协议比较简单，解析速度比较快。缺点：数据的描述性比XML差、不适合性能要求为ms级别的情况、额外空间开销比较大。适用场景（可替代ＸＭＬ）：跨防火墙访问、可调式性要求高、基于Web browser的Ajax请求、传输数据量相对小，实时性要求相对低（例如秒级别）的服务。 Fastjson，采用一种“假定有序快速匹配”的算法。优点：接口简单易用、目前java语言中最快的json库。缺点：过于注重快，而偏离了“标准”及功能性、代码质量不高，文档不全。适用场景：协议交互、Web输出、Android客户端 Thrift，不仅是序列化协议，还是一个RPC框架。优点：序列化后的体积小, 速度快、支持多种语言和丰富的数据类型、对于数据字段的增删具有较强的兼容性、支持二进制压缩编码。缺点：使用者较少、跨防火墙访问时，不安全、不具有可读性，调试代码时相对困难、不能与其他传输层协议共同使用（例如HTTP）、无法支持向持久层直接读写数据，即不适合做数据持久化序列化协议。适用场景：分布式系统的RPC解决方案 Avro，Hadoop的一个子项目，解决了JSON的冗长和没有IDL的问题。优点：支持丰富的数据类型、简单的动态语言结合功能、具有自我描述属性、提高了数据解析速度、快速可压缩的二进制数据形式、可以实现远程过程调用RPC、支持跨编程语言实现。缺点：对于习惯于静态类型语言的用户不直观。适用场景：在Hadoop中做Hive、Pig和MapReduce的持久化数据格式。 Protobuf，将数据结构以.proto文件进行描述，通过代码生成工具可以生成对应数据结构的POJO对象和Protobuf相关的方法和属性。优点：序列化后码流小，性能高、结构化数据存储格式（XML JSON等）、通过标识字段的顺序，可以实现协议的前向兼容、结构化的文档更容易管理和维护。缺点：需要依赖于工具生成代码、支持的语言相对较少，官方只支持Java 、C++ 、python。适用场景：对性能要求高的RPC调用、具有良好的跨防火墙的访问属性、适合应用层对象的持久化 其它 protostuff 基于protobuf协议，但不需要配置proto文件，直接导包即可 Jboss marshaling 可以直接序列化java类， 无须实java.io.Serializable接口 Message pack 一个高效的二进制序列化格式 Hessian 采用二进制协议的轻量级remoting onhttp工具 kryo 基于protobuf协议，只支持java语言,需要注册（Registration），然后序列化（Output），反序列化（Input） 7.如何选择序列化协议？ 具体场景 对于公司间的系统调用，如果性能要求在100ms以上的服务，基于XML的SOAP协议是一个值得考虑的方案。 基于Web browser的Ajax，以及Mobile app与服务端之间的通讯，JSON协议是首选。对于性能要求不太高，或者以动态类型语言为主，或者传输数据载荷很小的的运用场景，JSON也是非常不错的选择。 对于调试环境比较恶劣的场景，采用JSON或XML能够极大的提高调试效率，降低系统开发成本。 当对性能和简洁性有极高要求的场景，Protobuf，Thrift，Avro之间具有一定的竞争关系。 对于T级别的数据的持久化应用场景，Protobuf和Avro是首要选择。如果持久化后的数据存储在hadoop子项目里，Avro会是更好的选择。 对于持久层非Hadoop项目，以静态类型语言为主的应用场景，Protobuf会更符合静态类型语言工程师的开发习惯。由于Avro的设计理念偏向于动态类型语言，对于动态语言为主的应用场景，Avro是更好的选择。 如果需要提供一个完整的RPC解决方案，Thrift是一个好的选择。 如果序列化之后需要支持不同的传输层协议，或者需要跨防火墙访问的高性能场景，Protobuf可以优先考虑。 protobuf的数据类型有多种：bool、double、float、int32、int64、string、bytes、enum、message。protobuf的限定符：required: 必须赋值，不能为空、optional:字段可以赋值，也可以不赋值、repeated: 该字段可以重复任意次数（包括0次）、枚举；只能用指定的常量集中的一个值作为其值； protobuf的基本规则：每个消息中必须至少留有一个required类型的字段、包含0个或多个optional类型的字段；repeated表示的字段可以包含0个或多个数据；[1,15]之内的标识号在编码的时候会占用一个字节（常用），[16,2047]之内的标识号则占用2个字节，标识号一定不能重复、使用消息类型，也可以将消息嵌套任意多层，可用嵌套消息类型来代替组。 protobuf的消息升级原则：不要更改任何已有的字段的数值标识；不能移除已经存在的required字段，optional和repeated类型的字段可以被移除，但要保留标号不能被重用。新添加的字段必须是optional或repeated。因为旧版本程序无法读取或写入新增的required限定符的字段。 编译器为每一个消息类型生成了一个.java文件，以及一个特殊的Builder类（该类是用来创建消息类接口的）。如：UserProto.User.Builder builder = UserProto.User.newBuilder();builder.build()； Netty中的使用：ProtobufVarint32FrameDecoder 是用于处理半包消息的解码类；ProtobufDecoder(UserProto.User.getDefaultInstance())这是创建的UserProto.java文件中的解码类；ProtobufVarint32LengthFieldPrepender 对protobuf协议的消息头上加上一个长度为32的整形字段，用于标志这个消息的长度的类；ProtobufEncoder 是编码类 将StringBuilder转换为ByteBuf类型：copiedBuffer()方法 8.Netty的零拷贝实现 Netty的接收和发送ByteBuffer采用DIRECT BUFFERS，使用堆外直接内存进行Socket读写，不需要进行字节缓冲区的二次拷贝。堆内存多了一次内存拷贝，JVM会将堆内存Buffer拷贝一份到直接内存中，然后才写入Socket中。ByteBuffer由ChannelConfig分配，而ChannelConfig创建ByteBufAllocator默认使用Direct Buffer CompositeByteBuf 类可以将多个 ByteBuf 合并为一个逻辑上的 ByteBuf, 避免了传统通过内存拷贝的方式将几个小Buffer合并成一个大的Buffer。addComponents方法将 header 与 body 合并为一个逻辑上的 ByteBuf, 这两个 ByteBuf 在CompositeByteBuf 内部都是单独存在的, CompositeByteBuf 只是逻辑上是一个整体 通过 FileRegion 包装的FileChannel.tranferTo方法 实现文件传输, 可以直接将文件缓冲区的数据发送到目标 Channel，避免了传统通过循环write方式导致的内存拷贝问题。 通过 wrap方法, 我们可以将 byte[] 数组、ByteBuf、ByteBuffer等包装成一个 Netty ByteBuf 对象, 进而避免了拷贝操作。 Selector BUG：若Selector的轮询结果为空，也没有wakeup或新消息处理，则发生空轮询，CPU使用率100%， Netty的解决办法：对Selector的select操作周期进行统计，每完成一次空的select操作进行一次计数，若在某个周期内连续发生N次空轮询，则触发了epoll死循环bug。重建Selector，判断是否是其他线程发起的重建请求，若不是则将原SocketChannel从旧的Selector上去除注册，重新注册到新的Selector上，并将原来的Selector关闭。 9.Netty的高性能表现在哪些方面？ 心跳，对服务端：会定时清除闲置会话inactive(netty5)，对客户端:用来检测会话是否断开，是否重来，检测网络延迟，其中idleStateHandler类 用来检测会话状态 串行无锁化设计，即消息的处理尽可能在同一个线程内完成，期间不进行线程切换，这样就避免了多线程竞争和同步锁。表面上看，串行化设计似乎CPU利用率不高，并发程度不够。但是，通过调整NIO线程池的线程参数，可以同时启动多个串行化的线程并行运行，这种局部无锁化的串行线程设计相比一个队列-多个工作线程模型性能更优。 可靠性，链路有效性检测：链路空闲检测机制，读/写空闲超时机制；内存保护机制：通过内存池重用ByteBuf;ByteBuf的解码保护；优雅停机：不再接收新消息、退出前的预处理操作、资源的释放操作。 Netty安全性：支持的安全协议：SSL V2和V3，TLS，SSL单向认证、双向认证和第三方CA认证。 高效并发编程的体现：volatile的大量、正确使用；CAS和原子类的广泛使用；线程安全容器的使用；通过读写锁提升并发性能。IO通信性能三原则：传输（AIO）、协议（Http）、线程（主从多线程） 流量整型的作用（变压器）：防止由于上下游网元性能不均衡导致下游网元被压垮，业务流中断；防止由于通信模块接受消息过快，后端业务线程处理不及时导致撑死问题。 TCP参数配置：SO_RCVBUF和SO_SNDBUF：通常建议值为128K或者256K；SO_TCPNODELAY：NAGLE算法通过将缓冲区内的小封包自动相连，组成较大的封包，阻止大量小封包的发送阻塞网络，从而提高网络应用效率。但是对于时延敏感的应用场景需要关闭该优化算法； 10.NIOEventLoopGroup源码 NioEventLoopGroup(其实是MultithreadEventExecutorGroup) 内部维护一个类型为 EventExecutor children [], 默认大小是处理器核数 * 2, 这样就构成了一个线程池，初始化EventExecutor时NioEventLoopGroup重载newChild方法，所以children元素的实际类型为NioEventLoop。 线程启动时调用SingleThreadEventExecutor的构造方法，执行NioEventLoop类的run方法，首先会调用hasTasks()方法判断当前taskQueue是否有元素。如果taskQueue中有元素，执行 selectNow() 方法，最终执行selector.selectNow()，该方法会立即返回。如果taskQueue没有元素，执行 select(oldWakenUp) 方法 select ( oldWakenUp) 方法解决了 Nio 中的 bug，selectCnt 用来记录selector.select方法的执行次数和标识是否执行过selector.selectNow()，若触发了epoll的空轮询bug，则会反复执行selector.select(timeoutMillis)，变量selectCnt 会逐渐变大，当selectCnt 达到阈值（默认512），则执行rebuildSelector方法，进行selector重建，解决cpu占用100%的bug。 rebuildSelector方法先通过openSelector方法创建一个新的selector。然后将old selector的selectionKey执行cancel。最后将old selector的channel重新注册到新的selector中。rebuild后，需要重新执行方法selectNow，检查是否有已ready的selectionKey。 接下来调用processSelectedKeys 方法（处理I/O任务），当selectedKeys != null时，调用processSelectedKeysOptimized方法，迭代 selectedKeys 获取就绪的 IO 事件的selectkey存放在数组selectedKeys中, 然后为每个事件都调用 processSelectedKey 来处理它，processSelectedKey 中分别处理OP_READ；OP_WRITE；OP_CONNECT事件。 最后调用runAllTasks方法（非IO任务），该方法首先会调用fetchFromScheduledTaskQueue方法，把scheduledTaskQueue中已经超过延迟执行时间的任务移到taskQueue中等待被执行，然后依次从taskQueue中取任务执行，每执行64个任务，进行耗时检查，如果已执行时间超过预先设定的执行时间，则停止执行非IO任务，避免非IO任务太多，影响IO任务的执行。 每个NioEventLoop对应一个线程和一个Selector，NioServerSocketChannel会主动注册到某一个NioEventLoop的Selector上，NioEventLoop负责事件轮询。 Outbound 事件都是请求事件, 发起者是 Channel，处理者是 unsafe，通过 Outbound 事件进行通知，传播方向是 tail到head。Inbound 事件发起者是 unsafe，事件的处理者是 Channel, 是通知事件，传播方向是从头到尾。 内存管理机制，首先会预申请一大块内存Arena，Arena由许多Chunk组成，而每个Chunk默认由2048个page组成。Chunk通过AVL树的形式组织Page，每个叶子节点表示一个Page，而中间节点表示内存区域，节点自己记录它在整个Arena中的偏移地址。当区域被分配出去后，中间节点上的标记位会被标记，这样就表示这个中间节点以下的所有节点都已被分配了。大于8k的内存分配在poolChunkList中，而PoolSubpage用于分配小于8k的内存，它会把一个page分割成多段，进行内存分配。 ByteBuf的特点：支持自动扩容（4M），保证put方法不会抛出异常、通过内置的复合缓冲类型，实现零拷贝（zero-copy）；不需要调用flip()来切换读/写模式，读取和写入索引分开；方法链；引用计数基于AtomicIntegerFieldUpdater用于内存回收；PooledByteBuf采用二叉树来实现一个内存池，集中管理内存的分配和释放，不用每次使用都新建一个缓冲区对象。UnpooledHeapByteBuf每次都会新建一个缓冲区对象。]]></content>
      <categories>
        <category>Netty</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之手把手教你实现一个简单的RPC]]></title>
    <url>%2F2019%2F05%2F25%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E6%89%8B%E6%8A%8A%E6%89%8B%E6%95%99%E4%BD%A0%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84RPC%2F</url>
    <content type="text"><![CDATA[RPC的实现原理上面2讲我们已经讲过，RPC主要是为了解决的两个问题：解决分布式系统中，服务之间的调用问题。远程调用时，要能够像本地调用一样方便，让调用者感知不到远程调用的逻辑。还是以计算器Calculator为例，如果实现类CalculatorImpl是放在本地的，那么直接调用即可： 现在系统变成分布式了，CalculatorImpl和调用方不在同一个地址空间，那么就必须要进行远程过程调用： 那么如何实现远程过程调用，也就是RPC呢，一个完整的RPC流程，可以用下面这张图来描述： 其中左边的Client，对应的就是前面的Service A，而右边的Server，对应的则是Service B。下面一步一步详细解释一下。 Service A的应用层代码中，调用了Calculator的一个实现类的add方法，希望执行一个加法运算； 这个Calculator实现类，内部并不是直接实现计算器的加减乘除逻辑，而是通过远程调用Service B的RPC接口，来获取运算结果，因此称之为Stub； Stub怎么和Service B建立远程通讯呢？这时候就要用到远程通讯工具了，也就是图中的Run-time Library，这个工具将帮你实现远程通讯的功能，比如Java的Socket，就是这样一个库，当然，你也可以用基于Http协议的HttpClient，或者其他通讯工具类，都可以，RPC并没有规定说你要用何种协议进行通讯； Stub通过调用通讯工具提供的方法，和Service B建立起了通讯，然后将请求数据发给Service B。需要注意的是，由于底层的网络通讯是基于二进制格式的，因此这里Stub传给通讯工具类的数据也必须是二进制，比如calculator.add(1,2)，你必须把参数值1和2放到一个Request对象里头（这个Request对象当然不只这些信息，还包括要调用哪个服务的哪个RPC接口等其他信息），然后序列化为二进制，再传给通讯工具类，这一点也将在下面的代码实现中体现； 二进制的数据传到Service B这一边了，Service B当然也有自己的通讯工具，通过这个通讯工具接收二进制的请求； 既然数据是二进制的，那么自然要进行反序列化了，将二进制的数据反序列化为请求对象，然后将这个请求对象交给Service B的Stub处理； 和之前的Service A的Stub一样，这里的Stub也同样是个“假玩意”，它所负责的，只是去解析请求对象，知道调用方要调的是哪个RPC接口，传进来的参数又是什么，然后再把这些参数传给对应的RPC接口，也就是Calculator的实际实现类去执行。很明显，如果是Java，那这里肯定用到了反射。 RPC接口执行完毕，返回执行结果，现在轮到Service B要把数据发给Service A了，怎么发？一样的道理，一样的流程，只是现在Service B变成了Client，Service A变成了Server而已：Service B反序列化执行结果-&gt;传输给Service A-&gt;Service A反序列化执行结果 -&gt; 将结果返回给Application，完毕。 理论的讲完了，是时候把理论变成实践了。 把理论变成实践首先是Client端的应用层怎么发起RPC，ComsumerApp： 123456public class ComsumerApp &#123; public static void main(String[] args) &#123; Calculator calculator = new CalculatorRemoteImpl(); int result = calculator.add(1, 2); &#125;&#125; 通过一个CalculatorRemoteImpl，我们把RPC的逻辑封装进去了，客户端调用时感知不到远程调用的麻烦。下面再来看看CalculatorRemoteImpl，代码有些多，但是其实就是把上面的2、3、4几个步骤用代码实现了而已，CalculatorRemoteImpl： 123456789101112131415161718192021222324252627282930public class CalculatorRemoteImpl implements Calculator &#123; public int add(int a, int b) &#123; List&lt;String&gt; addressList = lookupProviders(&quot;Calculator.add&quot;); String address = chooseTarget(addressList); try &#123; Socket socket = new Socket(address, PORT); // 将请求序列化 CalculateRpcRequest calculateRpcRequest = generateRequest(a, b); ObjectOutputStream objectOutputStream = new ObjectOutputStream(socket.getOutputStream()); // 将请求发给服务提供方 objectOutputStream.writeObject(calculateRpcRequest); // 将响应体反序列化 ObjectInputStream objectInputStream = new ObjectInputStream(socket.getInputStream()); Object response = objectInputStream.readObject(); if (response instanceof Integer) &#123; return (Integer) response; &#125; else &#123; throw new InternalError(); &#125; &#125; catch (Exception e) &#123; log.error(&quot;fail&quot;, e); throw new InternalError(); &#125; &#125;&#125; add方法的前面两行，lookupProviders和chooseTarget，可能大家会觉得不明觉厉。 分布式应用下，一个服务可能有多个实例，比如Service B，可能有ip地址为198.168.1.11和198.168.1.13两个实例，lookupProviders，其实就是在寻找要调用的服务的实例列表。在分布式应用下，通常会有一个服务注册中心，来提供查询实例列表的功能。 查到实例列表之后要调用哪一个实例呢，只时候就需要chooseTarget了，其实内部就是一个负载均衡策略。 由于我们这里只是想实现一个简单的RPC，所以暂时不考虑服务注册中心和负载均衡，因此代码里写死了返回ip地址为127.0.0.1。 代码继续往下走，我们这里用到了Socket来进行远程通讯，同时利用ObjectOutputStream的writeObject和ObjectInputStream的readObject，来实现序列化和反序列化。 最后再来看看Server端的实现，和Client端非常类似，ProviderApp： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class ProviderApp &#123; private Calculator calculator = new CalculatorImpl(); public static void main(String[] args) throws IOException &#123; new ProviderApp().run(); &#125; private void run() throws IOException &#123; ServerSocket listener = new ServerSocket(9090); try &#123; while (true) &#123; Socket socket = listener.accept(); try &#123; // 将请求反序列化 ObjectInputStream objectInputStream = new ObjectInputStream(socket.getInputStream()); Object object = objectInputStream.readObject(); log.info(&quot;request is &#123;&#125;&quot;, object); // 调用服务 int result = 0; if (object instanceof CalculateRpcRequest) &#123; CalculateRpcRequest calculateRpcRequest = (CalculateRpcRequest) object; if (&quot;add&quot;.equals(calculateRpcRequest.getMethod())) &#123; result = calculator.add(calculateRpcRequest.getA(), calculateRpcRequest.getB()); &#125; else &#123; throw new UnsupportedOperationException(); &#125; &#125; // 返回结果 ObjectOutputStream objectOutputStream = new ObjectOutputStream(socket.getOutputStream()); objectOutputStream.writeObject(new Integer(result)); &#125; catch (Exception e) &#123; log.error(&quot;fail&quot;, e); &#125; finally &#123; socket.close(); &#125; &#125; &#125; finally &#123; listener.close(); &#125; &#125;&#125; Server端主要是通过ServerSocket的accept方法，来接收Client端的请求，接着就是反序列化请求-&gt;执行-&gt;序列化执行结果，最后将二进制格式的执行结果返回给Client。 就这样我们实现了一个简陋而又详细的RPC。 说它简陋，是因为这个实现确实比较挫，在下一小节会说它为什么挫。说它详细，是因为它一步一步的演示了一个RPC的执行流程，方便大家了解RPC的内部机制。 为什么说这个RPC实现很挫这个RPC实现只是为了给大家演示一下RPC的原理，要是想放到生产环境去用，那是绝对不行的。 1、缺乏通用性我通过给Calculator接口写了一个CalculatorRemoteImpl，来实现计算器的远程调用，下一次要是有别的接口需要远程调用，是不是又得再写对应的远程调用实现类？这肯定是很不方便的。 那该如何解决呢？先来看看使用Dubbo时是如何实现RPC调用的： 12345678@Referenceprivate Calculator calculator;...calculator.add(1,2);... Dubbo通过和Spring的集成，在Spring容器初始化的时候，如果扫描到对象加了@Reference注解，那么就给这个对象生成一个代理对象，这个代理对象会负责远程通讯，然后将代理对象放进容器中。所以代码运行期用到的calculator就是那个代理对象了。我们可以先不和Spring集成，也就是先不采用依赖注入，但是我们要做到像Dubbo一样，无需自己手动写代理对象，怎么做呢？那自然是要求所有的远程调用都遵循一套模板，把远程调用的信息放到一个RpcRequest对象里面，发给Server端，Server端解析之后就知道你要调用的是哪个RPC接口、以及入参是什么类型、入参的值又是什么，就像Dubbo的RpcInvocation： 12345678910111213public class RpcInvocation implements Invocation, Serializable &#123; private static final long serialVersionUID = -4355285085441097045L; private String methodName; private Class&lt;?&gt;[] parameterTypes; private Object[] arguments; private Map&lt;String, String&gt; attachments; private transient Invoker&lt;?&gt; invoker; 2、集成Spring在实现了代理对象通用化之后，下一步就可以考虑集成Spring的IOC功能了，通过Spring来创建代理对象，这一点就需要对Spring的bean初始化有一定掌握了。 3、长连接or短连接总不能每次要调用RPC接口时都去开启一个Socket建立连接吧？是不是可以保持若干个长连接，然后每次有rpc请求时，把请求放到任务队列中，然后由线程池去消费执行？只是一个思路，后续可以参考一下Dubbo是如何实现的。 4、 服务端线程池我们现在的Server端，是单线程的，每次都要等一个请求处理完，才能去accept另一个socket的连接，这样性能肯定很差，是不是可以通过一个线程池，来实现同时处理多个RPC请求？同样只是一个思路。 5、服务注册中心正如之前提到的，要调用服务，首先你需要一个服务注册中心，告诉你对方服务都有哪些实例。Dubbo的服务注册中心是可以配置的，官方推荐使用Zookeeper。如果使用Zookeeper的话，要怎样往上面注册实例，又要怎样获取实例，这些都是要实现的。 6、负载均衡如何从多个实例里挑选一个出来，进行调用，这就要用到负载均衡了。负载均衡的策略肯定不只一种，要怎样把策略做成可配置的？又要如何实现这些策略？同样可以参考Dubbo，Dubbo - 负载均衡 7、结果缓存每次调用查询接口时都要真的去Server端查询吗？是不是要考虑一下支持缓存？ 8、多版本控制服务端接口修改了，旧的接口怎么办？ 9、异步调用客户端调用完接口之后，不想等待服务端返回，想去干点别的事，可以支持不？ 10、优雅停机服务端要停机了，还没处理完的请求，怎么办？ 诸如此类的优化点还有很多，这也是为什么实现一个高性能高可用的RPC框架那么难的原因。当然，我们现在已经有很多很不错的RPC框架可以参考了，我们完全可以借鉴一下前人的智慧。]]></content>
      <categories>
        <category>RPC</category>
      </categories>
      <tags>
        <tag>实战</tag>
        <tag>大数据</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之RPC原理和框架]]></title>
    <url>%2F2019%2F05%2F23%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BRPC%E5%8E%9F%E7%90%86%E5%92%8C%E6%A1%86%E6%9E%B6%2F</url>
    <content type="text"><![CDATA[本文来自：csdn博客，作者在其中做了一些补充并添加了示例 Nelson 的论文中指出实现 RPC 的程序包括 5 个部分： User User-stub RPCRuntime Server-stub Server 这 5 个部分的关系如下图所示 这里 user 就是 client 端，当 user 想发起一个远程调用时，它实际是通过本地调用user-stub。user-stub 负责将调用的接口、方法和参数通过约定的协议规范进行编码并通过本地的 RPCRuntime 实例传输到远端的实例。远端 RPCRuntime 实例收到请求后交给 server-stub 进行解码后发起本地端调用，调用结果再返回给 user 端。 粗粒度的 RPC 实现概念结构，这里我们进一步细化它应该由哪些组件构成，如下图所示。 RPC 服务方通过 RpcServer 去导出（export）远程接口方法，而客户方通过 RpcClient 去引入（import）远程接口方法。客户方像调用本地方法一样去调用远程接口方法，RPC 框架提供接口的代理实现，实际的调用将委托给代理RpcProxy 。代理封装调用信息并将调用转交给RpcInvoker 去实际执行。在客户端的RpcInvoker 通过连接器RpcConnector 去维持与服务端的通道RpcChannel，并使用RpcProtocol 执行协议编码（encode）并将编码后的请求消息通过通道发送给服务方。 RPC 服务端接收器 RpcAcceptor 接收客户端的调用请求，同样使用RpcProtocol 执行协议解码（decode）。解码后的调用信息传递给RpcProcessor 去控制处理调用过程，最后再委托调用给RpcInvoker 去实际执行并返回调用结果。如下是各个部分的详细职责： 1. RpcServer 负责导出（export）远程接口 2. RpcClient 负责导入（import）远程接口的代理实现 3. RpcProxy 远程接口的代理实现 4. RpcInvoker 客户方实现：负责编码调用信息和发送调用请求到服务方并等待调用结果返回 服务方实现：负责调用服务端接口的具体实现并返回调用结果 5. RpcProtocol 负责协议编/解码 6. RpcConnector 负责维持客户方和服务方的连接通道和发送数据到服务方 7. RpcAcceptor 负责接收客户方请求并返回请求结果 8. RpcProcessor 负责在服务方控制调用过程，包括管理调用线程池、超时时间等 9. RpcChannel 数据传输通道 Java中常用的RPC框架目前常用的RPC框架如下： Thrift：thrift是一个软件框架，用来进行可扩展且跨语言的服务的开发。它结合了功能强大的软件堆栈和代码生成引擎，以构建在 C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js, Smalltalk, and OCaml 这些编程语言间无缝结合的、高效的服务。 Dubbo：Dubbo是一个分布式服务框架，以及SOA治理方案。其功能主要包括：高性能NIO通讯及多协议集成，服务动态寻址与路由，软负载均衡与容错，依赖分析与降级等。 Dubbo是阿里巴巴内部的SOA服务化治理方案的核心框架，Dubbo自2011年开源后，已被许多非阿里系公司使用。 Spring Cloud：Spring Cloud由众多子项目组成，如Spring Cloud Config、Spring Cloud Netflix、Spring Cloud Consul 等，提供了搭建分布式系统及微服务常用的工具，如配置管理、服务发现、断路器、智能路由、微代理、控制总线、一次性token、全局锁、选主、分布式会话和集群状态等，满足了构建微服务所需的所有解决方案。Spring Cloud基于Spring Boot, 使得开发部署极其简单。 RPC和消息队列的差异1. 功能差异在架构上，RPC和Message的差异点是，Message有一个中间结点Message Queue，可以把消息存储。消息的特点 Message Queue把请求的压力保存一下，逐渐释放出来，让处理者按照自己的节奏来处理。 Message Queue引入一下新的结点，系统的可靠性会受Message Queue结点的影响。 Message Queue是异步单向的消息。发送消息设计成是不需要等待消息处理的完成。所以对于有同步返回需求，用Message Queue则变得麻烦了。RPC的特点同步调用，对于要等待返回结果/处理结果的场景，RPC是可以非常自然直觉的使用方式(https://raw.githubusercontent.com/iCocos/icocos_hexo_images/master/2020/god_of_bd/branchs/RPC也可以是异步调用)。由于等待结果，Consumer（Client）会有线程消耗。如果以异步RPC的方式使用，Consumer（Client）线程消耗可以去掉。但不能做到像消息一样暂存消息/请求，压力会直接传导到服务Provider。2. 适用场合差异 希望同步得到结果的场合，RPC合适。 希望使用简单，则RPC；RPC操作基于接口，使用简单，使用方式模拟本地调用。异步的方式编程比较复杂。 不希望发送端（RPC Consumer、Message Sender）受限于处理端（RPC Provider、Message Receiver）的速度时，使用Message Queue。随着业务增长，有的处理端处理量会成为瓶颈，会进行同步调用到异步消息的改造。这样的改造实际上有调整业务的使用方式。比如原来一个操作页面提交后就下一个页面会看到处理结果；改造后异步消息后，下一个页面就会变成“操作已提交，完成后会得到通知”。3. 不适用场合说明 RPC同步调用使用Message Queue来传输调用信息。 上面分析可以知道，这样的做法，发送端是在等待，同时占用一个中间点的资源。变得复杂了，但没有对等的收益。 对于返回值是void的调用，可以这样做，因为实际上这个调用业务上往往不需要同步得到处理结果的，只要保证会处理即可。（RPC的方式可以保证调用返回即处理完成，使用消息方式后这一点不能保证了。） 返回值是void的调用，使用消息，效果上是把消息的使用方式Wrap成了服务调用（服务调用使用方式成简单，基于业务接口）。 RPC框架的核心技术点RPC框架实现的几个核心技术点： （1）服务暴露： 远程提供者需要以某种形式提供服务调用相关的信息，包括但不限于服务接口定义、数据结构、或者中间态的服务定义文件。例如Facebook的Thrift的IDL文件，Web service的WSDL文件；服务的调用者需要通过一定的途径获取远程服务调用相关的信息。 目前，大部分跨语言平台 RPC 框架采用根据 IDL 定义通过 code generator 去生成 stub 代码，这种方式下实际导入的过程就是通过代码生成器在编译期完成的。代码生成的方式对跨语言平台 RPC 框架而言是必然的选择，而对于同一语言平台的 RPC 则可以通过共享接口定义来实现。这里的导入方式本质也是一种代码生成技术，只不过是在运行时生成，比静态编译期的代码生成看起来更简洁些。 java 中还有一种比较特殊的调用就是多态，也就是一个接口可能有多个实现，那么远程调用时到底调用哪个？这个本地调用的语义是通过 jvm 提供的引用多态性隐式实现的，那么对于 RPC 来说跨进程的调用就没法隐式实现了。如果前面DemoService 接口有 2 个实现，那么在导出接口时就需要特殊标记不同的实现需要，那么远程调用时也需要传递该标记才能调用到正确的实现类，这样就解决了多态调用的语义问题。 （2）远程代理对象： 服务调用者用的服务实际是远程服务的本地代理。说白了就是通过动态代理来实现。 java 里至少提供了两种技术来提供动态代码生成，一种是 jdk 动态代理，另外一种是字节码生成。动态代理相比字节码生成使用起来更方便，但动态代理方式在性能上是要逊色于直接的字节码生成的，而字节码生成在代码可读性上要差很多。两者权衡起来，个人认为牺牲一些性能来获得代码可读性和可维护性显得更重要。 （3）通信： RPC框架与具体的协议无关。RPC 可基于 HTTP 或 TCP 协议，Web Service 就是基于 HTTP 协议的 RPC，它具有良好的跨平台性，但其性能却不如基于 TCP 协议的 RPC。 TCP/HTTP：众所周知，TCP 是传输层协议，HTTP 是应用层协议，而传输层较应用层更加底层，在数据传输方面，越底层越快，因此，在一般情况下，TCP 一定比 HTTP 快。 消息ID：RPC 的应用场景实质是一种可靠的请求应答消息流，和 HTTP 类似。因此选择长连接方式的 TCP 协议会更高效，与 HTTP 不同的是在协议层面我们定义了每个消息的唯一 id，因此可以更容易的复用连接。 IO方式：为了支持高并发，传统的阻塞式 IO 显然不太合适，因此我们需要异步的 IO，即 NIO。Java 提供了 NIO 的解决方案，Java 7 也提供了更优秀的 NIO.2 支持。 多连接：既然使用长连接，那么第一个问题是到底 client 和 server 之间需要多少根连接？实际上单连接和多连接在使用上没有区别，对于数据传输量较小的应用类型，单连接基本足够。单连接和多连接最大的区别在于，每根连接都有自己私有的发送和接收缓冲区，因此大数据量传输时分散在不同的连接缓冲区会得到更好的吞吐效率。所以，如果你的数据传输量不足以让单连接的缓冲区一直处于饱和状态的话，那么使用多连接并不会产生任何明显的提升，反而会增加连接管理的开销。 心跳：连接是由 client 端发起建立并维持。如果 client 和 server 之间是直连的，那么连接一般不会中断（当然物理链路故障除外）。如果 client 和 server 连接经过一些负载中转设备，有可能连接一段时间不活跃时会被这些中间设备中断。为了保持连接有必要定时为每个连接发送心跳数据以维持连接不中断。心跳消息是 RPC 框架库使用的内部消息，在前文协议头结构中也有一个专门的心跳位，就是用来标记心跳消息的，它对业务应用透明。 （4）序列化： 两方面会直接影响 RPC 的性能，一是传输方式，二是序列化。 序列化方式：毕竟是远程通信，需要将对象转化成二进制流进行传输。不同的RPC框架应用的场景不同，在序列化上也会采取不同的技术。 就序列化而言，Java 提供了默认的序列化方式，但在高并发的情况下，这种方式将会带来一些性能上的瓶颈，于是市面上出现了一系列优秀的序列化框架，比如：Protobuf、Kryo、Hessian、Jackson 等，它们可以取代 Java 默认的序列化，从而提供更高效的性能。 编码内容：出于效率考虑，编码的信息越少越好（传输数据少），编码的规则越简单越好（执行效率高）。如下是编码需具备的信息： 123456789101112131415-- 调用编码 -- 1. 接口方法 包括接口名、方法名 2. 方法参数 包括参数类型、参数值 3. 调用属性 包括调用属性信息，例如调用附件隐式参数、调用超时时间等 -- 返回编码 -- 1. 返回结果 接口方法中定义的返回值 2. 返回码 异常返回码 3. 返回异常信息 调用异常信息 除了以上这些必须的调用信息，我们可能还需要一些元信息以方便程序编解码以及未来可能的扩展。这样我们的编码消息里面就分成了两部分，一部分是元信息、另一部分是调用的必要信息。如果设计一种 RPC 协议消息的话，元信息我们把它放在协议消息头中，而必要信息放在协议消息体中。下面给出一种概念上的 RPC 协议消息设计格式： 123456789101112131415161718-- 消息头 -- magic : 协议魔数，为解码设计 header size: 协议头长度，为扩展设计 version : 协议版本，为兼容设计 st : 消息体序列化类型 hb : 心跳消息标记，为长连接传输层心跳设计 ow : 单向消息标记， rp : 响应消息标记，不置位默认是请求消息 status code: 响应消息状态码 reserved : 为字节对齐保留 message id : 消息 id body size : 消息体长度 -- 消息体 -- 采用序列化编码，常见有以下格式 xml : 如 webservie soap json : 如 JSON-RPC binary: 如 thrift; hession; kryo 等 RPC框架简易实现及其实例分析(1).服务端 服务端提供客户端所期待的服务，一般包括三个部分：服务接口，服务实现以及服务的注册暴露三部分，如下：服务接口 1234 public interface HelloService &#123; String hello(String name); String hi(String msg);&#125; 服务实现 1234567891011public class HelloServiceImpl implements HelloService&#123; @Override public String hello(String name) &#123; return &quot;Hello &quot; + name; &#125; @Override public String hi(String msg) &#123; return &quot;Hi, &quot; + msg; &#125;&#125; 服务暴露：只有把服务暴露出来，才能让客户端进行调用，这是RPC框架功能之一。 1234567public class RpcProvider &#123; public static void main(String[] args) throws Exception &#123; HelloService service = new HelloServiceImpl(); // RPC框架将服务暴露出来，供客户端消费 RpcFramework.export(service, 1234); &#125;&#125; (2).客户端 客户端消费服务端所提供的服务，一般包括两个部分：服务接口和服务引用两个部分，如下：服务接口：与服务端共享同一个服务接口 1234 public interface HelloService &#123; String hello(String name); String hi(String msg);&#125; 服务引用：消费端通过RPC框架进行远程调用，这也是RPC框架功能之一 123456789public class RpcConsumer &#123; public static void main(String[] args) throws Exception &#123; // 由RpcFramework生成的HelloService的代理 HelloService service = RpcFramework.refer(HelloService.class, &quot;127.0.0.1&quot;, 1234); String hello = service.hello(&quot;World&quot;); System.out.println(&quot;客户端收到远程调用的结果 ： &quot; + hello); &#125;&#125; (3).RPC框架原型实现 RPC框架主要包括两大功能：一个用于服务端暴露服务，一个用于客户端引用服务。服务端暴露服务 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576 /** * 暴露服务 * * @param service 服务实现 * @param port 服务端口 * @throws Exception */ public static void export(final Object service, int port) throws Exception &#123; if (service == null) &#123; throw new IllegalArgumentException(&quot;service instance == null&quot;); &#125; if (port &lt;= 0 || port &gt; 65535) &#123; throw new IllegalArgumentException(&quot;Invalid port &quot; + port); &#125; System.out.println(&quot;Export service &quot; + service.getClass().getName() + &quot; on port &quot; + port); // 建立Socket服务端 ServerSocket server = new ServerSocket(port); for (; ; ) &#123; try &#123; // 监听Socket请求 final Socket socket = server.accept(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; try &#123; /* 获取请求流，Server解析并获取请求*/ // 构建对象输入流，从源中读取对象到程序中 ObjectInputStream input = new ObjectInputStream( socket.getInputStream()); try &#123; System.out.println(&quot;\nServer解析请求 ： &quot;); String methodName = input.readUTF(); System.out.println(&quot;methodName : &quot; + methodName); // 泛型与数组是不兼容的，除了通配符作泛型参数以外 Class&lt;?&gt;[] parameterTypes = (Class&lt;?&gt;[])input.readObject(); System.out.println( &quot;parameterTypes : &quot; + Arrays.toString(parameterTypes)); Object[] arguments = (Object[])input.readObject(); System.out.println(&quot;arguments : &quot; + Arrays.toString(arguments)); /* Server 处理请求，进行响应*/ ObjectOutputStream output = new ObjectOutputStream( socket.getOutputStream()); try &#123; // service类型为Object的(可以发布任何服务)，故只能通过反射调用处理请求 // 反射调用，处理请求 Method method = service.getClass().getMethod(methodName, parameterTypes); Object result = method.invoke(service, arguments); System.out.println(&quot;\nServer 处理并生成响应 ：&quot;); System.out.println(&quot;result : &quot; + result); output.writeObject(result); &#125; catch (Throwable t) &#123; output.writeObject(t); &#125; finally &#123; output.close(); &#125; &#125; finally &#123; input.close(); &#125; &#125; finally &#123; socket.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; 从该RPC框架的简易实现来看，RPC服务端逻辑是：首先创建ServerSocket负责监听特定端口并接收客户连接请求，然后使用Java原生的序列化/反序列化机制来解析得到请求，包括所调用方法的名称、参数列表和实参，最后反射调用服务端对服务接口的具体实现并将得到的结果回传至客户端。至此，一次简单PRC调用的服务端流程执行完毕。客户端引用服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980 /** * 引用服务 * * @param &lt;T&gt; 接口泛型 * @param interfaceClass 接口类型 * @param host 服务器主机名 * @param port 服务器端口 * @return 远程服务，返回代理对象 * @throws Exception */ @SuppressWarnings(&quot;unchecked&quot;) public static &lt;T&gt; T refer(final Class&lt;T&gt; interfaceClass, final String host, final int port) throws Exception &#123; if (interfaceClass == null) &#123; throw new IllegalArgumentException(&quot;Interface class == null&quot;); &#125; // JDK 动态代理的约束，只能实现对接口的代理 if (!interfaceClass.isInterface()) &#123; throw new IllegalArgumentException( &quot;The &quot; + interfaceClass.getName() + &quot; must be interface class!&quot;); &#125; if (host == null || host.length() == 0) &#123; throw new IllegalArgumentException(&quot;Host == null!&quot;); &#125; if (port &lt;= 0 || port &gt; 65535) &#123; throw new IllegalArgumentException(&quot;Invalid port &quot; + port); &#125; System.out.println( &quot;Get remote service &quot; + interfaceClass.getName() + &quot; from server &quot; + host + &quot;:&quot; + port); // JDK 动态代理 T proxy = (T)Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class&lt;?&gt;[] &#123;interfaceClass&#125;, new InvocationHandler() &#123; // invoke方法本意是对目标方法的增强，在这里用于发送RPC请求和接收响应 @Override public Object invoke(Object proxy, Method method, Object[] arguments) throws Throwable &#123; // 创建Socket客户端，并与服务端建立链接 Socket socket = new Socket(host, port); try &#123; /* 客户端像服务端进行请求，并将请求参数写入流中*/ // 将对象写入到对象输出流，并将其发送到Socket流中去 ObjectOutputStream output = new ObjectOutputStream( socket.getOutputStream()); try &#123; // 发送请求 System.out.println(&quot;\nClient发送请求 ： &quot;); output.writeUTF(method.getName()); System.out.println(&quot;methodName : &quot; + method.getName()); output.writeObject(method.getParameterTypes()); System.out.println(&quot;parameterTypes : &quot; + Arrays.toString(method .getParameterTypes())); output.writeObject(arguments); System.out.println(&quot;arguments : &quot; + Arrays.toString(arguments)); /* 客户端读取并返回服务端的响应*/ ObjectInputStream input = new ObjectInputStream( socket.getInputStream()); try &#123; Object result = input.readObject(); if (result instanceof Throwable) &#123; throw (Throwable)result; &#125; System.out.println(&quot;\nClient收到响应 ： &quot;); System.out.println(&quot;result : &quot; + result); return result; &#125; finally &#123; input.close(); &#125; &#125; finally &#123; output.close(); &#125; &#125; finally &#123; socket.close(); &#125; &#125; &#125;); return proxy; &#125; 从该RPC框架的简易实现来看，RPC客户端逻辑是：首先创建Socket客户端并与服务端建立链接，然后使用Java原生的序列化/反序列化机制将调用请求发送给客户端，包括所调用方法的名称、参数列表将服务端的响应返回给用户即可。至此，一次简单PRC调用的客户端流程执行完毕。特别地，从代码实现来看，实现透明的PRC调用的关键就是 动态代理，这是RPC框架实现的灵魂所在。RPC原型实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160public class RpcFramework &#123; /** * 暴露服务 * * @param service 服务实现 * @param port 服务端口 * @throws Exception */ public static void export(final Object service, int port) throws Exception &#123; if (service == null) &#123; throw new IllegalArgumentException(&quot;service instance == null&quot;); &#125; if (port &lt;= 0 || port &gt; 65535) &#123; throw new IllegalArgumentException(&quot;Invalid port &quot; + port); &#125; System.out.println(&quot;Export service &quot; + service.getClass().getName() + &quot; on port &quot; + port); // 建立Socket服务端 ServerSocket server = new ServerSocket(port); for (; ; ) &#123; try &#123; // 监听Socket请求 final Socket socket = server.accept(); new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; try &#123; /* 获取请求流，Server解析并获取请求*/ // 构建对象输入流，从源中读取对象到程序中 ObjectInputStream input = new ObjectInputStream( socket.getInputStream()); try &#123; System.out.println(&quot;\nServer解析请求 ： &quot;); String methodName = input.readUTF(); System.out.println(&quot;methodName : &quot; + methodName); // 泛型与数组是不兼容的，除了通配符作泛型参数以外 Class&lt;?&gt;[] parameterTypes = (Class&lt;?&gt;[])input.readObject(); System.out.println( &quot;parameterTypes : &quot; + Arrays.toString(parameterTypes)); Object[] arguments = (Object[])input.readObject(); System.out.println(&quot;arguments : &quot; + Arrays.toString(arguments)); /* Server 处理请求，进行响应*/ ObjectOutputStream output = new ObjectOutputStream( socket.getOutputStream()); try &#123; // service类型为Object的(可以发布任何服务)，故只能通过反射调用处理请求 // 反射调用，处理请求 Method method = service.getClass().getMethod(methodName, parameterTypes); Object result = method.invoke(service, arguments); System.out.println(&quot;\nServer 处理并生成响应 ：&quot;); System.out.println(&quot;result : &quot; + result); output.writeObject(result); &#125; catch (Throwable t) &#123; output.writeObject(t); &#125; finally &#123; output.close(); &#125; &#125; finally &#123; input.close(); &#125; &#125; finally &#123; socket.close(); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;).start(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125; /** * 引用服务 * * @param &lt;T&gt; 接口泛型 * @param interfaceClass 接口类型 * @param host 服务器主机名 * @param port 服务器端口 * @return 远程服务，返回代理对象 * @throws Exception */ @SuppressWarnings(&quot;unchecked&quot;) public static &lt;T&gt; T refer(final Class&lt;T&gt; interfaceClass, final String host, final int port) throws Exception &#123; if (interfaceClass == null) &#123; throw new IllegalArgumentException(&quot;Interface class == null&quot;); &#125; // JDK 动态代理的约束，只能实现对接口的代理 if (!interfaceClass.isInterface()) &#123; throw new IllegalArgumentException( &quot;The &quot; + interfaceClass.getName() + &quot; must be interface class!&quot;); &#125; if (host == null || host.length() == 0) &#123; throw new IllegalArgumentException(&quot;Host == null!&quot;); &#125; if (port &lt;= 0 || port &gt; 65535) &#123; throw new IllegalArgumentException(&quot;Invalid port &quot; + port); &#125; System.out.println( &quot;Get remote service &quot; + interfaceClass.getName() + &quot; from server &quot; + host + &quot;:&quot; + port); // JDK 动态代理 T proxy = (T)Proxy.newProxyInstance(interfaceClass.getClassLoader(), new Class&lt;?&gt;[] &#123;interfaceClass&#125;, new InvocationHandler() &#123; // invoke方法本意是对目标方法的增强，在这里用于发送RPC请求和接收响应 @Override public Object invoke(Object proxy, Method method, Object[] arguments) throws Throwable &#123; // 创建Socket客户端，并与服务端建立链接 Socket socket = new Socket(host, port); try &#123; /* 客户端像服务端进行请求，并将请求参数写入流中*/ // 将对象写入到对象输出流，并将其发送到Socket流中去 ObjectOutputStream output = new ObjectOutputStream( socket.getOutputStream()); try &#123; // 发送请求 System.out.println(&quot;\nClient发送请求 ： &quot;); output.writeUTF(method.getName()); System.out.println(&quot;methodName : &quot; + method.getName()); output.writeObject(method.getParameterTypes()); System.out.println(&quot;parameterTypes : &quot; + Arrays.toString(method .getParameterTypes())); output.writeObject(arguments); System.out.println(&quot;arguments : &quot; + Arrays.toString(arguments)); /* 客户端读取并返回服务端的响应*/ ObjectInputStream input = new ObjectInputStream( socket.getInputStream()); try &#123; Object result = input.readObject(); if (result instanceof Throwable) &#123; throw (Throwable)result; &#125; System.out.println(&quot;\nClient收到响应 ： &quot;); System.out.println(&quot;result : &quot; + result); return result; &#125; finally &#123; input.close(); &#125; &#125; finally &#123; output.close(); &#125; &#125; finally &#123; socket.close(); &#125; &#125; &#125;); return proxy; &#125;&#125; 以上是简易RPC框架实现的简易完整代码。 关于RPC框架的若干问题说明(1).RPC框架如何做到透明化远程服务调用？ 如何封装通信细节才能让用户像以本地调用方式调用远程服务呢？就Java而言，动态代理恰是解决之道。Java动态代理有JDK动态代理和CGLIB动态代理两种方式。尽管字节码生成方式实现的代理更为强大和高效，但代码维护不易，因此RPC框架的大部分实现还是选择JDK动态代理的方式。在上面的例子中，RPCFramework实现中的invoke方法封装了与远端服务通信的细节，消费方首先从RPCFramework获得服务提供方的接口，当执行helloService.hi(“Panda”)方法时就会调用invoke方法。 (2).如何发布自己的服务？ 如何让别人使用我们的服务呢？难道就像我们上面的代码一样直接写死服务的IP以及端口就可以了吗？事实上，在实际生产实现中，使用人肉告知的方式是不现实的，因为实际生产中服务机器上/下线太频繁了。如果你发现一台机器提供服务不够，要再添加一台，这个时候就要告诉调用者我现在有两个IP了，你们要轮询调用来实现负载均衡；调用者咬咬牙改了，结果某天一台机器挂了，调用者发现服务有一半不可用，他又只能手动修改代码来删除挂掉那台机器的ip。这必然是相当痛苦的！ 有没有一种方法能实现自动告知，即机器的上线/下线对调用方透明，调用者不再需要写死服务提供方地址？当然可以，生产中的RPC框架都采用的是自动告知的方式，比如，阿里内部使用的RPC框架HSF是通过ConfigServer来完成这项任务的。此外，Zookeeper也被广泛用于实现服务自动注册与发现功能。不管具体采用何种技术，他们大都采用的都是 发布/订阅模式。 (3).序列化与反序列化 我们知道，Java对象是无法直接在网络中进行传输的。那么，我们的RPC请求如何发给服务端，客户端又如何接收来自服务端的响应呢？答案是，在传输Java对象时，首先对其进行序列化，然后在相应的终端进行反序列化还原对象以便进行处理。事实上，序列化/反序列化技术也有很多种，比如Java的原生序列化方式、JSON、阿里的Hessian和ProtoBuff序列化等，它们在效率上存在差异，但又有各自的特点。]]></content>
      <categories>
        <category>RPC</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>原理</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之RPC简单介绍]]></title>
    <url>%2F2019%2F05%2F23%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BRPC%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[RPC1. RPC是什么RPC（Remote Procedure Call Protocol）——远程过程调用协议，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。 RPC采用客户机/服务器模式。请求程序就是一个客户机，而服务提供程序就是一个服务器。首先，客户机调用进程发送一个有进程参数的调用信息到服务进程，然后等待应答信息。在服务器端，进程保持睡眠状态直到调用信息到达为止。当一个调用信息到达，服务器获得进程参数，计算结果，发送答复信息，然后等待下一个调用信息，最后，客户端调用进程接收答复信息，获得进程结果，然后调用执行继续进行。 2. 为什么要用RPC？其实这是应用开发到一定的阶段的强烈需求驱动的。 如果我们开发简单的单一应用，逻辑简单、用户不多、流量不大，那我们用不着； 当我们的系统访问量增大、业务增多时，我们会发现一台单机运行此系统已经无法承受。此时，我们可以将业务拆分成几个互不关联的应用，分别部署在各自机器上，以划清逻辑并减小压力。此时，我们也可以不需要RPC，因为应用之间是互不关联的。 当我们的业务越来越多、应用也越来越多时，自然的，我们会发现有些功能已经不能简单划分开来或者划分不出来。此时，可以将公共业务逻辑抽离出来，将之组成独立的服务Service应用 。而原有的、新增的应用都可以与那些独立的Service应用 交互，以此来完成完整的业务功能。所以此时，我们急需一种高效的应用程序之间的通讯手段来完成这种需求，所以你看，RPC大显身手的时候来了！ 其实3描述的场景也是服务化 、微服务 和分布式系统架构 的基础场景。即RPC框架就是实现以上结构的有力方式。]]></content>
      <categories>
        <category>RPC</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>RPC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之zookeeper开发实例]]></title>
    <url>%2F2019%2F05%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8Bzookeeper%E5%BC%80%E5%8F%91%E5%AE%9E%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[ZooKeeper中的组和成员我们可以把Zookeeper理解为一个高可用的文件系统。但是它没有文件和文件夹的概念，只有一个叫做znode的节点概念。那么znode即是数据的容器，也是其他节点的容器。（其实znode就可以理解为文件或者是文件夹）我们用父节点和子节点的关系来表示组和成员的关系。那么一个节点代表一个组，组节点下的子节点代表组内的成员.如下图所示： 创建组我们使用zookeeper的Java API来创建一个/zoo的组节点：1234567891011121314151617181920212223242526272829303132333435public class CreateGroup implements Watcher &#123; private static final int SESSION_TIMEOUT = 5000; private ZooKeeper zk; private CountDownLatch connectedSignal = new CountDownLatch(1); public void connect(String hosts) throws IOException, InterruptedException &#123; zk = new ZooKeeper(hosts, SESSION_TIMEOUT, this); connectedSignal.await(); &#125; @Override public void process(WatchedEvent event) &#123; // Watcher interface if (event.getState() == KeeperState.SyncConnected) &#123; connectedSignal.countDown(); &#125; &#125; public void create(String groupName) throws KeeperException, InterruptedException &#123; String path = &quot;/&quot; + groupName; String createdPath = zk.create(path, null/*data*/, Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); System.out.println(&quot;Created &quot; + createdPath); &#125; public void close() throws InterruptedException &#123; zk.close(); &#125; public static void main(String[] args) throws Exception &#123; CreateGroup createGroup = new CreateGroup(); createGroup.connect(args[0]); createGroup.create(args[1]); createGroup.close(); &#125; &#125; 当main()执行时，首先创建了一个CreateGroup的对象，然后调用connect()方法，通过zookeeper的API与zookeeper服务器连接。创建连接我们需要3个参数：一是服务器端主机名称以及端口号，二是客户端连接服务器session的超时时间，三是Watcher接口的一个实例。Watcher实例负责接收Zookeeper数据变化时产生的事件回调。 在连接函数中创建了zookeeper的实例，然后建立与服务器的连接。建立连接函数会立即返回，所以我们需要等待连接建立成功后再进行其他的操作。我们使用CountDownLatch来阻塞当前线程，直到zookeeper准备就绪。这时，我们就看到Watcher的作用了。我们实现了Watcher接口的一个方法：1public void process(WatchedEvent event); 当客户端连接上了zookeeper服务器，Watcher将由process()函数接收一个连接成功的事件。我们接下来调用CountDownLatch，释放之前的阻塞。 连接成功后，我们调用create()方法。我们在这个方法中调用zookeeper实例的create()方法来创建一个znode。参数包括：一是znode的path；二是znode的内容（一个二进制数组），三是一个access control list(ACL，访问控制列表，这里使用完全开放模式)，最后是znode的性质。 znode的性质分为ephemeral和persistent两种。ephemeral性质的znode在创建他的客户端的会话结束，或者客户端以其他原因断开与服务器的连接时，会被自动删除。而persistent性质的znode就不会被自动删除，除非客户端主动删除，而且不一定是创建它的客户端可以删除它，其他客户端也可以删除它。这里我们创建一个persistent的znode。create()将返回znode的path。我们讲新建znode的path打印出来。我们执行如上程序：1234% export CLASSPATH=ch21-zk/target/classes/:$ZOOKEEPER_HOME/*:\$ZOOKEEPER_HOME/lib/*:$ZOOKEEPER_HOME/conf% java CreateGroup localhost zooCreated /zoo 加入组接下来我们实现如何在一个组中注册成员。我们将使用ephemeral znode来创建这些成员节点。那么当客户端程序退出时，这些成员将被删除。我们创建一个ConnetionWatcher类，然后继承实现一个JoinGroup类： 123456789101112131415161718192021222324252627282930313233343536373839404142public class ConnectionWatcher implements Watcher &#123; private static final int SESSION_TIMEOUT = 5000; protected ZooKeeper zk; private CountDownLatch connectedSignal = new CountDownLatch(1); public void connect(String hosts) throws IOException, InterruptedException &#123; zk = new ZooKeeper(hosts, SESSION_TIMEOUT, this); connectedSignal.await(); &#125; @Override public void process(WatchedEvent event) &#123; if (event.getState() == KeeperState.SyncConnected) &#123; connectedSignal.countDown(); &#125; &#125; public void close() throws InterruptedException &#123; zk.close(); &#125;&#125;public class JoinGroup extends ConnectionWatcher &#123; public void join(String groupName, String memberName) throws KeeperException, InterruptedException &#123; String path = &quot;/&quot; + groupName + &quot;/&quot; + memberName; String createdPath = zk.create(path, null/*data*/, Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL); System.out.println(&quot;Created &quot; + createdPath); &#125; public static void main(String[] args) throws Exception &#123; JoinGroup joinGroup = new JoinGroup(); joinGroup.connect(args[0]); joinGroup.join(args[1], args[2]); // stay alive until process is killed or thread is interrupted Thread.sleep(Long.MAX_VALUE); &#125;&#125; 加入组与创建组非常相似。我们加入了一个ephemeral znode后，让线程阻塞住。然后我们可以使用命令行查看zookeeper中我们创建的znode。当我们将阻塞的程序强行关闭后，我们会发现我们创建的znode会自动消失。 成员列表下面我们实现一个程序来列出一个组中的所有成员。 123456789101112131415161718192021222324252627public class ListGroup extends ConnectionWatcher &#123; public void list(String groupName) throws KeeperException, InterruptedException &#123; String path = &quot;/&quot; + groupName; try &#123; List&lt;String&gt; children = zk.getChildren(path, false); if (children.isEmpty()) &#123; System.out.printf(&quot;No members in group %s\n&quot;, groupName); System.exit(1); &#125; for (String child : children) &#123; System.out.println(child); &#125; &#125; catch (KeeperException.NoNodeException e) &#123; System.out.printf(&quot;Group %s does not exist\n&quot;, groupName); System.exit(1); &#125; &#125; public static void main(String[] args) throws Exception &#123; ListGroup listGroup = new ListGroup(); listGroup.connect(args[0]); listGroup.list(args[1]); listGroup.close(); &#125;&#125; 我们在list()方法中通过调用getChildren()方法来获得某一个path下的子节点，然后打印出来。我们这里会试着捕获KeeperException.NoNodeException，当znode不存在时会抛出这个异常。我们运行程序，会看见如下结果，说明我们还没在zoo组中添加任何成员几点：12% java ListGroup localhost zooNo members in group zoo 我们可以运行之前的JoinGroup来添加成员。在后台运行一些JoinGroup程序，这些程序添加节点后都处于sleep状态：1234% java JoinGroup localhost zoo duck &amp;% java JoinGroup localhost zoo cow &amp;% java JoinGroup localhost zoo goat &amp;% goat_pid=$! 最后一行命令的作用是将最后一个启动的java程序的pid记录下来，我们好在列出zoo下面的成员后，将该进程kill掉。下面我们将zoo下的成员打印出来：12345678910% java ListGroup localhost zoogoatduckcow然后我们将kill掉最后启动的JoinGroup客户端：% kill $goat_pid过几秒后，我们发现goat节点不见了。因为之前我们创建的goat节点是一个ephemeral节点，而创建这个节点的客户端在ZooKeeper上的会话已经被终结了，因为这个回话在5秒后失效了（我们设置了会话的超时时间为5秒）：% java ListGroup localhost zooduckcow 让我们回过头来看看，我们到底都做了一些什么？我们首先创建了一个节点组，这些节点的创建者都在同一个分布式系统中。这些节点的创建者之间互相都不知情。一个创建者想使用这些节点数据进行一些工作，例如通过znode节点是否存在来判断节点的创建者是否存在。 最后一点，我们不能只依靠组成员关系来完全解决在与节点通信时的网络错误。当与一个集群组成员节点进行通信时，发生了通信失败，我们需要使用重试或者试验与组中其他的节点通信，来解决这次通信失败。 Zookeeper的命令行工具Zookeeper有一套命令行工具。我们可以像如下使用，来查找zoo下的成员节点： 12% zkCli.sh -server localhost ls /zoo[cow, duck] 你可以不加参数运行这个工具，来获得帮助。 删除分组下面让我们来看一下如何删除一个分组？ZooKeeper的API提供一个delete()方法来删除一个znode。我们通过输入znode的path和版本号（version number）来删除想要删除的znode。我们除了使用path来定位我们要删除的znode，还需要一个参数是版本号。只有当我们指定要删除的本版号，与znode当前的版本号一致时，ZooKeeper才允许我们将znode删除掉。这是一种optimistic locking机制，用来处理znode的读写冲突。我们也可以忽略版本号一致检查，做法就是版本号赋值为-1。删除一个znode之前，我们需要先删除它的子节点，就下如下代码中实现的那样： 12345678910111213141516171819202122232425public class DeleteGroup extends ConnectionWatcher &#123; public void delete(String groupName) throws KeeperException, InterruptedException &#123; String path = &quot;/&quot; + groupName; try &#123; List&lt;String&gt; children = zk.getChildren(path, false); for (String child : children) &#123; zk.delete(path + &quot;/&quot; + child, -1); &#125; zk.delete(path, -1); &#125; catch (KeeperException.NoNodeException e) &#123; System.out.printf(&quot;Group %s does not exist\n&quot;, groupName); System.exit(1); &#125; &#125; public static void main(String[] args) throws Exception &#123; DeleteGroup deleteGroup = new DeleteGroup(); deleteGroup.connect(args[0]); deleteGroup.delete(args[1]); deleteGroup.close(); &#125;&#125; 最后我们执行如下操作来删除zoo group： 123% java DeleteGroup localhost zoo% java ListGroup localhost zooGroup zoo does not exist]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>实战</tag>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之ZooKeeper应用程序]]></title>
    <url>%2F2019%2F05%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BZooKeeper%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[在对ZooKeeper有了一个深入的了解以后，我们来看一下用ZooKeeper可以实现哪些应用。 配置服务 Configuration Service一个基本的ZooKeeper实现的服务就是“配置服务”，集群中的服务器可以通过ZooKeeper共享一个通用的配置数据。从表面上，ZooKeeper可以理解为一个配置数据的高可用存储服务，为应用提供检索和更新配置数据服务。我们可以使用ZooKeeper的观察模式实现一个活动的配置服务，当配置数据发生变化时，可以通知与配置相关客户端。 接下来，我们来实现一个这样的活动配置服务。首先，我们设计用znode来存储key-value对，我们在znode中存储一个String类型的数据作为value，用znode的path来表示key。然后，我们实现一个client，这个client可以在任何时候对数据进行跟新操作。那么这个设计的ZooKeeper数据模型应该是：master来更新数据，其他的worker也随之将数据更新，就像HDFS的namenode那样。 我们在一个叫做ActiveKeyValueStore的类中编写代码如下： 123456789101112131415public class ActiveKeyValueStore extends ConnectionWatcher &#123; private static final Charset CHARSET = Charset.forName(&quot;UTF-8&quot;); public void write(String path, String value) throws InterruptedException, KeeperException &#123; Stat stat = zk.exists(path, false); if (stat == null) &#123; zk.create(path, value.getBytes(CHARSET), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; else &#123; zk.setData(path, value.getBytes(CHARSET), -1); &#125; &#125;&#125; write()方法主要实现将给定的key-value对写入到ZooKeeper中。这其中隐含了创建一个新的znode和更新一个已存在的znode的实现方法的不同。那么操作之前，我们需要根据exists()来判断znode是否存在，然后再根据情况进行相关的操作。其他值得一提的就是String类型的数据在转换成byte[]时，使用的字符集是UTF-8。 我们为了说明ActiveKeyValueStore怎么使用，我们考虑实现一个ConfigUpdater类来实现更新配置。下面代码实现了一个在一些随机时刻更新配置数据的应用。 1234567891011121314151617181920212223242526public class ConfigUpdater &#123; public static final String PATH = &quot;/config&quot;; private ActiveKeyValueStore store; private Random random = new Random(); public ConfigUpdater(String hosts) throws IOException, InterruptedException &#123; store = new ActiveKeyValueStore(); store.connect(hosts); &#125; public void run() throws InterruptedException, KeeperException &#123; while (true) &#123; String value = random.nextInt(100) + &quot;&quot;; store.write(PATH, value); System.out.printf(&quot;Set %s to %s\n&quot;, PATH, value); TimeUnit.SECONDS.sleep(random.nextInt(10)); &#125; &#125; public static void main(String[] args) throws Exception &#123; ConfigUpdater configUpdater = new ConfigUpdater(args[0]); configUpdater.run(); &#125;&#125; 上面的代码很简单。在ConfigUpdater的构造函数中，ActiveKeyValueStore对象连接到ZooKeeper服务。然后run()不断的循环运行，使用一个随机数不断的随机更新/configznode上的值。 下面我们来看一下，如何读取/config上的值。首先，我们在ActiveKeyValueStore中实现一个读方法。 12345public String read(String path, Watcher watcher) throws InterruptedException, KeeperException &#123; byte[] data = zk.getData(path, watcher, null/*stat*/); return new String(data, CHARSET); &#125; ZooKeeper的getData()方法的参数包含：path，一个Watcher对象和一个Stat对象。Stat对象中含有从getData()返回的值，并且负责接收回调信息。这种方式下，调用者不仅可以获得数据，还能够获得znode的metadata。 做为服务的consumer，ConfigWatcher以观察者身份，创建一个ActiveKeyValueStore对象，并且在启动以后调用read()函数（在dispalayConfig()函数中）获得相关数据。 下面的代码实现了一个以观察模式获得ZooKeeper中的数据更新的应用，并将值到后台中。 123456789101112131415161718192021222324252627282930313233343536public class ConfigWatcher implements Watcher &#123; private ActiveKeyValueStore store; public ConfigWatcher(String hosts) throws IOException, InterruptedException &#123; store = new ActiveKeyValueStore(); store.connect(hosts); &#125; public void displayConfig() throws InterruptedException, KeeperException &#123; String value = store.read(ConfigUpdater.PATH, this); System.out.printf(&quot;Read %s as %s\n&quot;, ConfigUpdater.PATH, value); &#125; @Override public void process(WatchedEvent event) &#123; if (event.getType() == EventType.NodeDataChanged) &#123; try &#123; displayConfig(); &#125; catch (InterruptedException e) &#123; System.err.println(&quot;Interrupted. Exiting.&quot;); Thread.currentThread().interrupt(); &#125; catch (KeeperException e) &#123; System.err.printf(&quot;KeeperException: %s. Exiting.\n&quot;, e); &#125; &#125; &#125; public static void main(String[] args) throws Exception &#123; ConfigWatcher configWatcher = new ConfigWatcher(args[0]); configWatcher.displayConfig(); // stay alive until process is killed or thread is interrupted Thread.sleep(Long.MAX_VALUE); &#125;&#125; 当ConfigUpadater更新znode时，ZooKeeper将触发一个EventType.NodeDataChanged的事件给观察者。ConfigWatcher将在他的process()函数中获得这个时间，并将显示读取到的最新的版本的配置数据。 由于观察模式的触发是一次性的，所以每次都要调用ActiveKeyValueStore的read()方法，这样才能获得未来的更新数据。我们不能确保一定能够接受到更新通知事件，因为在接受观察事件和下一次读取之间的窗口期内，znode可能被改变了（有可能很多次），但是client可能没有注册观察模式，所以client不会接到znode改变的通知。在配置服务中这不是一个什么问题，因为client只关心配置数据的最新版本。然而，建议读者关注一下这个潜在的问题。 让我们来看一下控制台打印的ConfigUpdater运行结果： 1234% java ConfigUpdater localhostSet /config to 79Set /config to 14Set /config to 78 然后立即在另外的控制台终端窗口中运行ConfigWatcher: 1234% java ConfigWatcher localhostRead /config as 79Read /config as 14Read /config as 78 坚韧的ZooKeeper应用分布式计算设计的第一谬误就是认为“网络是稳定的”。我们所实现的程序目前都是假设网络稳定的情况下实现的，所以当我们在一个真实的网络环境下，会有很多原因可以使程序执行失败。下面我们将阐述一些可能造成失败的场景，并且讲述如何正确的处理这些失败，让我们的程序在面对这些异常时更具韧性。在ZooKeeper的API中，每一个ZooKeeper的操作都会声明抛出两个异常：InterruptedException和KeeperException。 InterrupedException当一个操作被中断时，会抛出一个InterruptedException。在JAVA中有一个标准的阻塞机制用来取消程序的执行，就是在需要阻塞的的地方调用interrupt()。如果取消执行成功，会以抛出一个InterruptedException作为结果。ZooKeeper坚持了这个标准，所以我们可以用这种方式来取消client的对ZooKeeper的操作。用到ZooKeeper的类和库需要向上抛出InterruptedException，才能使我们的client实现取消操作。 InterruptedException并不意味着程序执行失败，可能是人为设计中断的，所以在上面配置应用的例子中，当向上抛出InterruptedException时，会引起应用终止。 KeeperException当ZooKeeper服务器出现错误信号，或者出现了通信方面的问题，就会抛出一个KeeperException。由于错误的不同原因，所以KeeperException有很多子类。例如，KeeperException.NoNodeException当操作一个znode时，而这个znode并不存在，就会抛出这个异常。 每一个之类都有一个异常码作为异常的类型。例如，KeeperException.NoNodeException的异常码就是KeeperException.Code.NONODE(一个枚举值)。有两种方法来处理KeeperException。一种是直接捕获KeeperException，然后根据异常码进行不同类型异常处理。另一种是捕获具体的子类，然后根据不同类型的异常进行处理。 KeeperException包含了3大类异常。 状态异常 State Exception当无法操作znode树造成操作失败时，会产生状态异常。通常引起状态异常的原因是有另外的程序在同时改变znode。例如，一个setData()操作时，会抛出KeeperException.BadVersionException。因为另外的一个程序已经在setData()操作之前修改了znode，造成setData()操作时版本号不匹配了。程序员必须了解，这种情况是很有可能发生的，我们必须靠编写处理这种异常的代码来解决他。 有的一些异常是编写代码时的疏忽造成的，例如KeeperException.NoChildrenForEphemeralsException。这个异常是当我们给一个enphemeral类型的znode添加子节点时抛出的。 重新获取异常 Recoverable Exception重新获取异常来至于那些能够获得同一个ZooKeeper session的应用。伴随的表现是抛出KeeperException.ConnectionLossException，表示与ZooKeeper的连接丢失。ZooKeeper将会尝试重新连接，大多数情况下重新连接都会成功并且能够保证session的完整性。 然而，ZooKeeper无法通知客户端操作由于KeeperException.ConnectionLossException而失败。这就是一个部分失败的例子。只能依靠程序员编写代码来处理这个不确定性。 在这点上，幂等操作和非幂等操作的差别就会变得非常有用了。一个幂等操作是指无论运行一次还是多次结果都是一样的，例如一个读请求，或者一个不设置任何值得setData操作。这些操作可以不断的重试。 一个非幂等操作不能被不分青红皂白的不停尝试执行，就像一些操作执行一次的效率和执行多次的效率是不同。我们将在之后会讨论如何利用非幂等操作来处理Recovreable Exception。 不能重新获取异常 Unrecoverable exceptions在一些情况下，ZooKeeper的session可能会变成不可用的——比如session过期，或者因为某些原因session被close掉（都会抛出KeeperException.SessionExpiredException），或者鉴权失败（KeeperException.AuthFailedException）。无论何种情况，ephemeral类型的znode上关联的session都会丢失，所以应用在重新连接到ZooKeeper之前都需要重新构建他的状态。 稳定的配置服务回过头来看一下ActiveKeyValueStore中的write()方法，其中调用了exists()方法来判断znode是否存在，然后决定是创建一个znode还是调用setData来更新数据。 12345678910public void write(String path, String value) throws InterruptedException, KeeperException &#123; Stat stat = zk.exists(path, false); if (stat == null) &#123; zk.create(path, value.getBytes(CHARSET), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; else &#123; zk.setData(path, value.getBytes(CHARSET), -1); &#125; &#125; 从整体上来看，write()方法是一个幂等方法，所以我们可以不断的尝试执行它。我们来修改一个新版本的write()方法，实现在循环中不断的尝试write操作。我们为尝试操作设置了一个最大尝试次数参数（MAX_RETRIES）和每次尝试间隔的休眠(RETRY_PERIOD_SECONDS)时长： 123456789101112131415161718192021222324public void write(String path, String value) throws InterruptedException, KeeperException &#123; int retries = 0; while (true) &#123; try &#123; Stat stat = zk.exists(path, false); if (stat == null) &#123; zk.create(path, value.getBytes(CHARSET), Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT); &#125; else &#123; zk.setData(path, value.getBytes(CHARSET), stat.getVersion()); &#125; return; &#125; catch (KeeperException.SessionExpiredException e) &#123; throw e; &#125; catch (KeeperException e) &#123; if (retries++ == MAX_RETRIES) &#123; throw e; &#125; // sleep then retry TimeUnit.SECONDS.sleep(RETRY_PERIOD_SECONDS); &#125; &#125; &#125; 细心的读者可能会发现我们并没有在捕获KeeperException.SessionExpiredException时继续重新尝试操作，这是因为当session过期后，ZooKeeper会变为CLOSED状态，就不能再重新连接了。我们只是简单的抛出一个异常，通知调用者去创建一个新的ZooKeeper实例，所以write()方法可以不断的尝试执行。一个简单的方式来创建一个ZooKeeper实例就是重新new一个ConfigUpdater实例。 123456789101112131415public static void main(String[] args) throws Exception &#123; while (true) &#123; try &#123; ResilientConfigUpdater configUpdater = new ResilientConfigUpdater(args[0]); configUpdater.run(); &#125; catch (KeeperException.SessionExpiredException e) &#123; // start a new session &#125; catch (KeeperException e) &#123; // already retried, so exit e.printStackTrace(); break; &#125; &#125; &#125; 另一个可以替代处理session过期的方法就是使用watcher来监控Expired的KeeperState，然后重新建立一个连接。这种方法下，我们只需要不断的尝试执行write()，如果我们得到了KeeperException.SessionExpiredException异常，连接最终也会被重新建立起来。那么我们抛开如何从一个过期的session中恢复问题，我们的重点是连接丢失的问题也可以这样解决，只是处理方法不同而已。 我们这里忽略了另外一种情况，在zookeeper实例不断的尝试连接了ensemble中的所有节点后发现都无法连接成功，就会抛出一个IOException，说明所有的集群节点都不可用。而有一些应用被设计为不断的尝试连接，直到ZooKeeper服务恢复可用为止。 锁服务分布式锁用来为一组程序提供互斥机制。任意一个时刻仅有一个进程能够获得锁。分布式锁可以用来实现大型分布式系统的leader选举算法，即leader就是获取到锁的那个进程。 不要把ZooKeeper的原生leader选举算法和我们这里所说的通用leader选举服务搞混淆了。ZooKeeper的原生leader选举算法并不是公开的算法，并不能向我们这里所说的通用leader选举服务那样，为一个分布式系统提供主进程选举服务。 为了使用ZooKeeper实现分布式锁，我们使用可排序的znode来实现进程对锁的竞争。思路其实很简单：首先，我们需要一个表示锁的znode，获得锁的进程就表示被这把锁给锁定了（命名为，/leader）。然后，client为了获得锁，就需要在锁的znode下创建ephemeral类型的子znode。在任何时间点上，只有排序序号最小的znode的client获得锁，即被锁定。例如，如果两个client同时创建znode /leader/lock-1和/leader/lock-2，所以创建/leader/lock-1的client获得锁，因为他的排序序号最小。ZooKeeper服务被看作是排序的权威管理者，因为是由他来安排排序的序号的。锁可能因为删除了/leader/lock-1znode而被简单的释放。另外，如果相应的客户端死掉，使用ephemeral znode的价值就在这里，znode可以被自动删除掉。创建/leader/lock-2的client就获得了锁，因为他的序号现在最小。当然客户端需要启动观察模式，在znode被删除时才能获得通知：此时他已经获得了锁。获得锁的伪代码如下： 在lock的znode下创建名字为lock-的ephemeral类型znode，并记录下创建的znode的path（会在创建函数中返回）。 获取lock znode的子节点列表，并开启对lock的子节点的watch模式。 如果创建的子节点的序号最小，则再执行一次第2步，那么就表示已经获得锁了。退出。 等待第2步的观察模式的通知，如果获得通知，则再执行第2步。 羊群效应虽然这个算法是正确的，但是还是有一些问题。第一个问题是羊群效应。试想一下，当有成千成百的client正在试图获得锁。每一个client都对lock节点开启了观察模式，等待lock的子节点的变化通知。每次锁的释放和获取，观察模式将被触发，每个client都会得到消息。那么羊群效应就是指像这样，大量的client都会获得相同的事件通知，而只有很小的一部分client会对事件通知有响应。我们这里，只有一个client将获得锁，但是所有的client都得到了通知。那么这就像在网络公路上撒了把钉子，增加了ZooKeeper服务器的压力。 为了避免羊群效应，通知的范围需要更精准。我们通过观察发现，只有当序号排在当前znode之前一个znode离开时，才有必要通知创建当前znode的client，而不必在任意一个znode删除或者创建时都通知client。在我们的例子中，如果client1、client2和client3创建了znode/leader/lock-1、/leader/lock-2和leader/lock-3，client3仅在/leader/lock-2消失时，才获得通知。而不需要在/leader/lock-1消失时，或者新建/leader/lock-4时，获得通知。 重新获取异常 Recoverable Exception这个锁算法的另一个问题是没有处理当连接中断造成的创建失败。在这种情况下，我们根本就不知道之前的创建是否成功了。创建一个可排序的znode是一个非等幂操作，所以我们不能简单重试，因为如果第一次我们创建成功了，那么第一次创建的znode就成了一个孤立的znode了，将永远不会被删除直到会话结束。 那么问题的关键在于，在重新连接以后，client不能确定是否之前创建过lock节点的子节点。我们在znode的名字中间嵌入一个client的ID，那么在重新连接后，就可以通过检查lock znode的子节点znode中是否有名字包含client ID的节点。如果有这样的节点，说明之前创建节点操作成功了，就不需要再创建了。如果没有这样的节点，那就重新创建一个。 Client的会话ID是一个长整型数据，并且在ZooKeeper中是唯一的。我们可以使用会话的ID在处理连接丢失事件过程中作为client的id。在ZooKeeper的JAVA API中，我们可以调用getSessionId()方法来获得会话的ID。 那么Ephemeral类型的可排序znode不要命名为lock--，所以当加上序号后就变成了lock--。那么序号虽然针对上一级名字是唯一的，但是上一级名字本身就是唯一的，所以这个方法既可以标记znode的创建者，也可以实现创建的顺序排序。 不能恢复异常 Unrecoverable Exception如果client的会话过期，那么他创建的ephemeral znode将被删除，client将立即失去锁（或者至少放弃获得锁的机会）。应用需要意识到他不再拥有锁，然后清理一切状态，重新创建一个锁对象，并尝试再次获得锁。注意，应用必须在得到通知的第一时间进行处理，因为应用不知道如何在znode被删除事后判断是否需要清理他的状态。 实现 Implementation考虑到所有的失败模式的处理的繁琐，所以实现一个正确的分布式锁是需要做很多细微的设计工作。好在ZooKeeper为我们提供了一个 产品级质量保证的锁的实现，我们叫做WriteLock。我们可以轻松的在client中应用。 更多的分布式数据结构和协议我们可以用ZooKeeper来构建很多分布式数据结构和协议，例如，barriers，queues和two-phase commit。有趣的是我们注意到这些都是同步协议，而我们却使用ZooKeeper的原生异步特征（比如通知机制）来构建他们。在ZooKeeper官网上提供了一些数据结构和协议的伪代码。并且提供了实现这些的数据结构和协议的标准教程（包括locks、leader选举和队列）；你可以在recipes目录中找到。 Apache Curator project也提供了一些简单客户端的教程。]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
        <tag>应用程序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之zookeeper服务]]></title>
    <url>%2F2019%2F05%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8Bzookeeper%E6%9C%8D%E5%8A%A1%2F</url>
    <content type="text"><![CDATA[Zookeeper 服务ZooKeeper 是一个高可用的高性能调度服务。这一节我们将讲述他的模型、操作和接口。 数据模型 Data ModelZooKeeper包含一个树形的数据模型，我们叫做znode。一个znode中包含了存储的数据和ACL（Access Control List）。ZooKeeper的设计适合存储少量的数据，并不适合存储大量数据，所以znode的存储限制最大不超过1M。 数据的访问被定义成原子性的。什么是原子性呢？一个客户端访问一个znode时，不会只得到一部分数据；客户端访问数据要么获得全部数据，要么读取失败，什么也得不到。相似的，写操作时，要么写入全部数据，要么写入失败，什么也写不进去。ZooKeeper能够保证写操作只有两个结果，成功和失败。绝对不会出现只写入了一部分数据的情况。与HDFS不同，ZooKeeper不支持字符的append（连接）操作。原因是HDFS是被设计成支持数据流访问（streaming data access）的大数据存储，而ZooKeeper则不是。 我们可以通过path来定位znode，就像Unix系统定位文件一样，使用斜杠来表示路径。但是，znode的路径只能使用绝对路径，而不能想Unix系统一样使用相对路径，即Zookeeper不能识别../和./这样的路径。 节点的名称是由Unicode字符组成的，除了zookeeper这个字符串，我们可以任意命名节点。为什么不能使用zookeeper命名节点呢？因为ZooKeeper已经默认使用zookeeper来命名了一个根节点，用来存储一些管理数据。 请注意，这里的path并不是URIs，在Java API中是一个String类型的变量。 Ephemeral znodes我们已经知道，znode有两种类型：ephemeral和persistent。在创建znode时，我们指定znode的类型，并且在之后不会再被修改。当创建znode的客户端的session结束后，ephemeral类型的znode将被删除。persistent类型的znode在创建以后，就与客户端没什么联系了，除非主动去删除它，否则他会一直存在。Ephemeral znode没有任何子节点。 虽然Ephemeral znode绑定了客户端session，但是对任何其他客户端都是可见的，当然是在他们的ACL策略下允许访问的情况下。当我们在创建分布式系统时，需要知道分布式资源是否可用。Ephemeral znode就是为这种场景应运而生的。正如我们之前讲述的例子中，使用Ephemeral znode来实现一个成员关系管理，任何一个客户端进程任何时候都可以知道其他成员是否可用。Znode的序号 如果在创建znode时，我们使用排序标志的话，ZooKeeper会在我们指定的znode名字后面增加一个数字。我们继续加入相同名字的znode时，这个数字会不断增加。这个序号的计数器是由这些排序znode的父节点来维护的。 如果我们请求创建一个znode，指定命名为/a/b-，那么ZooKeeper会为我们创建一个名字为/a/b-3的znode。我们再请求创建一个名字为/a/b-的znode，ZooKeeper会为我们创建一个名字/a/b-5的znode。ZooKeeper给我们指定的序号是不断增长的。Java API中的create()的返回结果就是znode的实际名字。 那么序号用来干什么呢？当然是用来排序用的！后面《A Lock Service》中我们将讲述如何使用znode的序号来构建一个share lock。 观察模式 Watches观察模式可以使客户端在某一个znode发生变化时得到通知。观察模式有ZooKeeper服务的某些操作启动，并由其他的一些操作来触发。例如，一个客户端对一个znode进行了exists操作，来判断目标znode是否存在，同时在znode上开启了观察模式。如果znode不存在，这exists将返回false。如果稍后，另外一个客户端创建了这个znode，观察模式将被触发，将znode的创建事件通知之前开启观察模式的客户端。我们将在以后详细介绍其他的操作和触发。 观察模式只能被触发一次。如果要一直获得znode的创建和删除的通知，那么就需要不断的在znode上开启观察模式。在上面的例子中，如果客户端还继续需要获得znode被删除的通知，那么在获得创建通知后，客户端还需要继续对这个znode进行exists操作，再开启一次观察模式。 在《A Configuration Service》中，有一个例子将讲述如何使用观察模式在集群中更新配置。 操作 Operations下面的表格中列出了9种ZooKeeper的操作。 操作 说明create Creates a znode (the parent znode must already exist)delete Deletes a znode (the znode must not have any children)exists Tests whether a znode exists and retrieves its metadatagetACL, setACL Gets/sets the ACL for a znodegetChildren Gets a list of the children of a znodegetData,setData Gets/sets the data associated with a znodesync Synchronizes a client’s view of a znode with ZooKeeper 调用delete和setData操作时，我们必须指定一个znode版本号（version number），即我们必须指定我们要删除或者更新znode数据的哪个版本。如果版本号不匹配，操作将会失败。失败的原因可能是在我们提交之前，该znode已经被修改过了，版本号发生了增量变化。那么我们该怎么办呢？我可以考虑重试，或者调用其他的操作。例如，我们提交更新失败后，可以重新获取znode当前的数据，看看当前的版本号是什么，再做更新操作。 ZooKeeper虽然可以被看作是一个文件系统，但是由于ZooKeeper文件很小，所以没有提供像一般文件系统所提供的open、close或者seek操作。 注意这里的sync操作与POSIX文件系统的fsync()操作是不同的。就像我们早前讲过的，ZooKeeper的写操作是原子性的，一个成功的写操作只保证数据被持久化到大多数ZooKeeper的服务器存储上。所以读操作可能会读取不到最新状态的数据，sync操作用来让client强制所访问的ZooKeeper服务器上的数据状态更新到最新状态。我们会在《一致性 Consistentcy》一节中详细介绍。 批量更新 MultiupdateZooKeeper支持将一些原始的操作组合成一个操作单元，然后执行这些操作。那么这种批量操作也是具有原子性的，只可能有两种执行结果，成功和失败。批量操作单元中的操作，不会出现一些操作执行成功，一些操作执行失败的情况，即要么都成功，要么都失败。 Multiupdate对于绑定一些结构化的全局变量很有用处。例如绑定一个无向图（undirected graph）。无向图的顶点（vertex）由znode来表示。添加和删除边（edge）的操作，由修改边的两个关联znode来实现。如果我们使用ZooKeeper的原始的操作来实现对边（edge）的操作，那么就有可能产生两个znode修改不一致的情况（一个修改成功，一个修改失败）。那么我们将修改两个znode的操作放入到一个Multi修改单元中，就能够保证两个znode，要么都修改成功，要么都修改失败。这样就能够避免修改无向图的边时产生修改不一致的现象。 APIsZooKeeper客户端使用的核心编程语言有JAVA和C；同时也支持Perl、Python和REST。执行操作的方式呢，分为同步执行和异步执行。我们之前已经见识过了同步的Java API中的exists。 12public Stat exists(String path, Watcher watcher) throws KeeperException, InterruptedException 下面代码则是异步方式的exists: 1public void exists(String path, Watcher watcher, StatCallback cb, Object ctx) Java API中，异步的方法的返回类型都是void，而操作的返回的结果将传递到回调对象的回调函数中。回调对象将实现StatCallback接口中的一个回调函数，来接收操作返回的结果。函数接口如下： 1public void processResult(int rc, String path, Object ctx, Stat stat); 参数rc表示返回码，请参考KeeperException中的定义。在stat参数为null的情况下，非0的值表示一种异常。参数path和ctx与客户端调用的exists方法中的参数相等，这两个参数通常用来确定回调中获得的响应是来至于哪个请求的。参数ctx可以是任意对象，只有当path参数不能消灭请求的歧义时才会用到。如果不需要参数ctx，可以设置为null。 应该使用同步API还是异步API呢?两种API提供了相同的功能，需要使用哪种API取决于你程序的模式。例如，你设计的程序模式是一个事件驱动模式的程序，那么你最好使用异步API。异步API也可以被用在追求一个比较好的数据吞吐量的场景。想象一下，如果你需要得去大量的znode数据，并且依靠独立的进程来处理他们。如果使用同步API,每次读取操作都会被阻塞住，直到返回结果。不如使用异步API，读取操作可以不必等待返回结果，继续执行。而使用另外的线程来处理返回结果。 观察模式触发器 Watch triggers读操作，例如：exists、getChildren、getData会在znode上开启观察模式，并且写操作会触发观察模式事件，例如：create、delete和setData。ACL(Access Control List)操作不会启动观察模式。观察模式被触发时，会生成一个事件，这个事件的类型取决于触发他的操作： 1, exists启动的观察模式，由创建znode，删除znode和更新znode操作来触发。 2,getData启动的观察模式，由删除znode和更新znode操作触发。创建znode不会触发，是因为getData操作成功的前提是znode必须已经存在。 3,getChildren启动的观察模式，由子节点创建和删除，或者本节点被删除时才会被触发。我们可以通过事件的类型来判断是本节点被删除还是子节点被删除：NodeChildrenChanged表示子节点被删除，而NodeDeleted表示本节点删除。 事件包含了触发事件的znode的path，所以我们通过NodeCreated和NodeDeleted事件就可以知道哪个znode被创建了或者删除了。如果我们需要在NodeChildrenChanged事件发生后知道哪个子节点被改变了，我们就需要再调用一次getChildren来获得一个新的子节点列表。与之类似，在NodeDataChanged事件发生后，我们需要调用getData来获得新的数据。我们在编写程序时，会在接收到事件通知后改变znode的状态，所以我们一定要清楚的记住znode的状态变化。 ACLs 访问控制操作znode的创建时，我们会给他一个ACL（Access Control List），来决定谁可以对znode做哪些操作。ZooKeeper通过鉴权来获得客户端的身份，然后通过ACL来控制客户端的访问。鉴权方式有如下几种： digest使用用户名和密码方式 sasl使用Kerberos鉴权 ip使用客户端的IP来鉴权 客户端可以在与ZooKeeper建立会话连接后，自己给自己授权。授权是并不是必须的，虽然znode的ACL要求客户端必须是身份合法的，在这种情况下，客户端可以自己授权来访问znode。下面的例子，客户端使用用户名和密码为自己授权： 1zk.addAuthInfo(&quot;digest&quot;, &quot;tom:secret&quot;.getBytes()); ACL是由鉴权方式、鉴权方式的ID和一个许可（permession）的集合组成。例如，我们想通过一个ip地址为10.0.0.1的客户端访问一个znode。那么，我们需要为znode设置一个ACL，鉴权方式使用IP鉴权方式，鉴权方式的ID为10.0.0.1，只允许读权限。使用JAVA我们将像如下方式创建一个ACL对象： 1new ACL(Perms.READ,new Id(&quot;ip&quot;, &quot;10.0.0.1&quot;)); 所有的许可权限将在下表中列出。请注意，exists操作不受ACL的控制，所以任何一个客户端都可以通过exists操作来获得任何znode的状态，从而得知znode是否真的存在。 在ZooDefs.Ids类中，有一些ACL的预定义变量，包括OPEN_ACL_UNSAFE，这个设置表示将赋予所有的许可给客户端（除了ADMIN的许可）。 另外，我们可以使用ZooKeeper鉴权的插件机制，来整合第三方的鉴权系统。 实现 ImplementationZooKeeper服务可以在两种模式下运行。在standalone模式下，我们可以运行一个单独的ZooKeeper服务器，我们可以在这种模式下进行基本功能的简单测试，但是这种模式没有办法体现ZooKeeper的高可用特性和快速恢复特性。在生产环境中，我们一般采用replicated（复制）模式安装在多台服务器上，组建一个叫做ensemble的集群。ZooKeeper在他的副本之间实现高可用性，并且只要ensemble集群中能够推举出主服务器，ZooKeeper的服务就可以一直不终断。例如，在一个5个节点的ensemble中，容忍有2个节点脱离集群，服务还是可用的。因为剩下的3个节点投票，可以产生超过集群半数的投票，来推选一台主服务器。而6个节点的ensemble中，也只能容忍2个节点的服务器死机。因为如果3个节点脱离集群，那么剩下的3个节点无论如何不能产生超过集群半数的投票来推选一个主服务器。所以，一般情况下ensemble中的服务器数量都是奇数。 从概念上来看，ZooKeeper其实是很简单的。他所做的一切就是保证每一次对znode树的修改，都能够复制到ensemble的大多数服务器上。如果非主服务器脱离集群，那么至少有一台服务器上的副本保存了最新状态。剩下的其他的服务器上的副本，会很快更新这个最新的状态。 为了实现这个简单而不平凡的设计思路，ZooKeeper使用了一个叫做Zab的协议。这个协议分为两阶段，并且不断的运行在ZooKeeper上： 阶段 1：领导选举（Leader election）Ensemble中的成员通过一个程序来选举出一个首领成员，我们叫做leader。其他的成员就叫做follower。在大多数（quorum）follower完成与leader状态同步时，这个阶段才结束。 阶段 2： 原子广播（Atomic broadcast）所有的写入请求都会发送给leader，leader在广播给follower。当大多数的follower已经完成了数据改变，leader才会将更新提交，客户端就会随之得到leader更新成功的消息。协议中的设计也是具有原子性的，所以写入操作只有成功和失败两个结果。 如果leader脱离了集群，剩下的节点将选举一个新的leader。如果之前的leader回到了集群中，那么将被视作一个follower。leader的选举很快，大概200ms就能够产生结果，所以不会影响执行效率。Ensemble中的所有节点都会在更新内存中的znode树的副本之前，先将更新数据写入到硬盘上。读操作可以请求任何一台ZooKeeper服务器，而且读取速度很快，因为读取是内存中的数据副本。 数据一致性 Consistency理解了ZooKeeper的实现原理，有助于理解ZooKeeper如何保证数据的一致性。就像字面上理解的“leader”和“follower”的意思一样，在ensemble中follower的update操作会滞后于leader的update完成。事实的结果使我们在提交更新数据之前，不必在每一台ZooKeeper服务器上执行持久化变更数据，而是仅需在主服务器上执行持久化变更数据。ZooKeeper客户端的最佳实践是全部链接到follower上。然而客户端是有可能连接到leader上的，并且客户端控制不了这个选择，甚至客户端并不知道连接到了follower还是leader。下图所示，读操作向follower请求即可，而写操作由leader来提交。 每一个对znode树的更新操作，都会被赋予一个全局唯一的ID，我们称之为zxid（ZooKeeper Transaction ID）。更新操作的ID按照发生的时间顺序升序排序。例如，例如z1小于z2，那么z1的操作就早于z2的操作。 ZooKeeper在数据一致性上实现了如下几个方面： 顺序一致性从客户端提交的更新操作是按照先后循序排序的。例如，如果一个客户端将一个znode z赋值为a，然后又将z的值改变成b，那么在这个过程中不会有客户端在z的值变为b后，取到的值是a。 原子性更新操作的结果不是失败就是成功。即，如果更新操作失败，其他的客户端是不会知道的。 系统视图唯一性无论客户端连接到哪个服务器，都将看见唯一的系统视图。如果客户端在同一个会话中去连接一个新的服务器，那么他所看见的视图的状态不会比之前服务器上看见的更旧。当ensemble中的一个服务器宕机，客户端去尝试连接另外一台服务器时，如果这台服务器的状态旧于之前宕机的服务器，那么服务器将不会接受客户端的连接请求，直到服务器的状态赶上之前宕机的服务器为止。 持久性一旦更新操作成功，数据将被持久化到服务器上，并且不能撤销。所以服务器宕机重启，也不会影响数据。时效性 系统视图的状态更新的延迟时间是有一个上限的，最多不过几十秒。如果服务器的状态落后于其他服务器太多，ZooKeeper会宁可关闭这个服务器上的服务，强制客户端去连接一个状态更新的服务器。 从执行效率上考虑，读操作的目标是内存中的缓存数据，并且读操作不会参与到写操作的全局排序中。这就会引起客户端在读取ZooKeeper的状态时产生不一致。例如，A客户端将znode z的值由a改变成a1，然后通知客户端B去读取z的值，但是B读取到的值是a，而不是修改后的a1，为了阻止这种情况出现，B在读取z的值之前，需要调用sync方法。sync方法会强制B连接的服务器状态与leader的状态同步，这样B在读取z的值就是A重新更改过的值了。 sync操作只在异步调用时才可用，原因是你不需要等待操作结束再去执行其他的操作。因此，ZooKeeper保证所有的子操作都会在sync结束后再执行，甚至在sync操作之前发出的操作请求也不例外。 会话 SessionsZooKeeper的客户端中，配置了一个ensemble服务器列表。当启动时，首先去尝试连接其中一个服务器。如果尝试连接失败，那么会继续尝试连接下一个服务器，直到连接成功或者全部尝试连接失败。 一旦连接成功，服务器就会为客户端创建一个会话（session）。session的过期时间由创建会话的客户端应用来设定，如果在这个时间期间，服务器没有收到客户端的任何请求，那么session将被视为过期，并且这个session不能被重新创建，而创建的ephemeral znode将随着session过期被删除掉。在会话长期存在的情况下，session的过期事件是比较少见的，但是应用程序如何处理好这个事件是很重要的。（我们将在《The Resilient ZooKeeper Application》中详细介绍）在长时间的空闲情况下，客户端会不断的发送ping请求来保持session。（ZooKeeper的客户端开发工具的liberay实现了自动发送ping请求，所以我们不必去考虑如何维持session）ping请求的间隔被设置成足够短，以便能够及时发现服务器失败（由读操作的超时时长来设置），并且能够及时的在session过期前连接到其他服务器上。容错连接到其他服务器上，是由ZooKeeper客户端自动完成的。重要的是在连接到其他服务器上后，之前的session以及epemeral节点还保持可用状态。在容错的过程中，应用将收到与服务断开连接和连接的通知。Watch模式的通知在断开链接时，是不会发送断开连接事件给客户端的，断开连接事件是在重新连接成功后发送给客户端的。如果在重新连接到其他节点时，应用尝试一个操作，这个操作是一定会失败的。对于这一点的处理，是一个ZooKeeper应用的重点。 时间 Time在ZooKeeper中有一些时间的参数。tick是ZooKeeper的基础时间单位，用来定义ensemble中服务器上运行的程序的时间表。其他时间相关的配置都是以tick为单位的，或者以tick的值为最大值或者最小值。例如，session的过期时间在2 ticks到20 ticks之间，那么你再设置时选择的session过期时间必须在2和20之间的一个数。 通常情况1 tick等于2秒。那么就是说session的过期时间的设置范围在4秒到40秒之间。在session过期时间的设置上有一些考虑。过期时间太短会造成加快物理失败的监测频率。在组成员关系的例子中，session的过期时间与从组中移除失败的成员花费的时间相等。如果设置过低的session过期时间，那么网络延迟就有可能造成非预期的session过期。这种情况下，就会出现在短时间内一台机器不断的离开组，然后又从新加入组中。 如果应用需要创建比较复杂的临时状态，那么就需要较长的session过期时间，因为重构花费的时间比较长。有一些情况下，需要在session的生命周期内重启，而且要保证重启完后session不过期（例如，应用维护和升级的情况）。服务器会给每一个session一个ID和密码，如果在连接创建时，ZooKeeper验证通过，那么session将被恢复使用（只要session没过期就行）。所以应用程序可以实现一个优雅的关机动作，在重启之前，将session的ID和密码存储在一个稳定的地方。重启之后，通过ID和密码恢复session。 这仅仅是在一些特殊的情况下，我们需要使用这个特性来使用比较长的session过期时间。大多数情况下，我们还是要考虑当出现非预期的异常失败时，如何处理session过期，或者仅需要优雅的关闭应用，在session过期前不用重启应用。 通常情况也越大规模的ensemble，就需要越长的session过期时间。Connetction Timeout、Read Timeout和Ping Periods都由一个以服务器数量为参数的函数计算得到，当ensemble的规模扩大，这些值需要逐渐减小。如果为了解决经常失去连接而需要增加timeout的时长，建议你先监控一下ZooKeeper的metrics，再去调整。 状态 StatesZooKeeper对象在他的生命周期内会有不同的状态，我们通过getState()来获得当前的状态。 1public States getState() 状态是一个枚举类型的数据。新构建的ZooKeeper对象在尝试连接ZooKeeper服务时的状态是CONNECTING，一旦与服务建立了连接那么状态就变成了CONNECTED。 客户端可以通过注册一个观察者对象来接收ZooKeeper对象状态的迁移。当通过CONNECTED状态后，观察者将接收到一个WatchedEvent事件，他的属性KeeperState的值是SyncConnected。 观察者有两个职能：一是接收ZooKeeper的状态改变通知；二是接收znode的改变通知。ZooKeeper对象构造时传递进去的watcher对象，默认是用来接收状态改变通知的，但是znode的改变通知也可能会共享使用默认的watcher对象，或者使用一个专用的watcher。我们可以通过一个Boolean变量来指定是否使用共享默认watcher。 ZooKeeper实例会与服务连接断开或者重新连接，状态会在CONNECTING和CONNECTED之间转换。如果连接断开，watcher会收到一个断开连接事件。请注意，这两个状态都是ZooKeeper实例自己初始化的，并且在断开连接后会自动进行重连接。 如果调用了close()或者session过期，ZooKeeper实例会转换为第三个状态CLOSED，此时在接受事件的KeeperState属性值为Expired。一旦ZooKeeper的状态变为CLOSED，说明实例已经不可用（可以通过isAlive()来判断），并且不能再被使用。如果要重新建立连接，就需要重新构建一个ZooKeeper实例。]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
        <tag>服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之zookeeper安装和运行]]></title>
    <url>%2F2019%2F05%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8Bzookeeper%E5%AE%89%E8%A3%85%E5%92%8C%E8%BF%90%E8%A1%8C%2F</url>
    <content type="text"><![CDATA[简介Zookeeper是Hadoop分布式调度服务，用来构建分布式应用系统。构建一个分布式应用是一个很复杂的事情，主要的原因是我们需要合理有效的处理分布式集群中的部分失败的问题。例如，集群中的节点在相互通信时，A节点向B节点发送消息。A节点如果想知道消息是否发送成功，只能由B节点告诉A节点。那么如果B节点关机或者由于其他的原因脱离集群网络，问题就出现了。A节点不断的向B发送消息，并且无法获得B的响应。B也没有办法通知A节点已经离线或者关机。集群中其他的节点完全不知道B发生了什么情况，还在不断的向B发送消息。这时，你的整个集群就发生了部分失败的故障。 Zookeeper不能让部分失败的问题彻底消失，但是它提供了一些工具能够让你的分布式应用安全合理的处理部分失败的问题。 安装和运行Zookeeper我们采用standalone模式，安装运行一个单独的zookeeper服务。安装前请确认您已经安装了Java运行环境。我们去Apache ZooKeeper releases page下载zookeeper安装包，并解压到本地： 1% tar xzf zookeeper-x.y.z.tar.gz ZooKeeper提供了一些可执行程序的工具，为了方便起见，我们将这些工具的路径加入到PATH环境变量中：12% export ZOOKEEPER_HOME=~/sw/zookeeper-x.y.z% export PATH=$PATH:$ZOOKEEPER_HOME/bin 运行ZooKeeper之前我们需要编写配置文件。配置文件一般在安装目录下的conf/zoo.cfg。我们可以把这个文件放在/etc/zookeeper下，或者放到其他目录下，并在环境变量设置ZOOCFGDIR指向这个个目录。下面是配置文件的内容：123tickTime=2000dataDir=/Users/tom/zookeeperclientPort=2181 tickTime是zookeeper中的基本时间单元，单位是毫秒。datadir是zookeeper持久化数据存放的目录。clientPort是zookeeper监听客户端连接的端口，默认是2181.启动命令：1% zkServer.sh start 我们通过nc或者telnet命令访问2181端口，通过执行ruok（Are you OK?）命令来检查zookeeper是否启动成功：12% echo ruok | nc localhost 2181imok 那么我看见zookeeper回答我们“I’m OK”。]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Zookeeper</tag>
        <tag>安装运行</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式ID生成器解决方案]]></title>
    <url>%2F2019%2F05%2F19%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8FID%E7%94%9F%E6%88%90%E5%99%A8%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[本文主要介绍在一个分布式系统中, 怎么样生成全局唯一的 ID 一, 问题描述在分布式系统存在多个 Shard 的场景中, 同时在各个 Shard 插入数据时, 怎么给这些数据生成全局的 unique ID? 在单机系统中 (例如一个 MySQL 实例), unique ID 的生成是非常简单的, 直接利用 MySQL 自带的自增 ID 功能就可以实现. 但在一个存在多个 Shards 的分布式系统 (例如多个 MySQL 实例组成一个集群, 在这个集群中插入数据), 这个问题会变得复杂, 所生成的全局的 unique ID 要满足以下需求: 保证生成的 ID 全局唯一 今后数据在多个 Shards 之间迁移不会受到 ID 生成方式的限制 生成的 ID 中最好能带上时间信息, 例如 ID 的前 k 位是 Timestamp, 这样能够直接通过对 ID 的前 k 位的排序来对数据按时间排序 生成的 ID 最好不大于 64 bits 生成 ID 的速度有要求. 例如, 在一个高吞吐量的场景中, 需要每秒生成几万个 ID (Twitter 最新的峰值到达了143,199 Tweets/s, 也就是 10万+/秒) 整个服务最好没有单点 如果没有上面这些限制, 问题会相对简单, 例如: 直接利用 UUID.randomUUID() 接口来生成 unique ID (http://www.ietf.org/rfc/rfc4122.txt). 但这个方案生成的 ID 有 128 bits, 另外, 生成的 ID 中也没有带 Timestamp 利用一个中心服务器来统一生成 unique ID. 但这种方案可能存在单点问题; 另外, 要支持高吞吐率的系统, 这个方案还要做很多改进工作 (例如, 每次从中心服务器批量获取一批 IDs, 提升 ID 产生的吞吐率) Flickr 的做法 (http://code.flickr.net/2010/02/08/ticket-servers-distributed-unique-primary-keys-on-the-cheap/). 但他这个方案 ID 中没有带 Timestamp, 生成的 ID 不能按时间排序 在要满足前面 6 点要求的场景中, 怎么来生成全局 unique ID 呢? Twitter 的 Snowflake 是一种比较好的做法. 下面主要介绍 Twitter Snowflake, 以及它的变种 二, Twitter Snowflakehttps://github.com/twitter/snowflake Snowflake 生成的 unique ID 的组成 (由高位到低位): 41 bits: Timestamp (毫秒级) 10 bits: 节点 ID (datacenter ID 5 bits + worker ID 5 bits) 12 bits: sequence number 一共 63 bits (最高位是 0) unique ID 生成过程: 10 bits 的机器号, 在 ID 分配 Worker 启动的时候, 从一个 Zookeeper 集群获取 (保证所有的 Worker 不会有重复的机器号) 41 bits 的 Timestamp: 每次要生成一个新 ID 的时候, 都会获取一下当前的 Timestamp, 然后分两种情况生成 sequence number: 如果当前的 Timestamp 和前一个已生成 ID 的 Timestamp 相同 (在同一毫秒中), 就用前一个 ID 的 sequence number + 1 作为新的 sequence number (12 bits); 如果本毫秒内的所有 ID 用完, 等到下一毫秒继续 (这个等待过程中, 不能分配出新的 ID) 如果当前的 Timestamp 比前一个 ID 的 Timestamp 大, 随机生成一个初始 sequence number (12 bits) 作为本毫秒内的第一个 sequence number整个过程中, 只是在 Worker 启动的时候会对外部有依赖 (需要从 Zookeeper 获取 Worker 号), 之后就可以独立工作了, 做到了去中心化. 异常情况讨论: 在获取当前 Timestamp 时, 如果获取到的时间戳比前一个已生成 ID 的 Timestamp 还要小怎么办? Snowflake 的做法是继续获取当前机器的时间, 直到获取到更大的 Timestamp 才能继续工作 (在这个等待过程中, 不能分配出新的 ID) 从这个异常情况可以看出, 如果 Snowflake 所运行的那些机器时钟有大的偏差时, 整个 Snowflake 系统不能正常工作 (偏差得越多, 分配新 ID 时等待的时间越久) 从 Snowflake 的官方文档 (https://github.com/twitter/snowflake/#system-clock-dependency) 中也可以看到, 它明确要求 “You should use NTP to keep your system clock accurate”. 而且最好把 NTP 配置成不会向后调整的模式. 也就是说, NTP 纠正时间时, 不会向后回拨机器时钟. 三, Snowflake 的其他变种Snowflake 有一些变种, 各个应用结合自己的实际场景对 Snowflake 做了一些改动. 这里主要介绍 3 种. 1. Boundary flakehttp://boundary.com/blog/2012/01/12/flake-a-decentralized-k-ordered-unique-id-generator-in-erlang/ 变化: ID 长度扩展到 128 bits:最高 64 bits 时间戳;然后是 48 bits 的 Worker 号 (和 Mac 地址一样长);最后是 16 bits 的 Seq Number由于它用 48 bits 作为 Worker ID, 和 Mac 地址的长度一样, 这样启动时不需要和 Zookeeper 通讯获取 Worker ID. 做到了完全的去中心化基于 Erlang它这样做的目的是用更多的 bits 实现更小的冲突概率, 这样就支持更多的 Worker 同时工作. 同时, 每毫秒能分配出更多的 ID 2. Simpleflakehttp://engineering.custommade.com/simpleflake-distributed-id-generation-for-the-lazy/ Simpleflake 的思路是取消 Worker 号, 保留 41 bits 的 Timestamp, 同时把 sequence number 扩展到 22 bits; Simpleflake 的特点: sequence number 完全靠随机产生 (这样也导致了生成的 ID 可能出现重复)没有 Worker 号, 也就不需要和 Zookeeper 通讯, 实现了完全去中心化Timestamp 保持和 Snowflake 一致, 今后可以无缝升级到 SnowflakeSimpleflake 的问题就是 sequence number 完全随机生成, 会导致生成的 ID 重复的可能. 这个生成 ID 重复的概率随着每秒生成的 ID 数的增长而增长. 所以, Simpleflake 的限制就是每秒生成的 ID 不能太多 (最好小于 100次/秒, 如果大于 100次/秒的场景, Simpleflake 就不适用了, 建议切换回 Snowflake). 3. instagram 的做法先简单介绍一下 instagram 的分布式存储方案: 先把每个 Table 划分为多个逻辑分片 (logic Shard), 逻辑分片的数量可以很大, 例如 2000 个逻辑分片 然后制定一个规则, 规定每个逻辑分片被存储到哪个数据库实例上面; 数据库实例不需要很多. 例如, 对有 2 个 PostgreSQL 实例的系统 (instagram 使用 PostgreSQL); 可以使用奇数逻辑分片存放到第一个数据库实例, 偶数逻辑分片存放到第二个数据库实例的规则 每个 Table 指定一个字段作为分片字段 (例如, 对用户表, 可以指定 uid 作为分片字段) 插入一个新的数据时, 先根据分片字段的值, 决定数据被分配到哪个逻辑分片 (logic Shard) 然后再根据 logic Shard 和 PostgreSQL 实例的对应关系, 确定这条数据应该被存放到哪台 PostgreSQL 实例上 instagram unique ID 的组成: 41 bits: Timestamp (毫秒) 13 bits: 每个 logic Shard 的代号 (最大支持 8 x 1024 个 logic Shards) 10 bits: sequence number; 每个 Shard 每毫秒最多可以生成 1024 个 ID 生成 unique ID 时, 41 bits 的 Timestamp 和 Snowflake 类似, 这里就不细说了. 主要介绍一下 13 bits 的 logic Shard 代号 和 10 bits 的 sequence number 怎么生成. logic Shard 代号: 假设插入一条新的用户记录, 插入时, 根据 uid 来判断这条记录应该被插入到哪个 logic Shard 中. 假设当前要插入的记录会被插入到第 1341 号 logic Shard 中 (假设当前的这个 Table 一共有 2000 个 logic Shard) 新生成 ID 的 13 bits 段要填的就是 1341 这个数字 sequence number 利用 PostgreSQL 每个 Table 上的 auto-increment sequence 来生成: 如果当前表上已经有 5000 条记录, 那么这个表的下一个 auto-increment sequence 就是 5001 (直接调用 PL/PGSQL 提供的方法可以获取到) 然后把 这个 5001 对 1024 取模就得到了 10 bits 的 sequence number instagram 这个方案的优势在于: 利用 logic Shard 号来替换 Snowflake 使用的 Worker 号, 就不需要到中心节点获取 Worker 号了. 做到了完全去中心化 另外一个附带的好处就是, 可以通过 ID 直接知道这条记录被存放在哪个 logic Shard 上 同时, 今后做数据迁移的时候, 也是按 logic Shard 为单位做数据迁移的, 所以这种做法也不会影响到今后的数据迁移]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>ID生成器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式事务解决方案]]></title>
    <url>%2F2019%2F05%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[分布式事务的解决方案有如下几种： 全局消息 基于可靠消息服务的分布式事务 TCC 最大努力通知 方案1：全局事务（DTP模型）全局事务基于DTP模型实现。DTP是由X/Open组织提出的一种分布式事务模型——X/Open Distributed Transaction Processing Reference Model。它规定了要实现分布式事务，需要三种角色： AP：Application 应用系统它就是我们开发的业务系统，在我们开发的过程中，可以使用资源管理器提供的事务接口来实现分布式事务。 TM：Transaction Manager 事务管理器 分布式事务的实现由事务管理器来完成，它会提供分布式事务的操作接口供我们的业务系统调用。这些接口称为TX接口。 事务管理器还管理着所有的资源管理器，通过它们提供的XA接口来同一调度这些资源管理器，以实现分布式事务。 DTP只是一套实现分布式事务的规范，并没有定义具体如何实现分布式事务，TM可以采用2PC、3PC、Paxos等协议实现分布式事务。 RM：Resource Manager 资源管理器 能够提供数据服务的对象都可以是资源管理器，比如：数据库、消息中间件、缓存等。大部分场景下，数据库即为分布式事务中的资源管理器。 资源管理器能够提供单数据库的事务能力，它们通过XA接口，将本数据库的提交、回滚等能力提供给事务管理器调用，以帮助事务管理器实现分布式的事务管理。 XA是DTP模型定义的接口，用于向事务管理器提供该资源管理器(该数据库)的提交、回滚等能力。 DTP只是一套实现分布式事务的规范，RM具体的实现是由数据库厂商来完成的。 实际方案：基于XA协议的两阶段提交XA是一个分布式事务协议，由Tuxedo提出。XA中大致分为两部分：事务管理器和本地资源管理器。其中本地资源管理器往往由数据库实现，比如Oracle、DB2这些商业数据库都实现了XA接口，而事务管理器作为全局的调度者，负责各个本地资源的提交和回滚。XA实现分布式事务的原理如下： 总的来说，XA协议比较简单，而且一旦商业数据库实现了XA协议，使用分布式事务的成本也比较低。但是，XA也有致命的缺点，那就是性能不理想，特别是在交易下单链路，往往并发量很高，XA无法满足高并发场景。XA目前在商业数据库支持的比较理想，在mysql数据库中支持的不太理想，mysql的XA实现，没有记录prepare阶段日志，主备切换回导致主库与备库数据不一致。许多nosql也没有支持XA，这让XA的应用场景变得非常狭隘。 方案2：基于可靠消息服务的分布式事务（事务消息中间件）这种实现分布式事务的方式需要通过消息中间件来实现。假设有A和B两个系统，分别可以处理任务A和任务B。此时系统A中存在一个业务流程，需要将任务A和任务B在同一个事务中处理。下面来介绍基于消息中间件来实现这种分布式事务。 在系统A处理任务A前，首先向消息中间件发送一条消息 消息中间件收到后将该条消息持久化，但并不投递。此时下游系统B仍然不知道该条消息的存在。 消息中间件持久化成功后，便向系统A返回一个确认应答； 系统A收到确认应答后，则可以开始处理任务A； 任务A处理完成后，向消息中间件发送Commit请求。该请求发送完成后，对系统A而言，该事务的处理过程就结束了，此时它可以处理别的任务了。但commit消息可能会在传输途中丢失，从而消息中间件并不会向系统B投递这条消息，从而系统就会出现不一致性。这个问题由消息中间件的事务回查机制完成，下文会介绍。 消息中间件收到Commit指令后，便向系统B投递该消息，从而触发任务B的执行； 当任务B执行完成后，系统B向消息中间件返回一个确认应答，告诉消息中间件该消息已经成功消费，此时，这个分布式事务完成。 上述过程可以得出如下几个结论： 消息中间件扮演者分布式事务协调者的角色。 系统A完成任务A后，到任务B执行完成之间，会存在一定的时间差。在这个时间差内，整个系统处于数据不一致的状态，但这短暂的不一致性是可以接受的，因为经过短暂的时间后，系统又可以保持数据一致性，满足BASE理论。 上述过程中，如果任务A处理失败，那么需要进入回滚流程，如下图所示： 若系统A在处理任务A时失败，那么就会向消息中间件发送Rollback请求。和发送Commit请求一样，系统A发完之后便可以认为回滚已经完成，它便可以去做其他的事情。 消息中间件收到回滚请求后，直接将该消息丢弃，而不投递给系统B，从而不会触发系统B的任务B。 此时系统又处于一致性状态，因为任务A和任务B都没有执行。 上面所介绍的Commit和Rollback都属于理想情况，但在实际系统中，Commit和Rollback指令都有可能在传输途中丢失。那么当出现这种情况的时候，消息中间件是如何保证数据一致性呢？——答案就是超时询问机制。 系统A除了实现正常的业务流程外，还需提供一个事务询问的接口，供消息中间件调用。当消息中间件收到一条事务型消息后便开始计时，如果到了超时时间也没收到系统A发来的Commit或Rollback指令的话，就会主动调用系统A提供的事务询问接口询问该系统目前的状态。该接口会返回三种结果： 提交若获得的状态是“提交”，则将该消息投递给系统B。 回滚若获得的状态是“回滚”，则直接将条消息丢弃。 处理中若获得的状态是“处理中”，则继续等待。 消息中间件的超时询问机制能够防止上游系统因在传输过程中丢失Commit/Rollback指令而导致的系统不一致情况，而且能降低上游系统的阻塞时间，上游系统只要发出Commit/Rollback指令后便可以处理其他任务，无需等待确认应答。而Commit/Rollback指令丢失的情况通过超时询问机制来弥补，这样大大降低上游系统的阻塞时间，提升系统的并发度。 下面来说一说消息投递过程的可靠性保证。当上游系统执行完任务并向消息中间件提交了Commit指令后，便可以处理其他任务了，此时它可以认为事务已经完成，接下来消息中间件一定会保证消息被下游系统成功消费掉！那么这是怎么做到的呢？这由消息中间件的投递流程来保证。 消息中间件向下游系统投递完消息后便进入阻塞等待状态，下游系统便立即进行任务的处理，任务处理完成后便向消息中间件返回应答。消息中间件收到确认应答后便认为该事务处理完毕！ 如果消息在投递过程中丢失，或消息的确认应答在返回途中丢失，那么消息中间件在等待确认应答超时之后就会重新投递，直到下游消费者返回消费成功响应为止。当然，一般消息中间件可以设置消息重试的次数和时间间隔，比如：当第一次投递失败后，每隔五分钟重试一次，一共重试3次。如果重试3次之后仍然投递失败，那么这条消息就需要人工干预。 有的同学可能要问：消息投递失败后为什么不回滚消息，而是不断尝试重新投递？ 这就涉及到整套分布式事务系统的实现成本问题。我们知道，当系统A将向消息中间件发送Commit指令后，它便去做别的事情了。如果此时消息投递失败，需要回滚的话，就需要让系统A事先提供回滚接口，这无疑增加了额外的开发成本，业务系统的复杂度也将提高。对于一个业务系统的设计目标是，在保证性能的前提下，最大限度地降低系统复杂度，从而能够降低系统的运维成本。 不知大家是否发现，上游系统A向消息中间件提交Commit/Rollback消息采用的是异步方式，也就是当上游系统提交完消息后便可以去做别的事情，接下来提交、回滚就完全交给消息中间件来完成，并且完全信任消息中间件，认为它一定能正确地完成事务的提交或回滚。然而，消息中间件向下游系统投递消息的过程是同步的。也就是消息中间件将消息投递给下游系统后，它会阻塞等待，等下游系统成功处理完任务返回确认应答后才取消阻塞等待。为什么这两者在设计上是不一致的呢？ 首先，上游系统和消息中间件之间采用异步通信是为了提高系统并发度。业务系统直接和用户打交道，用户体验尤为重要，因此这种异步通信方式能够极大程度地降低用户等待时间。此外，异步通信相对于同步通信而言，没有了长时间的阻塞等待，因此系统的并发性也大大增加。但异步通信可能会引起Commit/Rollback指令丢失的问题，这就由消息中间件的超时询问机制来弥补。 那么，消息中间件和下游系统之间为什么要采用同步通信呢？ 异步能提升系统性能，但随之会增加系统复杂度；而同步虽然降低系统并发度，但实现成本较低。因此，在对并发度要求不是很高的情况下，或者服务器资源较为充裕的情况下，我们可以选择同步来降低系统的复杂度。我们知道，消息中间件是一个独立于业务系统的第三方中间件，它不和任何业务系统产生直接的耦合，它也不和用户产生直接的关联，它一般部署在独立的服务器集群上，具有良好的可扩展性，所以不必太过于担心它的性能，如果处理速度无法满足我们的要求，可以增加机器来解决。而且，即使消息中间件处理速度有一定的延迟那也是可以接受的，因为前面所介绍的BASE理论就告诉我们了，我们追求的是最终一致性，而非实时一致性，因此消息中间件产生的时延导致事务短暂的不一致是可以接受的。 方案3：最大努力通知（定期校对）也叫本地消息表最大努力通知也被称为定期校对，其实在方案二中已经包含，这里再单独介绍，主要是为了知识体系的完整性。这种方案也需要消息中间件的参与，其过程如下： 上游系统在完成任务后，向消息中间件同步地发送一条消息，确保消息中间件成功持久化这条消息，然后上游系统可以去做别的事情了； 消息中间件收到消息后负责将该消息同步投递给相应的下游系统，并触发下游系统的任务执行； 当下游系统处理成功后，向消息中间件反馈确认应答，消息中间件便可以将该条消息删除，从而该事务完成。 上面是一个理想化的过程，但在实际场景中，往往会出现如下几种意外情况： 消息中间件向下游系统投递消息失败 上游系统向消息中间件发送消息失败 对于第一种情况，消息中间件具有重试机制，我们可以在消息中间件中设置消息的重试次数和重试时间间隔，对于网络不稳定导致的消息投递失败的情况，往往重试几次后消息便可以成功投递，如果超过了重试的上限仍然投递失败，那么消息中间件不再投递该消息，而是记录在失败消息表中，消息中间件需要提供失败消息的查询接口，下游系统会定期查询失败消息，并将其消费，这就是所谓的“定期校对”。 如果重复投递和定期校对都不能解决问题，往往是因为下游系统出现了严重的错误，此时就需要人工干预。 对于第二种情况，需要在上游系统中建立消息重发机制。可以在上游系统建立一张本地消息表，并将 任务处理过程 和 向本地消息表中插入消息 这两个步骤放在一个本地事务中完成。如果向本地消息表插入消息失败，那么就会触发回滚，之前的任务处理结果就会被取消。如果这量步都执行成功，那么该本地事务就完成了。接下来会有一个专门的消息发送者不断地发送本地消息表中的消息，如果发送失败它会返回重试。当然，也要给消息发送者设置重试的上限，一般而言，达到重试上限仍然发送失败，那就意味着消息中间件出现严重的问题，此时也只有人工干预才能解决问题。 对于不支持事务型消息的消息中间件，如果要实现分布式事务的话，就可以采用这种方式。它能够通过重试机制+定期校对实现分布式事务，但相比于第二种方案，它达到数据一致性的周期较长，而且还需要在上游系统中实现消息重试发布机制，以确保消息成功发布给消息中间件，这无疑增加了业务系统的开发成本，使得业务系统不够纯粹，并且这些额外的业务逻辑无疑会占用业务系统的硬件资源，从而影响性能。 因此，尽量选择支持事务型消息的消息中间件来实现分布式事务，如RocketMQ。 方案4：TCC（两阶段型、补偿型）跨应用的业务操作原子性要求，其实是比较常见的。比如在第三方支付场景中的组合支付，用户在电商网站购物后，要同时使用余额和红包支付该笔订单，而余额系统和红包系统分别是不同的应用系统，支付系统在调用这两个系统进行支付时，就需要保证余额扣减和红包使用要么同时成功，要么同时失败。 TCC事务的出现正是为了解决应用拆分带来的跨应用业务操作原子性的问题。当然，由于常规的XA事务(2PC，2 Phase Commit, 两阶段提交)性能上不尽如人意，也有通过TCC事务来解决数据库拆分的使用场景(如账务拆分)，这个本文后续部分再详述。 故从整个系统架构的角度来看，分布式事务的不同方案是存在层次结构的。 TCC的机制明眼一看就知道，TCC应该是三个英文单词的首字母缩写而来。没错，TCC分别对应Try、Confirm和Cancel三种操作，这三种操作的业务含义如下： Try：预留业务资源Confirm：确认执行业务操作Cancel：取消执行业务操作 稍稍对照下关系型数据库事务的三种操作：DML、Commit和Rollback，会发现和TCC有异曲同工之妙。在一个跨应用的业务操作中，Try操作是先把多个应用中的业务资源预留和锁定住，为后续的确认打下基础，类似的，DML操作要锁定数据库记录行，持有数据库资源；Confirm操作是在Try操作中涉及的所有应用均成功之后进行确认，使用预留的业务资源，和Commit类似；而Cancel则是当Try操作中涉及的所有应用没有全部成功，需要将已成功的应用进行取消(即Rollback回滚)。其中Confirm和Cancel操作是一对反向业务操作。 简而言之，TCC是应用层的2PC(2 Phase Commit, 两阶段提交)，如果你将应用看做资源管理器的话。详细来说，TCC每项操作需要做的事情如下： 1、Try：尝试执行业务。完成所有业务检查(一致性)预留必须业务资源(准隔离性)2、Confirm：确认执行业务。真正执行业务不做任何业务检查只使用Try阶段预留的业务资源3、Cancel：取消执行业务释放Try阶段预留的业务资源 一个完整的TCC事务参与方包括三部分： 主业务服务：主业务服务为整个业务活动的发起方，如前面提到的组合支付场景，支付系统即是主业务服务。从业务服务：从业务服务负责提供TCC业务操作，是整个业务活动的操作方。从业务服务必须实现Try、Confirm和Cancel三个接口，供主业务服务调用。由于Confirm和Cancel操作可能被重复调用，故要求Confirm和Cancel两个接口必须是幂等的。前面的组合支付场景中的余额系统和红包系统即为从业务服务。业务活动管理器：业务活动管理器管理控制整个业务活动，包括记录维护TCC全局事务的事务状态和每个从业务服务的子事务状态，并在业务活动提交时确认所有的TCC型操作的confirm操作，在业务活动取消时调用所有TCC型操作的cancel操作。可见整个TCC事务对于主业务服务来说是透明的，其中业务活动管理器和从业务服务各自干了一部分工作。 TCC的优点和限制TCC事务的优点如下：解决了跨应用业务操作的原子性问题，在诸如组合支付、账务拆分场景非常实用。TCC实际上把数据库层的二阶段提交上提到了应用层来实现，对于数据库来说是一阶段提交，规避了数据库层的2PC性能低下问题。 TCC事务的缺点，主要就一个：TCC的Try、Confirm和Cancel操作功能需业务提供，开发成本高。当然，对TCC事务的这个缺点是否是缺点，是一个见仁见智的事情。 一个案例理解TCC说实话，TCC的理论有点让人费解。故接下来将以账务拆分为例，对TCC事务的流程做一个描述，希望对理解TCC有所帮助。账务拆分的业务场景如下，分别位于三个不同分库的帐户A、B、C，A和B一起向C转帐共80元：分布式事务之说说TCC事务 1、Try：尝试执行业务。完成所有业务检查(一致性)：检查A、B、C的帐户状态是否正常，帐户A的余额是否不少于30元，帐户B的余额是否不少于50元。预留必须业务资源(准隔离性)：帐户A的冻结金额增加30元，帐户B的冻结金额增加50元，这样就保证不会出现其他并发进程扣减了这两个帐户的余额而导致在后续的真正转帐操作过程中，帐户A和B的可用余额不够的情况。 2、Confirm：确认执行业务。真正执行业务：如果Try阶段帐户A、B、C状态正常，且帐户A、B余额够用，则执行帐户A给账户C转账30元、帐户B给账户C转账50元的转帐操作。不做任何业务检查：这时已经不需要做业务检查，Try阶段已经完成了业务检查。只使用Try阶段预留的业务资源：只需要使用Try阶段帐户A和帐户B冻结的金额即可。 3、Cancel：取消执行业务释放Try阶段预留的业务资源：如果Try阶段部分成功，比如帐户A的余额够用，且冻结相应金额成功，帐户B的余额不够而冻结失败，则需要对帐户A做Cancel操作，将帐户A被冻结的金额解冻掉。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>事务</tag>
        <tag>大数据</tag>
        <tag>分布式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式锁解决方案(二)]]></title>
    <url>%2F2019%2F05%2F11%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88(%E4%BA%8C)%2F</url>
    <content type="text"><![CDATA[目前几乎很多大型网站及应用都是分布式部署的，分布式场景中的数据一致性问题一直是一个比较重要的话题。分布式的CAP理论告诉我们“任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项。”所以，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。 在很多场景中，我们为了保证数据的最终一致性，需要很多的技术方案来支持，比如分布式事务、分布式锁等。有的时候，我们需要保证一个方法在同一时间内只能被同一个线程执行。在单机环境中，Java中其实提供了很多并发处理相关的API，但是这些API在分布式场景中就无能为力了。也就是说单纯的Java Api并不能提供分布式锁的能力。所以针对分布式锁的实现目前有多种方案。 针对分布式锁的实现，目前比较常用的有以下几种方案： 基于数据库实现分布式锁 基于缓存（redis，memcached，tair）实现分布式锁 基于Zookeeper实现分布式锁 在分析这几种实现方案之前我们先来想一下，我们需要的分布式锁应该是怎么样的？（这里以方法锁为例，资源锁同理） 可以保证在分布式部署的应用集群中，同一个方法在同一时间只能被一台机器上的一个线程执行 这把锁要是一把可重入锁（避免死锁） 这把锁最好是一把阻塞锁（根据业务需求考虑要不要这条） 有高可用的获取锁和释放锁功能 获取锁和释放锁的性能要好 基于数据库实现分布式锁基于数据库表要实现分布式锁，最简单的方式可能就是直接创建一张锁表，然后通过操作该表中的数据来实现了。 当我们要锁住某个方法或资源时，我们就在该表中增加一条记录，想要释放锁的时候就删除这条记录。 创建这样一张数据库表： 12345678CREATE TABLE `methodLock` ( `id` int(11) NOT NULL AUTO_INCREMENT COMMENT &apos;主键&apos;, `method_name` varchar(64) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;锁定的方法名&apos;, `desc` varchar(1024) NOT NULL DEFAULT &apos;备注信息&apos;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &apos;保存数据时间，自动生成&apos;, PRIMARY KEY (`id`), UNIQUE KEY `uidx_method_name` (`method_name `) USING BTREE) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=&apos;锁定中的方法&apos;; 当我们想要锁住某个方法时，执行以下SQL： 1insert into methodLock(method_name,desc) values (‘method_name’,‘desc’) 因为我们对method_name做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁，可以执行方法体内容。 当方法执行完毕之后，想要释放锁的话，需要执行以下Sql: 1delete from methodLock where method_name =&apos;method_name&apos; 上面这种简单的实现有以下几个问题： 1、这把锁强依赖数据库的可用性，数据库是一个单点，一旦数据库挂掉，会导致业务系统不可用。 2、这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在数据库中，其他线程无法再获得到锁。 3、这把锁只能是非阻塞的，因为数据的insert操作，一旦插入失败就会直接报错。没有获得锁的线程并不会进入排队队列，要想再次获得锁就要再次触发获得锁操作。 4、这把锁是非重入的，同一个线程在没有释放锁之前无法再次获得该锁。因为数据中数据已经存在了。 当然，我们也可以有其他方式解决上面的问题。 数据库是单点？搞两个数据库，数据之前双向同步。一旦挂掉快速切换到备库上。 没有失效时间？只要做一个定时任务，每隔一定时间把数据库中的超时数据清理一遍。 非阻塞的？搞一个while循环，直到insert成功再返回成功。 非重入的？在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给他就可以了。 基于数据库排他锁除了可以通过增删操作数据表中的记录以外，其实还可以借助数据中自带的锁来实现分布式的锁。 我们还用刚刚创建的那张数据库表。可以通过数据库的排他锁来实现分布式锁。 基于MySql的InnoDB引擎，可以使用以下方法来实现加锁操作： 123456789101112131415public boolean lock()&#123; connection.setAutoCommit(false) while(true)&#123; try&#123; result = select * from methodLock where method_name=xxx for update; if(result==null)&#123; return true; &#125; &#125;catch(Exception e)&#123; &#125; sleep(1000); &#125; return false;&#125; 在查询语句后面增加for update，数据库会在查询过程中给数据库表增加排他锁（这里再多提一句，InnoDB引擎在加锁的时候，只有通过索引进行检索的时候才会使用行级锁，否则会使用表级锁。这里我们希望使用行级锁，就要给method_name添加索引，值得注意的是，这个索引一定要创建成唯一索引，否则会出现多个重载方法之间无法同时被访问的问题。重载方法的话建议把参数类型也加上。）。当某条记录被加上排他锁之后，其他线程无法再在该行记录上增加排他锁。 我们可以认为获得排它锁的线程即可获得分布式锁，当获取到锁之后，可以执行方法的业务逻辑，执行完方法之后，再通过以下方法解锁： 123public void unlock()&#123; connection.commit();&#125; 通过connection.commit()操作来释放锁。 通过connection.commit()操作来释放锁。 这种方法可以有效的解决上面提到的无法释放锁和阻塞锁的问题。阻塞锁？ for update语句会在执行成功后立即返回，在执行失败时一直处于阻塞状态，直到成功。锁定之后服务宕机，无法释放？使用这种方式，服务宕机之后数据库会自己把锁释放掉。但是还是无法直接解决数据库单点和可重入问题。 这里还可能存在另外一个问题，虽然我们对method_name 使用了唯一索引，并且显示使用for update来使用行级锁。但是，MySql会对查询进行优化，即便在条件中使用了索引字段，但是否使用索引来检索数据是由 MySQL 通过判断不同执行计划的代价来决定的，如果 MySQL 认为全表扫效率更高，比如对一些很小的表，它就不会使用索引，这种情况下 InnoDB 将使用表锁，而不是行锁。如果发生这种情况就悲剧了。 还有一个问题，就是我们要使用排他锁来进行分布式锁的lock，那么一个排他锁长时间不提交，就会占用数据库连接。一旦类似的连接变得多了，就可能把数据库连接池撑爆 总结总结一下使用数据库来实现分布式锁的方式，这两种方式都是依赖数据库的一张表，一种是通过表中的记录的存在情况确定当前是否有锁存在，另外一种是通过数据库的排他锁来实现分布式锁。 数据库实现分布式锁的优点: 直接借助数据库，容易理解。 数据库实现分布式锁的缺点: 会有各种各样的问题，在解决问题的过程中会使整个方案变得越来越复杂。 操作数据库需要一定的开销，性能问题需要考虑。 使用数据库的行级锁并不一定靠谱，尤其是当我们的锁表并不大的时候。 基于缓存实现分布式锁相比较于基于数据库实现分布式锁的方案来说，基于缓存来实现在性能方面会表现的更好一点。而且很多缓存是可以集群部署的，可以解决单点问题。 目前有很多成熟的缓存产品，包括Redis，memcached以及阿里巴巴的Tair。 这里以Tair为例来分析下使用缓存实现分布式锁的方案。关于Redis和memcached在网络上有很多相关的文章，并且也有一些成熟的框架及算法可以直接使用。 基于Tair的实现分布式锁其实和Redis类似，其中主要的实现方式是使用TairManager.put方法来实现。 12345678910public boolean trylock(String key) &#123; ResultCode code = ldbTairManager.put(NAMESPACE, key, &quot;This is a Lock.&quot;, 2, 0); if (ResultCode.SUCCESS.equals(code)) return true; else return false;&#125;public boolean unlock(String key) &#123; ldbTairManager.invalid(NAMESPACE, key);&#125; 以上实现方式同样存在几个问题： 1、这把锁没有失效时间，一旦解锁操作失败，就会导致锁记录一直在tair中，其他线程无法再获得到锁。 2、这把锁只能是非阻塞的，无论成功还是失败都直接返回。 3、这把锁是非重入的，一个线程获得锁之后，在释放锁之前，无法再次获得该锁，因为使用到的key在tair中已经存在。无法再执行put操作。 当然，同样有方式可以解决。 没有失效时间？tair的put方法支持传入失效时间，到达时间之后数据会自动删除。 非阻塞？while重复执行。 非可重入？在一个线程获取到锁之后，把当前主机信息和线程信息保存起来，下次再获取之前先检查自己是不是当前锁的拥有者。 但是，失效时间我设置多长时间为好？如何设置的失效时间太短，方法没等执行完，锁就自动释放了，那么就会产生并发问题。如果设置的时间太长，其他获取锁的线程就可能要平白的多等一段时间。这个问题使用数据库实现分布式锁同样存在 总结可以使用缓存来代替数据库来实现分布式锁，这个可以提供更好的性能，同时，很多缓存服务都是集群部署的，可以避免单点问题。并且很多缓存服务都提供了可以用来实现分布式锁的方法，比如Tair的put方法，redis的setnx方法等。并且，这些缓存服务也都提供了对数据的过期自动删除的支持，可以直接设置超时时间来控制锁的释放。 使用缓存实现分布式锁的优点 性能好，实现起来较为方便。 使用缓存实现分布式锁的缺点 通过超时时间来控制锁的失效时间并不是十分的靠谱。 基于Zookeeper实现分布式锁基于zookeeper临时有序节点可以实现的分布式锁。 大致思想即为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。 来看下Zookeeper能不能解决前面提到的问题。 锁无法释放？使用Zookeeper可以有效的解决锁无法释放的问题，因为在创建锁的时候，客户端会在ZK中创建一个临时节点，一旦客户端获取到锁之后突然挂掉（Session连接断开），那么这个临时节点就会自动删除掉。其他客户端就可以再次获得锁。 非阻塞锁？使用Zookeeper可以实现阻塞的锁，客户端可以通过在ZK中创建顺序节点，并且在节点上绑定监听器，一旦节点有变化，Zookeeper会通知客户端，客户端可以检查自己创建的节点是不是当前所有节点中序号最小的，如果是，那么自己就获取到锁，便可以执行业务逻辑了。 不可重入？使用Zookeeper也可以有效的解决不可重入的问题，客户端在创建节点的时候，把当前客户端的主机信息和线程信息直接写入到节点中，下次想要获取锁的时候和当前最小的节点中的数据比对一下就可以了。如果和自己的信息一样，那么自己直接获取到锁，如果不一样就再创建一个临时的顺序节点，参与排队。 单点问题？使用Zookeeper可以有效的解决单点问题，ZK是集群部署的，只要集群中有半数以上的机器存活，就可以对外提供服务。 可以直接使用zookeeper第三方库Curator客户端，这个客户端中封装了一个可重入的锁服务。 123456789101112131415161718public boolean tryLock(long timeout, TimeUnit unit) throws InterruptedException &#123; try &#123; return interProcessMutex.acquire(timeout, unit); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; return true;&#125;public boolean unlock() &#123; try &#123; interProcessMutex.release(); &#125; catch (Throwable e) &#123; log.error(e.getMessage(), e); &#125; finally &#123; executorService.schedule(new Cleaner(client, path), delayTimeForClean, TimeUnit.MILLISECONDS); &#125; return true;&#125; Curator提供的InterProcessMutex是分布式锁的实现。acquire方法用户获取锁，release方法用于释放锁。 使用ZK实现的分布式锁好像完全符合了本文开头我们对一个分布式锁的所有期望。但是，其实并不是，Zookeeper实现的分布式锁其实存在一个缺点，那就是性能上可能并没有缓存服务那么高。因为每次在创建锁和释放锁的过程中，都要动态创建、销毁瞬时节点来实现锁功能。ZK中创建和删除节点只能通过Leader服务器来执行，然后将数据同不到所有的Follower机器上。 其实，使用Zookeeper也有可能带来并发问题，只是并不常见而已。考虑这样的情况，由于网络抖动，客户端可ZK集群的session连接断了，那么zk以为客户端挂了，就会删除临时节点，这时候其他客户端就可以获取到分布式锁了。就可能产生并发问题。这个问题不常见是因为zk有重试机制，一旦zk集群检测不到客户端的心跳，就会重试，Curator客户端支持多种重试策略。多次重试之后还不行的话才会删除临时节点。（所以，选择一个合适的重试策略也比较重要，要在锁的粒度和并发之间找一个平衡。） 总结使用Zookeeper实现分布式锁的优点 有效的解决单点问题，不可重入问题，非阻塞问题以及锁无法释放的问题。实现起来较为简单。 使用Zookeeper实现分布式锁的缺点 性能上不如使用缓存实现分布式锁。 需要对ZK的原理有所了解。 本文作者：Hollis]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式锁解决方案(一)]]></title>
    <url>%2F2019%2F05%2F11%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88(%E4%B8%80)%2F</url>
    <content type="text"><![CDATA[前言随着互联网技术的不断发展，数据量的不断增加，业务逻辑日趋复杂，在这种背景下，传统的集中式系统已经无法满足我们的业务需求，分布式系统被应用在更多的场景，而在分布式系统中访问共享资源就需要一种互斥机制，来防止彼此之间的互相干扰，以保证一致性，在这种情况下，我们就需要用到分布式锁。 分布式一致性问题首先我们先来看一个小例子： 假设某商城有一个商品库存剩10个，用户A想要买6个，用户B想要买5个，在理想状态下，用户A先买走了6了，库存减少6个还剩4个，此时用户B应该无法购买5个，给出数量不足的提示；而在真实情况下，用户A和B同时获取到商品剩10个，A买走6个，在A更新库存之前，B又买走了5个，此时B更新库存，商品还剩5个，这就是典型的电商“秒杀”活动。 从上述例子不难看出，在高并发情况下，如果不做处理将会出现各种不可预知的后果。那么在这种高并发多线程的情况下，解决问题最有效最普遍的方法就是给共享资源或对共享资源的操作加一把锁，来保证对资源的访问互斥。在Java JDK已经为我们提供了这样的锁，利用ReentrantLcok或者synchronized，即可达到资源互斥访问的目的。但是在分布式系统中，由于分布式系统的分布性，即多线程和多进程并且分布在不同机器中，这两种锁将失去原有锁的效果，需要我们自己实现分布式锁——分布式锁。 分布式锁需要具备哪些条件 获取锁和释放锁的性能要好 判断是否获得锁必须是原子性的，否则可能导致多个请求都获取到锁 网络中断或宕机无法释放锁时，锁必须被清楚，不然会发生死锁 可重入一个线程中可以多次获取同一把锁，比如一个线程在执行一个带锁的方法，该方法中又调用了另一个需要相同锁的方法，则该线程可以直接执行调用的方法，而无需重新获得锁； 5.阻塞锁和非阻塞锁，阻塞锁即没有获取到锁，则继续等待获取锁；非阻塞锁即没有获取到锁后，不继续等待，直接返回锁失败。 分布式锁实现方式一、数据库锁一般很少使用数据库锁，性能不好并且容易产生死锁。 1. 基于MySQL锁表该实现方式完全依靠数据库唯一索引来实现，当想要获得锁时，即向数据库中插入一条记录，释放锁时就删除这条记录。这种方式存在以下几个问题： (1) 锁没有失效时间，解锁失败会导致死锁，其他线程无法再获取到锁，因为唯一索引insert都会返回失败。 (2) 只能是非阻塞锁，insert失败直接就报错了，无法进入队列进行重试 (3) 不可重入，同一线程在没有释放锁之前无法再获取到锁 2. 采用乐观锁增加版本号根据版本号来判断更新之前有没有其他线程更新过，如果被更新过，则获取锁失败。 二、缓存锁具体实例可以参考我讲述Redis的系列文章，里面有完整的Redis分布式锁实现方案 这里我们主要介绍几种基于redis实现的分布式锁： 1. 基于setnx、expire两个命令来实现基于setnx（set if not exist）的特点，当缓存里key不存在时，才会去set，否则直接返回false。如果返回true则获取到锁，否则获取锁失败，为了防止死锁，我们再用expire命令对这个key设置一个超时时间来避免。但是这里看似完美，实则有缺陷，当我们setnx成功后，线程发生异常中断，expire还没来的及设置，那么就会产生死锁。 解决上述问题有两种方案 第一种是采用redis2.6.12版本以后的set，它提供了一系列选项 EX seconds – 设置键key的过期时间，单位时秒 PX milliseconds – 设置键key的过期时间，单位时毫秒 NX – 只有键key不存在的时候才会设置key的值 XX – 只有键key存在的时候才会设置key的值 第二种采用setnx()，get()，getset()实现，大体的实现过程如下： (1) 线程Asetnx，值为超时的时间戳(t1)，如果返回true，获得锁。 (2) 线程B用get 命令获取t1，与当前时间戳比较，判断是否超时，没超时false，如果已超时执行步骤3 (3) 计算新的超时时间t2，使用getset命令返回t3(这个值可能其他线程已经修改过)，如果t1==t3,获得锁,如果t1!=t3说明锁被其他线程获取了 (4) 获取锁后，处理完业务逻辑，再去判断锁是否超时，如果没超时删除锁，如果已超时，不用处理（防止删除其他线程的锁） 2. RedLock算法redlock算法是redis作者推荐的一种分布式锁实现方式，算法的内容如下： (1) 获取当前时间； (2) 尝试从5个相互独立redis客户端获取锁； (3) 计算获取所有锁消耗的时间，当且仅当客户端从多数节点获取锁，并且获取锁的时间小于锁的有效时间，认为获得锁； (4) 重新计算有效期时间，原有效时间减去获取锁消耗的时间； (5) 删除所有实例的锁 redlock算法相对于单节点redis锁可靠性要更高，但是实现起来条件也较为苛刻。 (1) 必须部署5个节点才能让Redlock的可靠性更强。 (2) 需要请求5个节点才能获取到锁，通过Future的方式，先并发向5个节点请求，再一起获得响应结果，能缩短响应时间，不过还是比单节点redis锁要耗费更多时间。 然后由于必须获取到5个节点中的3个以上，所以可能出现获取锁冲突，即大家都获得了1-2把锁，结果谁也不能获取到锁，这个问题，redis作者借鉴了raft算法的精髓，通过冲突后在随机时间开始，可以大大降低冲突时间，但是这问题并不能很好的避免，特别是在第一次获取锁的时候，所以获取锁的时间成本增加了。 如果5个节点有2个宕机，此时锁的可用性会极大降低，首先必须等待这两个宕机节点的结果超时才能返回，另外只有3个节点，客户端必须获取到这全部3个节点的锁才能拥有锁，难度也加大了。 如果出现网络分区，那么可能出现客户端永远也无法获取锁的情况，介于这种情况，下面我们来看一种更可靠的分布式锁zookeeper锁。 zookeeper分布式锁首先我们来了解一下zookeeper的特性，看看它为什么适合做分布式锁， zookeeper是一个为分布式应用提供一致性服务的软件，它内部是一个分层的文件系统目录树结构，规定统一个目录下只能有一个唯一文件名。 数据模型： 永久节点：节点创建后，不会因为会话失效而消失 临时节点：与永久节点相反，如果客户端连接失效，则立即删除节点 顺序节点：与上述两个节点特性类似，如果指定创建这类节点时，zk会自动在节点名后加一个数字后缀，并且是有序的。 监视器（watcher）： 当创建一个节点时，可以注册一个该节点的监视器，当节点状态发生改变时，watch被触发时，ZooKeeper将会向客户端发送且仅发送一条通知，因为watch只能被触发一次。 根据zookeeper的这些特性，我们来看看如何利用这些特性来实现分布式锁： 创建一个锁目录lock 希望获得锁的线程A就在lock目录下，创建临时顺序节点 获取锁目录下所有的子节点，然后获取比自己小的兄弟节点，如果不存在，则说明当前线程顺序号最小，获得锁 线程B获取所有节点，判断自己不是最小节点，设置监听(watcher)比自己次小的节点（只关注比自己次小的节点是为了防止发生“羊群效应”） 线程A处理完，删除自己的节点，线程B监听到变更事件，判断自己是最小的节点，获得锁。 小结 在分布式系统中，共享资源互斥访问问题非常普遍，而针对访问共享资源的互斥问题，常用的解决方案就是使用分布式锁，这里只介绍了几种常用的分布式锁，分布式锁的实现方式还有有很多种，根据业务选择合适的分布式锁，下面对上述几种锁进行一下比较： 数据库锁： 优点：直接使用数据库，使用简单。 缺点：分布式系统大多数瓶颈都在数据库，使用数据库锁会增加数据库负担。 缓存锁： 优点：性能高，实现起来较为方便，在允许偶发的锁失效情况，不影响系统正常使用，建议采用缓存锁。 缺点：通过锁超时机制不是十分可靠，当线程获得锁后，处理时间过长导致锁超时，就失效了锁的作用。 zookeeper锁： 优点：不依靠超时时间释放锁；可靠性高；系统要求高可靠性时，建议采用zookeeper锁。 缺点：性能比不上缓存锁，因为要频繁的创建节点删除节点。 事实上，大家只要参考引入下面的代码：https://github.com/yujiasun/Distributed-Kit这个过程有基于redis和zookeeper分布式工具集-包括:分布式锁实现,分布式速率限制器,分布式ID生成器等。 重要的事情说三遍：不要去用一个没有经过严酷环境考验的自己写的分布式锁不要去用一个没有经过严酷环境考验的自己写的分布式锁不要去用一个没有经过严酷环境考验的自己写的分布式锁]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>分布式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式系统进阶(选举,多数派,租约)]]></title>
    <url>%2F2019%2F05%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%BF%9B%E9%98%B6(%E9%80%89%E4%B8%BE%2C%E5%A4%9A%E6%95%B0%E6%B4%BE%2C%E7%A7%9F%E7%BA%A6)%2F</url>
    <content type="text"><![CDATA[选举(election)是分布式系统实践中常见的问题，通过打破节点间的对等关系，选得的leader(或叫master、coordinator)有助于实现事务原子性、提升决议效率。 多数派(quorum)的思路帮助我们在网络分化的情况下达成决议一致性，在leader选举的场景下帮助我们选出唯一leader。租约(lease)在一定期限内给予节点特定权利，也可以用于实现leader选举。 下面我们就来学习分布式系统理论中的选举、多数派和租约。 选举(election)一致性问题(consistency)是独立的节点间如何达成决议的问题，选出大家都认可的leader本质上也是一致性问题，因而如何应对宕机恢复、网络分化等在leader选举中也需要考量。 Bully算法[1]是最常见的选举算法，其要求每个节点对应一个序号，序号最高的节点为leader。leader宕机后次高序号的节点被重选为leader，过程如下： (a). 节点4发现leader不可达，向序号比自己高的节点发起重新选举，重新选举消息中带上自己的序号 (b)(c). 节点5、6接收到重选信息后进行序号比较，发现自身的序号更大，向节点4返回OK消息并各自向更高序号节点发起重新选举 (d). 节点5收到节点6的OK消息，而节点6经过超时时间后收不到更高序号节点的OK消息，则认为自己是leader (e). 节点6把自己成为leader的信息广播到所有节点 Bully算法中有2PC的身影，都具有提议(propose)和收集反馈(vote)的过程。 在一致性算法Paxos、ZAB[2]、Raft[3]中，为提升决议效率均有节点充当leader的角色。ZAB、Raft中描述了具体的leader选举实现，与Bully算法类似ZAB中使用zxid标识节点，具有最大zxid的节点表示其所具备的事务(transaction)最新、被选为leader。 多数派(quorum)在网络分化的场景下以上Bully算法会遇到一个问题，被分隔的节点都认为自己具有最大的序号、将产生多个leader，这时候就需要引入多数派(quorum)[4]。多数派的思路在分布式系统中很常见，其确保网络分化情况下决议唯一。 多数派的原理说起来很简单，假如节点总数为2f+1，则一项决议得到多于 f 节点赞成则获得通过。leader选举中，网络分化场景下只有具备多数派节点的部分才可能选出leader，这避免了多leader的产生。 多数派的思路还被应用于副本(replica)管理，根据业务实际读写比例调整写副本数Vw、读副本数Vr，用以在可靠性和性能方面取得平衡[5]。 租约(lease)选举中很重要的一个问题，以上尚未提到：怎么判断leader不可用、什么时候应该发起重新选举？最先可能想到会通过心跳(heart beat)判别leader状态是否正常，但在网络拥塞或瞬断的情况下，这容易导致出现双主。 租约(lease)是解决该问题的常用方法，其最初提出时用于解决分布式缓存一致性问题[6]，后面在分布式锁[7]等很多方面都有应用。 租约的原理同样不复杂，中心思想是每次租约时长内只有一个节点获得租约、到期后必须重新颁发租约。假设我们有租约颁发节点Z，节点0、1和2竞选leader，租约过程如下： (a). 节点0、1、2在Z上注册自己，Z根据一定的规则(例如先到先得)颁发租约给节点，该租约同时对应一个有效时长；这里假设节点0获得租约、成为leader (b). leader宕机时，只有租约到期(timeout)后才重新发起选举，这里节点1获得租约、成为leader 租约机制确保了一个时刻最多只有一个leader，避免只使用心跳机制产生双主的问题。在实践应用中，zookeeper、ectd可用于租约颁发。 小结在分布式系统理论和实践中，常见leader、quorum和lease的身影。分布式系统内不一定事事协商、事事民主，leader的存在有助于提升决议效率。 本文以leader选举作为例子引入和讲述quorum、lease，当然quorum和lease是两种思想，并不限于leader选举应用。 最后提一个有趣的问题与大家思考，leader选举的本质是一致性问题，Paxos、Raft和ZAB等解决一致性问题的协议和算法本身又需要或依赖于leader，怎么理解这个看似“蛋生鸡、鸡生蛋”的问题？[8] 引用列表[1] Elections in a Distributed Computing System, Hector Garcia-Molina, 1982 [2] ZooKeeper’s atomic broadcast protocol: Theory and practice, Andre Medeiros, 2012 [3] In Search of an Understandable Consensus Algorithm, Diego Ongaro and John Ousterhout, 2013 [4] A quorum-based commit protocol, Dale Skeen, 1982 [5] Weighted Voting for Replicated Data, David K. Gifford, 1979 [6] Leases: An Efficient Fault-Tolerant Mechanism for Distributed File Cache Consistency, Cary G. Gray and David R. Cheriton, 1989 [7] The Chubby lock service for loosely-coupled distributed systems, Mike Burrows, 2006 [8] Why is Paxos leader election not done using Paxos?]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>选举,多数派,租约</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式系统进阶(Raft,Zab)]]></title>
    <url>%2F2019%2F05%2F08%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%BF%9B%E9%98%B6(Raft%2CZab)%2F</url>
    <content type="text"><![CDATA[引言《分布式系统理论进阶 - Paxos》介绍了一致性协议Paxos，今天我们来学习另外两个常见的一致性协议——Raft和Zab。通过与Paxos对比，了解Raft和Zab的核心思想、加深对一致性协议的认识。 RaftPaxos偏向于理论、对如何应用到工程实践提及较少。理解的难度加上现实的骨感，在生产环境中基于Paxos实现一个正确的分布式系统非常难[1]： There are significant gaps between the description of the Paxos algorithm and the needs of a real-world system. In order to build a real-world system, an expert needs to use numerous ideas scattered in the literature and make several relatively small protocol extensions. The cumulative effort will be substantial and the final system will be based on an unproven protocol. Raft[2][3]在2013年提出，提出的时间虽然不长，但已经有很多系统基于Raft实现。相比Paxos，Raft的买点就是更利于理解、更易于实行。 为达到更容易理解和实行的目的，Raft将问题分解和具体化：Leader统一处理变更操作请求，一致性协议的作用具化为保证节点间操作日志副本(log replication)一致，以term作为逻辑时钟(logical clock)保证时序，节点运行相同状态机(state machine)[4]得到一致结果。Raft协议具体过程如下： Client发起请求，每一条请求包含操作指令 请求交由Leader处理，Leader将操作指令(entry)追加(append)至操作日志，紧接着对Follower发起AppendEntries请求、尝试让操作日志副本在Follower落地 如果Follower多数派(quorum)同意AppendEntries请求，Leader进行commit操作、把指令交由状态机处理 状态机处理完成后将结果返回给Client 指令通过log index(指令id)和term number保证时序，正常情况下Leader、Follower状态机按相同顺序执行指令，得出相同结果、状态一致。 宕机、网络分化等情况可引起Leader重新选举(每次选举产生新Leader的同时，产生新的term)、Leader/Follower间状态不一致。Raft中Leader为自己和所有Follower各维护一个nextIndex值，其表示Leader紧接下来要处理的指令id以及将要发给Follower的指令id，LnextIndex不等于FnextIndex时代表Leader操作日志和Follower操作日志存在不一致，这时将从Follower操作日志中最初不一致的地方开始，由Leader操作日志覆盖Follower，直到LnextIndex、FnextIndex相等。 Paxos中Leader的存在是为了提升决议效率，Leader的有无和数目并不影响决议一致性，Raft要求具备唯一Leader，并把一致性问题具体化为保持日志副本的一致性，以此实现相较Paxos而言更容易理解、更容易实现的目标。 ZabZab[5][6]的全称是Zookeeper atomic broadcast protocol，是Zookeeper内部用到的一致性协议。相比Paxos，Zab最大的特点是保证强一致性(strong consistency，或叫线性一致性linearizable consistency)。 和Raft一样，Zab要求唯一Leader参与决议，Zab可以分解成discovery、sync、broadcast三个阶段： discovery: 选举产生PL(prospective leader)，PL收集Follower epoch(cepoch)，根据Follower的反馈PL产生newepoch(每次选举产生新Leader的同时产生新epoch，类似Raft的term) sync: PL补齐相比Follower多数派缺失的状态、之后各Follower再补齐相比PL缺失的状态，PL和Follower完成状态同步后PL变为正式Leader(established leader) broadcast: Leader处理Client的写操作，并将状态变更广播至Follower，Follower多数派通过之后Leader发起将状态变更落地(deliver/commit) Leader和Follower之间通过心跳判别健康状态，正常情况下Zab处在broadcast阶段，出现Leader宕机、网络隔离等异常情况时Zab重新回到discovery阶段。 了解完Zab的基本原理，我们再来看Zab怎样保证强一致性，Zab通过约束事务先后顺序达到强一致性，先广播的事务先commit、FIFO，Zab称之为primary order(以下简称PO)。实现PO的核心是zxid。 Zab中每个事务对应一个zxid，它由两部分组成：&lt;e, c&gt;，e即Leader选举时生成的epoch，c表示当次epoch内事务的编号、依次递增。假设有两个事务的zxid分别是z、z’，当满足 z.e &lt; z’.e 或者 z.e = z’.e &amp;&amp; z.c &lt; z’.c 时，定义z先于z’发生(z &lt; z’)。 为实现PO，Zab对Follower、Leader有以下约束： 有事务z和z’，如果Leader先广播z，则Follower需保证先commit z对应的事务 有事务z和z’，z由Leader p广播，z’由Leader q广播，Leader p先于Leader q，则Follower需保证先commit z对应的事务 有事务z和z’，z由Leader p广播，z’由Leader q广播，Leader p先于Leader q，如果Follower已经commit z，则q需保证已commit z才能广播z’ 第1、2点保证事务FIFO，第3点保证Leader上具备所有已commit的事务。 相比Paxos，Zab约束了事务顺序、适用于有强一致性需求的场景。 Paxos、Raft、Zab再比较除Paxos、Raft和Zab外，Viewstamped Replication(简称VR)[7/8]也是讨论比较多的一致性协议。这些协议包含很多共同的内容(Leader、quorum、state machine等)，因而我们不禁要问：Paxos、Raft、Zab和VR等分布式一致性协议区别到底在哪，还是根本就是一回事？[9] Paxos、Raft、Zab和VR都是解决一致性问题的协议，Paxos协议原文倾向于理论，Raft、Zab、VR倾向于实践，一致性保证程度等的不同也导致这些协议间存在差异。下图帮助我们理解这些协议的相似点和区别[10]： 相比Raft、Zab、VR，Paxos更纯粹、更接近一致性问题本源，尽管Paxos倾向理论，但不代表Paxos不能应用于工程。基于Paxos的工程实践，须考虑具体需求场景(如一致性要达到什么程度)，再在Paxos原始语意上进行包装。 小结以上介绍分布式一致性协议Raft、Zab的核心思想，分析Raft、Zab与Paxos的异同。实现分布式系统时，先从具体需求和场景考虑，Raft、Zab、VR、Paxos等协议没有绝对地好与不好，只是适不适合。 引用列表[1] Paxos made live - An engineering perspective, Tushar Chandra, Robert Griesemer and Joshua Redstone, 2007 [2] In Search of an Understandable Consensus Algorithm, Diego Ongaro and John Ousterhout, 2013 [3] In Search of an Understandable Consensus Algorithm (Extended Version), Diego Ongaro and John Ousterhout, 2013 [4] Implementing Fault-Tolerant Services Using the State Machine, Fred B. Schneider, 1990 [5] Zab:High-performance broadcast for primary-backup systems, FlavioP.Junqueira,BenjaminC.Reed,andMarcoSeraﬁni, 2011 [6] ZooKeeper’s atomic broadcast protocol: Theory and practice, Andr´e Medeiros, 2012 [7] Viewstamped Replication A New Primary Copy Method to Support Highly-Available Distributed Systems, Brian M.Oki and Barbar H.Liskov, 1988 [8] Viewstamped Replication Revisited, Barbara Liskov and James Cowling, Barbara Liskov and James Cowling ,2012 [9] Can’t we all just agree? The morning paper, 2015 [10] Vive La Difference: Paxos vs. Viewstamped Replication vs. Zab, Robbert van Renesse, Nicolas Schiper and Fred B. Schneider, 2014]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>Raft,Zab</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式系统进阶(Paxos)]]></title>
    <url>%2F2019%2F05%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E8%BF%9B%E9%98%B6(Paxos)%2F</url>
    <content type="text"><![CDATA[引言《分布式系统理论基础 - 一致性、2PC和3PC》一文介绍了一致性、达成一致性需要面临的各种问题以及2PC、3PC模型，Paxos协议在节点宕机恢复、消息无序或丢失、网络分化的场景下能保证决议的一致性，是被讨论最广泛的一致性协议。 Paxos协议同时又以其“艰深晦涩”著称，下面结合 Paxos Made Simple、The Part-Time Parliament 两篇论文，尝试通过Paxos推演、学习和了解Paxos协议。 Basic Paxos何为一致性问题？简单而言，一致性问题是在节点宕机、消息无序等场景可能出现的情况下，相互独立的节点之间如何达成决议的问题，作为解决一致性问题的协议，Paxos的核心是节点间如何确定并只确定一个值(value)。 也许你会疑惑只确定一个值能起什么作用，在Paxos协议里确定并只确定一个值是确定多值的基础，如何确定多值将在第二部分Multi Paxos中介绍，这部分我们聚焦在“Paxos如何确定并只确定一个值”这一问题上。 和2PC类似，Paxos先把节点分成两类，发起提议(proposal)的一方为proposer，参与决议的一方为acceptor。假如只有一个proposer发起提议，并且节点不宕机、消息不丢包，那么acceptor做到以下这点就可以确定一个值：1P1. 一个acceptor接受它收到的第一项提议 当然上面要求的前提条件有些严苛，节点不能宕机、消息不能丢包，还只能由一个proposer发起提议。我们尝试放宽条件，假设多个proposer可以同时发起提议，又怎样才能做到确定并只确定一个值呢？ 首先proposer和acceptor需要满足以下两个条件： proposer发起的每项提议分别用一个ID标识，提议的组成因此变为(ID, value) acceptor可以接受(accept)不止一项提议，当多数(quorum) acceptor接受一项提议时该提议被确定(chosen) (注: 注意以上“接受”和“确定”的区别）我们约定后面发起的提议的ID比前面提议的ID大，并假设可以有多项提议被确定，为做到确定并只确定一个值acceptor要做到以下这点： 1P2. 如果一项值为v的提议被确定，那么后续只确定值为v的提议 (注: 乍看这个条件不太好理解，谨记目标是“确定并只确定一个值”) 由于一项提议被确定(chosen)前必须先被多数派acceptor接受(accepted)，为实现P2，实质上acceptor需要做到：1P2a. 如果一项值为v的提议被确定，那么acceptor后续只接受值为v的提议 满足P2a则P2成立 (P2a =&gt; P2)。 目前在多个proposer可以同时发起提议的情况下，满足P1、P2a即能做到确定并只确定一个值。如果再加上节点宕机恢复、消息丢包的考量呢？ 假设acceptor c 宕机一段时间后恢复，c 宕机期间其他acceptor已经确定了一项值为v的决议但c 因为宕机并不知晓；c 恢复后如果有proposer马上发起一项值不是v的提议，由于条件P1，c 会接受该提议，这与P2a矛盾。为了避免这样的情况出现，进一步地我们对proposer作约束： 1P2b. 如果一项值为v的提议被确定，那么proposer后续只发起值为v的提议 满足P2b则P2a成立 (P2b =&gt; P2a =&gt; P2)。 P2b约束的是提议被确定(chosen)后proposer的行为，我们更关心提议被确定前proposer应该怎么做：1P2c. 对于提议(n,v)，acceptor的多数派S中，如果存在acceptor最近一次(即ID值最大)接受的提议的值为v&apos;，那么要求v = v&apos;；否则v可为任意值 满足P2c则P2b成立 (P2c =&gt; P2b =&gt; P2a =&gt; P2)。 条件P2c是Basic Paxos的核心，光看P2c的描述可能会觉得一头雾水，我们通过 The Part-Time Parliament 中的例子加深理解： 假设有A~E 5个acceptor，- 表示acceptor因宕机等原因缺席当次决议，x 表示acceptor不接受提议，o 表示接受提议；多数派acceptor接受提议后提议被确定，以上表格对应的决议过程如下： 1.ID为2的提议最早提出，根据P2c其提议值可为任意值，这里假设为a2.acceptor A/B/C/E 在之前的决议中没有接受(accept)任何提议，因而ID为5的提议的值也可以为任意值，这里假设为b3.acceptor B/D/E，其中D曾接受ID为2的提议，根据P2c，该轮ID为14的提议的值必须与ID为2的提议的值相同，为a4.acceptor A/C/D，其中D曾接受ID为2的提议、C曾接受ID为5的提议，相比之下ID 5较ID 2大，根据P2c，该轮ID为27的提议的值必须与ID为5的提议的值相同，为b；该轮决议被多数派acceptor接受，因此该轮决议得以确定5.acceptor B/C/D，3个acceptor之前都接受过提议，相比之下C、D曾接受的ID 27的ID号最大，该轮ID为29的提议的值必须与ID为27的提议的值相同，为b 以上提到的各项约束条件可以归纳为3点，如果proposer/acceptor满足下面3点，那么在少数节点宕机、网络分化隔离的情况下，在“确定并只确定一个值”这件事情上可以保证一致性(consistency)：B1(ß): ß中每一轮决议都有唯一的ID标识B2(ß): 如果决议B被acceptor多数派接受，则确定决议BB3(ß): 对于ß中的任意提议B(n,v)，acceptor的多数派中如果存在acceptor最近一次(即ID值最大)接受的提议的值为v’，那么要求v = v’；否则v可为任意值 (注: 希腊字母ß表示多轮决议的集合，字母B表示一轮决议) 另外为保证P2c，我们对acceptor作两个要求： 记录曾接受的ID最大的提议，因proposer需要问询该信息以决定提议值 在回应提议ID为n的proposer自己曾接受过ID最大的提议时，acceptor同时保证(promise)不再接受ID小于n的提议 至此，proposer/acceptor完成一轮决议可归纳为prepare和accept两个阶段。prepare阶段proposer发起提议问询提议值、acceptor回应问询并进行promise；accept阶段完成决议，图示如下： 还有一个问题需要考量，假如proposer A发起ID为n的提议，在提议未完成前proposer B又发起ID为n+1的提议，在n+1提议未完成前proposer C又发起ID为n+2的提议…… 如此acceptor不能完成决议、形成活锁(livelock)，虽然这不影响一致性，但我们一般不想让这样的情况发生。解决的方法是从proposer中选出一个leader，提议统一由leader发起。 最后我们再引入一个新的角色：learner，learner依附于acceptor，用于习得已确定的决议。以上决议过程都只要求acceptor多数派参与，而我们希望尽量所有acceptor的状态一致。如果部分acceptor因宕机等原因未知晓已确定决议，宕机恢复后可经本机learner采用pull的方式从其他acceptor习得。 Multi Paxos通过以上步骤分布式系统已经能确定一个值，“只确定一个值有什么用？这可解决不了我面临的问题。” 你心中可能有这样的疑问。 其实不断地进行“确定一个值”的过程、再为每个过程编上序号，就能得到具有全序关系(total order)的系列值，进而能应用在数据库副本存储等很多场景。我们把单次“确定一个值”的过程称为实例(instance)，它由proposer/acceptor/learner组成，下图说明了A/B/C三机上的实例： 不同序号的实例之间互相不影响，A/B/C三机输入相同、过程实质等同于执行相同序列的状态机(state machine)指令 ，因而将得到一致的结果。 proposer leader在Multi Paxos中还有助于提升性能，常态下统一由leader发起提议，可节省prepare步骤(leader不用问询acceptor曾接受过的ID最大的提议、只有leader提议也不需要acceptor进行promise)直至发生leader宕机、重新选主。 小结以上介绍了Paxos的推演过程、如何在Basic Paxos的基础上通过状态机构建Multi Paxos。Paxos协议比较“艰深晦涩”，但多读几遍论文一般能理解其内涵，更难的是如何将Paxos真正应用到工程实践。 31 13:12:09后台开发同学实现并开源了一套基于Paxos协议的多机状态拷贝类库PhxPaxos，PhxPaxos用于将单机服务扩展到多机，其经过线上系统验证并在一致性保证、性能等方面作了很多考量。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>Paxos</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式系统基础(时间,时钟,事件顺序)]]></title>
    <url>%2F2019%2F05%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80(%E6%97%B6%E9%97%B4%2C%E6%97%B6%E9%92%9F%2C%E4%BA%8B%E4%BB%B6%E9%A1%BA%E5%BA%8F)%2F</url>
    <content type="text"><![CDATA[现实生活中时间是很重要的概念，时间可以记录事情发生的时刻、比较事情发生的先后顺序。分布式系统的一些场景也需要记录和比较不同节点间事件发生的顺序，但不同于日常生活使用物理时钟记录时间，分布式系统使用逻辑时钟记录事件顺序关系，下面我们来看分布式系统中几种常见的逻辑时钟。 物理时钟 vs 逻辑时钟可能有人会问，为什么分布式系统不使用物理时钟(physical clock)记录事件？每个事件对应打上一个时间戳，当需要比较顺序的时候比较相应时间戳就好了。 这是因为现实生活中物理时间有统一的标准，而分布式系统中每个节点记录的时间并不一样，即使设置了 NTP 时间同步节点间也存在毫秒级别的偏差。因而分布式系统需要有另外的方法记录事件顺序关系，这就是逻辑时钟(logical clock)。 Lamport timestampsLeslie Lamport 在1978年提出逻辑时钟的概念，并描述了一种逻辑时钟的表示方法，这个方法被称为Lamport时间戳(Lamport timestamps)。 分布式系统中按是否存在节点交互可分为三类事件，一类发生于节点内部，二是发送事件，三是接收事件。Lamport时间戳原理如下： 图1: Lamport timestamps space time (图片来源: wikipedia) 每个事件对应一个Lamport时间戳，初始值为0如果事件在节点内发生，时间戳加1如果事件属于发送事件，时间戳加1并在消息中带上该时间戳如果事件属于接收事件，时间戳 = Max(本地时间戳，消息中的时间戳) + 1 假设有事件a、b，C(a)、C(b)分别表示事件a、b对应的Lamport时间戳，如果C(a) &lt; C(b)，则有a发生在b之前(happened before)，记作 a -&gt; b，例如图1中有 C1 -&gt; B1。通过该定义，事件集中Lamport时间戳不等的事件可进行比较，我们获得事件的偏序关系(partial order)。 如果C(a) = C(b)，那a、b事件的顺序又是怎样的？假设a、b分别在节点P、Q上发生，Pi、Qj分别表示我们给P、Q的编号，如果 C(a) = C(b) 并且 Pi &lt; Qj，同样定义为a发生在b之前，记作 a =&gt; b。假如我们对图1的A、B、C分别编号Ai = 1、Bj = 2、Ck = 3，因 C(B4) = C(C3) 并且 Bj &lt; Ck，则 B4 =&gt; C3。 通过以上定义，我们可以对所有事件排序、获得事件的全序关系(total order)。上图例子，我们可以从C1到A4进行排序。 Vector clockLamport时间戳帮助我们得到事件顺序关系，但还有一种顺序关系不能用Lamport时间戳很好地表示出来，那就是同时发生关系(concurrent)。例如图1中事件B4和事件C3没有因果关系，属于同时发生事件，但Lamport时间戳定义两者有先后顺序。 Vector clock是在Lamport时间戳基础上演进的另一种逻辑时钟方法，它通过vector结构不但记录本节点的Lamport时间戳，同时也记录了其他节点的Lamport时间戳。Vector clock的原理与Lamport时间戳类似，使用图例如下： 图2: Vector clock space time (图片来源: wikipedia) 假设有事件a、b分别在节点P、Q上发生，Vector clock分别为Ta、Tb，如果 Tb[Q] &gt; Ta[Q] 并且 Tb[P] &gt;= Ta[P]，则a发生于b之前，记作 a -&gt; b。到目前为止还和Lamport时间戳差别不大，那Vector clock怎么判别同时发生关系呢？ 如果 Tb[Q] &gt; Ta[Q] 并且 Tb[P] &lt; Ta[P]，则认为a、b同时发生，记作 a b。例如图2中节点B上的第4个事件 (A:2，B:4，C:1) 与节点C上的第2个事件 (B:3，C:2) 没有因果关系、属于同时发生事件。 Version vector基于Vector clock我们可以获得任意两个事件的顺序关系，结果或为先后顺序或为同时发生，识别事件顺序在工程实践中有很重要的引申应用，最常见的应用是发现数据冲突(detect conflict)。 分布式系统中数据一般存在多个副本(replication)，多个副本可能被同时更新，这会引起副本间数据不一致，Version vector的实现与Vector clock非常类似[8]，目的用于发现数据冲突。下面通过一个例子说明Version vector的用法： 图3: Version vector client端写入数据，该请求被Sx处理并创建相应的vector ([Sx, 1])，记为数据D1 第2次请求也被Sx处理，数据修改为D2，vector修改为([Sx, 2]) 第3、第4次请求分别被Sy、Sz处理，client端先读取到D2，然后D3、D4被写入Sy、Sz 第5次更新时client端读取到D2、D3和D4 3个数据版本，通过类似Vector clock判断同时发生关系的方法可判断D3、D4存在数据冲突，最终通过一定方法解决数据冲突并写入D5 Vector clock只用于发现数据冲突，不能解决数据冲突。如何解决数据冲突因场景而异，具体方法有以最后更新为准(last write win)，或将冲突的数据交给client由client端决定如何处理，或通过quorum决议事先避免数据冲突的情况发生。 由于记录了所有数据在所有节点上的逻辑时钟信息，Vector clock和Version vector在实际应用中可能面临的一个问题是vector过大，用于数据管理的元数据(meta data)甚至大于数据本身。 解决该问题的方法是使用server id取代client id创建vector (因为server的数量相对client稳定)，或设定最大的size、如果超过该size值则淘汰最旧的vector信息。 小结 以上介绍了分布式系统里逻辑时钟的表示方法，通过Lamport timestamps可以建立事件的全序关系，通过Vector clock可以比较任意两个事件的顺序关系并且能表示无因果关系的事件，将Vector clock的方法用于发现数据版本冲突，于是有了Version vector。 参考资料：[1] Time is an illusion, George Neville-Neil, 2016 [2] There is No Now, Justin Sheehy, 2015 [3] Time, Clocks, and the Ordering of Events in a Distributed System, Leslie Lamport, 1978 [4] Timestamps in Message-Passing Systems That Preserve the Partial Ordering, Colin J. Fidge, 1988 [5] Virtual Time and Global States of Distributed Systems, Friedemann Mattern, 1988 [6] Why Vector Clocks are Easy, Bryan Fink, 2010 [7] Conflict Management, CouchDB [8] Version Vectors are not Vector Clocks, Carlos Baquero, 2011 [9] Detection of Mutual Inconsistency in Distributed Systems, IEEE Transactions on Software Engineering , 1983 [10] Dynamo: Amazon’s Highly Available Key-value Store, Amazon, 2007 [11] Conflict Resolution, Jeff Darcy , 2010 [12] Why Vector Clocks Are Hard, Justin Sheehy, 2010 [13] Causality Is Expensive (and What To Do About It), Peter Bailis ,2014]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>时间,时钟,事件顺序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式系统基础(CAP)]]></title>
    <url>%2F2019%2F05%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80(CAP)%2F</url>
    <content type="text"><![CDATA[引言CAP是分布式系统、特别是分布式存储领域中被讨论最多的理论，“什么是CAP定理？”在Quora 分布式系统分类下排名 FAQ 的 No.1。CAP在程序员中也有较广的普及，它不仅仅是“C、A、P不能同时满足，最多只能3选2”，以下尝试综合各方观点，从发展历史、工程实践等角度讲述CAP理论。希望大家透过本文对CAP理论有更多地了解和认识。 CAP定理CAP由Eric Brewer在2000年PODC会议上提出[1][2]，是Eric Brewer在Inktomi[3]期间研发搜索引擎、分布式web缓存时得出的关于数据一致性(consistency)、服务可用性(availability)、分区容错性(partition-tolerance)的猜想：It is impossible for a web service to provide the three following guarantees : Consistency, Availability and Partition-tolerance. 该猜想在提出两年后被证明成立[4]，成为我们熟知的CAP定理： 数据一致性(consistency)：如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性(strong consistency) (又叫原子性 atomic、线性一致性 linearizable consistency) 服务可用性(availability)：所有读写请求在一定时间内得到响应，可终止、不会一直等待 分区容错性(partition-tolerance)：在网络分区的情况下，被分隔的节点仍能正常对外服务 在某时刻如果满足AP，分隔的节点同时对外服务但不能相互通信，将导致状态不一致，即不能满足C；如果满足CP，网络分区的情况下为达成C，请求只能一直等待，即不满足A；如果要满足CA，在一定时间内要达到节点状态一致，要求不能出现网络分区，则不能满足P。 C、A、P三者最多只能满足其中两个，和FLP定理一样，CAP定理也指示了一个不可达的结果(impossibility result)。 CAP的工程启示CAP理论提出7、8年后，NoSql圈将CAP理论当作对抗传统关系型数据库的依据、阐明自己放宽对数据一致性(consistency)要求的正确性[6]，随后引起了大范围关于CAP理论的讨论。 CAP理论看似给我们出了一道3选2的选择题，但在工程实践中存在很多现实限制条件，需要我们做更多地考量与权衡，避免进入CAP认识误区[7]。 1、关于 P 的理解Partition字面意思是网络分区，即因网络因素将系统分隔为多个单独的部分，有人可能会说，网络分区的情况发生概率非常小啊，是不是不用考虑P，保证CA就好。要理解P，我们看回CAP证明中P的定义：In order to model partition tolerance, the network will be allowed to lose arbitrarily many messages sent from one node to another. 网络分区的情况符合该定义，网络丢包的情况也符合以上定义，另外节点宕机，其他节点发往宕机节点的包也将丢失，这种情况同样符合定义。现实情况下我们面对的是一个不可靠的网络、有一定概率宕机的设备，这两个因素都会导致Partition，因而分布式系统实现中 P 是一个必须项，而不是可选项。 对于分布式系统工程实践，CAP理论更合适的描述是：在满足分区容错的前提下，没有算法能同时满足数据一致性和服务可用性[11]：In a network subject to communication failures, it is impossible for any web service to implement an atomic read/write shared memory that guarantees a response to every request. 2、CA非0/1的选择CAP定理证明中的一致性指强一致性，强一致性要求多节点组成的被调要能像单节点一样运作、操作具备原子性，数据在时间、时序上都有要求。如果放宽这些要求，还有其他一致性类型： 序列一致性(sequential consistency)：不要求时序一致，A操作先于B操作，在B操作后如果所有调用端读操作得到A操作的结果，满足序列一致性 最终一致性(eventual consistency)：放宽对时间的要求，在被调完成操作响应后的某个时间点，被调多个节点的数据最终达成一致 可用性在CAP定理里指所有读写操作必须要能终止，实际应用中从主调、被调两个不同的视角，可用性具有不同的含义。当P(网络分区)出现时，主调可以只支持读操作，通过牺牲部分可用性达成数据一致。 工程实践中，较常见的做法是通过异步拷贝副本(asynchronous replication)、quorum/NRW，实现在调用端看来数据强一致、被调端最终一致，在调用端看来服务可用、被调端允许部分节点不可用(或被网络分隔)的效果。 3、跳出CAPCAP理论对实现分布式系统具有指导意义，但CAP理论并没有涵盖分布式工程实践中的所有重要因素。 例如延时(latency)，它是衡量系统可用性、与用户体验直接相关的一项重要指标。CAP理论中的可用性要求操作能终止、不无休止地进行，除此之外，我们还关心到底需要多长时间能结束操作，这就是延时，它值得我们设计、实现分布式系统时单列出来考虑。 延时与数据一致性也是一对“冤家”，如果要达到强一致性、多个副本数据一致，必然增加延时。加上延时的考量，我们得到一个CAP理论的修改版本PACELC：如果出现P(网络分区)，如何在A(服务可用性)、C(数据一致性)之间选择；否则，如何在L(延时)、C(数据一致性)之间选择。 小结以上介绍了CAP理论的源起和发展，介绍了CAP理论给分布式系统工程实践带来的启示。 CAP理论对分布式系统实现有非常重大的影响，我们可以根据自身的业务特点，在数据一致性和服务可用性之间作出倾向性地选择。通过放松约束条件，我们可以实现在不同时间点满足CAP(此CAP非CAP定理中的CAP，如C替换为最终一致性)。 有非常非常多文章讨论和研究CAP理论，希望这篇对你认识和了解CAP理论有帮助。 [1] Harvest, Yield, and Scalable Tolerant Systems, Armando Fox , Eric Brewer, 1999 [2] Towards Robust Distributed Systems, Eric Brewer, 2000 [3] Inktomi’s wild ride - A personal view of the Internet bubble, Eric Brewer, 2004 [4] Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant Web, Seth Gilbert, Nancy Lynch, 2002 [5] Linearizability: A Correctness Condition for Concurrent Objects, Maurice P. Herlihy,Jeannette M. Wing, 1990 [6] Brewer’s CAP Theorem - The kool aid Amazon and Ebay have been drinking, Julian Browne, 2009 [7] CAP Theorem between Claims and Misunderstandings: What is to be Sacrificed?, Balla Wade Diack,Samba Ndiaye,Yahya Slimani, 2013 [8] Errors in Database Systems, Eventual Consistency, and the CAP Theorem, Michael Stonebraker, 2010 [9] CAP Confusion: Problems with ‘partition tolerance’, Henry Robinson, 2010 [10] You Can’t Sacrifice Partition Tolerance, Coda Hale, 2010 [11] Perspectives on the CAP Theorem, Seth Gilbert, Nancy Lynch, 2012 [12] CAP Twelve Years Later: How the “Rules” Have Changed, Eric Brewer, 2012 [13] How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs, Lamport Leslie, 1979 [14] Eventual Consistent Databases: State of the Art, Mawahib Elbushra , Jan Lindström, 2014 [15] Eventually Consistent, Werner Vogels, 2008 [16] Speed Matters for Google Web Search, Jake Brutlag, 2009 [17] Consistency Tradeoffs in Modern Distributed Database System Design, Daniel J. Abadi, 2012 [18] A CAP Solution (Proving Brewer Wrong), Guy’s blog, 2008 [19] How to beat the CAP theorem, nathanmarz , 2011 [20] The CAP FAQ, Henry Robinson]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>CAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式系统基础(一致性,2PC,3PC)]]></title>
    <url>%2F2019%2F05%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%9F%BA%E7%A1%80(%E4%B8%80%E8%87%B4%E6%80%A7%2C2PC%2C3PC)%2F</url>
    <content type="text"><![CDATA[引言狭义的分布式系统指由网络连接的计算机系统，每个节点独立地承担计算或存储任务，节点间通过网络协同工作。 广义的分布式系统是一个相对的概念，正如Leslie Lamport所说[1]： What is a distributed systeme. Distribution is in the eye of the beholder.To the user sitting at the keyboard, his IBM personal computer is a nondistributed system. To a flea crawling around on the circuit board, or to the engineer who designed it, it’s very much a distributed system. 一致性是分布式理论中的根本性问题，近半个世纪以来，科学家们围绕着一致性问题提出了很多理论模型，依据这些理论模型，业界也出现了很多工程实践投影。下面我们从一致性问题、特定条件下解决一致性问题的两种方法(2PC、3PC)入门，了解最基础的分布式系统理论。 一致性(consensus)何为一致性问题？简单而言，一致性问题就是相互独立的节点之间如何达成一项决议的问题。分布式系统中，进行数据库事务提交(commit transaction)、Leader选举、序列号生成等都会遇到一致性问题。这个问题在我们的日常生活中也很常见，比如牌友怎么商定几点在哪打几圈麻将： 假设一个具有N个节点的分布式系统，当其满足以下条件时，我们说这个系统满足一致性： 全认同(agreement): 所有N个节点都认同一个结果 值合法(validity): 该结果必须由N个节点中的节点提出 可结束(termination): 决议过程在一定时间内结束，不会无休止地进行下去 有人可能会说，决定什么时候在哪搓搓麻将，4个人商量一下就ok，这不很简单吗？ 但就这样看似简单的事情，分布式系统实现起来并不轻松，因为它面临着这些问题： 消息传递异步无序(asynchronous): 现实网络不是一个可靠的信道，存在消息延时、丢失，节点间消息传递做不到同步有序(synchronous) 节点宕机(fail-stop): 节点持续宕机，不会恢复 节点宕机恢复(fail-recover): 节点宕机一段时间后恢复，在分布式系统中最常见 网络分化(network partition): 网络链路出现问题，将N个节点隔离成多个部分 拜占庭将军问题(byzantine failure): 节点或宕机或逻辑失败，甚至不按套路出牌抛出干扰决议的信息 假设现实场景中也存在这样的问题，我们看看结果会怎样： 还能不能一起愉快地玩耍… 我们把以上所列的问题称为系统模型(system model)，讨论分布式系统理论和工程实践的时候，必先划定模型。例如有以下两种模型： 异步环境(asynchronous)下，节点宕机(fail-stop)异步环境(asynchronous)下，节点宕机恢复(fail-recover)、网络分化(network partition)2比1多了节点恢复、网络分化的考量，因而对这两种模型的理论研究和工程解决方案必定是不同的，在还没有明晰所要解决的问题前谈解决方案都是一本正经地耍流氓。 一致性还具备两个属性，一个是强一致(safety)，它要求所有节点状态一致、共进退；一个是可用(liveness)，它要求分布式系统24x7无间断对外服务。FLP定理(FLP impossibility)已经证明在一个收窄的模型中(异步环境并只存在节点宕机)，不能同时满足 safety 和 liveness。 FLP定理是分布式系统理论中的基础理论，正如物理学中的能量守恒定律彻底否定了永动机的存在，FLP定理否定了同时满足safety 和 liveness 的一致性协议的存在。 工程实践上根据具体的业务场景，或保证强一致(safety)，或在节点宕机、网络分化的时候保证可用(liveness)。2PC、3PC是相对简单的解决一致性问题的协议，下面我们就来了解2PC和3PC。 2PC2PC(tow phase commit)两阶段提交[5]顾名思义它分成两个阶段，先由一方进行提议(propose)并收集其他节点的反馈(vote)，再根据反馈决定提交(commit)或中止(abort)事务。我们将提议的节点称为协调者(coordinator)，其他参与决议节点称为参与者(participants, 或cohorts)： 2PC, phase one在阶段1中，coordinator发起一个提议，分别问询各participant是否接受。 2PC, phase two在阶段2中，coordinator根据participant的反馈，提交或中止事务，如果participant全部同意则提交，只要有一个participant不同意就中止。 在异步环境(asynchronous)并且没有节点宕机(fail-stop)的模型下，2PC可以满足全认同、值合法、可结束，是解决一致性问题的一种协议。但如果再加上节点宕机(fail-recover)的考虑，2PC是否还能解决一致性问题呢？ coordinator如果在发起提议后宕机，那么participant将进入阻塞(block)状态、一直等待coordinator回应以完成该次决议。这时需要另一角色把系统从不可结束的状态中带出来，我们把新增的这一角色叫协调者备份(coordinator watchdog)。coordinator宕机一定时间后，watchdog接替原coordinator工作，通过问询(query) 各participant的状态，决定阶段2是提交还是中止。这也要求 coordinator/participant 记录(logging)历史状态，以备coordinator宕机后watchdog对participant查询、coordinator宕机恢复后重新找回状态。 从coordinator接收到一次事务请求、发起提议到事务完成，经过2PC协议后增加了2次RTT(propose+commit)，带来的时延(latency)增加相对较少。 3PC3PC(three phase commit)即三阶段提交[6][7]，既然2PC可以在异步网络+节点宕机恢复的模型下实现一致性，那还需要3PC做什么，3PC是什么鬼？ 在2PC中一个participant的状态只有它自己和coordinator知晓，假如coordinator提议后自身宕机，在watchdog启用前一个participant又宕机，其他participant就会进入既不能回滚、又不能强制commit的阻塞状态，直到participant宕机恢复。这引出两个疑问： 能不能去掉阻塞，使系统可以在commit/abort前回滚(rollback)到决议发起前的初始状态当次决议中，participant间能不能相互知道对方的状态，又或者participant间根本不依赖对方的状态 图片截取自wikipediacoordinator接收完participant的反馈(vote)之后，进入阶段2，给各个participant发送准备提交(prepare to commit)指令。participant接到准备提交指令后可以锁资源，但要求相关操作必须可回滚。coordinator接收完确认(ACK)后进入阶段3、进行commit/abort，3PC的阶段3与2PC的阶段2无异。协调者备份(coordinator watchdog)、状态记录(logging)同样应用在3PC。 participant如果在不同阶段宕机，我们来看看3PC如何应对： 阶段1: coordinator或watchdog未收到宕机participant的vote，直接中止事务；宕机的participant恢复后，读取logging发现未发出赞成vote，自行中止该次事务 阶段2: coordinator未收到宕机participant的precommit ACK，但因为之前已经收到了宕机participant的赞成反馈(不然也不会进入到阶段2)，coordinator进行commit；watchdog可以通过问询其他participant获得这些信息，过程同理；宕机的participant恢复后发现收到precommit或已经发出赞成vote，则自行commit该次事务 阶段3: 即便coordinator或watchdog未收到宕机participant的commit ACK，也结束该次事务；宕机的participant恢复后发现收到commit或者precommit，也将自行commit该次事务 因为有了准备提交(prepare to commit)阶段，3PC的事务处理延时也增加了1个RTT，变为3个RTT(propose+precommit+commit)，但是它防止participant宕机后整个系统进入阻塞态，增强了系统的可用性，对一些现实业务场景是非常值得的。 小结以上介绍了分布式系统理论中的部分基础知识，阐述了一致性(consensus)的定义和实现一致性所要面临的问题，最后讨论在异步网络(asynchronous)、节点宕机恢复(fail-recover)模型下2PC、3PC怎么解决一致性问题。 阅读前人对分布式系统的各项理论研究，其中有严谨地推理、证明，有一种数学的美；观现实中的分布式系统实现，是综合各种因素下妥协的结果。 参考文献[1] Solved Problems, Unsolved Problems and Problems in Concurrency, Leslie Lamport, 1983[2] The Byzantine Generals Problem, Leslie Lamport,Robert Shostak and Marshall Pease, 1982[3] Impossibility of Distributed Consensus with One Faulty Process, Fischer, Lynch and Patterson, 1985[4] FLP Impossibility的证明, Daniel Wu, 2015[5] Consensus Protocols: Two-Phase Commit, Henry Robinson, 2008[6] Consensus Protocols: Three-phase Commit, Henry Robinson, 2008[7] Three-phase commit protocol, Wikipedia]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>一致性,2PC,3PC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之分布式系统基本概念]]></title>
    <url>%2F2019%2F05%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[分布式小明的公司又3个系统：系统A，系统B和系统C，这三个系统所做的业务不同，被部署在3个独立的机器上运行，他们之间互相调用（当然是跨域网络的），通力合作完成公司的业务流程。 将不同的业务分部在不同的地方，就构成了一个分布式的系统，现在问题来了，系统A是整个分布式系统的脸面，用户直接访问，用户访问量大的时候要么是速度巨慢，要么直接挂掉，怎么办？ 由于系统A只有一份，所以会引起单点失败。。。 集群（Cluster）小明的公司不差钱，就多买几台机器吧， 小明把系统A一下子部署了好几份（例如下图的3个服务器），每一份都是系统A的一个实例，对外提供同样的服务，这样，就不怕其中一个坏掉了，还有另外两个呢。 这三个服务器的系统就组成了一个集群。 可是对用户来说，一下子出现这么多系统A，每个系统的IP地址都不一样，到底访问哪一个呢？ 如果所有人都访问服务器1.1，那服务器1.1会被累死，剩下两个闲死，成了浪费钱的摆设 负载均衡（Load Balancer）小明要尽可能的让3个机器上的系统A工作均衡一些，比如有3万个请求，那就让3个服务器各处理1万个（理想情况），这叫负载均衡 很明显，这个负载均衡的工作最好独立出来，放到独立的服务器上（例如nginx）：后来小明发现，这个负载均衡的服务器虽然工作内容简单，就是拿到请求，分发请求，但是它还是有可能挂掉，单点失败还是会出现。 没办法，只好把负载均衡也搞成一个集群，bug和系统A的集群有两点不同： 1.这个新的集群中虽然有两个机器，但是我们可以用某种办法，让这个机器对外只提供一个IP地址，也就是用户看到的好像只有一个机器。 2.同一时刻，我们只让一个负载均衡的机器工作，另外一个原地待命，如果工作的那个拐到了，待命的那个就顶上去。 4、弹性 如果3个系统A的实例还是满足不了大量请求，例如双十一，可以申请增加服务器，双十一过后，新增的服务器闲置，成了摆设，于是小明决定尝试云计算，在云端可以轻松的创建，删除虚拟的服务器，那样就可以轻松的随着用户的请求动图的增减服务器了。 5、失效转移 上面的系统看起来很美好，但是做了一个不切实际的假设： 所有的服务都是无状态的，换句话说，假设用户的两次请求直接是没有关联的。 但是现实是，大部分服务都是有状态的，例如购物车。 用户访问系统，在服务器上创建了一个购物车，并向其中加了几个商品，然后服务器1.1挂掉了，用户后续访问就找不到服务器1.1了，这时候就要做失效转移，让另外几个服务器去接管，去处理用户的请求。 可是问题来了，在服务器1.2,1.3上有用户的购物车吗？如果没有，用户就会抱怨，我刚创建的购物车哪里去了？ 还有更严重的，假设用户登录过得信息保存到了该服务器1.1上登录的，用户登录过的信息保存到了该服务器的session中，现在这个服务器挂了，用的session就不见了，会把用户踢到了登录界面，让用户再次登录！ 处理不好状态的问题，集群的威力就大打折扣，无法完成真正的失效转移，甚至无法使用。 怎么办？ 一种办法是把状态信息在集群的各个服务器之间复制，让集群的各个服务器达成一致，谁来干这个事情？只能像Webspher，Weblogic这样的应用服务器了。 还有一种办法， 就是把状态信息几种存储在一个地方，让集群服务器的各个服务器都能访问到： 小明听说Redis不错，那就用Redis来保存吧！ 认识分布式架构随着计算机系统规模变得越来越大，将所有的业务单元集中部署在一个或若干个大型机上的体系结构，已经越来越不能满足当今计算机系统，尤其是大型互联网系统的快速发展，各种灵活多变的系统架构模型层出不穷。分布式的处理方式越来越受到业界的青睐——计算机系统正在经历一场前所未有的从集中式向分布式架构的变革。 集中式与分布式集中式系统 所谓的集中式系统就是指由一台或多台主计算机组成中心节点，数据集中存储于这个中心节点中，并且整个系统的所有业务单元都集中部署在这个中心节点上，系统的所有功能均由其集中处理。 集中式系统的最大的特点就是部署结构非常简单，底层一般采用从IBM、HP等厂商购买到的昂贵的大型主机。因此无需考虑如何对服务进行多节点的部署，也就不用考虑各节点之间的分布式协作问题。但是，由于采用单机部署，很可能带来系统大而复杂、难于维护、发生单点故障（单个点发生故障的时候会波及到整个系统或者网络，从而导致整个系统或者网络的瘫痪）、扩展性差等问题。 分布式系统 分布式系统是一个硬件或软件组件分布在不同的网络计算机上，彼此之间仅仅通过消息传递进行通信和协调的系统。简单来说就是一群独立计算机集合共同对外提供服务，但是对于系统的用户来说，就像是一台计算机在提供服务一样。分布式意味着可以采用更多的普通计算机（相对于昂贵的大型机）组成分布式集群对外提供服务。计算机越多，CPU、内存、存储资源等也就越多，能够处理的并发访问量也就越大。 从分布式系统的概念中我们知道，各个主机之间通信和协调主要通过网络进行，所以分布式系统中的计算机在空间上几乎没有任何限制，这些计算机可能被放在不同的机柜上，也可能被部署在不同的机房中，还可能在不同的城市中，对于大型的网站甚至可能分布在不同的国家和地区。但是，无论空间上如何分布，一个标准的分布式系统应该具有以下几个主要特征： 分布性 分布式系统中的多台计算机之间在空间位置上可以随意分布，同时，机器的分布情况也会随时变动。 对等性 分布式系统中的计算机没有主／从之分，即没有控制整个系统的主机，也没有被控制的从机，组成分布式系统的所有计算机节点都是对等的。副本（Replica）是分布式系统最常见的概念之一，指的是分布式系统对数据和服务提供的一种冗余方式。在常见的分布式系统中，为了对外提供高可用的服务，我们往往会对数据和服务进行副本处理。数据副本是指在不同节点上持久化同一份数据，当某一个节点上存储的数据丢失时，可以从副本上读取该数据，这是解决分布式系统数据丢失问题最为有效的手段。另一类副本是服务副本，指多个节点提供同样的服务，每个节点都有能力接收来自外部的请求并进行相应的处理。 并发性 在一个计算机网络中，程序运行过程的并发性操作是非常常见的行为。例如同一个分布式系统中的多个节点，可能会并发地操作一些共享的资源，如何准确并高效地协调分布式并发操作也成为了分布式系统架构与设计中最大的挑战之一。 缺乏全局时钟 在分布式系统中，很难定义两个事件究竟谁先谁后，原因就是因为分布式系统缺乏一个全局的时钟序列控制。 故障总是会发生 组成分布式系统的所有计算机，都有可能发生任何形式的故障。除非需求指标允许，在系统设计时不能放过任何异常情况。 分布式系统面临的问题 通信异常 分布式系统需要在各个节点之间进行网络通信，因此网络通信都会伴随着网络不可用的风险或是系统不可用都会导致最终分布式系统无法顺利完成一次网络通信。另外，即使分布式系统各节点之间的网络通信能够正常进行，其延时也会远大于单机操作，会影响消息的收发的过程，因此消息丢失和消息延迟变得非常普遍。 网络分区 当网络由于发生异常情况，导致分布式系统中部分节点之间的网络延时不断增大，最终导致组成分布式系统的所有节点中，只有部分节点之间能够进行正常通信，而另一些节点则不能——我们将这个现象称为网络分区，就是俗称的“脑裂”。当网络分区出现时，分布式系统会出现局部小集群，在极端情况下，这些局部小集群会独立完成原本需要整个分布式才能完成的功能，这就对分布式一致性提出类非常大的挑战。 三态 分布式系统的每一次请求与响应，存在特有的“三态”概念，即成功、失败与超时。当出现超时现象时，网络通信的发起方是无法确定当前请求是否被成功处理的。 节点故障 节点故障则是分布式环境下另一个比较常见的问题，指的是组成分布式系统的服务器节点出现的宕机或“僵死”现象。 分布式理论(一) - CAP定理前言CAP原则又称CAP定理，指的是在一个分布式系统中，Consistency（一致性）、 Availability（可用性）、Partition tolerance（分区容错性）这三个基本需求，最多只能同时满足其中的2个。 初探分布式理论1. CAP原则简介选项 描述Consistency（一致性） 指数据在多个副本之间能够保持一致的特性（严格的一致性）Availability（可用性） 指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应（不保证获取的数据为最新数据）Partition tolerance（分区容错性） 分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障什么是分区？ 在分布式系统中，不同的节点分布在不同的子网络中，由于一些特殊的原因，这些子节点之间出现了网络不通的状态，但他们的内部子网络是正常的。从而导致了整个系统的环境被切分成了若干个孤立的区域，这就是分区。 2. CAP原则论证如图所示，是我们证明CAP的基本场景，网络中有两个节点N1和N2，可以简单的理解N1和N2分别是两台计算机，他们之间网络可以连通，N1中有一个应用程序A，和一个数据库V，N2也有一个应用程序B和一个数据库V。现在，A和B是分布式系统的两个部分，V是分布式系统的数据存储的两个子数据库。 在满足一致性的时候，N1和N2中的数据是一样的，V0=V0。 在满足可用性的时候，用户不管是请求N1或者N2，都会得到立即响应。 在满足分区容错性的情况下，N1和N2有任何一方宕机，或者网络不通的时候，都不会影响N1和N2彼此之间的正常运作。 如图所示，这是分布式系统正常运转的流程，用户向N1机器请求数据更新，程序A更新数据库V0为V1。分布式系统将数据进行同步操作M，将V1同步的N2中V0，使得N2中的数据V0也更新为V1，N2中的数据再响应N2的请求。 根据CAP原则定义，系统的一致性、可用性和分区容错性细分如下： 一致性：N1和N2的数据库V之间的数据是否完全一样。可用性：N1和N2的对外部的请求能否做出正常的响应。分区容错性：N1和N2之间的网络是否互通。 这是正常运作的场景，也是理想的场景。作为一个分布式系统，它和单机系统的最大区别，就在于网络。现在假设一种极端情况，N1和N2之间的网络断开了，我们要支持这种网络异常。相当于要满足分区容错性，能不能同时满足一致性和可用性呢？还是说要对他们进行取舍？ 假设在N1和N2之间网络断开的时候，有用户向N1发送数据更新请求，那N1中的数据V0将被更新为V1。由于网络是断开的，所以分布式系统同步操作M，所以N2中的数据依旧是V0。这个时候，有用户向N2发送数据读取请求，由于数据还没有进行同步，应用程序没办法立即给用户返回最新的数据V1，怎么办呢？ 这里有两种选择： 第一：牺牲数据一致性，保证可用性。响应旧的数据V0给用户。第二：牺牲可用性，保证数据一致性。阻塞等待，直到网络连接恢复，数据更新操作M完成之后，再给用户响应最新的数据V1。这个过程，证明了要满足分区容错性的分布式系统，只能在一致性和可用性两者中，选择其中一个。 CAP原则权衡通过CAP理论，我们知道无法同时满足一致性、可用性和分区容错性这三个特性，那要舍弃哪个呢？ 3.1. CA without P如果不要求P（不允许分区），则C（强一致性）和A（可用性）是可以保证的。但其实分区不是你想不想的问题，而是始终会存在，因此CA的系统更多的是允许分区后各子系统依然保持CA。 3.2. CP without A如果不要求A（可用），相当于每个请求都需要在Server之间强一致，而P（分区）会导致同步时间无限延长，如此CP也是可以保证的。很多传统的数据库分布式事务都属于这种模式。 3.3. AP wihtout C要高可用并允许分区，则需放弃一致性。一旦分区发生，节点之间可能会失去联系，为了高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。现在众多的NoSQL都属于此类。 小结对于多数大型互联网应用的场景，主机众多、部署分散。而且现在的集群规模越来越大，所以节点故障、网络故障是常态。这种应用一般要保证服务可用性达到N个9，即保证P和A，只有舍弃C（退而求其次保证最终一致性）。虽然某些地方会影响客户体验，但没达到造成用户流程的严重程度。 对于涉及到钱财这样不能有一丝让步的场景，C必须保证。网络发生故障宁可停止服务，这是保证CA，舍弃P。貌似这几年国内银行业发生了不下10起事故，但影响面不大，报到也不多，广大群众知道的少。还有一种是保证CP，舍弃A，例如网络故障时只读不写。 孰优孰劣，没有定论，只能根据场景定夺，适合的才是最好的。 分布式理论(二) - BASE理论前言BASE理论是由eBay架构师提出的。BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网分布式系统实践的总结，是基于CAP定律逐步演化而来。其核心思想是即使无法做到强一致性，但每个应用都可以根据自身业务特点，才用适当的方式来使系统打到最终一致性。 1. CAP的3选2伪命题实际上，不是为了P（分区容错性），必须在C（一致性）和A（可用性）之间任选其一。分区的情况很少出现，CAP在大多时间能够同时满足C和A。 对于分区存在或者探知其影响的情况下，需要提供一种预备策略做出处理： 探知分区的发生；进入显示的分区模式，限制某些操作；启动恢复过程，恢复数据一致性，补偿分区发生期间的错误。 2. BASE理论简介BASE理论是Basically Available(基本可用)，Soft State（软状态）和Eventually Consistent（最终一致性）三个短语的缩写。 其核心思想是： 既是无法做到强一致性（Strong consistency），但每个应用都可以根据自身的业务特点，采用适当的方式来使系统达到最终一致性（Eventual consistency）。 3. BASE理论的内容基本可用（Basically Available）软状态（Soft State）最终一致性（Eventually Consistent）下面展开讨论： 3.1. 基本可用什么是基本可用呢？假设系统，出现了不可预知的故障，但还是能用，相比较正常的系统而言： 响应时间上的损失：正常情况下的搜索引擎0.5秒即返回给用户结果，而基本可用的搜索引擎可以在2秒作用返回结果。 功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单。但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 3.2. 软状态什么是软状态呢？相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种“硬状态”。 软状态指的是：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。 3.3. 最终一致性上面说软状态，然后不可能一直是软状态，必须有个时间期限。在期限过后，应当保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间期限取决于网络延时、系统负载、数据复制方案设计等等因素。 而在实际工程实践中，最终一致性分为5种： 3.3.1. 因果一致性（Causal consistency） 因果一致性指的是：如果节点A在更新完某个数据后通知了节点B，那么节点B之后对该数据的访问和修改都是基于A更新后的值。于此同时，和节点A无因果关系的节点C的数据访问则没有这样的限制。 3.3.2. 读己之所写（Read your writes） 读己之所写指的是：节点A更新一个数据后，它自身总是能访问到自身更新过的最新值，而不会看到旧值。其实也算一种因果一致性。 3.3.3. 会话一致性（Session consistency） 会话一致性将对系统数据的访问过程框定在了一个会话当中：系统能保证在同一个有效的会话中实现 “读己之所写” 的一致性，也就是说，执行更新操作之后，客户端能够在同一个会话中始终读取到该数据项的最新值。 3.3.4. 单调读一致性（Monotonic read consistency） 单调读一致性指的是：如果一个节点从系统中读取出一个数据项的某个值后，那么系统对于该节点后续的任何数据访问都不应该返回更旧的值。 3.3.5. 单调写一致性（Monotonic write consistency） 单调写一致性指的是：一个系统要能够保证来自同一个节点的写操作被顺序的执行。 在实际的实践中，这5种系统往往会结合使用，以构建一个具有最终一致性的分布式系统。 实际上，不只是分布式系统使用最终一致性，关系型数据库在某个功能上，也是使用最终一致性的。比如备份，数据库的复制过程是需要时间的，这个复制过程中，业务读取到的值就是旧的。当然，最终还是达成了数据一致性。这也算是一个最终一致性的经典案例。 更具体的分布式问题一、分布式事务指事务的操作位于不同的节点上，需要保证事务的 AICD 特性。例如在下单场景下，库存和订单如果不在同一个节点上，就需要涉及分布式事务。 本地消息 原理本地消息表与业务数据表处于同一个数据库中，这样就能利用本地事务来保证在对这两个表的操作满足事务特性。 在分布式事务操作的一方，它完成写业务数据的操作之后向本地消息表发送一个消息，本地事务能保证这个消息一定会被写入本地消息表中。之后将本地消息表中的消息转发到 Kafka 等消息队列（MQ）中，如果转发成功则将消息从本地消息表中删除，否则继续重新转发。在分布式事务操作的另一方从消息队列中读取一个消息，并执行消息中的操作。 分析本地消息表利用了本地事务来实现分布式事务，并且使用了消息队列来保证最终一致性。 两阶段提交协议2PC参考这里：https://www.cnblogs.com/AndyAo/p/8228099.html 二、分布式锁可以使用 Java 提供的内置锁来实现进程同步：由 JVM 实现的 synchronized 和 JDK 提供的 Lock。但是在分布式场景下，需要同步的进程可能位于不同的节点上，那么就需要使用分布式锁来同步。 原理锁可以有阻塞锁和乐观锁两种实现方式，这里主要探讨阻塞锁实现。阻塞锁通常使用互斥量来实现，互斥量为 1 表示有其它进程在使用锁，此时处于锁定状态，互斥量为 0 表示未锁定状态。1 和 0 可以用一个整型值来存储，也可以用某个数据存在或者不存在来存储，某个数据存在表示互斥量为 1，也就是锁定状态。 实现 1. 数据库的唯一索引当想要获得锁时，就向表中插入一条记录，释放锁时就删除这条记录。唯一索引可以保证该记录只被插入一次，那么就可以用这个记录是否存在来判断是否存于锁定状态。 这种方式存在以下几个问题： 锁没有失效时间，解锁失败会导致死锁，其他线程无法再获得锁。只能是非阻塞锁，插入失败直接就报错了，无法重试。不可重入，同一线程在没有释放锁之前无法再获得锁。 2. Redis 的 SETNX 指令使用 SETNX（set if not exist）指令插入一个键值对，如果 Key 已经存在，那么会返回 False，否则插入成功并返回 True。 SETNX 指令和数据库的唯一索引类似，可以保证只存在一个 Key 的键值对，可以用一个 Key 的键值对是否存在来判断是否存于锁定状态。 EXPIRE 指令可以为一个键值对设置一个过期时间，从而避免了死锁的发生。 3. Redis 的 RedLock 算法使用了多个 Redis 实例来实现分布式锁，这是为了保证在发生单点故障时仍然可用。 尝试从 N 个相互独立 Redis 实例获取锁，如果一个实例不可用，应该尽快尝试下一个。计算获取锁消耗的时间，只有当这个时间小于锁的过期时间，并且从大多数（N/2+1）实例上获取了锁，那么就认为锁获取成功了。如果锁获取失败，会到每个实例上释放锁。 4. Zookeeper 的有序节点Zookeeper 是一个为分布式应用提供一致性服务的软件，例如配置管理、分布式协同以及命名的中心化等，这些都是分布式系统中非常底层而且是必不可少的基本功能，但是如果自己实现这些功能而且要达到高吞吐、低延迟同时还要保持一致性和可用性，实际上非常困难。 （一）抽象模型 Zookeeper 提供了一种树形结构级的命名空间，/app1/p_1 节点表示它的父节点为 /app1。 （二）节点类型 永久节点：不会因为会话结束或者超时而消失；临时节点：如果会话结束或者超时就会消失；有序节点：会在节点名的后面加一个数字后缀，并且是有序的，例如生成的有序节点为 /lock/node-0000000000，它的下一个有序节点则为 /lock/node-0000000001，依次类推。（三）监听器 为一个节点注册监听器，在节点状态发生改变时，会给客户端发送消息。 （四）分布式锁实现 创建一个锁目录 /lock；在 /lock 下创建临时的且有序的子节点，第一个客户端对应的子节点为 /lock/lock-0000000000，第二个为 /lock/lock-0000000001，以此类推；客户端获取 /lock 下的子节点列表，判断自己创建的子节点是否为当前子节点列表中序号最小的子节点，如果是则认为获得锁；否则监听自己的前一个子节点，获得子节点的变更通知后重复此步骤直至获得锁；执行业务代码，完成后，删除对应的子节点。（五）会话超时 如果一个已经获得锁的会话超时了，因为创建的是临时节点，所以该会话对应的临时节点会被删除，其它会话就可以获得锁了。可以看到，Zookeeper 分布式锁不会出现数据库的唯一索引实现分布式锁的死锁问题。 （六）羊群效应 一个节点未获得锁，需要监听自己的前一个子节点，这是因为如果监听所有的子节点，那么任意一个子节点状态改变，其它所有子节点都会收到通知（羊群效应），而我们只希望它的后一个子节点收到通知。 三、分布式 Session在分布式场景下，一个用户的 Session 如果只存储在一个服务器上，那么当负载均衡器把用户的下一个请求转发到另一个服务器上，该服务器没有用户的 Session，就可能导致用户需要重新进行登录等操作。 Sticky Sessions需要配置负载均衡器，使得一个用户的所有请求都路由到一个服务器节点上，这样就可以把用户的 Session 存放在该服务器节点中。 缺点：当服务器节点宕机时，将丢失该服务器节点上的所有 Session。 Session Replication在服务器节点之间进行 Session 同步操作，这样的话用户可以访问任何一个服务器节点。 缺点：需要更好的服务器硬件条件；需要对服务器进行配置。 Persistent DataStore将 Session 信息持久化到一个数据库中。 缺点：有可能需要去实现存取 Session 的代码。 In-Memory DataStore可以使用 Redis 和 Memcached 这种内存型数据库对 Session 进行存储，可以大大提高 Session 的读写效率。内存型数据库同样可以持久化数据到磁盘中来保证数据的安全性。 四、负载均衡算法 轮询（Round Robin）轮询算法把每个请求轮流发送到每个服务器上。下图中，一共有 6 个客户端产生了 6 个请求，这 6 个请求按 (1, 2, 3, 4, 5, 6) 的顺序发送。最后，(1, 3, 5) 的请求会被发送到服务器 1，(2, 4, 6) 的请求会被发送到服务器 2。 该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载（下图的 Server 2）。 加权轮询（Weighted Round Robbin）加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值。例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 (1, 2, 3, 4, 5) 请求会被发送到服务器 1，(6) 请求会被发送到服务器 2。 最少连接（least Connections）由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。例如下图中，(1, 3, 5) 请求会被发送到服务器 1，但是 (1, 3) 很快就断开连接，此时只有 (5) 请求连接服务器 1；(2, 4, 6) 请求被发送到服务器 2，只有 (2) 的连接断开。该系统继续运行时，服务器 2 会承担过大的负载。 最少连接算法就是将请求发送给当前最少连接数的服务器上。例如下图中，服务器 1 当前连接数最小，那么新到来的请求 6 就会被发送到服务器 1 上。 加权最少连接（Weighted Least Connection）在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。 随机算法（Random）把请求随机发送到服务器上。和轮询算法类似，该算法比较适合服务器性能差不多的场景。 源地址哈希法 (IP Hash)源地址哈希通过对客户端 IP 哈希计算得到的一个数值，用该数值对服务器数量进行取模运算，取模结果便是目标服务器的序号。 优点：保证同一 IP 的客户端都会被 hash 到同一台服务器上。缺点：不利于集群扩展，后台服务器数量变更都会影响 hash 结果。可以采用一致性 Hash 改进。 实现 HTTP 重定向HTTP 重定向负载均衡服务器收到 HTTP 请求之后会返回服务器的地址，并将该地址写入 HTTP 重定向响应中返回给浏览器，浏览器收到后需要再次发送请求。 缺点： 用户访问的延迟会增加；如果负载均衡器宕机，就无法访问该站点。 DNS 重定向使用 DNS 作为负载均衡器，根据负载情况返回不同服务器的 IP 地址。大型网站基本使用了这种方式做为第一级负载均衡手段，然后在内部使用其它方式做第二级负载均衡。 缺点： DNS 查找表可能会被客户端缓存起来，那么之后的所有请求都会被重定向到同一个服务器。 修改 MAC 地址使用 LVS（Linux Virtual Server）这种链路层负载均衡器，根据负载情况修改请求的 MAC 地址。 修改 IP 地址在网络层修改请求的目的 IP 地址。 代理自动配置正向代理与反向代理的区别： 正向代理：发生在客户端，是由用户主动发起的。比如翻墙，客户端通过主动访问代理服务器，让代理服务器获得需要的外网数据，然后转发回客户端。反向代理：发生在服务器端，用户不知道代理的存在。PAC 服务器是用来判断一个请求是否要经过代理。 高可用之“脑裂”在涉及到高可用性时，经常会听到”脑裂“，到底啥是”脑裂“？ 一句话：当两（多）个节点同时认为自已是唯一处于活动状态的服务器从而出现争用资源的情况，这种争用资源的场景即是所谓的“脑裂”（split-brain）或”区间集群“（partitioned cluster）。 HeartBeat原理： HeartBeat运行于备用主机上的Heartbeat可以通过以太网连接检测主服务器的运行状态，一旦其无法检测到主服务器的”心跳”则自动接管主服务器的资源。通常情况下，主、备服务器间的心跳连接是一个独立的物理连接，这个连接可以是串行线缆、一个由”交叉线”实现的以太网连接。Heartbeat甚至可同时通过多个物理连接检测主服务器的工作状态，而其只要能通过其中一个连接收到主服务器处于活动状态的信息，就会认为主服务器处于正常状态。从实践经验的角度来说，建议为Heartbeat配置多条独立的物理连接，以避免Heartbeat通信线路本身存在单点故障。 在“双机热备”高可用（HA）系统中，当联系2个节点的“心跳线”断开时，本来为一整体、动作协调的HA系统，就分裂成为2个独立的个体。由于相互失去了联系，都以为是对方出了故障，2个节点上的HA软件像“裂脑人”一样，“本能”地争抢“共享资源”、争起“应用服务”，就会发生严重后果：或者共享资源被瓜分、2边“服务”都起不来了；或者2边“服务”都起来了，但同时读写“共享存储”，导致数据损坏（常见如数据库轮询着的联机日志出错）。运行于备用主机上的Heartbeat可以通过以太网连接检测主服务器的运行状态，一旦其无法检测到主服务器的“心跳”则自动接管主服务器的资源。通常情况下，主、备服务器间的心跳连接是一个独立的物理连接，这个连接可以是串行线缆、一个由“交叉线”实现的以太网连接。Heartbeat甚至可同时通过多个物理连接检测主服务器的工作状态，而其只要能通过其中一个连接收到主服务器处于活动状态的信息，就会认为主服务器处于正常状态。从实践经验的角度来说，建议为Heartbeat配置多条独立的物理连接，以避免Heartbeat通信线路本身存在单点故障。1、串行电缆：被认为是比以太网连接安全性稍好些的连接方式，因为hacker无法通过串行连接运行诸如telnet、ssh或rsh类的程序，从而可以降低其通过已劫持的服务器再次侵入备份服务器的几率。但串行线缆受限于可用长度，因此主、备服务器的距离必须非常短。2、以太网连接：使用此方式可以消除串行线缆的在长度方面限制，并且可以通过此连接在主备服务器间同步文件系统，从而减少了从正常通信连接带宽的占用。 基于冗余的角度考虑，应该在主、备服务器使用两个物理连接传输heartbeat的控制信息；这样可以避免在一个网络或线缆故障时导致两个节点同时认为自已是唯一处于活动状态的服务器从而出现争用资源的情况，这种争用资源的场景即是所谓的“脑裂”（split-brain）或“partitioned cluster”。在两个节点共享同一个物理设备资源的情况下，脑裂会产生相当可怕的后果。 为了避免出现脑裂，可采用下面的预防措施：添加冗余的心跳线，例如双线条线。尽量减少“裂脑”发生机会。启用磁盘锁。正在服务一方锁住共享磁盘，“裂脑”发生时，让对方完全“抢不走”共享磁盘资源。但使用锁磁盘也会有一个不小的问题，如果占用共享盘的一方不主动“解锁”，另一方就永远得不到共享磁盘。现实中假如服务节点突然死机或崩溃，就不可能执行解锁命令。后备节点也就接管不了共享资源和应用服务。于是有人在HA中设计了“智能”锁。即，正在服务的一方只在发现心跳线全部断开（察觉不到对端）时才启用磁盘锁。平时就不上锁了。设置仲裁机制。例如设置参考IP（如网关IP），当心跳线完全断开时，2个节点都各自ping一下 参考IP，不通则表明断点就出在本端，不仅“心跳”、还兼对外“服务”的本端网络链路断了，即使启动（或继续）应用服务也没有用了，那就主动放弃竞争，让能够ping通参考IP的一端去起服务。更保险一些，ping不通参考IP的一方干脆就自我重启，以彻底释放有可能还占用着的那些共享资源。]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>分布式</tag>
        <tag>基本概念</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之JavaGC分析]]></title>
    <url>%2F2019%2F04%2F30%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJavaGC%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Java GC就是JVM记录仪，书画了JVM各个分区的表演。 什么是 Java GCJava GC（Garbage Collection，垃圾收集，垃圾回收）机制，是Java与C++/C的主要区别之一，作为Java开发者，一般不需要专门编写内存回收和垃圾清理代码，对内存泄露和溢出的问题，也不需要像C程序员那样战战兢兢。这是因为在Java虚拟机中，存在自动内存管理和垃圾清扫机制。概括地说，该机制对JVM（Java Virtual Machine）中的内存进行标记，并确定哪些内存需要回收，根据一定的回收策略，自动的回收内存，永不停息（Nerver Stop）的保证JVM中的内存空间，防止出现内存泄露和溢出问题。 在Java语言出现之前，就有GC机制的存在，如Lisp语言），Java GC机制已经日臻完善，几乎可以自动的为我们做绝大多数的事情。然而，如果我们从事较大型的应用软件开发，曾经出现过内存优化的需求，就必定要研究Java GC机制。 简单总结一下，Java GC就是通过GC收集器回收不在存活的对象，保证JVM更加高效的运转。 如何获取 Java GC日志一般情况可以通过两种方式来获取GC日志，一种是使用命令动态查看，一种是在容器中设置相关参数打印GC日志。 命令动态查看Java 自动的工具行命令，jstat可以用来动态监控JVM内存的使用，统计垃圾回收的各项信息。 比如常用命令，jstat -gc 统计垃圾回收堆的行为123$ jstat -gc 1262 S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT 26112.0 24064.0 6562.5 0.0 564224.0 76274.5 434176.0 388518.3 524288.0 42724.7 320 6.417 1 0.398 6.815 也可以设置间隔固定时间来打印：1$ jstat -gc 1262 2000 20 这个命令意思就是每隔2000ms输出1262的gc情况，一共输出20次 GC参数JVM的GC日志的主要参数包括如下几个： -XX:+PrintGC 输出GC日志-XX:+PrintGCDetails 输出GC的详细日志-XX:+PrintGCTimeStamps 输出GC的时间戳（以基准时间的形式）-XX:+PrintGCDateStamps 输出GC的时间戳（以日期的形式，如 2017-09-04T21:53:59.234+0800）-XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息-Xloggc:../logs/gc.log 日志文件的输出路径在生产环境中，根据需要配置相应的参数来监控JVM运行情况。 Tomcat 设置示例 我们经常在tomcat的启动参数中添加JVM相关参数，这里有一个典型的示例： 123456JAVA_OPTS=&quot;-server -Xms2000m -Xmx2000m -Xmn800m -XX:PermSize=64m -XX:MaxPermSize=256m -XX:SurvivorRatio=4-verbose:gc -Xloggc:$CATALINA_HOME/logs/gc.log -Djava.awt.headless=true -XX:+PrintGCTimeStamps -XX:+PrintGCDetails -Dsun.rmi.dgc.server.gcInterval=600000 -Dsun.rmi.dgc.client.gcInterval=600000-XX:+UseConcMarkSweepGC -XX:MaxTenuringThreshold=15&quot; 根据上面的参数我们来做一下解析： -Xms2000m -Xmx2000m -Xmn800m -XX:PermSize=64m -XX:MaxPermSize=256mXms，即为jvm启动时得JVM初始堆大小,Xmx为jvm的最大堆大小，xmn为新生代的大小，permsize为永久代的初始大小，MaxPermSize为永久代的最大空间。 -XX:SurvivorRatio=4SurvivorRatio为新生代空间中的Eden区和救助空间Survivor区的大小比值，默认是8，则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10。调小这个参数将增大survivor区，让对象尽量在survitor区呆长一点，减少进入年老代的对象。去掉救助空间的想法是让大部分不能马上回收的数据尽快进入年老代，加快年老代的回收频率，减少年老代暴涨的可能性，这个是通过将-XX:SurvivorRatio 设置成比较大的值（比如65536)来做到。 -verbose:gc -Xloggc:$CATALINA_HOME/logs/gc.log将虚拟机每次垃圾回收的信息写到日志文件中，文件名由file指定，文件格式是平文件，内容和-verbose:gc输出内容相同。 -Djava.awt.headless=true Headless模式是系统的一种配置模式。在该模式下，系统缺少了显示设备、键盘或鼠标。 -XX:+PrintGCTimeStamps -XX:+PrintGCDetails设置gc日志的格式 -Dsun.rmi.dgc.server.gcInterval=600000 -Dsun.rmi.dgc.client.gcInterval=600000指定rmi调用时gc的时间间隔 -XX:+UseConcMarkSweepGC -XX:MaxTenuringThreshold=15 采用并发gc方式，经过15次minor gc 后进入年老代 如何分析GC日志摘录GC日志一部分 Young GC回收日志:12016-07-05T10:43:18.093+0800: 25.395: [GC [PSYoungGen: 274931K-&gt;10738K(274944K)] 371093K-&gt;147186K(450048K), 0.0668480 secs] [Times: user=0.17 sys=0.08, real=0.07 secs] Full GC回收日志:12016-07-05T10:43:18.160+0800: 25.462: [Full GC [PSYoungGen: 10738K-&gt;0K(274944K)] [ParOldGen: 136447K-&gt;140379K(302592K)] 147186K-&gt;140379K(577536K) [PSPermGen: 85411K-&gt;85376K(171008K)], 0.6763541 secs] [Times: user=1.75 sys=0.02, real=0.68 secs] 通过上面日志分析得出，PSYoungGen、ParOldGen、PSPermGen属于Parallel收集器。其中PSYoungGen表示gc回收前后年轻代的内存变化；ParOldGen表示gc回收前后老年代的内存变化；PSPermGen表示gc回收前后永久区的内存变化。young gc 主要是针对年轻代进行内存回收比较频繁，耗时短；full gc 会对整个堆内存进行回城，耗时长，因此一般尽量减少full gc的次数 通过两张图非常明显看出gc日志构成： Young GC日志:Java%20GC%20%E5%88%86%E6%9E%90.resources/253C4E10-C025-406F-BCEC-360BD0B901AC.png) Full GC日志:Java%20GC%20%E5%88%86%E6%9E%90.resources/E41583CF-6306-4B8F-95D7-396A3B91FBB1.png) GC分析工具GChistoGChisto是一款专业分析gc日志的工具，可以通过gc日志来分析：Minor GC、full gc的时间、频率等等，通过列表、报表、图表等不同的形式来反应gc的情况。虽然界面略显粗糙，但是功能还是不错的。配置好本地的jdk环境之后，双击GChisto.jar,在弹出的输入框中点击 add 选择gc.log日志 Java%20GC%20%E5%88%86%E6%9E%90.resources/3BC499FA-2D44-4448-9720-AA4734BA2290.jpg) GC Pause Stats:可以查看GC 的次数、GC的时间、GC的开销、最大GC时间和最小GC时间等，以及相应的柱状图 Java%20GC%20%E5%88%86%E6%9E%90.resources/175F8410-B0BD-4288-A2C2-5C35AF57F933.jpg) GC Pause Distribution:查看GC停顿的详细分布，x轴表示垃圾收集停顿时间，y轴表示是停顿次数。GC Timeline：显示整个时间线上的垃圾收集 Java%20GC%20%E5%88%86%E6%9E%90.resources/1550316713780.jpg) 不过这款工具已经不再维护GC Easy这是一个web工具,在线使用非常方便.地址: http://gceasy.io进入官网，讲打包好的zip或者gz为后缀的压缩包上传，过一会就会拿到分析结果。 Java%20GC%20%E5%88%86%E6%9E%90.resources/1550316679385.jpg) 推荐使用此工具进行gc分析。 Java%20GC%20%E5%88%86%E6%9E%90.resources/1550316713780.jpg)]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>GC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之JVM调优命令大全]]></title>
    <url>%2F2019%2F04%2F30%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJVM%E8%B0%83%E4%BC%98%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8%2F</url>
    <content type="text"><![CDATA[简介运用jvm自带的命令可以方便的在生产监控和打印堆栈的日志信息帮忙我们来定位问题！虽然jvm调优成熟的工具已经有很多：jconsole、大名鼎鼎的VisualVM，IBM的Memory Analyzer等等，但是在生产环境出现问题的时候，一方面工具的使用会有所限制，另一方面喜欢装X的我们，总喜欢在出现问题的时候在终端输入一些命令来解决。所有的工具几乎都是依赖于jdk的接口和底层的这些命令，研究这些命令的使用也让我们更能了解jvm构成和特性。Sun JDK监控和故障处理命令有jps jstat jmap jhat jstack jinfo下面做一一介绍 jpsJVM Process Status Tool,显示指定系统内所有的HotSpot虚拟机进程。 命令格式jps [options] [hostid]option参数-l : 输出主类全名或jar路径-q : 只输出LVMID-m : 输出JVM启动时传递给main()的参数-v : 输出JVM启动时显示指定的JVM参数其中[option]、[hostid]参数也可以不写。 示例1234$ jps -l -m 28920 org.apache.catalina.startup.Bootstrap start 11589 org.apache.catalina.startup.Bootstrap start 25816 sun.tools.jps.Jps -l -m jstatjstat(JVM statistics Monitoring)是用于监视虚拟机运行时状态信息的命令，它可以显示出虚拟机进程中的类装载、内存、垃圾收集、JIT编译等运行数据。 命令格式jstat [option] LVMID [interval] [count]参数[option] : 操作参数LVMID : 本地虚拟机进程ID[interval] : 连续输出的时间间隔[count] : 连续输出的次数 option 参数总览jvm调优-命令大全（jps jstat jmap jhat jstack jinfo）.resources/3358A9D6-B4C7-4831-B26B-AD24973357EB.png) option 参数详解-class监视类装载、卸载数量、总空间以及耗费的时间12345678910$ jstat -class 11589 Loaded Bytes Unloaded Bytes Time 7035 14506.3 0 0.0 3.67Loaded : 加载class的数量Bytes : class字节大小Unloaded : 未加载class的数量Bytes : 未加载class的字节大小Time : 加载时间-compiler输出JIT编译过的方法数量耗时等 $ jstat -compiler 1262Compiled Failed Invalid Time FailedType FailedMethod 2573 1 0 47.60 1 org/apache/catalina/loader/WebappClassLoader findResourceInternalCompiled : 编译数量Failed : 编译失败数量Invalid : 无效数量Time : 编译耗时FailedType : 失败类型FailedMethod : 失败方法的全限定名-gc垃圾回收堆的行为统计，常用命令123$ jstat -gc 1262 S0C S1C S0U S1U EC EU OC OU PC PU YGC YGCT FGC FGCT GCT 26112.0 24064.0 6562.5 0.0 564224.0 76274.5 434176.0 388518.3 524288.0 42724.7 320 6.417 1 0.398 6.815 C即Capacity 总容量，U即Used 已使用的容量 S0C : survivor0区的总容量S1C : survivor1区的总容量S0U : survivor0区已使用的容量S1U : survivor1区已使用的容量EC : Eden区的总容量EU : Eden区已使用的容量OC : Old区的总容量OU : Old区已使用的容量PC 当前perm的容量 (KB)PU perm的使用 (KB)YGC : 新生代垃圾回收次数YGCT : 新生代垃圾回收时间FGC : 老年代垃圾回收次数FGCT : 老年代垃圾回收时间GCT : 垃圾回收总消耗时间1$ jstat -gc 1262 2000 20 这个命令意思就是每隔2000ms输出1262的gc情况，一共输出20次 -gccapacity同-gc，不过还会输出Java堆各区域使用到的最大、最小空间123$ jstat -gccapacity 1262 NGCMN NGCMX NGC S0C S1C EC OGCMN OGCMX OGC OC PGCMN PGCMX PGC PC YGC FGC 614400.0 614400.0 614400.0 26112.0 24064.0 564224.0 434176.0 434176.0 434176.0 434176.0 524288.0 1048576.0 524288.0 524288.0 320 1 NGCMN : 新生代占用的最小空间NGCMX : 新生代占用的最大空间OGCMN : 老年代占用的最小空间OGCMX : 老年代占用的最大空间OGC：当前年老代的容量 (KB)OC：当前年老代的空间 (KB)PGCMN : perm占用的最小空间PGCMX : perm占用的最大空间 -gcutil 同-gc，不过输出的是已使用空间占总空间的百分比123$ jstat -gcutil 28920 S0 S1 E O P YGC YGCT FGC FGCT GCT 12.45 0.00 33.85 0.00 4.44 4 0.242 0 0.000 0.242 -gccause 垃圾收集统计概述（同-gcutil），附加最近两次垃圾回收事件的原因123$ jstat -gccause 28920 S0 S1 E O P YGC YGCT FGC FGCT GCT LGCC GCC 12.45 0.00 33.85 0.00 4.44 4 0.242 0 0.000 0.242 Allocation Failure No GC LGCC：最近垃圾回收的原因GCC：当前垃圾回收的原因-gcnew统计新生代的行为123$ jstat -gcnew 28920 S0C S1C S0U S1U TT MTT DSS EC EU YGC YGCT 419392.0 419392.0 52231.8 0.0 6 6 209696.0 3355520.0 1172246.0 4 0.242 TT：Tenuring threshold(提升阈值)MTT：最大的tenuring thresholdDSS：survivor区域大小 (KB) -gcnewcapacity 新生代与其相应的内存空间的统计123$ jstat -gcnewcapacity 28920 NGCMN NGCMX NGC S0CMX S0C S1CMX S1C ECMX EC YGC FGC 4194304.0 4194304.0 4194304.0 419392.0 419392.0 419392.0 419392.0 3355520.0 3355520.0 4 0 NGC:当前年轻代的容量 (KB)S0CMX:最大的S0空间 (KB)S0C:当前S0空间 (KB)ECMX:最大eden空间 (KB)EC:当前eden空间 (KB) -gcold 统计旧生代的行为1234$ jstat -gcold 28920 PC PU OC OU YGC FGC FGCT GCT 1048576.0 46561.7 6291456.0 0.0 4 0 0.000 0.242-gcoldcapacity 统计旧生代的大小和空间123$ jstat -gcoldcapacity 28920 OGCMN OGCMX OGC OC YGC FGC FGCT GCT 6291456.0 6291456.0 6291456.0 6291456.0 4 0 0.000 0.242 -gcpermcapacity 永生代行为统计123$ jstat -gcpermcapacity 28920 PGCMN PGCMX PGC PC YGC FGC FGCT GCT 1048576.0 2097152.0 1048576.0 1048576.0 4 0 0.000 0.242 -printcompilation hotspot编译方法统计123$ jstat -printcompilation 28920 Compiled Size Type Method 1291 78 1 java/util/ArrayList indexOf Compiled：被执行的编译任务的数量Size：方法字节码的字节数Type：编译类型Method：编译方法的类名和方法名。类名使用”/” 代替 “.” 作为空间分隔符. 方法名是给出类的方法名. 格式是一致于HotSpot - XX:+PrintComplation 选项 jmapjmap(JVM Memory Map)命令用于生成heap dump文件，如果不使用这个命令，还阔以使用-XX:+HeapDumpOnOutOfMemoryError参数来让虚拟机出现OOM的时候·自动生成dump文件。 jmap不仅能生成dump文件，还阔以查询finalize执行队列、Java堆和永久代的详细信息，如当前使用率、当前使用的是哪种收集器等。 命令格式jmap [option] LVMID option参数 dump : 生成堆转储快照finalizerinfo : 显示在F-Queue队列等待Finalizer线程执行finalizer方法的对象heap : 显示Java堆详细信息histo : 显示堆中对象的统计信息permstat : to print permanent generation statisticsF : 当-dump没有响应时，强制生成dump快照 示例 -dump常用格式1-dump::live,format=b,file=&lt;filename&gt; pid dump堆到文件,format指定输出格式，live指明是活着的对象,file指定文件名123$ jmap -dump:live,format=b,file=dump.hprof 28920 Dumping heap to /home/xxx/dump.hprof ... Heap dump file created dump.hprof这个后缀是为了后续可以直接用MAT(Memory Anlysis Tool)打开。 -finalizerinfo打印等待回收对象的信息 123456$ jmap -finalizerinfo 28920 Attaching to process ID 28920, please wait... Debugger attached successfully. Server compiler detected. JVM version is 24.71-b01 Number of objects pending for finalization: 0 可以看到当前F-QUEUE队列中并没有等待Finalizer线程执行finalizer方法的对象。 -heap打印heap的概要信息，GC使用的算法，heap的配置及wise heap的使用情况,可以用此来判断内存目前的使用情况以及垃圾回收情况 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051$ jmap -heap 28920 Attaching to process ID 28920, please wait... Debugger attached successfully. Server compiler detected. JVM version is 24.71-b01 using thread-local object allocation. Parallel GC with 4 thread(s)//GC 方式 Heap Configuration: //堆内存初始化配置 MinHeapFreeRatio = 0 //对应jvm启动参数-XX:MinHeapFreeRatio设置JVM堆最小空闲比率(default 40) MaxHeapFreeRatio = 100 //对应jvm启动参数 -XX:MaxHeapFreeRatio设置JVM堆最大空闲比率(default 70) MaxHeapSize = 2082471936 (1986.0MB) //对应jvm启动参数-XX:MaxHeapSize=设置JVM堆的最大大小 NewSize = 1310720 (1.25MB)//对应jvm启动参数-XX:NewSize=设置JVM堆的‘新生代’的默认大小 MaxNewSize = 17592186044415 MB//对应jvm启动参数-XX:MaxNewSize=设置JVM堆的‘新生代’的最大大小 OldSize = 5439488 (5.1875MB)//对应jvm启动参数-XX:OldSize=&lt;value&gt;:设置JVM堆的‘老生代’的大小 NewRatio = 2 //对应jvm启动参数-XX:NewRatio=:‘新生代’和‘老生代’的大小比率 SurvivorRatio = 8 //对应jvm启动参数-XX:SurvivorRatio=设置年轻代中Eden区与Survivor区的大小比值 PermSize = 21757952 (20.75MB) //对应jvm启动参数-XX:PermSize=&lt;value&gt;:设置JVM堆的‘永生代’的初始大小 MaxPermSize = 85983232 (82.0MB)//对应jvm启动参数-XX:MaxPermSize=&lt;value&gt;:设置JVM堆的‘永生代’的最大大小 G1HeapRegionSize = 0 (0.0MB) Heap Usage://堆内存使用情况 PS Young Generation Eden Space://Eden区内存分布 capacity = 33030144 (31.5MB)//Eden区总容量 used = 1524040 (1.4534378051757812MB) //Eden区已使用 free = 31506104 (30.04656219482422MB) //Eden区剩余容量 4.614088270399305% used //Eden区使用比率 From Space: //其中一个Survivor区的内存分布 capacity = 5242880 (5.0MB) used = 0 (0.0MB) free = 5242880 (5.0MB) 0.0% used To Space: //另一个Survivor区的内存分布 capacity = 5242880 (5.0MB) used = 0 (0.0MB) free = 5242880 (5.0MB) 0.0% used PS Old Generation //当前的Old区内存分布 capacity = 86507520 (82.5MB) used = 0 (0.0MB) free = 86507520 (82.5MB) 0.0% used PS Perm Generation//当前的 “永生代” 内存分布 capacity = 22020096 (21.0MB) used = 2496528 (2.3808746337890625MB) free = 19523568 (18.619125366210938MB) 11.337498256138392% used 670 interned Strings occupying 43720 bytes. 可以很清楚的看到Java堆中各个区域目前的情况。 -histo打印堆的对象统计，包括对象数、内存大小等等 （因为在dump:live前会进行full gc，如果带上live则只统计活对象，因此不加live的堆大小要大于加live堆的大小 ） 1234567891011121314$ jmap -histo:live 28920 | more num #instances #bytes class name---------------------------------------------- 1: 83613 12012248 &lt;constMethodKlass&gt; 2: 23868 11450280 [B 3: 83613 10716064 &lt;methodKlass&gt; 4: 76287 10412128 [C 5: 8227 9021176 &lt;constantPoolKlass&gt; 6: 8227 5830256 &lt;instanceKlassKlass&gt; 7: 7031 5156480 &lt;constantPoolCacheKlass&gt; 8: 73627 1767048 java.lang.String 9: 2260 1348848 &lt;methodDataKlass&gt; 10: 8856 849296 java.lang.Class .... 仅仅打印了前10行 xml class name是对象类型，说明如下： B byteC charD doubleF floatI intJ longZ boolean[ 数组，如[I表示int[][L+类名 其他对象 -permstat打印Java堆内存的永久保存区域的类加载器的智能统计信息。对于每个类加载器而言，它的名称、活跃度、地址、父类加载器、它所加载的类的数量和大小都会被打印。此外，包含的字符串数量和大小也会被打印。 123456789101112131415$ jmap -permstat 28920 Attaching to process ID 28920, please wait... Debugger attached successfully. Server compiler detected. JVM version is 24.71-b01 finding class loader instances ..done. computing per loader stat ..done. please wait.. computing liveness.liveness analysis may be inaccurate ... class_loader classes bytes parent_loader alive? type &lt;bootstrap&gt; 3111 18154296 null live &lt;internal&gt; 0x0000000600905cf8 1 1888 0x0000000600087f08 dead sun/reflect/DelegatingClassLoader@0x00000007800500a0 0x00000006008fcb48 1 1888 0x0000000600087f08 dead sun/reflect/DelegatingClassLoader@0x00000007800500a0 0x00000006016db798 0 0 0x00000006008d3fc0 dead java/util/ResourceBundle$RBClassLoader@0x0000000780626ec0 0x00000006008d6810 1 3056 null dead sun/reflect/DelegatingClassLoader@0x00000007800500a0 -F强制模式。如果指定的pid没有响应，请使用jmap -dump或jmap -histo选项。此模式下，不支持live子选项。 jhatjhat(JVM Heap Analysis Tool)命令是与jmap搭配使用，用来分析jmap生成的dump，jhat内置了一个微型的HTTP/HTML服务器，生成dump的分析结果后，可以在浏览器中查看。在此要注意，一般不会直接在服务器上进行分析，因为jhat是一个耗时并且耗费硬件资源的过程，一般把服务器生成的dump文件复制到本地或其他机器上进行分析。 命令格式jhat [dumpfile] 参数 -stack false|true 关闭对象分配调用栈跟踪(tracking object allocation call stack)。 如果分配位置信息在堆转储中不可用. 则必须将此标志设置为 false. 默认值为 true.&gt; -refs false|true 关闭对象引用跟踪(tracking of references to objects)。 默认值为 true. 默认情况下, 返回的指针是指向其他特定对象的对象,如反向链接或输入引用(referrers or incoming references), 会统计/计算堆中的所有对象。&gt; -port port-number 设置 jhat HTTP server 的端口号. 默认值 7000.&gt; -exclude exclude-file 指定对象查询时需要排除的数据成员列表文件(a file that lists data members that should be excluded from the reachable objects query)。 例如, 如果文件列列出了 java.lang.String.value , 那么当从某个特定对象 Object o 计算可达的对象列表时, 引用路径涉及 java.lang.String.value 的都会被排除。&gt; -baseline exclude-file 指定一个基准堆转储(baseline heap dump)。 在两个 heap dumps 中有相同 object ID 的对象会被标记为不是新的(marked as not being new). 其他对象被标记为新的(new). 在比较两个不同的堆转储时很有用.&gt; -debug int 设置 debug 级别. 0 表示不输出调试信息。 值越大则表示输出更详细的 debug 信息.&gt; -version 启动后只显示版本信息就退出&gt; -J&lt; flag &gt; 因为 jhat 命令实际上会启动一个JVM来执行, 通过 -J 可以在启动JVM时传入一些启动参数. 例如, -J-Xmx512m 则指定运行 jhat 的Java虚拟机使用的最大堆内存为 512 MB. 如果需要使用多个JVM启动参数,则传入多个 -Jxxxxxx. 示例 12345678910$ jhat -J-Xmx512m dump.hprof eading from dump.hprof... Dump file created Fri Mar 11 17:13:42 CST 2016 Snapshot read, resolving... Resolving 271678 objects... Chasing references, expect 54 dots...................................................... Eliminating duplicate references...................................................... Snapshot resolved. Started HTTP server on port 7000 Server is ready. 中间的-J-Xmx512m是在dump快照很大的情况下分配512M内存去启动HTTP服务器，运行完之后就可在浏览器打开Http://localhost:7000进行快照分析 堆快照分析主要在最后面的Heap Histogram里，里面根据class列出了dump的时候所有存活对象。 分析同样一个dump快照，MAT需要的额外内存比jhat要小的多的多，所以建议使用MAT来进行分析，当然也看个人偏好。 分析打开浏览器Http://localhost:7000，该页面提供了几个查询功能可供使用： 1234567All classes including platformShow all members of the rootsetShow instance counts for all classes (including platform)Show instance counts for all classes (excluding platform)Show heap histogramShow finalizer summaryExecute Object Query Language (OQL) query 一般查看堆异常情况主要看这个两个部分： Show instance counts for all classes (excluding platform)，平台外的所有对象信息。如下图：jvm调优-命令大全（jps jstat jmap jhat jstack jinfo）.resources/E7DB83E0-2344-4149-8603-C606D78AB943.png)Show heap histogram 以树状图形式展示堆情况。如下图：jvm调优-命令大全（jps jstat jmap jhat jstack jinfo）.resources/8DE752AB-6C96-4A58-8E01-8970BA3E2014.png)具体排查时需要结合代码，观察是否大量应该被回收的对象在一直被引用或者是否有占用内存特别大的对象无法被回收。一般情况，会down到客户端用工具来分析 jstackjstack用于生成java虚拟机当前时刻的线程快照。线程快照是当前java虚拟机内每一条线程正在执行的方法堆栈的集合，生成线程快照的主要目的是定位线程出现长时间停顿的原因，如线程间死锁、死循环、请求外部资源导致的长时间等待等。 线程出现停顿的时候通过jstack来查看各个线程的调用堆栈，就可以知道没有响应的线程到底在后台做什么事情，或者等待什么资源。 如果java程序崩溃生成core文件，jstack工具可以用来获得core文件的java stack和native stack的信息，从而可以轻松地知道java程序是如何崩溃和在程序何处发生问题。另外，jstack工具还可以附属到正在运行的java程序中，看到当时运行的java程序的java stack和native stack的信息, 如果现在运行的java程序呈现hung的状态，jstack是非常有用的。 命令格式jstack [option] LVMIDoption参数-F : 当正常输出请求不被响应时，强制输出线程堆栈-l : 除堆栈外，显示关于锁的附加信息-m : 如果调用到本地方法的话，可以显示C/C++的堆栈 示例12345678910111213141516171819202122232425262728$ jstack -l 11494|more2016-07-28 13:40:04Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.71-b01 mixed mode):&quot;Attach Listener&quot; daemon prio=10 tid=0x00007febb0002000 nid=0x6b6f waiting on condition [0x0000000000000000] java.lang.Thread.State: RUNNABLE Locked ownable synchronizers: - None&quot;http-bio-8005-exec-2&quot; daemon prio=10 tid=0x00007feb94028000 nid=0x7b8c waiting on condition [0x00007fea8f56e000] java.lang.Thread.State: WAITING (parking) at sun.misc.Unsafe.park(Native Method) - parking to wait for &lt;0x00000000cae09b80&gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject) at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186) at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043) at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:104) at org.apache.tomcat.util.threads.TaskQueue.take(TaskQueue.java:32) at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run(TaskThread.java:61) at java.lang.Thread.run(Thread.java:745) Locked ownable synchronizers: - None ..... 这里有一篇文章解释的很好 分析打印出的文件内容jinfojinfo(JVM Configuration info)这个命令作用是实时查看和调整虚拟机运行参数。 之前的jps -v口令只能查看到显示指定的参数，如果想要查看未被显示指定的参数的值就要使用jinfo口令 jinfojinfo(JVM Configuration info)这个命令作用是实时查看和调整虚拟机运行参数。 之前的jps -v口令只能查看到显示指定的参数，如果想要查看未被显示指定的参数的值就要使用jinfo口令 命令格式jinfo [option] [args] LVMIDoption参数-flag : 输出指定args参数的值-flags : 不需要args参数，输出所有JVM参数的值-sysprops : 输出系统属性，等同于System.getProperties()示例12$ jinfo -flag 11494-XX:CMSInitiatingOccupancyFraction=80]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>调优命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之GC算法垃圾收集器]]></title>
    <url>%2F2019%2F04%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BGC%E7%AE%97%E6%B3%95%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[概述垃圾收集 Garbage Collection 通常被称为“GC”，它诞生于1960年 MIT 的 Lisp 语言，经过半个多世纪，目前已经十分成熟了。 jvm 中，程序计数器、虚拟机栈、本地方法栈都是随线程而生随线程而灭，栈帧随着方法的进入和退出做入栈和出栈操作，实现了自动的内存清理，因此，我们的内存垃圾回收主要集中于 java 堆和方法区中，在程序运行期间，这部分内存的分配和使用都是动态的. 对象存活判断判断对象是否存活一般有两种方式： 引用计数：每个对象有一个引用计数属性，新增一个引用时计数加1，引用释放时计数减1，计数为0时可以回收。此方法简单，无法解决对象相互循环引用的问题。 可达性分析（Reachability Analysis）：从GC Roots开始向下搜索，搜索所走过的路径称为引用链。当一个对象到GC Roots没有任何引用链相连时，则证明此对象是不可用的。不可达对象。 在Java语言中，GC Roots包括： 虚拟机栈中引用的对象。 方法区中类静态属性实体引用的对象。 方法区中常量引用的对象。 本地方法栈中JNI引用的对象。 垃圾收集算法标记-清除算法 “标记-清除”（Mark-Sweep）算法，如它的名字一样，算法分为“标记”和“清除”两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收掉所有被标记的对象。之所以说它是最基础的收集算法，是因为后续的收集算法都是基于这种思路并对其缺点进行改进而得到的。 它的主要缺点有两个：一个是效率问题，标记和清除过程的效率都不高；另外一个是空间问题，标记清除之后会产生大量不连续的内存碎片，空间碎片太多可能会导致，当程序在以后的运行过程中需要分配较大对象时无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 GC算法 垃圾收集器.resources/1082B388-1A45-4177-98E2-30394876769B.png) 复制算法“复制”（Copying）的收集算法，它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。 这样使得每次都是对其中的一块进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。只是这种算法的代价是将内存缩小为原来的一半，持续复制长生存期的对象则导致效率降低。GC算法 垃圾收集器.resources/02057726-396E-47F9-9339-2DEF5B338ED2.png) 标记-压缩算法复制收集算法在对象存活率较高时就要执行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。 根据老年代的特点，有人提出了另外一种“标记-整理”（Mark-Compact）算法，标记过程仍然与“标记-清除”算法一样，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存 GC算法 垃圾收集器.resources/7F6F3B11-DDA0-4E80-8162-442B66A71632.png) 分代收集算法GC分代的基本假设：绝大部分对象的生命周期都非常短暂，存活时间短。 “分代收集”（Generational Collection）算法，把Java堆分为新生代和老年代，这样就可以根据各个年代的特点采用最适当的收集算法。在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需要付出少量存活对象的复制成本就可以完成收集。而老年代中因为对象存活率高、没有额外空间对它进行分配担保，就必须使用“标记-清理”或“标记-整理”算法来进行回收。 垃圾收集器如果说收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现 Serial收集器串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停） 参数控制：-XX:+UseSerialGC 串行收集器 GC算法 垃圾收集器.resources/BC391A2A-4905-4708-9052-2422C0E9D692.png) ParNew收集器ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩 参数控制：-XX:+UseParNewGC ParNew收集器-XX:ParallelGCThreads 限制线程数量 GC算法 垃圾收集器.resources/B6AC79A9-21A8-48B4-B29E-CF755E0970FC.png) Parallel收集器Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩 参数控制：-XX:+UseParallelGC 使用Parallel收集器+ 老年代串行 Parallel Old 收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。这个收集器是在JDK 1.6中才开始提供 参数控制： -XX:+UseParallelOldGC 使用Parallel收集器+ 老年代并行 CMS收集器CMS（Concurrent Mark Sweep）收集器是一种以获取最短回收停顿时间为目标的收集器。目前很大一部分的Java应用都集中在互联网站或B/S系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。 从名字（包含“Mark Sweep”）上就可以看出CMS收集器是基于“标记-清除”算法实现的，它的运作过程相对于前面几种收集器来说要更复杂一些，整个过程分为4个步骤，包括： 初始标记（CMS initial mark） 并发标记（CMS concurrent mark） 重新标记（CMS remark） 并发清除（CMS concurrent sweep） 其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。 由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。老年代收集器（新生代使用ParNew） 优点:并发收集、低停顿 缺点：产生大量空间碎片、并发阶段会降低吞吐量 参数控制：-XX:+UseConcMarkSweepGC 使用CMS收集器 -XX:+ UseCMSCompactAtFullCollection Full GC后，进行一次碎片整理；整理过程是独占的，会引起停顿时间变长 -XX:+CMSFullGCsBeforeCompaction 设置进行几次Full GC后，进行一次碎片整理 -XX:ParallelCMSThreads 设定CMS的线程数量（一般情况约等于可用CPU数量） GC算法 垃圾收集器.resources/4F09EDEB-C98A-43BB-BF13-BAE28C01B961.png) G1收集器G1是目前技术发展的最前沿成果之一，HotSpot开发团队赋予它的使命是未来可以替换掉JDK1.5中发布的CMS收集器。与CMS收集器相比G1收集器有以下特点： 空间整合，G1收集器采用标记整理算法，不会产生内存空间碎片。分配大对象时不会因为无法找到连续空间而提前触发下一次GC。 可预测停顿，这是G1的另一大优势，降低停顿时间是G1和CMS的共同关注点，但G1除了追求低停顿外，还能建立可预测的停顿时间模型，能让使用者明确指定在一个长度为N毫秒的时间片段内，消耗在垃圾收集上的时间不得超过N毫秒，这几乎已经是实时Java（RTSJ）的垃圾收集器的特征了。 上面提到的垃圾收集器，收集的范围都是整个新生代或者老年代，而G1不再是这样。使用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合。 GC算法 垃圾收集器.resources/C3FF8C8A-97F5-4DC1-8E90-54FFAACD3D25.jpg) G1的新生代收集跟ParNew类似，当新生代占用达到一定比例的时候，开始出发收集。和CMS类似，G1收集器收集老年代对象会有短暂停顿。 收集步骤： 1、标记阶段，首先初始标记(Initial-Mark),这个阶段是停顿的(Stop the World Event)，并且会触发一次普通Mintor GC。对应GC log:GC pause (young) (inital-mark) 2、Root Region Scanning，程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在young GC之前完成。 3、Concurrent Marking，在整个堆中进行并发标记(和应用程序并发执行)，此过程可能被young GC中断。在并发标记阶段，若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)。 GC算法 垃圾收集器.resources/E8512714-576B-47AD-928D-72543F14014C.png) 4、Remark, 再标记，会有短暂停顿(STW)。再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)；G1中采用了比CMS更快的初始快照算法:snapshot-at-the-beginning (SATB)。 5、Copy/Clean up，多线程清除失活对象，会有STW。G1将回收区域的存活对象拷贝到新区域，清除Remember Sets，并发清空回收区域并把它返回到空闲区域链表中。 GC算法 垃圾收集器.resources/64EEE171-56CA-4292-B8C8-74512B629639.png) 6、复制/清除过程后。回收区域的活性对象已经被集中回收到深蓝色和深绿色区域。 GC算法 垃圾收集器.resources/A426A5C0-99CE-47B1-933C-EE2E0F34D408.png) 常用的收集器组合 GC算法 垃圾收集器.resources/7B29E97B-2070-485D-AACD-B47E5D8B6B65.png)]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>垃圾收集器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之java类的加载机制]]></title>
    <url>%2F2019%2F04%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8Bjava%E7%B1%BB%E7%9A%84%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[什么是类的加载类的加载指的是将类的.class文件中的二进制数据读入到内存中，将其放在运行时数据区的方法区内，然后在堆区创建一个 java.lang.Class对象，用来封装类在方法区内的数据结构。类的加载的最终产品是位于堆区中的 Class对象， Class对象封装了类在方法区内的数据结构，并且向Java程序员提供了访问方法区内的数据结构的接口。 java类的加载机制.resources/39F977A1-EAA5-4870-9752-0DAD0C9B2955.jpg) 类加载器并不需要等到某个类被“首次主动使用”时再加载它，JVM规范允许类加载器在预料某个类将要被使用时就预先加载它，如果在预先加载的过程中遇到了.class文件缺失或存在错误，类加载器必须在程序首次主动使用该类时才报告错误（LinkageError错误）如果这个类一直没有被程序主动使用，那么类加载器就不会报告错误 加载.class文件的方式 从本地系统中直接加载通过网络下载.class文件 从zip，jar等归档文件中加载.class文件 从专有数据库中提取.class文件将Java源文件动态编译为.class文件 类的生命周期java类的加载机制.resources/B33959C2-DD9E-4FA5-8BDE-0CFC04DDDB11.jpg)其中类加载的过程包括了加载、验证、准备、解析、初始化五个阶段。在这五个阶段中，加载、验证、准备和初始化这四个阶段发生的顺序是确定的，而解析阶段则不一定，它在某些情况下可以在初始化阶段之后开始，这是为了支持Java语言的运行时绑定（也成为动态绑定或晚期绑定）。另外注意这里的几个阶段是按顺序开始，而不是按顺序进行或完成，因为这些阶段通常都是互相交叉地混合进行的，通常在一个阶段执行的过程中调用或激活另一个阶段。 加载查找并加载类的二进制数据加载时类加载过程的第一个阶段，在加载阶段，虚拟机需要完成以下三件事情： 通过一个类的全限定名来获取其定义的二进制字节流。 将这个字节流所代表的静态存储结构转化为方法区的运行时数据结构。 在Java堆中生成一个代表这个类的 java.lang.Class对象，作为对方法区中这些数据的访问入口。 相对于类加载的其他阶段而言，加载阶段（准确地说，是加载阶段获取类的二进制字节流的动作）是可控性最强的阶段，因为开发人员既可以使用系统提供的类加载器来完成加载，也可以自定义自己的类加载器来完成加载。加载阶段完成后，虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在方法区之中，而且在Java堆中也创建一个 java.lang.Class类的对象，这样便可以通过该对象访问方法区中的这些数据。 连接验证：确保被加载的类的正确性验证是连接阶段的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段大致会完成4个阶段的检验动作： 文件格式验证：验证字节流是否符合Class文件格式的规范；例如：是否以 0xCAFEBABE开头、主次版本号是否在当前虚拟机的处理范围之内、常量池中的常量是否有不被支持的类型。 元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了 java.lang.Object之外。 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行。验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用 -Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 准备：为类的 静态变量分配内存，并将其初始化为默认值准备阶段是正式为类变量分配内存并设置类变量初始值的阶段，这些内存都将在方法区中分配。对于该阶段有以下几点需要注意： 1、这时候进行内存分配的仅包括类变量（static），而不包括实例变量，实例变量会在对象实例化时随着对象一块分配在Java堆中。 2、这里所设置的初始值通常情况下是数据类型默认的零值（如0、0L、null、false等），而不是被在Java代码中被显式地赋予的值。假设一个类变量的定义为： publicstaticintvalue=3； 那么变量value在准备阶段过后的初始值为0，而不是3，因为这时候尚未开始执行任何Java方法，而把value赋值为3的 publicstatic指令是在程序编译后，存放于类构造器 &lt;clinit&gt;（）方法之中的，所以把value赋值为3的动作将在初始化阶段才会执行。这里还需要注意如下几点： 对基本数据类型来说，对于类变量（static）和全局变量，如果不显式地对其赋值而直接使用，则系统会为其赋予默认的零值，而对于局部变量来说，在使用前必须显式地为其赋值，否则编译时不通过。对于同时被static和final修饰的常量，必须在声明的时候就为其显式地赋值，否则编译时不通过；而只被final修饰的常量则既可以在声明时显式地为其赋值，也可以在类初始化时显式地为其赋值，总之，在使用前必须为其显式地赋值，系统不会为其赋予默认零值。对于引用数据类型reference来说，如数组引用、对象引用等，如果没有对其进行显式地赋值而直接使用，系统都会为其赋予默认的零值，即null。如果在数组初始化时没有对数组中的各元素赋值，那么其中的元素将根据对应的数据类型而被赋予默认的零值。 3、如果类字段的字段属性表中存在 ConstantValue属性，即同时被final和static修饰，那么在准备阶段变量value就会被初始化为ConstValue属性所指定的值。假设上面的类变量value被定义为： publicstaticfinalintvalue=3； 编译时Javac将会为value生成ConstantValue属性，在准备阶段虚拟机就会根据 ConstantValue的设置将value赋值为3。我们可以理解为static final常量在编译期就将其结果放入了调用它的类的常量池中 解析：把类中的符号引用转换为直接引用解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。符号引用就是一组符号来描述目标，可以是任何字面量。直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄。 初始化初始化，为类的静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化。在Java中对类变量进行初始值设定有两种方式： ①声明类变量是指定初始值 ②使用静态代码块为类变量指定初始值 JVM初始化步骤 1、假如这个类还没有被加载和连接，则程序先加载并连接该类 2、假如该类的直接父类还没有被初始化，则先初始化其直接父类 3、假如类中有初始化语句，则系统依次执行这些初始化语句 类初始化时机：只有当对类的主动使用的时候才会导致类的初始化，类的主动使用包括以下六种： 创建类的实例，也就是new的方式 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（如 Class.forName(“com.shengsiyuan.Test”)） 初始化某个类的子类，则其父类也会被初始化 Java虚拟机启动时被标明为启动类的类（ JavaTest），直接使用 java.exe命令来运行某个主类 结束生命周期 执行了 System.exit()方法 程序正常执行结束 程序在执行过程中遇到了异常或错误而异常终止 由于操作系统出现错误而导致Java虚拟机进程终止 3、类加载器寻找类加载器，先来一个小例子 123456789package com.neo.classloader;public class ClassLoaderTest &#123; public static void main(String[] args) &#123; ClassLoader loader = Thread.currentThread().getContextClassLoader(); System.out.println(loader); System.out.println(loader.getParent()); System.out.println(loader.getParent().getParent()); &#125;&#125; 运行后，输出结果：123sun.misc.Launcher$AppClassLoader@64fef26asun.misc.Launcher$ExtClassLoader@1ddd40f3null 从上面的结果可以看出，并没有获取到ExtClassLoader的父Loader，原因是Bootstrap Loader（引导类加载器）是用C语言实现的，找不到一个确定的返回父Loader的方式，于是就返回null。 这几种类加载器的层次关系如下图所示：java类的加载机制.resources/660D04B1-E9B5-4609-8CD6-6674BC130BA9.jpg) 注意：这里父类加载器并不是通过继承关系来实现的，而是采用组合实现的。 站在Java虚拟机的角度来讲，只存在两种不同的类加载器：启动类加载器：它使用C++实现（这里仅限于Hotspot，也就是JDK1.5之后默认的虚拟机，有很多其他的虚拟机是用Java语言实现的），是虚拟机自身的一部分；所有其他的类加载器：这些类加载器都由Java语言实现，独立于虚拟机之外，并且全部继承自抽象类java.lang.ClassLoader，这些类加载器需要由启动类加载器加载到内存中之后才能去加载其他的类。 站在Java开发人员的角度来看，类加载器可以大致划分为以下三类： 启动类加载器： Bootstrap ClassLoader，负责加载存放在JDK\jre\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库（如rt.jar，所有的java.*开头的类均被Bootstrap ClassLoader加载）。启动类加载器是无法被Java程序直接引用的。 扩展类加载器： Extension ClassLoader，该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载DK\jre\lib\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类），开发者可以直接使用扩展类加载器。 应用程序类加载器： Application ClassLoader，该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）所指定的类，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 应用程序都是由这三种类加载器互相配合进行加载的，如果有必要，我们还可以加入自定义的类加载器。因为JVM自带的ClassLoader只是懂得从本地文件系统加载标准的java class文件，因此如果编写了自己的ClassLoader，便可以做到如下几点： 1）在执行非置信代码之前，自动验证数字签名。 2）动态地创建符合用户特定需要的定制化构建类。 3）从特定的场所取得java class，例如数据库中和网络中。 JVM类加载机制 •全盘负责，当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 •父类委托，先让父类加载器试图加载该类，只有在父类加载器无法加载该类时才尝试从自己的类路径中加载该类 •缓存机制，缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效 4、类的加载类加载有三种方式： 1、命令行启动应用时候由JVM初始化加载 2、通过Class.forName()方法动态加载 3、通过ClassLoader.loadClass()方法动态加载 例子：12345678910111213package com.neo.classloader;public class loaderTest &#123; public static void main(String[] args) throws ClassNotFoundException &#123; ClassLoader loader = HelloWorld.class.getClassLoader(); System.out.println(loader); //使用ClassLoader.loadClass()来加载类，不会执行初始化块 loader.loadClass(&quot;Test2&quot;); //使用Class.forName()来加载类，默认会执行初始化块 // Class.forName(&quot;Test2&quot;); //使用Class.forName()来加载类，并指定ClassLoader，初始化时不执行静态块 // Class.forName(&quot;Test2&quot;, false, loader); &#125; &#125; demo 类12345public class Test2 &#123; static &#123; System.out.println(&quot;静态初始化块执行了！&quot;); &#125; &#125; 分别切换加载方式，会有不同的输出结果。 Class.forName()和ClassLoader.loadClass()区别Class.forName()：将类的.class文件加载到jvm中之外，还会对类进行解释，执行类中的static块；ClassLoader.loadClass()：只干一件事情，就是将.class文件加载到jvm中，不会执行static中的内容,只有在newInstance才会去执行static块。注：Class.forName(name, initialize, loader)带参函数也可控制是否加载static块。并且只有调用了newInstance()方法采用调用构造函数，创建类的对象。 5、双亲委派模型双亲委派模型的工作流程是：如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把请求委托给父加载器去完成，依次向上，因此，所有的类加载请求最终都应该被传递到顶层的启动类加载器中，只有当父加载器在它的搜索范围中没有找到所需的类时，即无法完成该加载，子加载器才会尝试自己去加载该类。 双亲委派机制:1、当AppClassLoader加载一个class时，它首先不会自己去尝试加载这个类，而是把类加载请求委派给父类加载器ExtClassLoader去完成。 2、当ExtClassLoader加载一个class时，它首先也不会自己去尝试加载这个类，而是把类加载请求委派给BootStrapClassLoader去完成。 3、如果BootStrapClassLoader加载失败（例如在$JAVA_HOME/jre/lib里未查找到该class），会使用ExtClassLoader来尝试加载； 4、若ExtClassLoader也加载失败，则会使用AppClassLoader来加载，如果AppClassLoader也加载失败，则会报出异常ClassNotFoundException。 123456789101112131415161718192021222324252627public Class&lt;?&gt; loadClass(String name)throws ClassNotFoundException &#123; return loadClass(name, false); &#125; protected synchronized Class&lt;?&gt; loadClass(String name, boolean resolve)throws ClassNotFoundException &#123; // 首先判断该类型是否已经被加载 Class c = findLoadedClass(name); if (c == null) &#123; //如果没有被加载，就委托给父类加载或者委派给启动类加载器加载 try &#123; if (parent != null) &#123; //如果存在父类加载器，就委派给父类加载器加载 c = parent.loadClass(name, false); &#125; else &#123; //如果不存在父类加载器，就检查是否是由启动类加载器加载的类，通过调用本地方法native Class findBootstrapClass(String name) c = findBootstrapClass0(name); &#125; &#125; catch (ClassNotFoundException e) &#123; // 如果父类加载器和启动类加载器都不能完成加载任务，才调用自身的加载功能 c = findClass(name); &#125; &#125; if (resolve) &#123; resolveClass(c); &#125; return c; &#125; 双亲委派模型意义： -系统类防止内存中出现多份同样的字节码 -保证Java程序安全稳定运行 6、自定义类加载器通常情况下，我们都是直接使用系统类加载器。但是，有的时候，我们也需要自定义类加载器。比如应用是通过网络来传输 Java 类的字节码，为保证安全性，这些字节码经过了加密处理，这时系统类加载器就无法对其进行加载，这样则需要自定义类加载器来实现。自定义类加载器一般都是继承自 ClassLoader 类，从上面对 loadClass 方法来分析来看，我们只需要重写 findClass 方法即可。下面我们通过一个示例来演示自定义类加载器的流程： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364package com.neo.classloader;import java.io.*;public class MyClassLoader extends ClassLoader &#123; private String root; protected Class&lt;?&gt; findClass(String name) throws ClassNotFoundException &#123; byte[] classData = loadClassData(name); if (classData == null) &#123; throw new ClassNotFoundException(); &#125; else &#123; return defineClass(name, classData, 0, classData.length); &#125; &#125; private byte[] loadClassData(String className) &#123; String fileName = root + File.separatorChar + className.replace(&apos;.&apos;, File.separatorChar) + &quot;.class&quot;; try &#123; InputStream ins = new FileInputStream(fileName); ByteArrayOutputStream baos = new ByteArrayOutputStream(); int bufferSize = 1024; byte[] buffer = new byte[bufferSize]; int length = 0; while ((length = ins.read(buffer)) != -1) &#123; baos.write(buffer, 0, length); &#125; return baos.toByteArray(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; return null; &#125; public String getRoot() &#123; return root; &#125; public void setRoot(String root) &#123; this.root = root; &#125; public static void main(String[] args) &#123; MyClassLoader classLoader = new MyClassLoader(); classLoader.setRoot(&quot;E:\\temp&quot;); Class&lt;?&gt; testClass = null; try &#123; testClass = classLoader.loadClass(&quot;com.neo.classloader.Test2&quot;); Object object = testClass.newInstance(); System.out.println(object.getClass().getClassLoader()); &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125; catch (InstantiationException e) &#123; e.printStackTrace(); &#125; catch (IllegalAccessException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 自定义类加载器的核心在于对字节码文件的获取，如果是加密的字节码则需要在该类中对文件进行解密。由于这里只是演示，我并未对class文件进行加密，因此没有解密的过程。这里有几点需要注意： 1、这里传递的文件名需要是类的全限定性名称，即com.paddx.test.classloading.Test格式的，因为 defineClass 方法是按这种格式进行处理的。 2、最好不要重写loadClass方法，因为这样容易破坏双亲委托模式。 3、这类Test 类本身可以被 AppClassLoader 类加载，因此我们不能把 com/paddx/test/classloading/Test.class 放在类路径下。否则，由于双亲委托机制的存在，会直接导致该类由 AppClassLoader 加载，而不会通过我们自定义类加载器来加载。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>类的加载机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之JVM内存结构-高级篇]]></title>
    <url>%2F2019%2F04%2F28%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84-%E9%AB%98%E7%BA%A7%E7%AF%87%2F</url>
    <content type="text"><![CDATA[所有的Java开发人员可能会遇到这样的困惑？我该为堆内存设置多大空间呢？OutOfMemoryError的异常到底涉及到运行时数据的哪块区域？该怎么解决呢？其实如果你经常解决服务器性能问题，那么这些问题就会变的非常常见，了解JVM内存也是为了服务器出现性能问题的时候可以快速的了解那块的内存区域出现问题，以便于快速的解决生产故障。先看一张图，这张图能很清晰的说明JVM内存结构布局。 Java的内存结构：JVM内存结构.resources/476AD977-8808-4422-8349-06754DB023F5.png)JVM内存结构主要有三大块：堆内存、方法区和栈。堆内存是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配； 方法区存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)；栈又分为java虚拟机栈和本地方法栈主要用于方法的执行。 在通过一张图来了解如何通过参数来控制各区域的内存大小JVM内存结构.resources/150E44D4-976A-4736-98B8-6E8681BA307B.png) 控制参数-Xms设置堆的最小空间大小。 -Xmx设置堆的最大空间大小。 -XX:NewSize设置新生代最小空间大小。 -XX:MaxNewSize设置新生代最大空间大小。 -XX:PermSize设置永久代最小空间大小。 -XX:MaxPermSize设置永久代最大空间大小。 -Xss设置每个线程的堆栈大小。 没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制。 老年代空间大小=堆空间大小-年轻代大空间大小 从更高的一个维度再次来看JVM和系统调用之间的关系 JVM内存结构.resources/CD206F3C-4EF8-47AC-9DB5-8B67574E8ADB.png)方法区和对是所有线程共享的内存区域；而java栈、本地方法栈和程序员计数器是运行是线程私有的内存区域。 下面我们详细介绍每个区域的作用 Java堆（Heap） 对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。 Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。 根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。 如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。 方法区（Method Area） 方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。 对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。 Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。 根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。 程序计数器（Program Counter Register）程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。 如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。 此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。 JVM栈（JVM Stacks）与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。 其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。 在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。 本地方法栈（Native Method Stacks）本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。 参考：http://ifeve.com/under-the-hood-runtime-data-areas-javas-memory-model/ 《深入理解Java虚拟机：JVM高级特性与最佳实践_周志明.高清扫描版.pdf》 下载地址：http://download.csdn.net/detail/ityouknow/9557109]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>内存结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java类加载器]]></title>
    <url>%2F2019%2F04%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[类加载器类与类加载器判断类是否“相等”任意一个类，都由加载它的类加载器和这个类本身一同确立其在 Java 虚拟机中的唯一性，每一个类加载器，都有一个独立的类名称空间。 因此，比较两个类是否“相等”，只有在这两个类是由同一个类加载器加载的前提下才有意义，否则，即使这两个类来源于同一个 Class 文件，被同一个虚拟机加载，只要加载它们的类加载器不同，那么这两个类就必定不相等。 这里的“相等”，包括代表类的 Class 对象的 equals() 方法、isInstance() 方法的返回结果，也包括使用 instanceof 关键字做对象所属关系判定等情况。 加载器种类系统提供了 3 种类加载器： 启动类加载器（Bootstrap ClassLoader）： 负责将存放在 &lt;JAVA_HOME&gt;\lib 目录中的，并且能被虚拟机识别的（仅按照文件名识别，如 rt.jar，名字不符合的类库即使放在 lib 目录中也不会被加载）类库加载到虚拟机内存中。 扩展类加载器（Extension ClassLoader）： 负责加载 &lt;JAVA_HOME&gt;\lib\ext 目录中的所有类库，开发者可以直接使用扩展类加载器。 应用程序类加载器（Application ClassLoader）： 由于这个类加载器是 ClassLoader 中的 getSystemClassLoader() 方法的返回值，所以一般也称它为“系统类加载器”。它负责加载用户类路径（classpath）上所指定的类库，开发者可以直接使用这个类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 当然，如果有必要，还可以加入自己定义的类加载器。 双亲委派模型什么是双亲委派模型双亲委派模型是描述类加载器之间的层次关系。它要求除了顶层的启动类加载器外，其余的类加载器都应当有自己的父类加载器。（父子关系一般不会以继承的关系实现，而是以组合关系来复用父加载器的代码） 工作过程如果一个类加载器收到了类加载的请求，它首先不会自己去尝试加载这个类，而是把这个请求委派给父类加载器去完成，每一个层次的类加载器都是如此，因此所有的加载请求最终都应该传送到顶层的启动类加载器中，只有当父加载器反馈自己无法完成这个加载请求（找不到所需的类）时，子加载器才会尝试自己去加载。 在 java.lang.ClassLoader 中的 loadClass() 方法中实现该过程。 为什么使用双亲委派模型像 java.lang.Object 这些存放在 rt.jar 中的类，无论使用哪个类加载器加载，最终都会委派给最顶端的启动类加载器加载，从而使得不同加载器加载的 Object 类都是同一个。 相反，如果没有使用双亲委派模型，由各个类加载器自行去加载的话，如果用户自己编写了一个称为 java.lang.Object 的类，并放在 classpath 下，那么系统将会出现多个不同的 Object 类，Java 类型体系中最基础的行为也就无法保证。 （完）]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>类加载器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java类加载的过程]]></title>
    <url>%2F2019%2F04%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[类加载的过程类加载过程包括 5 个阶段：加载、验证、准备、解析和初始化。 加载加载的过程“加载”是“类加载”过程的一个阶段，不能混淆这两个名词。在加载阶段，虚拟机需要完成 3 件事： 通过类的全限定名获取该类的二进制字节流。 将二进制字节流所代表的静态结构转化为方法区的运行时数据结构。 在内存中创建一个代表该类的 java.lang.Class 对象，作为方法区这个类的各种数据的访问入口。 获取二进制字节流对于 Class 文件，虚拟机没有指明要从哪里获取、怎样获取。除了直接从编译好的 .class 文件中读取，还有以下几种方式： 从 zip 包中读取，如 jar、war等 从网络中获取，如 Applect 通过动态代理计数生成代理类的二进制字节流 由 JSP 文件生成对应的 Class 类 从数据库中读取，如 有些中间件服务器可以选择把程序安装到数据库中来完成程序代码在集群间的分发。 “非数组类”与“数组类”加载比较 非数组类加载阶段可以使用系统提供的引导类加载器，也可以由用户自定义的类加载器完成，开发人员可以通过定义自己的类加载器控制字节流的获取方式（如重写一个类加载器的 loadClass() 方法） 数组类本身不通过类加载器创建，它是由 Java 虚拟机直接创建的，再由类加载器创建数组中的元素类。 注意事项 虚拟机规范未规定 Class 对象的存储位置，对于 HotSpot 虚拟机而言，Class 对象比较特殊，它虽然是对象，但存放在方法区中。 加载阶段与连接阶段的部分内容交叉进行，加载阶段尚未完成，连接阶段可能已经开始了。但这两个阶段的开始实践仍然保持着固定的先后顺序。 验证验证的重要性验证阶段确保 Class 文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。 验证的过程 文件格式验证 验证字节流是否符合 Class 文件格式的规范，并且能被当前版本的虚拟机处理，验证点如下： 是否以魔数 0XCAFEBABE 开头 主次版本号是否在当前虚拟机处理范围内 常量池是否有不被支持的常量类型 指向常量的索引值是否指向了不存在的常量 CONSTANT_Utf8_info 型的常量是否有不符合 UTF8 编码的数据 …… 元数据验证 对字节码描述信息进行语义分析，确保其符合 Java 语法规范。 字节码验证 本阶段是验证过程中最复杂的一个阶段，是对方法体进行语义分析，保证方法在运行时不会出现危害虚拟机的事件。 符号引用验证 本阶段发生在解析阶段，确保解析正常执行。 准备准备阶段是正式为类变量（或称“静态成员变量”）分配内存并设置初始值的阶段。这些变量（不包括实例变量）所使用的内存都在方法区中进行分配。 初始值“通常情况下”是数据类型的零值（0, null…），假设一个类变量的定义为： 1public static int value = 123; 那么变量 value 在准备阶段过后的初始值为 0 而不是 123，因为这时候尚未开始执行任何 Java 方法。 存在“特殊情况”：如果类字段的字段属性表中存在 ConstantValue 属性，那么在准备阶段 value 就会被初始化为 ConstantValue 属性所指定的值，假设上面类变量 value 的定义变为： 1public static final int value = 123; 那么在准备阶段虚拟机会根据 ConstantValue 的设置将 value 赋值为 123。 解析解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。 初始化类初始化阶段是类加载过程的最后一步，是执行类构造器 &lt;clinit&gt;() 方法的过程。 &lt;clinit&gt;() 方法是由编译器自动收集类中的所有类变量的赋值动作和静态语句块（static {} 块）中的语句合并产生的，编译器收集的顺序是由语句在源文件中出现的顺序所决定的。 静态语句块中只能访问定义在静态语句块之前的变量，定义在它之后的变量，在前面的静态语句块中可以赋值，但不能访问。如下方代码所示： 1234567public class Test &#123; static &#123; i = 0; // 给变量赋值可以正常编译通过 System.out.println(i); // 这句编译器会提示“非法向前引用” &#125; static int i = 1;&#125; &lt;clinit&gt;() 方法不需要显式调用父类构造器，虚拟机会保证在子类的 &lt;clinit&gt;() 方法执行之前，父类的 &lt;clinit&gt;() 方法已经执行完毕。 由于父类的 &lt;clinit&gt;() 方法先执行，意味着父类中定义的静态语句块要优先于子类的变量赋值操作。如下方代码所示： 1234567891011121314static class Parent &#123; public static int A = 1; static &#123; A = 2; &#125;&#125;static class Sub extends Parent &#123; public static int B = A;&#125;public static void main(String[] args) &#123; System.out.println(Sub.B); // 输出 2&#125; &lt;clinit&gt;() 方法不是必需的，如果一个类没有静态语句块，也没有对类变量的赋值操作，那么编译器可以不为这个类生成 &lt;clinit&gt;() 方法。 接口中不能使用静态代码块，但接口也需要通过 &lt;clinit&gt;() 方法为接口中定义的静态成员变量显式初始化。但接口与类不同，接口的 &lt;clinit&gt;() 方法不需要先执行父类的 &lt;clinit&gt;() 方法，只有当父接口中定义的变量使用时，父接口才会初始化。 虚拟机会保证一个类的 &lt;clinit&gt;() 方法在多线程环境中被正确加锁、同步。如果多个线程同时去初始化一个类，那么只会有一个线程去执行这个类的 &lt;clinit&gt;() 方法。 （完）]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>类加载的过程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java类加载的时机]]></title>
    <url>%2F2019%2F04%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E6%97%B6%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[类加载的时机类的生命周期类从被加载到虚拟机内存开始，到卸载出内存为止，它的整个生命周期包括以下 7 个阶段： 加载 验证 准备 解析 初始化 使用 卸载 验证、准备、解析 3 个阶段统称为连接。 加载、验证、准备、初始化和卸载这 5 个阶段的顺序是确定的，类的加载过程必须按照这种顺序按部就班地开始（注意是“开始”，而不是“进行”或“完成”），而解析阶段则不一定：它在某些情况下可以在初始化后再开始，这是为了支持 Java 语言的运行时绑定。 类加载过程中“初始化”开始的时机Java 虚拟机规范没有强制约束类加载过程的第一阶段（即：加载）什么时候开始，但对于“初始化”阶段，有着严格的规定。有且仅有 5 种情况必须立即对类进行“初始化”： 在遇到 new、putstatic、getstatic、invokestatic 字节码指令时，如果类尚未初始化，则需要先触发其初始化。 对类进行反射调用时，如果类还没有初始化，则需要先触发其初始化。 初始化一个类时，如果其父类还没有初始化，则需要先初始化父类。 虚拟机启动时，用于需要指定一个包含 main() 方法的主类，虚拟机会先初始化这个主类。 当使用 JDK 1.7 的动态语言支持时，如果一个 java.lang.invoke.MethodHandle 实例最后的解析结果为 REF_getStatic、REF_putStatic、REF_invokeStatic 的方法句柄，并且这个方法句柄所对应的类还没初始化，则需要先触发其初始化。 这 5 种场景中的行为称为对一个类进行主动引用，除此之外，其它所有引用类的方式都不会触发初始化，称为被动引用。 被动引用演示 DemoDemo11234567891011121314151617181920212223242526272829/** * 被动引用 Demo1: * 通过子类引用父类的静态字段，不会导致子类初始化。 * * @author ylb * */class SuperClass &#123; static &#123; System.out.println("SuperClass init!"); &#125; public static int value = 123;&#125;class SubClass extends SuperClass &#123; static &#123; System.out.println("SubClass init!"); &#125;&#125;public class NotInitialization &#123; public static void main(String[] args) &#123; System.out.println(SubClass.value); // SuperClass init! &#125;&#125; 对于静态字段，只有直接定义这个字段的类才会被初始化，因此通过其子类来引用父类中定义的静态字段，只会触发父类的初始化而不会触发子类的初始化。 Demo2123456789101112131415/** * 被动引用 Demo2: * 通过数组定义来引用类，不会触发此类的初始化。 * * @author ylb * */public class NotInitialization &#123; public static void main(String[] args) &#123; SuperClass[] superClasses = new SuperClass[10]; &#125;&#125; 这段代码不会触发父类的初始化，但会触发“[L 全类名”这个类的初始化，它由虚拟机自动生成，直接继承自 java.lang.Object，创建动作由字节码指令 newarray 触发。 Demo31234567891011121314151617181920212223/** * 被动引用 Demo3: * 常量在编译阶段会存入调用类的常量池中，本质上并没有直接引用到定义常量的类，因此不会触发定义常量的类的初始化。 * * @author ylb * */class ConstClass &#123; static &#123; System.out.println("ConstClass init!"); &#125; public static final String HELLO_BINGO = "Hello Bingo";&#125;public class NotInitialization &#123; public static void main(String[] args) &#123; System.out.println(ConstClass.HELLO_BINGO); &#125;&#125; 编译通过之后，常量存储到 NotInitialization 类的常量池中，NotInitialization 的 Class 文件中并没有 ConstClass 类的符号引用入口，这两个类在编译成 Class 之后就没有任何联系了。 接口的加载过程接口加载过程与类加载过程稍有不同。 当一个类在初始化时，要求其父类全部都已经初始化过了，但是一个接口在初始化时，并不要求其父接口全部都完成了初始化，当真正用到父接口的时候才会初始化。 （完）]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>类加载的时机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java类文件结构]]></title>
    <url>%2F2019%2F04%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E7%B1%BB%E6%96%87%E4%BB%B6%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[类文件结构JVM 的“无关性”谈论 JVM 的无关性，主要有以下两个： 平台无关性：任何操作系统都能运行 Java 代码 语言无关性： JVM 能运行除 Java 以外的其他代码 Java 源代码首先需要使用 Javac 编译器编译成 .class 文件，然后由 JVM 执行 .class 文件，从而程序开始运行。 JVM 只认识 .class 文件，它不关心是何种语言生成了 .class 文件，只要 .class 文件符合 JVM 的规范就能运行。 目前已经有 JRuby、Jython、Scala 等语言能够在 JVM 上运行。它们有各自的语法规则，不过它们的编译器 都能将各自的源码编译成符合 JVM 规范的 .class 文件，从而能够借助 JVM 运行它们。 Java 语言中的各种变量、关键字和运算符号的语义最终都是由多条字节码命令组合而成的， 因此字节码命令所能提供的语义描述能力肯定会比 Java 语言本身更加强大。 因此，有一些 Java 语言本身无法有效支持的语言特性，不代表字节码本身无法有效支持。 Class 文件结构Class 文件时二进制文件，它的内容具有严格的规范，文件中没有任何空格，全都是连续的 0/1。Class 文件 中的所有内容被分为两种类型：无符号数、表。 无符号数 无符号数表示 Class 文件中的值，这些值没有任何类型，但有不同的长度。u1、u2、u4、u8 分别代表 1/2/4/8 字节的无符号数。 表 由多个无符号数或者其他表作为数据项构成的符合数据类型。 Class 文件具体由以下几个构成: 魔数 版本信息 常量池 访问标志 类索引、父类索引、接口索引集合 字段表集合 方法表集合 属性表集合 魔数Class 文件的头 4 个字节称为魔数，用来表示这个 Class 文件的类型。 Class 文件的魔数是用 16 进制表示的“CAFE BABE”，是不是很具有浪漫色彩？ 魔数相当于文件后缀名，只不过后缀名容易被修改，不安全，因此在 Class 文件中标识文件类型比较合适。 版本信息紧接着魔数的 4 个字节是版本信息，5-6 字节表示次版本号，7-8 字节表示主版本号，它们表示当前 Class 文件中使用的是哪个版本的 JDK。 高版本的 JDK 能向下兼容以前版本的 Class 文件，但不能运行以后版本的 Class 文件，即时文件格式并未发生任何变化，虚拟机也必需拒绝执行超过其版本号的 Class 文件。 常量池版本信息之后就是常量池，常量池中存放两种类型的常量： 字面值常量 字面值常量就是我们在程序中定义的字符串、被 final 修饰的值。 符号引用 符号引用就是我们定义的各种名字：类和接口的全限定名、字段的名字和描述符、方法的名字和描述符。 常量池的特点 常量池中常量数量不固定，因此常量池开头放置一个 u2 类型的无符号数，用来存储当前常量池的容量。 常量池的每一项常量都是一个表，表开始的第一位是一个 u1 类型的标志位（tag），代表当前这个常量属于哪种常量类型。 常量池中常量类型 类型 tag 描述 CONSTANT_utf8_info 1 UTF-8编码的字符串 CONSTANT_Integer_info 3 整型字面量 CONSTANT_Float_info 4 浮点型字面量 CONSTANT_Long_info 5 长整型字面量 CONSTANT_Double_info 6 双精度浮点型字面量 CONSTANT_Class_info 7 类或接口的符号引用 CONSTANT_String_info 8 字符串类型字面量 CONSTANT_Fieldref_info 9 字段的符号引用 CONSTANT_Methodref_info 10 类中方法的符号引用 CONSTANT_InterfaceMethodref_info 11 接口中方法的符号引用 CONSTANT_NameAndType_info 12 字段或方法的符号引用 CONSTANT_MethodHandle_info 15 表示方法句柄 CONSTANT_MethodType_info 16 标识方法类型 CONSTANT_InvokeDynamic_info 18 表示一个动态方法调用点 对于 CONSTANT_Class_info（此类型的常量代表一个类或者接口的符号引用），它的二维表结构如下： 类型 名称 数量 u1 tag 1 u2 name_index 1 tag 是标志位，用于区分常量类型；name_index 是一个索引值，它指向常量池中一个 CONSTANT_Utf8_info 类型常量，此常量代表这个类（或接口）的全限定名，这里 name_index 值若为 0x0002，也即是指向了常量池中的第二项常量。 CONSTANT_Utf8_info 型常量的结构如下： 类型 名称 数量 u1 tag 1 u2 length 1 u1 bytes length tag 是当前常量的类型；length 表示这个字符串的长度；bytes 是这个字符串的内容（采用缩略的 UTF8 编码） 访问标志在常量池结束之后，紧接着的两个字节代表访问标志，这个标志用于识别一些类或者接口层次的访问信息，包括：这个 Class 是类还是接口；是否定义为 public 类型；是否被 abstract/final 修饰。 类索引、父类索引、接口索引集合类索引和父类索引都是一个 u2 类型的数据，而接口索引集合是一组 u2 类型的数据的集合，Class 文件中由这三项数据来确定类的继承关系。类索引用于确定这个类的全限定名，父类索引用于确定这个类的父类的全限定名。 由于 Java 不允许多重继承，所以父类索引只有一个，除了 java.lang.Object 之外，所有的 Java 类都有父类，因此除了 java.lang.Object 外，所有 Java 类的父类索引都不为 0。一个类可能实现了多个接口，因此用接口索引集合来描述。这个集合第一项为 u2 类型的数据，表示索引表的容量，接下来就是接口的名字索引。 类索引和父类索引用两个 u2 类型的索引值表示，它们各自指向一个类型为 CONSTANT_Class_info 的类描述符常量，通过该常量总的索引值可以找到定义在 CONSTANT_Utf8_info 类型的常量中的全限定名字符串。 字段表集合字段表集合存储本类涉及到的成员变量，包括实例变量和类变量，但不包括方法中的局部变量。 每一个字段表只表示一个成员变量，本类中的所有成员变量构成了字段表集合。字段表结构如下： 类型 名称 数量 说明 u2 access_flags 1 字段的访问标志，与类稍有不同 u2 name_index 1 字段名字的索引 u2 descriptor_index 1 描述符，用于描述字段的数据类型。 基本数据类型用大写字母表示； 对象类型用“L 对象类型的全限定名”表示。 u2 attributes_count 1 属性表集合的长度 u2 attributes attributes_count 属性表集合，用于存放属性的额外信息，如属性的值。 字段表集合中不会出现从父类（或接口）中继承而来的字段，但有可能出现原本 Java 代码中不存在的字段，譬如在内部类中为了保持对外部类的访问性，会自动添加指向外部类实例的字段。 方法表集合方法表结构与属性表类似。 volatile 关键字 和 transient 关键字不能修饰方法，所以方法表的访问标志中没有 ACC_VOLATILE 和 ACC_TRANSIENT 标志。 方法表的属性表集合中有一张 Code 属性表，用于存储当前方法经编译器编译后的字节码指令。 属性表集合每个属性对应一张属性表，属性表的结构如下： 类型 名称 数量 u2 attribute_name_index 1 u4 attribute_length 1 u1 info attribute_length （完）]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>文件结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之JVM性能调优]]></title>
    <url>%2F2019%2F04%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJVM%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[JVM 性能调优在高性能硬件上部署程序，目前主要有两种方式： 通过 64 位 JDK 来使用大内存； 使用若干个 32 位虚拟机建立逻辑集群来利用硬件资源。 使用64位JDK管理大内存堆内存变大后，虽然垃圾收集的频率减少了，但每次垃圾回收的时间变长。 如果堆内存为14 G，那么每次 Full GC 将长达数十秒。如果 Full GC 频繁发生，那么对于一个网站来说是无法忍受的。 对于用户交互性强、对停顿时间敏感的系统，可以给 Java 虚拟机分配超大堆的前提是有把握把应用程序的 Full GC 频率控制得足够低，至少要低到不会影响用户使用。 可能面临的问题： 内存回收导致的长时间停顿； 现阶段，64位 JDK 的性能普遍比 32 位 JDK 低； 需要保证程序足够稳定，因为这种应用要是产生堆溢出几乎就无法产生堆转储快照（因为要产生超过 10GB 的 Dump 文件），哪怕产生了快照也几乎无法进行分析； 相同程序在 64 位 JDK 消耗的内存一般比 32 位 JDK 大，这是由于指针膨胀，以及数据类型对齐补白等因素导致的。 使用32位JVM建立逻辑集群在一台物理机器上启动多个应用服务器进程，每个服务器进程分配不同端口， 然后在前端搭建一个负载均衡器，以反向代理的方式来分配访问请求。 考虑到在一台物理机器上建立逻辑集群的目的仅仅是为了尽可能利用硬件资源，并不需要关心状态保留、热转移之类的高可用性能需求， 也不需要保证每个虚拟机进程有绝对的均衡负载，因此使用无 Session 复制的亲合式集群是一个不错的选择。 我们仅仅需要保障集群具备亲合性，也就是均衡器按一定的规则算法（一般根据 SessionID 分配） 将一个固定的用户请求永远分配到固定的一个集群节点进行处理即可。 可能遇到的问题： 尽量避免节点竞争全局资源，如磁盘竞争，各个节点如果同时访问某个磁盘文件的话，很可能导致 IO 异常； 很难高效利用资源池，如连接池，一般都是在节点建立自己独立的连接池，这样有可能导致一些节点池满了而另外一些节点仍有较多空余； 各个节点受到 32 位的内存限制； 大量使用本地缓存的应用，在逻辑集群中会造成较大的内存浪费，因为每个逻辑节点都有一份缓存，这时候可以考虑把本地缓存改成集中式缓存。 调优案例分析与实战场景描述一个小型系统，使用 32 位 JDK，4G 内存，测试期间发现服务端不定时抛出内存溢出异常。 加入 -XX:+HeapDumpOnOutOfMemoryError（添加这个参数后，堆内存溢出时就会输出异常日志）， 但再次发生内存溢出时，没有生成相关异常日志。 分析在 32 位 JDK 上，1.6G 分配给堆，还有一部分分配给 JVM 的其他内存，直接内存最大也只能在剩余的 0.4G 空间中分出一部分， 如果使用了 NIO，JVM 会在 JVM 内存之外分配内存空间，那么就要小心“直接内存”不足时发生内存溢出异常了。 直接内存的回收过程直接内存虽然不是 JVM 内存空间，但它的垃圾回收也由 JVM 负责。 垃圾收集进行时，虚拟机虽然会对直接内存进行回收， 但是直接内存却不能像新生代、老年代那样，发现空间不足了就通知收集器进行垃圾回收， 它只能等老年代满了后 Full GC，然后“顺便”帮它清理掉内存的废弃对象。 否则只能一直等到抛出内存溢出异常时，先 catch 掉，再在 catch 块里大喊 “System.gc()”。 要是虚拟机还是不听，那就只能眼睁睁看着堆中还有许多空闲内存，自己却不得不抛出内存溢出异常了。 （完）]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>性能调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之JVM内存分配与回收策略]]></title>
    <url>%2F2019%2F04%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJVM%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%B8%8E%E5%9B%9E%E6%94%B6%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[内存分配与回收策略对象的内存分配，就是在堆上分配（也可能经过 JIT 编译后被拆散为标量类型并间接在栈上分配），对象主要分配在新生代的 Eden 区上，少数情况下可能直接分配在老年代，分配规则不固定，取决于当前使用的垃圾收集器组合以及相关的参数配置。 以下列举几条最普遍的内存分配规则，供大家学习。 对象优先在 Eden 分配大多数情况下，对象在新生代 Eden 区中分配。当 Eden 区没有足够空间进行分配时，虚拟机将发起一次 Minor GC。 👇Minor GC vs Major GC/Full GC： Minor GC：回收新生代（包括 Eden 和 Survivor 区域），因为 Java 对象大多都具备朝生夕灭的特性，所以 Minor GC 非常频繁，一般回收速度也比较快。 Major GC / Full GC: 回收老年代，出现了 Major GC，经常会伴随至少一次的 Minor GC，但这并非绝对。Major GC 的速度一般会比 Minor GC 慢 10 倍 以上。 在 JVM 规范中，Major GC 和 Full GC 都没有一个正式的定义，所以有人也简单地认为 Major GC 清理老年代，而 Full GC 清理整个内存堆。 大对象直接进入老年代大对象是指需要大量连续内存空间的 Java 对象，如很长的字符串或数据。 一个大对象能够存入 Eden 区的概率比较小，发生分配担保的概率比较大，而分配担保需要涉及大量的复制，就会造成效率低下。 虚拟机提供了一个 -XX:PretenureSizeThreshold 参数，令大于这个设置值的对象直接在老年代分配，这样做的目的是避免在 Eden 区及两个 Survivor 区之间发生大量的内存复制。（还记得吗，新生代采用复制算法回收垃圾） 长期存活的对象将进入老年代JVM 给每个对象定义了一个对象年龄计数器。当新生代发生一次 Minor GC 后，存活下来的对象年龄 +1，当年龄超过一定值时，就将超过该值的所有对象转移到老年代中去。 使用 -XXMaxTenuringThreshold 设置新生代的最大年龄，只要超过该参数的新生代对象都会被转移到老年代中去。 动态对象年龄判定如果当前新生代的 Survivor 中，相同年龄所有对象大小的总和大于 Survivor 空间的一半，年龄 &gt;= 该年龄的对象就可以直接进入老年代，无须等到 MaxTenuringThreshold 中要求的年龄。 空间分配担保JDK 6 Update 24 之前的规则是这样的： 在发生 Minor GC 之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间， 如果这个条件成立，Minor GC 可以确保是安全的； 如果不成立，则虚拟机会查看 HandlePromotionFailure 值是否设置为允许担保失败， 如果是，那么会继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小， 如果大于，将尝试进行一次 Minor GC,尽管这次 Minor GC 是有风险的； 如果小于，或者 HandlePromotionFailure 设置不允许冒险，那此时也要改为进行一次 Full GC。 JDK 6 Update 24 之后的规则变为： 只要老年代的连续空间大于新生代对象总大小或者历次晋升的平均大小，就会进行 Minor GC，否则将进行 Full GC。 通过清除老年代中废弃数据来扩大老年代空闲空间，以便给新生代作担保。 这个过程就是分配担保。 👇总结一下有哪些情况可能会触发 JVM 进行 Full GC。 System.gc() 方法的调用此方法的调用是建议 JVM 进行 Full GC，注意这只是建议而非一定，但在很多情况下它会触发 Full GC，从而增加 Full GC 的频率。通常情况下我们只需要让虚拟机自己去管理内存即可，我们可以通过 -XX:+ DisableExplicitGC 来禁止调用 System.gc()。 老年代空间不足老年代空间不足会触发 Full GC操作，若进行该操作后空间依然不足，则会抛出如下错误：java.lang.OutOfMemoryError: Java heap space 永久代空间不足JVM 规范中运行时数据区域中的方法区，在 HotSpot 虚拟机中也称为永久代（Permanet Generation），存放一些类信息、常量、静态变量等数据，当系统要加载的类、反射的类和调用的方法较多时，永久代可能会被占满，会触发 Full GC。如果经过 Full GC 仍然回收不了，那么 JVM 会抛出如下错误信息：java.lang.OutOfMemoryError: PermGen space CMS GC 时出现 promotion failed 和 concurrent mode failurepromotion failed，就是上文所说的担保失败，而 concurrent mode failure 是在执行 CMS GC 的过程中同时有对象要放入老年代，而此时老年代空间不足造成的。 统计得到的Minor GC晋升到旧生代的平均大小大于老年代的剩余空间 👉 Previous👉 Next👉 Back to README]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>内存分配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之HotSpot垃圾收集器]]></title>
    <url>%2F2019%2F04%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHotSpot%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E5%99%A8%2F</url>
    <content type="text"><![CDATA[HotSpot垃圾收集器HotSpot 虚拟机提供了多种垃圾收集器，每种收集器都有各自的特点，虽然我们要对各个收集器进行比较，但并非为了挑选出一个最好的收集器。我们选择的只是对具体应用最合适的收集器。 新生代垃圾收集器Serial 垃圾收集器（单线程）只开启一条 GC 线程进行垃圾回收，并且在垃圾收集过程中停止一切用户线程(Stop The World)。 一般客户端应用所需内存较小，不会创建太多对象，而且堆内存不大，因此垃圾收集器回收时间短，即使在这段时间停止一切用户线程，也不会感觉明显卡顿。因此 Serial 垃圾收集器适合客户端使用。 由于 Serial 收集器只使用一条 GC 线程，避免了线程切换的开销，从而简单高效。 ParNew 垃圾收集器（多线程）ParNew 是 Serial 的多线程版本。由多条 GC 线程并行地进行垃圾清理。但清理过程依然需要 Stop The World。 ParNew 追求“低停顿时间”,与 Serial 唯一区别就是使用了多线程进行垃圾收集，在多 CPU 环境下性能比 Serial 会有一定程度的提升；但线程切换需要额外的开销，因此在单 CPU 环境中表现不如 Serial。 Parallel Scavenge 垃圾收集器（多线程）Parallel Scavenge 和 ParNew 一样，都是多线程、新生代垃圾收集器。但是两者有巨大的不同点： Parallel Scavenge：追求 CPU 吞吐量，能够在较短时间内完成指定任务，因此适合没有交互的后台计算。 ParNew：追求降低用户停顿时间，适合交互式应用。 吞吐量 = 运行用户代码时间 / (运行用户代码时间 + 垃圾收集时间) 追求高吞吐量，可以通过减少 GC 执行实际工作的时间，然而，仅仅偶尔运行 GC 意味着每当 GC 运行时将有许多工作要做，因为在此期间积累在堆中的对象数量很高。单个 GC 需要花更多的时间来完成，从而导致更高的暂停时间。而考虑到低暂停时间，最好频繁运行 GC 以便更快速完成，反过来又导致吞吐量下降。 通过参数 -XX:GCTimeRadio 设置垃圾回收时间占总 CPU 时间的百分比。 通过参数 -XX:MaxGCPauseMillis 设置垃圾处理过程最久停顿时间。 通过命令 -XX:+UseAdaptiveSizePolicy 开启自适应策略。我们只要设置好堆的大小和 MaxGCPauseMillis 或 GCTimeRadio，收集器会自动调整新生代的大小、Eden 和 Survivor 的比例、对象进入老年代的年龄，以最大程度上接近我们设置的 MaxGCPauseMillis 或 GCTimeRadio。 老年代垃圾收集器Serial Old 垃圾收集器（单线程）Serial Old 收集器是 Serial 的老年代版本，都是单线程收集器，只启用一条 GC 线程，都适合客户端应用。它们唯一的区别就是：Serial Old 工作在老年代，使用“标记-整理”算法；Serial 工作在新生代，使用“复制”算法。 Parallel Old 垃圾收集器（多线程）Parallel Old 收集器是 Parallel Scavenge 的老年代版本，追求 CPU 吞吐量。 CMS 垃圾收集器CMS(Concurrent Mark Sweep，并发标记清除)收集器是以获取最短回收停顿时间为目标的收集器（追求低停顿），它在垃圾收集时使得用户线程和 GC 线程并发执行，因此在垃圾收集过程中用户也不会感到明显的卡顿。 初始标记：Stop The World，仅使用一条初始标记线程对所有与 GC Roots 直接关联的对象进行标记。 并发标记：使用多条标记线程，与用户线程并发执行。此过程进行可达性分析，标记出所有废弃对象。速度很慢。 重新标记：Stop The World，使用多条标记线程并发执行，将刚才并发标记过程中新出现的废弃对象标记出来。 并发清除：只使用一条 GC 线程，与用户线程并发执行，清除刚才标记的对象。这个过程非常耗时。 并发标记与并发清除过程耗时最长，且可以与用户线程一起工作，因此，总体上说，CMS 收集器的内存回收过程是与用户线程一起并发执行的。 CMS 的缺点： 吞吐量低 无法处理浮动垃圾，导致频繁 Full GC 使用“标记-清除”算法产生碎片空间 对于产生碎片空间的问题，可以通过开启 -XX:+UseCMSCompactAtFullCollection，在每次 Full GC 完成后都会进行一次内存压缩整理，将零散在各处的对象整理到一块。设置参数 -XX:CMSFullGCsBeforeCompaction告诉 CMS，经过了 N 次 Full GC 之后再进行一次内存整理。 G1 通用垃圾收集器G1 是一款面向服务端应用的垃圾收集器，它没有新生代和老年代的概念，而是将堆划分为一块块独立的 Region。当要进行垃圾收集时，首先估计每个 Region 中垃圾的数量，每次都从垃圾回收价值最大的 Region 开始回收，因此可以获得最大的回收效率。 从整体上看， G1 是基于“标记-整理”算法实现的收集器，从局部（两个 Region 之间）上看是基于“复制”算法实现的，这意味着运行期间不会产生内存空间碎片。 这里抛个问题👇一个对象和它内部所引用的对象可能不在同一个 Region 中，那么当垃圾回收时，是否需要扫描整个堆内存才能完整地进行一次可达性分析？ 并不！每个 Region 都有一个 Remembered Set，用于记录本区域中所有对象引用的对象所在的区域，进行可达性分析时，只要在 GC Roots 中再加上 Remembered Set 即可防止对整个堆内存进行遍历。 如果不计算维护 Remembered Set 的操作，G1 收集器的工作过程分为以下几个步骤： 初始标记：Stop The World，仅使用一条初始标记线程对所有与 GC Roots 直接关联的对象进行标记。 并发标记：使用一条标记线程与用户线程并发执行。此过程进行可达性分析，速度很慢。 最终标记：Stop The World，使用多条标记线程并发执行。 筛选回收：回收废弃对象，此时也要 Stop The World，并使用多条筛选回收线程并发执行。 （完）]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>HotSpot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java垃圾收集策略与算法]]></title>
    <url>%2F2019%2F04%2F25%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E7%AD%96%E7%95%A5%E4%B8%8E%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[垃圾收集策略与算法程序计数器、虚拟机栈、本地方法栈随线程而生，也随线程而灭；栈帧随着方法的开始而入栈，随着方法的结束而出栈。这几个区域的内存分配和回收都具有确定性，在这几个区域内不需要过多考虑回收的问题，因为方法结束或者线程结束时，内存自然就跟随着回收了。 而对于 Java 堆和方法区，我们只有在程序运行期间才能知道会创建哪些对象，这部分内存的分配和回收都是动态的，垃圾收集器所关注的正是这部分内存。 判定对象是否存活若一个对象不被任何对象或变量引用，那么它就是无效对象，需要被回收。 引用计数法在对象头维护着一个 counter 计数器，对象被引用一次则计数器 +1；若引用失效则计数器 -1。当计数器为 0 时，就认为该对象无效了。 引用计数算法的实现简单，判定效率也很高，在大部分情况下它都是一个不错的算法。但是主流的 Java 虚拟机里没有选用引用计数算法来管理内存，主要是因为它很难解决对象之间循环引用的问题。 举个栗子👉对象 objA 和 objB 都有字段 instance，令 objA.instance = objB 并且 objB.instance = objA，由于它们互相引用着对方，导致它们的引用计数都不为 0，于是引用计数算法无法通知 GC 收集器回收它们。 可达性分析法所有和 GC Roots 直接或间接关联的对象都是有效对象，和 GC Roots 没有关联的对象就是无效对象。 GC Roots 是指： Java 虚拟机栈（栈帧中的本地变量表）中引用的对象 本地方法栈中引用的对象 方法区中常量引用的对象 方法区中类静态属性引用的对象 GC Roots 并不包括堆中对象所引用的对象，这样就不会有循环引用的问题。 引用的种类判定对象是否存活与“引用”有关。在 JDK 1.2 以前，Java 中的引用定义很传统，一个对象只有被引用或者没有被引用两种状态，我们希望能描述这一类对象：当内存空间还足够时，则保留在内存中；如果内存空间在进行垃圾手收集后还是非常紧张，则可以抛弃这些对象。很多系统的缓存功能都符合这样的应用场景。 在 JDK 1.2 之后，Java 对引用的概念进行了扩充，将引用分为了以下四种。不同的引用类型，主要体现的是对象不同的可达性状态reachable和垃圾收集的影响。 强引用（Strong Reference）类似 “Object obj = new Object()” 这类的引用，就是强引用，只要强引用存在，垃圾收集器永远不会回收被引用的对象。但是，如果我们错误地保持了强引用，比如：赋值给了 static 变量，那么对象在很长一段时间内不会被回收，会产生内存泄漏。 软引用（Soft Reference）软引用是一种相对强引用弱化一些的引用，可以让对象豁免一些垃圾收集，只有当 JVM 认为内存不足时，才会去试图回收软引用指向的对象。JVM 会确保在抛出 OutOfMemoryError 之前，清理软引用指向的对象。软引用通常用来实现内存敏感的缓存，如果还有空闲内存，就可以暂时保留缓存，当内存不足时清理掉，这样就保证了使用缓存的同时，不会耗尽内存。 弱引用（Weak Reference）弱引用的强度比软引用更弱一些。当 JVM 进行垃圾回收时，无论内存是否充足，都会回收被软引用关联的对象。 虚引用（Phantom Reference）虚引用也称幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响。它仅仅是提供了一种确保对象被 finalize 以后，做某些事情的机制，比如，通常用来做所谓的 Post-Mortem 清理机制。 回收堆中无效对象对于可达性分析中不可达的对象，也并不是没有存活的可能。 判定 finalize() 是否有必要执行JVM 会判断此对象是否有必要执行 finalize() 方法，如果对象没有覆盖 finalize() 方法，或者 finalize() 方法已经被虚拟机调用过，那么视为“没有必要执行”。那么对象基本上就真的被回收了。 如果对象被判定为有必要执行 finalize() 方法，那么对象会被放入一个 F-Queue 队列中，虚拟机会以较低的优先级执行这些 finalize()方法，但不会确保所有的 finalize() 方法都会执行结束。如果 finalize() 方法出现耗时操作，虚拟机就直接停止指向该方法，将对象清除。 对象重生或死亡如果在执行 finalize() 方法时，将 this 赋给了某一个引用，那么该对象就重生了。如果没有，那么就会被垃圾收集器清除。 任何一个对象的 finalize() 方法只会被系统自动调用一次，如果对象面临下一次回收，它的 finalize() 方法不会被再次执行，想继续在 finalize() 中自救就失效了。 回收方法区内存方法区中存放生命周期较长的类信息、常量、静态变量，每次垃圾收集只有少量的垃圾被清除。方法区中主要清除两种垃圾： 废弃常量 无用的类 判定废弃常量只要常量池中的常量不被任何变量或对象引用，那么这些常量就会被清除掉。比如，一个字符串 “bingo” 进入了常量池，但是当前系统没有任何一个 String 对象引用常量池中的 “bingo” 常量，也没有其它地方引用这个字面量，必要的话，”bingo”常量会被清理出常量池。 判定无用的类判定一个类是否是“无用的类”，条件较为苛刻。 该类的所有对象都已经被清除 加载该类的 ClassLoader 已经被回收 该类的 java.lang.Class 对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 一个类被虚拟机加载进方法区，那么在堆中就会有一个代表该类的对象：java.lang.Class。这个对象在类被加载进方法区时创建，在方法区该类被删除时清除。 垃圾收集算法学会了如何判定无效对象、无用类、废弃常量之后，剩余工作就是回收这些垃圾。常见的垃圾收集算法有以下几个： 标记-清除算法判断哪些数据需要清除，并对它们进行标记，然后清除被标记的数据。 这种方法有两个不足： 效率问题：标记和清除两个过程的效率都不高。 空间问题：标记清除之后会产生大量不连续的内存碎片，碎片太多可能导致以后需要分配较大对象时，无法找到足够的连续内存而不得不提前触发另一次垃圾收集动作。 复制算法（新生代）为了解决效率问题，“复制”收集算法出现了。它将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块内存用完，需要进行垃圾收集时，就将存活者的对象复制到另一块上面，然后将第一块内存全部清除。这种算法有优有劣： 优点：不会有内存碎片的问题。 缺点：内存缩小为原来的一半，浪费空间。 为了解决空间利用率问题，可以将内存分为三块： Eden、From Survivor、To Survivor，比例是 8:1:1，每次使用 Eden 和其中一块 Survivor。回收时，将 Eden 和 Survivor 中还存活的对象一次性复制到另外一块 Survivor 空间上，最后清理掉 Eden 和刚才使用的 Survivor 空间。这样只有 10% 的内存被浪费。 但是我们无法保证每次回收都只有不多于 10% 的对象存活，当 Survivor 空间不够，需要依赖其他内存（指老年代）进行分配担保。 分配担保为对象分配内存空间时，如果 Eden+Survivor 中空闲区域无法装下该对象，会触发 MinorGC 进行垃圾收集。但如果 Minor GC 过后依然有超过 10% 的对象存活，这样存活的对象直接通过分配担保机制进入老年代，然后再将新对象存入 Eden 区。 标记-整理算法（老年代）在回收垃圾前，首先将废弃对象做上标记，然后将未标记的对象移到一边，最后清空另一边区域即可。 这是一种老年代的垃圾收集算法。老年代的对象一般寿命比较长，因此每次垃圾回收会有大量对象存活，如果采用复制算法，每次需要复制大量存活的对象，效率很低。 分代收集算法根据对象存活周期的不同，将内存划分为几块。一般是把 Java 堆分为新生代和老年代，针对各个年代的特点采用最适当的收集算法。 新生代：复制算法 老年代：标记-清除算法、标记-整理算法 （完）]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>垃圾收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之HotSpot虚拟机对象探秘]]></title>
    <url>%2F2019%2F04%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BHotSpot%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AF%B9%E8%B1%A1%E6%8E%A2%E7%A7%98%2F</url>
    <content type="text"><![CDATA[HotSpot虚拟机对象探秘对象的内存布局在 HotSpot 虚拟机中，对象的内存布局分为以下 3 块区域： 对象头（Header） 实例数据（Instance Data） 对齐填充（Padding） 对象头对象头记录了对象在运行过程中所需要使用的一些数据： 哈希码 GC 分代年龄 锁状态标志 线程持有的锁 偏向线程 ID 偏向时间戳 对象头可能包含类型指针，通过该指针能确定对象属于哪个类。如果对象是一个数组，那么对象头还会包括数组长度。 实例数据实例数据部分就是成员变量的值，其中包括父类成员变量和本类成员变量。 对齐填充用于确保对象的总长度为 8 字节的整数倍。 HotSpot VM 的自动内存管理系统要求对象的大小必须是 8 字节的整数倍。而对象头部分正好是 8 字节的倍数（1 倍或 2 倍），因此，当对象实例数据部分没有对齐时，就需要通过对齐填充来补全。 对齐填充并不是必然存在，也没有特别的含义，它仅仅起着占位符的作用。 对象的创建过程类加载检查虚拟机在解析.class文件时，若遇到一条 new 指令，首先它会去检查常量池中是否有这个类的符号引用，并且检查这个符号引用所代表的类是否已被加载、解析和初始化过。如果没有，那么必须先执行相应的类加载过程。 为新生对象分配内存对象所需内存的大小在类加载完成后便可完全确定，接下来从堆中划分一块对应大小的内存空间给新的对象。分配堆中内存有两种方式： 指针碰撞如果 Java 堆中内存绝对规整（说明采用的是“复制算法”或“标记整理法”），空闲内存和已使用内存中间放着一个指针作为分界点指示器，那么分配内存时只需要把指针向空闲内存挪动一段与对象大小一样的距离，这种分配方式称为“指针碰撞”。 空闲列表如果 Java 堆中内存并不规整，已使用的内存和空闲内存交错（说明采用的是标记-清除法，有碎片），此时没法简单进行指针碰撞， VM 必须维护一个列表，记录其中哪些内存块空闲可用。分配之时从空闲列表中找到一块足够大的内存空间划分给对象实例。这种方式称为“空闲列表”。 初始化分配完内存后，为对象中的成员变量赋上初始值，设置对象头信息，调用对象的构造函数方法进行初始化。 至此，整个对象的创建过程就完成了。 对象的访问方式所有对象的存储空间都是在堆中分配的，但是这个对象的引用却是在堆栈中分配的。也就是说在建立一个对象时两个地方都分配内存，在堆中分配的内存实际建立这个对象，而在堆栈中分配的内存只是一个指向这个堆对象的指针（引用）而已。 那么根据引用存放的地址类型的不同，对象有不同的访问方式。 句柄访问方式堆中需要有一块叫做“句柄池”的内存空间，句柄中包含了对象实例数据与类型数据各自的具体地址信息。 引用类型的变量存放的是该对象的句柄地址（reference）。访问对象时，首先需要通过引用类型的变量找到该对象的句柄，然后根据句柄中对象的地址找到对象。 直接指针访问方式引用类型的变量直接存放对象的地址，从而不需要句柄池，通过引用能够直接访问对象。但对象所在的内存空间需要额外的策略存储对象所属的类信息的地址。 需要说明的是，HotSpot 采用第二种方式，即直接指针方式来访问对象，只需要一次寻址操作，所以在性能上比句柄访问方式快一倍。但像上面所说，它需要额外的策略来存储对象在方法区中类信息的地址。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>HotSpot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之JVM内存结构-基础篇]]></title>
    <url>%2F2019%2F04%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84-%E5%9F%BA%E7%A1%80%E7%AF%87%2F</url>
    <content type="text"><![CDATA[JVM 内存结构Java 虚拟机的内存空间分为 5 个部分： 程序计数器 Java 虚拟机栈 本地方法栈 堆 方法区JDK 1.8 同 JDK 1.7 比，最大的差别就是：元数据区取代了永久代。元空间的本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元数据空间并不在虚拟机中，而是使用本地内存。 程序计数器（PC 寄存器）程序计数器的定义程序计数器是一块较小的内存空间，是当前线程正在执行的那条字节码指令的地址。若当前线程正在执行的是一个本地方法，那么此时程序计数器为Undefined。 程序计数器的作用 字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制。 在多线程情况下，程序计数器记录的是当前线程执行的位置，从而当线程切换回来时，就知道上次线程执行到哪了。 程序计数器的特点 是一块较小的内存空间。 线程私有，每条线程都有自己的程序计数器。 生命周期：随着线程的创建而创建，随着线程的结束而销毁。 是唯一一个不会出现OutOfMemoryError的内存区域。 Java 虚拟机栈（Java 栈）Java 虚拟机栈的定义Java 虚拟机栈是描述 Java 方法运行过程的内存模型。 Java 虚拟机栈会为每一个即将运行的 Java 方法创建一块叫做“栈帧”的区域，用于存放该方法运行过程中的一些信息，如： 局部变量表 操作数栈 动态链接 方法出口信息 …… 压栈出栈过程当方法运行过程中需要创建局部变量时，就将局部变量的值存入栈帧中的局部变量表中。 Java 虚拟机栈的栈顶的栈帧是当前正在执行的活动栈，也就是当前正在执行的方法，PC 寄存器也会指向这个地址。只有这个活动的栈帧的本地变量可以被操作数栈使用，当在这个栈帧中调用另一个方法，与之对应的栈帧又会被创建，新创建的栈帧压入栈顶，变为当前的活动栈帧。 方法结束后，当前栈帧被移出，栈帧的返回值变成新的活动栈帧中操作数栈的一个操作数。如果没有返回值，那么新的活动栈帧中操作数栈的操作数没有变化。 由于Java 虚拟机栈是与线程对应的，数据不是线程共享的，因此不用关心数据一致性问题，也不会存在同步锁的问题。 Java 虚拟机栈的特点 局部变量表随着栈帧的创建而创建，它的大小在编译时确定，创建时只需分配事先规定的大小即可。在方法运行过程中，局部变量表的大小不会发生改变。 Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。 StackOverFlowError 若 Java 虚拟机栈的大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度时，抛出 StackOverFlowError 异常。 OutOfMemoryError 若允许动态扩展，那么当线程请求栈时内存用完了，无法再动态扩展时，抛出 OutOfMemoryError 异常。 Java 虚拟机栈也是线程私有，随着线程创建而创建，随着线程的结束而销毁。 出现 StackOverFlowError 时，内存空间可能还有很多。 本地方法栈（C 栈）本地方法栈的定义本地方法栈是为 JVM 运行 Native 方法准备的空间，由于很多 Native 方法都是用 C 语言实现的，所以它通常又叫 C 栈。它与 Java 虚拟机栈实现的功能类似，只不过本地方法栈是描述本地方法运行过程的内存模型。 栈帧变化过程本地方法被执行时，在本地方法栈也会创建一块栈帧，用于存放该方法的局部变量表、操作数栈、动态链接、方法出口信息等。 方法执行结束后，相应的栈帧也会出栈，并释放内存空间。也会抛出 StackOverFlowError 和 OutOfMemoryError 异常。 如果 Java 虚拟机本身不支持 Native 方法，或是本身不依赖于传统栈，那么可以不提供本地方法栈。如果支持本地方法栈，那么这个栈一般会在线程创建的时候按线程分配。 堆堆的定义堆是用来存放对象的内存空间，几乎所有的对象都存储在堆中。 堆的特点 线程共享，整个 Java 虚拟机只有一个堆，所有的线程都访问同一个堆。而程序计数器、Java 虚拟机栈、本地方法栈都是一个线程对应一个。 在虚拟机启动时创建。 是垃圾回收的主要场所。 进一步可分为：新生代(Eden区 From Survior To Survivor)、老年代。 不同的区域存放不同生命周期的对象，这样可以根据不同的区域使用不同的垃圾回收算法，更具有针对性。 堆的大小既可以固定也可以扩展，但对于主流的虚拟机，堆的大小是可扩展的，因此当线程请求分配内存，但堆已满，且内存已无法再扩展时，就抛出 OutOfMemoryError 异常。 Java 堆所使用的内存不需要保证是连续的。而由于堆是被所有线程共享的，所以对它的访问需要注意同步问题，方法和对应的属性都需要保证一致性。 方法区方法区的定义Java 虚拟机规范中定义方法区是堆的一个逻辑部分。方法区存放以下信息： 已经被虚拟机加载的类信息 常量 静态变量 即时编译器编译后的代码 方法区的特点 线程共享。 方法区是堆的一个逻辑部分，因此和堆一样，都是线程共享的。整个虚拟机中只有一个方法区。 永久代。 方法区中的信息一般需要长期存在，而且它又是堆的逻辑分区，因此用堆的划分方法，把方法区称为“永久代”。 内存回收效率低。 方法区中的信息一般需要长期存在，回收一遍之后可能只有少量信息无效。主要回收目标是：对常量池的回收；对类型的卸载。 Java 虚拟机规范对方法区的要求比较宽松。 和堆一样，允许固定大小，也允许动态扩展，还允许不实现垃圾回收。 运行时常量池方法区中存放：类信息、常量、静态变量、即时编译器编译后的代码。常量就存放在运行时常量池中。 当类被 Java 虚拟机加载后， .class 文件中的常量就存放在方法区的运行时常量池中。而且在运行期间，可以向常量池中添加新的常量。如 String 类的 intern() 方法就能在运行期间向常量池中添加字符串常量。 直接内存（堆外内存）直接内存是除 Java 虚拟机之外的内存，但也可能被 Java 使用。 操作直接内存在 NIO 中引入了一种基于通道和缓冲的 IO 方式。它可以通过调用本地方法直接分配 Java 虚拟机之外的内存，然后通过一个存储在堆中的DirectByteBuffer对象直接操作该内存，而无须先将外部内存中的数据复制到堆中再进行操作，从而提高了数据操作的效率。 直接内存的大小不受 Java 虚拟机控制，但既然是内存，当内存不足时就会抛出 OutOfMemoryError 异常。 直接内存与堆内存比较 直接内存申请空间耗费更高的性能 直接内存读取 IO 的性能要优于普通的堆内存。 直接内存作用链： 本地 IO -&gt; 直接内存 -&gt; 本地 IO 堆内存作用链：本地 IO -&gt; 直接内存 -&gt; 非直接内存 -&gt; 直接内存 -&gt; 本地 IO 服务器管理员在配置虚拟机参数时，会根据实际内存设置-Xmx等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制，从而导致动态扩展时出现OutOfMemoryError异常。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JVM</tag>
        <tag>内存结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(ArrayBlockingQueue)]]></title>
    <url>%2F2019%2F04%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(ArrayBlockingQueue)%2F</url>
    <content type="text"><![CDATA[ArrayBlockingQueue介绍ArrayBlockingQueue是数组实现的线程安全的有界的阻塞队列。 线程安全是指，ArrayBlockingQueue内部通过“互斥锁”保护竞争资源，实现了多线程对竞争资源的互斥访问。而有界，则是指ArrayBlockingQueue对应的数组是有界限的。 阻塞队列，是指多线程访问竞争资源时，当竞争资源已被某线程获取时，其它要获取该资源的线程需要阻塞等待；而且，ArrayBlockingQueue是按 FIFO（先进先出）原则对元素进行排序，元素都是从尾部插入到队列，从头部开始返回。 注意:ArrayBlockingQueue不同于ConcurrentLinkedQueue，ArrayBlockingQueue是数组实现的，并且是有界限的;而ConcurrentLinkedQueue是链表实现的，是无界限的. ArrayBlockingQueue原理和数据结构ArrayBlockingQueue的数据结构，如下图所示：.resources/0BA99FFA-FAC7-470F-AB00-4523EDCCFF80.jpg)说明： ArrayBlockingQueue继承于AbstractQueue，并且它实现了BlockingQueue接口。 ArrayBlockingQueue内部是通过Object[]数组保存数据的，也就是说ArrayBlockingQueue本质上是通过数组实现的。ArrayBlockingQueue的大小，即数组的容量是创建ArrayBlockingQueue时指定的。 ArrayBlockingQueue与ReentrantLock是组合关系，ArrayBlockingQueue中包含一个ReentrantLock对象(lock)。ReentrantLock是可重入的互斥锁，ArrayBlockingQueue就是根据该互斥锁实现“多线程对竞争资源的互斥访问”。而且，ReentrantLock分为公平锁和非公平锁，关于具体使用公平锁还是非公平锁，在创建ArrayBlockingQueue时可以指定；而且，ArrayBlockingQueue默认会使用非公平锁。 ArrayBlockingQueue与Condition是组合关系，ArrayBlockingQueue中包含两个Condition对象(notEmpty和notFull)。而且，Condition又依赖于ArrayBlockingQueue而存在，通过Condition可以实现对ArrayBlockingQueue的更精确的访问 – (01)若某线程(线程A)要取数据时，数组正好为空，则该线程会执行notEmpty.await()进行等待；当其它某个线程(线程B)向数组中插入了数据之后，会调用notEmpty.signal()唤醒“notEmpty上的等待线程”。此时，线程A会被唤醒从而得以继续运行。(02)若某线程(线程H)要插入数据时，数组已满，则该线程会它执行notFull.await()进行等待；当其它某个线程(线程I)取出数据之后，会调用notFull.signal()唤醒“notFull上的等待线程”。此时，线程H就会被唤醒从而得以继续运行。 ArrayBlockingQueue函数列表123456789101112131415161718192021222324252627282930313233343536373839404142434445// 创建一个带有给定的（固定）容量和默认访问策略的 ArrayBlockingQueue。ArrayBlockingQueue(int capacity)// 创建一个具有给定的（固定）容量和指定访问策略的 ArrayBlockingQueue。ArrayBlockingQueue(int capacity, boolean fair)// 创建一个具有给定的（固定）容量和指定访问策略的 ArrayBlockingQueue，它最初包含给定 collection 的元素，并以 collection 迭代器的遍历顺序添加元素。ArrayBlockingQueue(int capacity, boolean fair, Collection&lt;? extends E&gt; c)// 将指定的元素插入到此队列的尾部（如果立即可行且不会超过该队列的容量），在成功时返回 true，如果此队列已满，则抛出 IllegalStateException。boolean add(E e)// 自动移除此队列中的所有元素。void clear()// 如果此队列包含指定的元素，则返回 true。boolean contains(Object o)// 移除此队列中所有可用的元素，并将它们添加到给定 collection 中。int drainTo(Collection&lt;? super E&gt; c)// 最多从此队列中移除给定数量的可用元素，并将这些元素添加到给定 collection 中。int drainTo(Collection&lt;? super E&gt; c, int maxElements)// 返回在此队列中的元素上按适当顺序进行迭代的迭代器。Iterator&lt;E&gt; iterator()// 将指定的元素插入到此队列的尾部（如果立即可行且不会超过该队列的容量），在成功时返回 true，如果此队列已满，则返回 false。boolean offer(E e)// 将指定的元素插入此队列的尾部，如果该队列已满，则在到达指定的等待时间之前等待可用的空间。boolean offer(E e, long timeout, TimeUnit unit)// 获取但不移除此队列的头；如果此队列为空，则返回 null。E peek()// 获取并移除此队列的头，如果此队列为空，则返回 null。E poll()// 获取并移除此队列的头部，在指定的等待时间前等待可用的元素（如果有必要）。E poll(long timeout, TimeUnit unit)// 将指定的元素插入此队列的尾部，如果该队列已满，则等待可用的空间。void put(E e)// 返回在无阻塞的理想情况下（不存在内存或资源约束）此队列能接受的其他元素数量。int remainingCapacity()// 从此队列中移除指定元素的单个实例（如果存在）。boolean remove(Object o)// 返回此队列中元素的数量。int size()// 获取并移除此队列的头部，在元素变得可用之前一直等待（如果有必要）。E take()// 返回一个按适当顺序包含此队列中所有元素的数组。Object[] toArray()// 返回一个按适当顺序包含此队列中所有元素的数组；返回数组的运行时类型是指定数组的运行时类型。&lt;T&gt; T[] toArray(T[] a)// 返回此 collection 的字符串表示形式。String toString() ArrayBlockingQueue源码分析下面从ArrayBlockingQueue的创建，添加，取出，遍历这几个方面对ArrayBlockingQueue进行分析。1. 创建下面以ArrayBlockingQueue(int capacity, boolean fair)来进行说明。12345678public ArrayBlockingQueue(int capacity, boolean fair) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.items = new Object[capacity]; lock = new ReentrantLock(fair); notEmpty = lock.newCondition(); notFull = lock.newCondition();&#125; 说明：(01) items是保存“阻塞队列”数据的数组。它的定义如下：1final Object[] items; (02) fair是“可重入的独占锁(ReentrantLock)”的类型。fair为true，表示是公平锁；fair为false，表示是非公平锁。notEmpty和notFull是锁的两个Condition条件。它们的定义如下：123final ReentrantLock lock;private final Condition notEmpty;private final Condition notFull; Lock的作用是提供独占锁机制，来保护竞争资源；而Condition是为了更加精细的对锁进行控制，它依赖于Lock，通过某个条件对多线程进行控制。notEmpty表示“锁的非空条件”。当某线程想从队列中取数据时，而此时又没有数据，则该线程通过notEmpty.await()进行等待；当其它线程向队列中插入了元素之后，就调用notEmpty.signal()唤醒“之前通过notEmpty.await()进入等待状态的线程”。同理，notFull表示“锁的满条件”。当某线程想向队列中插入元素，而此时队列已满时，该线程等待；当其它线程从队列中取出元素之后，就唤醒该等待的线程。 2. 添加 下面以offer(E e)为例，对ArrayBlockingQueue的添加方法进行说明。1234567891011121314151617181920public boolean offer(E e) &#123; // 创建插入的元素是否为null，是的话抛出NullPointerException异常 checkNotNull(e); // 获取“该阻塞队列的独占锁” final ReentrantLock lock = this.lock; lock.lock(); try &#123; // 如果队列已满，则返回false。 if (count == items.length) return false; else &#123; // 如果队列未满，则插入e，并返回true。 insert(e); return true; &#125; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 说明：offer(E e)的作用是将e插入阻塞队列的尾部。如果队列已满，则返回false，表示插入失败；否则，插入元素，并返回true。(01) count表示”队列中的元素个数“。除此之外，队列中还有另外两个遍历takeIndex和putIndex。takeIndex表示下一个被取出元素的索引，putIndex表示下一个被添加元素的索引。它们的定义如下：123456// 队列中的元素个数int takeIndex;// 下一个被取出元素的索引int putIndex;// 下一个被添加元素的索引int count; (02) insert()的源码如下：12345678910private void insert(E x) &#123; // 将x添加到”队列“中 items[putIndex] = x; // 设置”下一个被取出元素的索引“ putIndex = inc(putIndex); // 将”队列中的元素个数”+1 ++count; // 唤醒notEmpty上的等待线程 notEmpty.signal();&#125; insert()在插入元素之后，会唤醒notEmpty上面的等待线程。inc()的源码如下：123final int inc(int i) &#123; return (++i == items.length) ? 0 : i;&#125; 若i+1的值等于“队列的长度”，即添加元素之后，队列满；则设置“下一个被添加元素的索引”为0。3. 取出 下面以take()为例，对ArrayBlockingQueue的取出方法进行说明。12345678910111213141516public E take() throws InterruptedException &#123; // 获取“队列的独占锁” final ReentrantLock lock = this.lock; // 获取“锁”，若当前线程是中断状态，则抛出InterruptedException异常 lock.lockInterruptibly(); try &#123; // 若“队列为空”，则一直等待。 while (count == 0) notEmpty.await(); // 取出元素 return extract(); &#125; finally &#123; // 释放“锁” lock.unlock(); &#125;&#125; 说明：take()的作用是取出并返回队列的头。若队列为空，则一直等待。extract()的源码如下：1234567891011121314private E extract() &#123; final Object[] items = this.items; // 强制将元素转换为“泛型E” E x = this.&lt;E&gt;cast(items[takeIndex]); // 将第takeIndex元素设为null，即删除。同时，帮助GC回收。 items[takeIndex] = null; // 设置“下一个被取出元素的索引” takeIndex = inc(takeIndex); // 将“队列中元素数量”-1 --count; // 唤醒notFull上的等待线程。 notFull.signal(); return x;&#125; 说明：extract()在删除元素之后，会唤醒notFull上的等待线程。 4. 遍历下面对ArrayBlockingQueue的遍历方法进行说明。123public Iterator&lt;E&gt; iterator() &#123; return new Itr();&#125; Itr是实现了Iterator接口的类，它的源码如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778private class Itr implements Iterator&lt;E&gt; &#123; // 队列中剩余元素的个数 private int remaining; // Number of elements yet to be returned // 下一次调用next()返回的元素的索引 private int nextIndex; // Index of element to be returned by next // 下一次调用next()返回的元素 private E nextItem; // Element to be returned by next call to next // 上一次调用next()返回的元素 private E lastItem; // Element returned by last call to next // 上一次调用next()返回的元素的索引 private int lastRet; // Index of last element returned, or -1 if none Itr() &#123; // 获取“阻塞队列”的锁 final ReentrantLock lock = ArrayBlockingQueue.this.lock; lock.lock(); try &#123; lastRet = -1; if ((remaining = count) &gt; 0) nextItem = itemAt(nextIndex = takeIndex); &#125; finally &#123; // 释放“锁” lock.unlock(); &#125; &#125; public boolean hasNext() &#123; return remaining &gt; 0; &#125; public E next() &#123; // 获取“阻塞队列”的锁 final ReentrantLock lock = ArrayBlockingQueue.this.lock; lock.lock(); try &#123; // 若“剩余元素&lt;=0”，则抛出异常。 if (remaining &lt;= 0) throw new NoSuchElementException(); lastRet = nextIndex; // 获取第nextIndex位置的元素 E x = itemAt(nextIndex); // check for fresher value if (x == null) &#123; x = nextItem; // we are forced to report old value lastItem = null; // but ensure remove fails &#125; else lastItem = x; while (--remaining &gt; 0 &amp;&amp; // skip over nulls (nextItem = itemAt(nextIndex = inc(nextIndex))) == null) ; return x; &#125; finally &#123; lock.unlock(); &#125; &#125; public void remove() &#123; final ReentrantLock lock = ArrayBlockingQueue.this.lock; lock.lock(); try &#123; int i = lastRet; if (i == -1) throw new IllegalStateException(); lastRet = -1; E x = lastItem; lastItem = null; // only remove if item still at index if (x != null &amp;&amp; x == items[i]) &#123; boolean removingHead = (i == takeIndex); removeAt(i); if (!removingHead) nextIndex = dec(nextIndex); &#125; &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; ArrayBlockingQueue示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.*;import java.util.concurrent.*;/* * ArrayBlockingQueue是“线程安全”的队列，而LinkedList是非线程安全的。 * * 下面是“多个线程同时操作并且遍历queue”的示例 * (01) 当queue是ArrayBlockingQueue对象时，程序能正常运行。 * (02) 当queue是LinkedList对象时，程序会产生ConcurrentModificationException异常。 */public class ArrayBlockingQueueDemo1&#123; // TODO: queue是LinkedList对象时，程序会出错。 //private static Queue&lt;String&gt; queue = new LinkedList&lt;String&gt;(); private static Queue&lt;String&gt; queue = new ArrayBlockingQueue&lt;String&gt;(20); public static void main(String[] args) &#123; // 同时启动两个线程对queue进行操作！ new MyThread(&quot;ta&quot;).start(); new MyThread(&quot;tb&quot;).start(); &#125; private static void printAll() &#123; String value; Iterator iter = queue.iterator(); while(iter.hasNext()) &#123; value = (String)iter.next(); System.out.print(value+&quot;, &quot;); &#125; System.out.println(); &#125; private static class MyThread extends Thread &#123; MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; int i = 0; while (i++ &lt; 6) &#123; // “线程名” + &quot;-&quot; + &quot;序号&quot; String val = Thread.currentThread().getName()+i; queue.add(val); // 通过“Iterator”遍历queue。 printAll(); &#125; &#125; &#125;&#125; 其中一次运行结果：123456789101112ta1, ta1, tb1, ta1, tb1, ta1, ta2, tb1, ta1, ta2, tb1, tb2, ta2, ta1, tb2, tb1, ta3, ta2, ta1, tb2, tb1, ta3, ta2, tb3, tb2, ta1, ta3, tb1, tb3, ta2, ta4, tb2, ta1, ta3, tb1, tb3, ta2, ta4, tb2, tb4, ta3, ta1, tb3, tb1, ta4, ta2, tb4, tb2, ta5, ta3, ta1, tb3, tb1, ta4, ta2, tb4, tb2, ta5, ta3, tb5, tb3, ta1, ta4, tb1, tb4, ta2, ta5, tb2, tb5, ta3, ta6, tb3, ta4, tb4, ta5, tb5, ta6, tb6, 结果说明：如果将源码中的queue改成LinkedList对象时，程序会产生ConcurrentModificationException异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>ArrayBlockingQueue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(ConcurrentHashMap)]]></title>
    <url>%2F2019%2F04%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(ConcurrentHashMap)%2F</url>
    <content type="text"><![CDATA[前言HashMap是我们平时开发过程中用的比较多的集合，但它是非线程安全的，在涉及到多线程并发的情况，进行get操作有可能会引起死循环，导致CPU利用率接近100%。 12345678910final HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(2);for (int i = 0; i &lt; 10000; i++) &#123; new Thread(new Runnable() &#123; @Override public void run() &#123; map.put(UUID.randomUUID().toString(), &quot;&quot;); &#125; &#125;).start();&#125; 解决方案有Hashtable和Collections.synchronizedMap(hashMap)，不过这两个方案基本上是对读写进行加锁操作，一个线程在读写元素，其余线程必须等待，性能可想而知。 JDK1.7分析ConcurrentHashMap采用 分段锁的机制，实现并发的更新操作，底层采用数组+链表的存储结构。其包含两个核心静态内部类 Segment和HashEntry。 Segment继承ReentrantLock用来充当锁的角色，每个 Segment 对象守护每个散列映射表的若干个桶。HashEntry 用来封装映射表的键/值对；每个桶是由若干个 HashEntry 对象链接起来的链表。一个 ConcurrentHashMap 实例中包含由若干个 Segment 对象组成的数组，下面我们通过一个图来演示一下 ConcurrentHashMap 的结构：.resources/4C9E2E7A-9471-46E4-BC40-3F7803EA8C2B.png) JDK1.8分析1.8的实现已经抛弃了Segment分段锁机制，利用CAS+Synchronized来保证并发更新的安全，底层采用数组+链表+红黑树的存储结构。.resources/33571F0A-5AF5-4191-AC11-06AC6A8D8E03.png)重要概念在开始之前，有些重要的概念需要介绍一下: table:默认为null,初始化发生在第一次插入操作,默认大小为16的数组,用来存储Node节点数据,扩容时大小总是2的幂次方。 nextTable:默认为null，扩容时新生成的数组，其大小为原数组的两倍。 sizeCtl:默认为0，用来控制table的初始化和扩容操作，具体应用在后续会体现出来。 -1代表table正在初始化 -N表示有N-1个线程正在进行扩容操作 其余情况： 1、如果table未初始化，表示table需要初始化的大小。 2、如果table初始化完成，表示table的容量，默认是table大小的0.75倍，居然用这个公式算0.75（n - (n &gt;&gt;&gt; 2)）。 Node：保存key，value及key的hash值的数据结构。 1234567class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; volatile V val; volatile Node&lt;K,V&gt; next; ... 省略部分代码&#125; 其中value和next都用volatile修饰，保证并发的可见性。 ForwardingNode：一个特殊的Node节点，hash值为-1，其中存储nextTable的引用。1234567final class ForwardingNode&lt;K,V&gt; extends Node&lt;K,V&gt; &#123; final Node&lt;K,V&gt;[] nextTable; ForwardingNode(Node&lt;K,V&gt;[] tab) &#123; super(MOVED, null, null, null); this.nextTable = tab; &#125;&#125; 只有table发生扩容的时候，ForwardingNode才会发挥作用，作为一个占位符放在table中表示当前节点为null或则已经被移动。实例初始化实例化ConcurrentHashMap时带参数时，会根据参数调整table的大小，假设参数为100，最终会调整成256，确保table的大小总是2的幂次方，算法如下：12345678910ConcurrentHashMap&lt;String, String&gt; hashMap = new ConcurrentHashMap&lt;&gt;(100);private static final int tableSizeFor(int c) &#123; int n = c - 1; n |= n &gt;&gt;&gt; 1; n |= n &gt;&gt;&gt; 2; n |= n &gt;&gt;&gt; 4; n |= n &gt;&gt;&gt; 8; n |= n &gt;&gt;&gt; 16; return (n &lt; 0) ? 1 : (n &gt;= MAXIMUM_CAPACITY) ? MAXIMUM_CAPACITY : n + 1;&#125; 注意，ConcurrentHashMap在构造函数中只会初始化sizeCtl值，并不会直接初始化table，而是延缓到第一次put操作。table初始化前面已经提到过，table初始化操作会延缓到第一次put行为。但是put是可以并发执行的，Doug Lea是如何实现table只初始化一次的？让我们来看看源码的实现。1234567891011121314151617181920212223private final Node&lt;K,V&gt;[] initTable() &#123; Node&lt;K,V&gt;[] tab; int sc; while ((tab = table) == null || tab.length == 0) &#123;//如果一个线程发现sizeCtl&lt;0，意味着另外的线程执行CAS操作成功，当前线程只需要让出cpu时间片 if ((sc = sizeCtl) &lt; 0) Thread.yield(); // lost initialization race; just spin else if (U.compareAndSwapInt(this, SIZECTL, sc, -1)) &#123; try &#123; if ((tab = table) == null || tab.length == 0) &#123; int n = (sc &gt; 0) ? sc : DEFAULT_CAPACITY; @SuppressWarnings(&quot;unchecked&quot;) Node&lt;K,V&gt;[] nt = (Node&lt;K,V&gt;[])new Node&lt;?,?&gt;[n]; table = tab = nt; sc = n - (n &gt;&gt;&gt; 2); &#125; &#125; finally &#123; sizeCtl = sc; &#125; break; &#125; &#125; return tab;&#125; sizeCtl默认为0，如果ConcurrentHashMap实例化时有传参数，sizeCtl会是一个2的幂次方的值。所以执行第一次put操作的线程会执行Unsafe.compareAndSwapInt方法修改sizeCtl为-1，有且只有一个线程能够修改成功，其它线程通过Thread.yield()让出CPU时间片等待table初始化完成。put操作假设table已经初始化完成，put操作采用CAS+synchronized实现并发插入或更新操作，具体实现如下。12345678910111213141516171819final V putVal(K key, V value, boolean onlyIfAbsent) &#123; if (key == null || value == null) throw new NullPointerException(); int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K,V&gt;[] tab = table;;) &#123; Node&lt;K,V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) &#123; if (casTabAt(tab, i, null, new Node&lt;K,V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin &#125; else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); ...省略部分代码 &#125; addCount(1L, binCount); return null;&#125; hash算法1static final int spread(int h) &#123;return (h ^ (h &gt;&gt;&gt; 16)) &amp; HASH_BITS;&#125; table中定位索引位置，n是table的大小1int index = (n - 1) &amp; hash 获取table中对应索引的元素f:Doug Lea采用Unsafe.getObjectVolatile来获取，也许有人质疑，直接table[index]不可以么，为什么要这么复杂？在java内存模型中，我们已经知道每个线程都有一个工作内存，里面存储着table的副本，虽然table是volatile修饰的，但不能保证线程每次都拿到table中的最新元素，Unsafe.getObjectVolatile可以直接获取指定内存的数据，保证了每次拿到数据都是最新的。如果f为null，说明table中这个位置第一次插入元素，利用Unsafe.compareAndSwapObject方法插入Node节点。如果CAS成功，说明Node节点已经插入，随后addCount(1L, binCount)方法会检查当前容量是否需要进行扩容。如果CAS失败，说明有其它线程提前插入了节点，自旋重新尝试在这个位置插入节点。如果f的hash值为-1，说明当前f是ForwardingNode节点，意味有其它线程正在扩容，则一起进行扩容操作。其余情况把新的Node节点按链表或红黑树的方式插入到合适的位置，这个过程采用同步内置锁实现并发，代码如下:12345678910111213141516171819202122232425262728293031323334synchronized (f) &#123; if (tabAt(tab, i) == f) &#123; if (fh &gt;= 0) &#123; binCount = 1; for (Node&lt;K,V&gt; e = f;; ++binCount) &#123; K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) &#123; oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; &#125; Node&lt;K,V&gt; pred = e; if ((e = e.next) == null) &#123; pred.next = new Node&lt;K,V&gt;(hash, key, value, null); break; &#125; &#125; &#125; else if (f instanceof TreeBin) &#123; Node&lt;K,V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K,V&gt;)f).putTreeVal(hash, key, value)) != null) &#123; oldVal = p.val; if (!onlyIfAbsent) p.val = value; &#125; &#125; &#125;&#125; 在节点f上进行同步，节点插入之前，再次利用tabAt(tab, i) == f判断，防止被其它线程修改。 如果f.hash &gt;= 0，说明f是链表结构的头结点，遍历链表，如果找到对应的node节点，则修改value，否则在链表尾部加入节点。 如果f是TreeBin类型节点，说明f是红黑树根节点，则在树结构上遍历元素，更新或增加节点。 如果链表中节点数binCount &gt;= TREEIFY_THRESHOLD(默认是8)，则把链表转化为红黑树结构。table扩容当table容量不足的时候，即table的元素数量达到容量阈值sizeCtl，需要对table进行扩容。整个扩容分为两部分： 构建一个nextTable，大小为table的两倍。 把table的数据复制到nextTable中。 这两个过程在单线程下实现很简单，但是ConcurrentHashMap是支持并发插入的，扩容操作自然也会有并发的出现，这种情况下，第二步可以支持节点的并发复制，这样性能自然提升不少，但实现的复杂度也上升了一个台阶。先看第一步，构建nextTable，毫无疑问，这个过程只能只有单个线程进行nextTable的初始化，具体实现如下：12345678910111213141516171819202122private final void addCount(long x, int check) &#123; ... 省略部分代码 if (check &gt;= 0) &#123; Node&lt;K,V&gt;[] tab, nt; int n, sc; while (s &gt;= (long)(sc = sizeCtl) &amp;&amp; (tab = table) != null &amp;&amp; (n = tab.length) &lt; MAXIMUM_CAPACITY) &#123; int rs = resizeStamp(n); if (sc &lt; 0) &#123; if ((sc &gt;&gt;&gt; RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex &lt;= 0) break; if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) transfer(tab, nt); &#125; else if (U.compareAndSwapInt(this, SIZECTL, sc, (rs &lt;&lt; RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); &#125; &#125;&#125; 通过Unsafe.compareAndSwapInt修改sizeCtl值，保证只有一个线程能够初始化nextTable，扩容后的数组长度为原来的两倍，但是容量是原来的1.5。节点从table移动到nextTable，大体思想是遍历、复制的过程。 首先根据运算得到需要遍历的次数i，然后利用tabAt方法获得i位置的元素f，初始化一个forwardNode实例fwd。 如果f == null，则在table中的i位置放入fwd，这个过程是采用Unsafe.compareAndSwapObjectf方法实现的，很巧妙的实现了节点的并发移动。 如果f是链表的头节点，就构造一个反序链表，把他们分别放在nextTable的i和i+n的位置上，移动完成，采用Unsafe.putObjectVolatile方法给table原位置赋值fwd。 如果f是TreeBin节点，也做一个反序处理，并判断是否需要untreeify，把处理的结果分别放在nextTable的i和i+n的位置上，移动完成，同样采用Unsafe.putObjectVolatile方法给table原位置赋值fwd。遍历过所有的节点以后就完成了复制工作，把table指向nextTable，并更新sizeCtl为新数组大小的0.75倍 ，扩容完成。红黑树构造注意：如果链表结构中元素超过TREEIFY_THRESHOLD阈值，默认为8个，则把链表转化为红黑树，提高遍历查询效率。1234567if (binCount != 0) &#123; if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break;&#125; 接下来我们看看如何构造树结构，代码如下：12345678910111213141516171819202122232425private final void treeifyBin(Node&lt;K,V&gt;[] tab, int index) &#123; Node&lt;K,V&gt; b; int n, sc; if (tab != null) &#123; if ((n = tab.length) &lt; MIN_TREEIFY_CAPACITY) tryPresize(n &lt;&lt; 1); else if ((b = tabAt(tab, index)) != null &amp;&amp; b.hash &gt;= 0) &#123; synchronized (b) &#123; if (tabAt(tab, index) == b) &#123; TreeNode&lt;K,V&gt; hd = null, tl = null; for (Node&lt;K,V&gt; e = b; e != null; e = e.next) &#123; TreeNode&lt;K,V&gt; p = new TreeNode&lt;K,V&gt;(e.hash, e.key, e.val, null, null); if ((p.prev = tl) == null) hd = p; else tl.next = p; tl = p; &#125; setTabAt(tab, index, new TreeBin&lt;K,V&gt;(hd)); &#125; &#125; &#125; &#125;&#125; 可以看出，生成树节点的代码块是同步的，进入同步代码块之后，再次验证table中index位置元素是否被修改过。1、根据table中index位置Node链表，重新生成一个hd为头结点的TreeNode链表。2、根据hd头结点，生成TreeBin树结构，并把树结构的root节点写到table的index位置的内存中，具体实现如下：12345678910111213141516171819202122232425262728293031323334353637383940414243TreeBin(TreeNode&lt;K,V&gt; b) &#123; super(TREEBIN, null, null, null); this.first = b; TreeNode&lt;K,V&gt; r = null; for (TreeNode&lt;K,V&gt; x = b, next; x != null; x = next) &#123; next = (TreeNode&lt;K,V&gt;)x.next; x.left = x.right = null; if (r == null) &#123; x.parent = null; x.red = false; r = x; &#125; else &#123; K k = x.key; int h = x.hash; Class&lt;?&gt; kc = null; for (TreeNode&lt;K,V&gt; p = r;;) &#123; int dir, ph; K pk = p.key; if ((ph = p.hash) &gt; h) dir = -1; else if (ph &lt; h) dir = 1; else if ((kc == null &amp;&amp; (kc = comparableClassFor(k)) == null) || (dir = compareComparables(kc, k, pk)) == 0) dir = tieBreakOrder(k, pk); TreeNode&lt;K,V&gt; xp = p; if ((p = (dir &lt;= 0) ? p.left : p.right) == null) &#123; x.parent = xp; if (dir &lt;= 0) xp.left = x; else xp.right = x; r = balanceInsertion(r, x); break; &#125; &#125; &#125; &#125; this.root = r; assert checkInvariants(root);&#125; 主要根据Node节点的hash值大小构建二叉树。这个红黑树的构造过程实在有点复杂，感兴趣的同学可以看看源码。get操作get操作和put操作相比，显得简单了许多12345678910111213141516171819public V get(Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; e, p; int n, eh; K ek; int h = spread(key.hashCode()); if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (e = tabAt(tab, (n - 1) &amp; h)) != null) &#123; if ((eh = e.hash) == h) &#123; if ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek))) return e.val; &#125; else if (eh &lt; 0) return (p = e.find(h, key)) != null ? p.val : null; while ((e = e.next) != null) &#123; if (e.hash == h &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) return e.val; &#125; &#125; return null;&#125; 1.判断table是否为空，如果为空，直接返回null。2.计算key的hash值，并获取指定table中指定位置的Node节点，通过遍历链表或则树结构找到对应的节点，返回value值。总结ConcurrentHashMap是一个并发散列映射表的实现,它允许完全并发的读取,并且支持给定数量的并发更新。相比于HashTable和同步包装器包装的 HashMap,使用一个全局的锁来同步不同线程间的并发访问,同一时间点,只能有一个线程持有锁,也就是说在同一时间点,只能有一个线程能访问容器,这虽然保证多线程间的安全并发访问，但同时也导致对容器的访问变成串行化的了。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>ConcurrentHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(ConcurrentLinkedQueue)]]></title>
    <url>%2F2019%2F04%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(ConcurrentLinkedQueue)%2F</url>
    <content type="text"><![CDATA[ConcurrentLinkedQueue介绍ConcurrentLinkedQueue是线程安全的队列，它适用于“高并发”的场景。它是一个基于链接节点的无界线程安全队列，按照 FIFO（先进先出）原则对元素进行排序。队列元素中不可以放置null元素（内部实现的特殊节点除外）。 ConcurrentLinkedQueue原理和数据结构ConcurrentLinkedQueue的数据结构，如下图所示：.resources/2C447958-48AF-4B02-A30E-52AA0038497C.jpg)说明： ConcurrentLinkedQueue继承于AbstractQueue。 ConcurrentLinkedQueue内部是通过链表来实现的。它同时包含链表的头节点head和尾节点tail。ConcurrentLinkedQueue按照FIFO（先进先出）原则对元素进行排序。元素都是从尾部插入到链表，从头部开始返回。 ConcurrentLinkedQueue的链表Node中的next的类型是volatile，而且链表数据item的类型也是volatile。关于volatile，我们知道它的语义包含：”即对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入”。ConcurrentLinkedQueue就是通过volatile来实现多线程对竞争资源的互斥访问的. ConcurrentLinkedQueue函数列表123456789101112131415161718192021222324252627// 创建一个最初为空的 ConcurrentLinkedQueue。ConcurrentLinkedQueue()// 创建一个最初包含给定 collection 元素的 ConcurrentLinkedQueue，按照此 collection 迭代器的遍历顺序来添加元素。ConcurrentLinkedQueue(Collection&lt;? extends E&gt; c)// 将指定元素插入此队列的尾部。boolean add(E e)// 如果此队列包含指定元素，则返回 true。boolean contains(Object o)// 如果此队列不包含任何元素，则返回 true。boolean isEmpty()// 返回在此队列元素上以恰当顺序进行迭代的迭代器。Iterator&lt;E&gt; iterator()// 将指定元素插入此队列的尾部。boolean offer(E e)// 获取但不移除此队列的头；如果此队列为空，则返回 null。E peek()// 获取并移除此队列的头，如果此队列为空，则返回 null。E poll()// 从队列中移除指定元素的单个实例（如果存在）。boolean remove(Object o)// 返回此队列中的元素数量。int size()// 返回以恰当顺序包含此队列所有元素的数组。Object[] toArray()// 返回以恰当顺序包含此队列所有元素的数组；返回数组的运行时类型是指定数组的运行时类型。&lt;T&gt; T[] toArray(T[] a) ConcurrentLinkedQueue源码分析下面从ConcurrentLinkedQueue的创建，添加，删除这几个方面对它进行分析。1 创建下面以ConcurrentLinkedQueue()来进行说明。123public ConcurrentLinkedQueue() &#123; head = tail = new Node&lt;E&gt;(null);&#125; 说明：在构造函数中，新建了一个“内容为null的节点”，并设置表头head和表尾tail的值为新节点。 head和tail的定义如下：12private transient volatile Node&lt;E&gt; head;private transient volatile Node&lt;E&gt; tail; head和tail都是volatile类型，他们具有volatile赋予的含义：“即对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入”。Node的声明如下：1234567891011121314151617181920212223242526272829303132333435363738private static class Node&lt;E&gt; &#123; volatile E item; volatile Node&lt;E&gt; next; Node(E item) &#123; UNSAFE.putObject(this, itemOffset, item); &#125; boolean casItem(E cmp, E val) &#123; return UNSAFE.compareAndSwapObject(this, itemOffset, cmp, val); &#125; void lazySetNext(Node&lt;E&gt; val) &#123; UNSAFE.putOrderedObject(this, nextOffset, val); &#125; boolean casNext(Node&lt;E&gt; cmp, Node&lt;E&gt; val) &#123; return UNSAFE.compareAndSwapObject(this, nextOffset, cmp, val); &#125; // Unsafe mechanics private static final sun.misc.Unsafe UNSAFE; private static final long itemOffset; private static final long nextOffset; static &#123; try &#123; UNSAFE = sun.misc.Unsafe.getUnsafe(); Class k = Node.class; itemOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;item&quot;)); nextOffset = UNSAFE.objectFieldOffset (k.getDeclaredField(&quot;next&quot;)); &#125; catch (Exception e) &#123; throw new Error(e); &#125; &#125;&#125; 说明：Node是个单向链表节点，next用于指向下一个Node，item用于存储数据。Node中操作节点数据的API，都是通过Unsafe机制的CAS函数实现的；例如casNext()是通过CAS函数“比较并设置节点的下一个节点”。 2. 添加 下面以add(E e)为例对ConcurrentLinkedQueue中的添加进行说明。 123public boolean add(E e) &#123; return offer(e);&#125; 说明：add()实际上是调用的offer()来完成添加操作的。 offer()的源码如下：12345678910111213141516171819202122232425262728public boolean offer(E e) &#123; // 检查e是不是null，是的话抛出NullPointerException异常。 checkNotNull(e); // 创建新的节点 final Node&lt;E&gt; newNode = new Node&lt;E&gt;(e); // 将“新的节点”添加到链表的末尾。 for (Node&lt;E&gt; t = tail, p = t;;) &#123; Node&lt;E&gt; q = p.next; // 情况1：q为空 if (q == null) &#123; // CAS操作：如果“p的下一个节点为null”(即p为尾节点)，则设置p的下一个节点为newNode。 // 如果该CAS操作成功的话，则比较“p和t”(若p不等于t，则设置newNode为新的尾节点)，然后返回true。 // 如果该CAS操作失败，这意味着“其它线程对尾节点进行了修改”，则重新循环。 if (p.casNext(null, newNode)) &#123; if (p != t) // hop two nodes at a time casTail(t, newNode); // Failure is OK. return true; &#125; &#125; // 情况2：p和q相等 else if (p == q) p = (t != (t = tail)) ? t : head; // 情况3：其它 else p = (p != t &amp;&amp; t != (t = tail)) ? t : q; &#125;&#125; 说明：offer(E e)的作用就是将元素e添加到链表的末尾。offer()比较的地方是理解for循环，下面区分3种情况对for进行分析。 情况1 – q为空。这意味着q是尾节点的下一个节点。此时，通过p.casNext(null, newNode)将“p的下一个节点设为newNode”，若设置成功的话，则比较“p和t”(若p不等于t，则设置newNode为新的尾节点)，然后返回true。否则的话(意味着“其它线程对尾节点进行了修改”)，什么也不做，继续进行for循环。p.casNext(null, newNode)，是调用CAS对p进行操作。若“p的下一个节点等于null”，则设置“p的下一个节点等于newNode”；设置成功的话，返回true，失败的话返回false。 情况2 – p和q相等。这种情况什么时候会发生呢？通过“情况3”，我们知道，经过“情况3”的处理后，p的值可能等于q。此时，若尾节点没有发生变化的话，那么，应该是头节点发生了变化，则设置p为头节点，然后重新遍历链表；否则(尾节点变化的话)，则设置p为尾节点。 情况3 – 其它。我们将p = (p != t &amp;&amp; t != (t = tail)) ? t : q;转换成如下代码。1234567891011if (p==t) &#123; p = q;&#125; else &#123; Node&lt;E&gt; tmp=t; t = tail; if (tmp==t) &#123; p=q; &#125; else &#123; p=t; &#125;&#125; 如果p和t相等，则设置p为q。否则的话，判断“尾节点是否发生变化”，没有变化的话，则设置p为q；否则，设置p为尾节点。 checkNotNull()的源码如下： 1234private static void checkNotNull(Object v) &#123; if (v == null) throw new NullPointerException();&#125; 3. 删除 下面以poll()为例对ConcurrentLinkedQueue中的删除进行说明。 1234567891011121314151617181920212223242526272829303132public E poll() &#123; // 设置“标记” restartFromHead: for (;;) &#123; for (Node&lt;E&gt; h = head, p = h, q;;) &#123; E item = p.item; // 情况1 // 表头的数据不为null，并且“设置表头的数据为null”这个操作成功的话; // 则比较“p和h”(若p!=h，即表头发生了变化，则更新表头，即设置表头为p)，然后返回原表头的item值。 if (item != null &amp;&amp; p.casItem(item, null)) &#123; if (p != h) // hop two nodes at a time updateHead(h, ((q = p.next) != null) ? q : p); return item; &#125; // 情况2 // 表头的下一个节点为null，即链表只有一个“内容为null的表头节点”。则更新表头为p，并返回null。 else if ((q = p.next) == null) &#123; updateHead(h, p); return null; &#125; // 情况3 // 这可能到由于“情况4”的发生导致p=q，在该情况下跳转到restartFromHead标记重新操作。 else if (p == q) continue restartFromHead; // 情况4 // 设置p为q else p = q; &#125; &#125;&#125; 说明：poll()的作用就是删除链表的表头节点，并返回被删节点对应的值。poll()的实现原理和offer()比较类似，下面根将or循环划分为4种情况进行分析。 情况1：“表头节点的数据”不为null，并且“设置表头节点的数据为null”这个操作成功。p.casItem(item, null) – 调用CAS函数，比较“节点p的数据值”与item是否相等，是的话，设置节点p的数据值为null。在情况1发生时，先比较“p和h”，若p!=h，即表头发生了变化，则调用updateHead()更新表头；然后返回删除节点的item值。updateHead()的源码如下：1234final void updateHead(Node&lt;E&gt; h, Node&lt;E&gt; p) &#123; if (h != p &amp;&amp; casHead(h, p)) h.lazySetNext(h);&#125; 说明：updateHead()的最终目的是更新表头为p，并设置h的下一个节点为h本身。casHead(h,p)是通过CAS函数设置表头，若表头等于h的话，则设置表头为p。lazySetNext()的源码如下：123void lazySetNext(Node&lt;E&gt; val) &#123; UNSAFE.putOrderedObject(this, nextOffset, val);&#125; putOrderedObject()函数，我们在前面一章“TODO”中介绍过。h.lazySetNext(h)的作用是通过CAS函数设置h的下一个节点为h自身，该设置可能会延迟执行。 情况2：如果表头的下一个节点为null，即链表只有一个“内容为null的表头节点”。则调用updateHead(h, p)，将表头更新p；然后返回null。 情况3：p=q在“情况4”的发生后，会导致p=q；此时，“情况3”就会发生。当“情况3”发生后，它会跳转到restartFromHead标记重新操作。 情况4：其它情况。设置p=q。 ConcurrentLinkedQueue示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.*;import java.util.concurrent.*;/* * ConcurrentLinkedQueue是“线程安全”的队列，而LinkedList是非线程安全的。 * * 下面是“多个线程同时操作并且遍历queue”的示例 * (01) 当queue是ConcurrentLinkedQueue对象时，程序能正常运行。 * (02) 当queue是LinkedList对象时，程序会产生ConcurrentModificationException异常。 * * @author skywang */public class ConcurrentLinkedQueueDemo1 &#123; // TODO: queue是LinkedList对象时，程序会出错。 //private static Queue&lt;String&gt; queue = new LinkedList&lt;String&gt;(); private static Queue&lt;String&gt; queue = new ConcurrentLinkedQueue&lt;String&gt;(); public static void main(String[] args) &#123; // 同时启动两个线程对queue进行操作！ new MyThread(&quot;ta&quot;).start(); new MyThread(&quot;tb&quot;).start(); &#125; private static void printAll() &#123; String value; Iterator iter = queue.iterator(); while(iter.hasNext()) &#123; value = (String)iter.next(); System.out.print(value+&quot;, &quot;); &#125; System.out.println(); &#125; private static class MyThread extends Thread &#123; MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; int i = 0; while (i++ &lt; 6) &#123; // “线程名” + &quot;-&quot; + &quot;序号&quot; String val = Thread.currentThread().getName()+i; queue.add(val); // 通过“Iterator”遍历queue。 printAll(); &#125; &#125; &#125;&#125; 其中一次运行结果：123456789101112ta1, ta1, tb1, tb1,ta1, ta1, tb1, tb1, ta2, ta2, tb2, tb2, ta1, ta1, tb1, tb1, ta2, ta2, tb2, tb2, ta3, tb3, ta3, ta1, tb3, tb1, ta4, ta2, ta1, tb2, tb1, ta3, ta2, tb3, tb2, ta4, ta3, tb4, tb3, ta1, ta4, tb1, tb4, ta2, ta5, tb2, ta1, ta3, tb1, tb3, ta2, ta4, tb2, tb4, ta3, ta5, tb3, tb5, ta4, ta1, tb4, tb1, ta5, ta2, tb5, tb2, ta6, ta3, ta1, tb3, tb1, ta4, ta2, tb4, tb2, ta5, ta3, tb5, tb3, ta6, ta4, tb6, tb4, ta5, tb5, ta6, tb6, 结果说明：如果将源码中的queue改成LinkedList对象时，程序会产生ConcurrentModificationException异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>ConcurrentLinkedQueue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(ConcurrentSkipListMap)]]></title>
    <url>%2F2019%2F04%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(ConcurrentSkipListMap)%2F</url>
    <content type="text"><![CDATA[ConcurrentSkipListMap介绍ConcurrentSkipListMap是线程安全的有序的哈希表，适用于高并发的场景。ConcurrentSkipListMap和TreeMap，它们虽然都是有序的哈希表。但是，第一，它们的线程安全机制不同，TreeMap是非线程安全的，而ConcurrentSkipListMap是线程安全的。 第二，ConcurrentSkipListMap是通过跳表实现的，而TreeMap是通过红黑树实现的。关于跳表(Skip List)，它是平衡树的一种替代的数据结构，但是和红黑树不相同的是，跳表对于树的平衡的实现是基于一种随机化的算法的，这样也就是说跳表的插入和删除的工作是比较简单的。 ConcurrentSkipListMap原理和数据结构ConcurrentSkipListMap的数据结构，如下图所示：.resources/AA985055-21D7-47B9-B826-0BBF8CC3F359.jpg)说明：先以数据“7,14,21,32,37,71,85”序列为例，来对跳表进行简单说明。跳表分为许多层(level)，每一层都可以看作是数据的索引，这些索引的意义就是加快跳表查找数据速度。每一层的数据都是有序的，上一层数据是下一层数据的子集，并且第一层(level 1)包含了全部的数据；层次越高，跳跃性越大，包含的数据越少。跳表包含一个表头，它查找数据时，是从上往下，从左往右进行查找。现在“需要找出值为32的节点”为例，来对比说明跳表和普遍的链表。情况1：链表中查找“32”节点路径如下图1-02所示：.resources/1E5B73C5-5DDD-4A44-8328-0A34F43BE2B1.jpg)需要4步(红色部分表示路径)。情况2：跳表中查找“32”节点路径如下图1-03所示:.resources/0F2F9532-CF90-471C-BEE7-FB3AD0126E1C.jpg)忽略索引垂直线路上路径的情况下，只需要2步(红色部分表示路径)。下面说说Java中ConcurrentSkipListMap的数据结构。(01) ConcurrentSkipListMap继承于AbstractMap类，也就意味着它是一个哈希表。(02) Index是ConcurrentSkipListMap的内部类，它与“跳表中的索引相对应”。HeadIndex继承于Index，ConcurrentSkipListMap中含有一个HeadIndex的对象head，head是“跳表的表头”。(03) Index是跳表中的索引，它包含“右索引的指针(right)”，“下索引的指针(down)”和“哈希表节点node”。node是Node的对象，Node也是ConcurrentSkipListMap中的内部类。 ConcurrentSkipListMap函数列表12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// 构造一个新的空映射，该映射按照键的自然顺序进行排序。ConcurrentSkipListMap()// 构造一个新的空映射，该映射按照指定的比较器进行排序。ConcurrentSkipListMap(Comparator&lt;? super K&gt; comparator)// 构造一个新映射，该映射所包含的映射关系与给定映射包含的映射关系相同，并按照键的自然顺序进行排序。ConcurrentSkipListMap(Map&lt;? extends K,? extends V&gt; m)// 构造一个新映射，该映射所包含的映射关系与指定的有序映射包含的映射关系相同，使用的顺序也相同。ConcurrentSkipListMap(SortedMap&lt;K,? extends V&gt; m)// 返回与大于等于给定键的最小键关联的键-值映射关系；如果不存在这样的条目，则返回 null。Map.Entry&lt;K,V&gt; ceilingEntry(K key)// 返回大于等于给定键的最小键；如果不存在这样的键，则返回 null。K ceilingKey(K key)// 从此映射中移除所有映射关系。void clear()// 返回此 ConcurrentSkipListMap 实例的浅表副本。ConcurrentSkipListMap&lt;K,V&gt; clone()// 返回对此映射中的键进行排序的比较器；如果此映射使用键的自然顺序，则返回 null。Comparator&lt;? super K&gt; comparator()// 如果此映射包含指定键的映射关系，则返回 true。boolean containsKey(Object key)// 如果此映射为指定值映射一个或多个键，则返回 true。boolean containsValue(Object value)// 返回此映射中所包含键的逆序 NavigableSet 视图。NavigableSet&lt;K&gt; descendingKeySet()// 返回此映射中所包含映射关系的逆序视图。ConcurrentNavigableMap&lt;K,V&gt; descendingMap()// 返回此映射中所包含的映射关系的 Set 视图。Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet()// 比较指定对象与此映射的相等性。boolean equals(Object o)// 返回与此映射中的最小键关联的键-值映射关系；如果该映射为空，则返回 null。Map.Entry&lt;K,V&gt; firstEntry()// 返回此映射中当前第一个（最低）键。K firstKey()// 返回与小于等于给定键的最大键关联的键-值映射关系；如果不存在这样的键，则返回 null。Map.Entry&lt;K,V&gt; floorEntry(K key)// 返回小于等于给定键的最大键；如果不存在这样的键，则返回 null。K floorKey(K key)// 返回指定键所映射到的值；如果此映射不包含该键的映射关系，则返回 null。V get(Object key)// 返回此映射的部分视图，其键值严格小于 toKey。ConcurrentNavigableMap&lt;K,V&gt; headMap(K toKey)// 返回此映射的部分视图，其键小于（或等于，如果 inclusive 为 true）toKey。ConcurrentNavigableMap&lt;K,V&gt; headMap(K toKey, boolean inclusive)// 返回与严格大于给定键的最小键关联的键-值映射关系；如果不存在这样的键，则返回 null。Map.Entry&lt;K,V&gt; higherEntry(K key)// 返回严格大于给定键的最小键；如果不存在这样的键，则返回 null。K higherKey(K key)// 如果此映射未包含键-值映射关系，则返回 true。boolean isEmpty()// 返回此映射中所包含键的 NavigableSet 视图。NavigableSet&lt;K&gt; keySet()// 返回与此映射中的最大键关联的键-值映射关系；如果该映射为空，则返回 null。Map.Entry&lt;K,V&gt; lastEntry()// 返回映射中当前最后一个（最高）键。K lastKey()// 返回与严格小于给定键的最大键关联的键-值映射关系；如果不存在这样的键，则返回 null。Map.Entry&lt;K,V&gt; lowerEntry(K key)// 返回严格小于给定键的最大键；如果不存在这样的键，则返回 null。K lowerKey(K key)// 返回此映射中所包含键的 NavigableSet 视图。NavigableSet&lt;K&gt; navigableKeySet()// 移除并返回与此映射中的最小键关联的键-值映射关系；如果该映射为空，则返回 null。Map.Entry&lt;K,V&gt; pollFirstEntry()// 移除并返回与此映射中的最大键关联的键-值映射关系；如果该映射为空，则返回 null。Map.Entry&lt;K,V&gt; pollLastEntry()// 将指定值与此映射中的指定键关联。V put(K key, V value)// 如果指定键已经不再与某个值相关联，则将它与给定值关联。V putIfAbsent(K key, V value)// 从此映射中移除指定键的映射关系（如果存在）。V remove(Object key)// 只有目前将键的条目映射到给定值时，才移除该键的条目。boolean remove(Object key, Object value)// 只有目前将键的条目映射到某一值时，才替换该键的条目。V replace(K key, V value)// 只有目前将键的条目映射到给定值时，才替换该键的条目。boolean replace(K key, V oldValue, V newValue)// 返回此映射中的键-值映射关系数。int size()// 返回此映射的部分视图，其键的范围从 fromKey 到 toKey。ConcurrentNavigableMap&lt;K,V&gt; subMap(K fromKey, boolean fromInclusive, K toKey, boolean toInclusive)// 返回此映射的部分视图，其键值的范围从 fromKey（包括）到 toKey（不包括）。ConcurrentNavigableMap&lt;K,V&gt; subMap(K fromKey, K toKey)// 返回此映射的部分视图，其键大于等于 fromKey。ConcurrentNavigableMap&lt;K,V&gt; tailMap(K fromKey)// 返回此映射的部分视图，其键大于（或等于，如果 inclusive 为 true）fromKey。ConcurrentNavigableMap&lt;K,V&gt; tailMap(K fromKey, boolean inclusive)// 返回此映射中所包含值的 Collection 视图。Collection&lt;V&gt; values() ConcurrentSkipListMap源码分析下面从ConcurrentSkipListMap的添加，删除，获取这3个方面对它进行分析。1. 添加下面以put(K key, V value)为例，对ConcurrentSkipListMap的添加方法进行说明。12345public V put(K key, V value) &#123; if (value == null) throw new NullPointerException(); return doPut(key, value, false);&#125; 实际上，put()是通过doPut()将key-value键值对添加到ConcurrentSkipListMap中的。doPut()的源码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private V doPut(K kkey, V value, boolean onlyIfAbsent) &#123; Comparable&lt;? super K&gt; key = comparable(kkey); for (;;) &#123; // 找到key的前继节点 Node&lt;K,V&gt; b = findPredecessor(key); // 设置n为“key的前继节点的后继节点”，即n应该是“插入节点”的“后继节点” Node&lt;K,V&gt; n = b.next; for (;;) &#123; if (n != null) &#123; Node&lt;K,V&gt; f = n.next; // 如果两次获得的b.next不是相同的Node，就跳转到”外层for循环“，重新获得b和n后再遍历。 if (n != b.next) break; // v是“n的值” Object v = n.value; // 当n的值为null(意味着其它线程删除了n)；此时删除b的下一个节点，然后跳转到”外层for循环“，重新获得b和n后再遍历。 if (v == null) &#123; // n is deleted n.helpDelete(b, f); break; &#125; // 如果其它线程删除了b；则跳转到”外层for循环“，重新获得b和n后再遍历。 if (v == n || b.value == null) // b is deleted break; // 比较key和n.key int c = key.compareTo(n.key); if (c &gt; 0) &#123; b = n; n = f; continue; &#125; if (c == 0) &#123; if (onlyIfAbsent || n.casValue(v, value)) return (V)v; else break; // restart if lost race to replace value &#125; // else c &lt; 0; fall through &#125; // 新建节点(对应是“要插入的键值对”) Node&lt;K,V&gt; z = new Node&lt;K,V&gt;(kkey, value, n); // 设置“b的后继节点”为z if (!b.casNext(n, z)) break; // 多线程情况下，break才可能发生(其它线程对b进行了操作) // 随机获取一个level // 然后在“第1层”到“第level层”的链表中都插入新建节点 int level = randomLevel(); if (level &gt; 0) insertIndex(z, level); return null; &#125; &#125;&#125; 说明：doPut() 的作用就是将键值对添加到“跳表”中。要想搞清doPut()，首先要弄清楚它的主干部分 —— 我们先单纯的只考虑“单线程的情况下，将key-value添加到跳表中”，即忽略“多线程相关的内容”。它的流程如下：第1步：找到“插入位置”。即，找到“key的前继节点(b)”和“key的后继节点(n)”；key是要插入节点的键。第2步：新建并插入节点。即，新建节点z(key对应的节点)，并将新节点z插入到“跳表”中(设置“b的后继节点为z”，“z的后继节点为n”)。第3步：更新跳表。即，随机获取一个level，然后在“跳表”的第1层～第level层之间，每一层都插入节点z；在第level层之上就不再插入节点了。若level数值大于“跳表的层次”，则新建一层。主干部分“对应的精简后的doPut()的代码”如下(仅供参考)：1234567891011121314151617181920212223private V doPut(K kkey, V value, boolean onlyIfAbsent) &#123; Comparable&lt;? super K&gt; key = comparable(kkey); for (;;) &#123; // 找到key的前继节点 Node&lt;K,V&gt; b = findPredecessor(key); // 设置n为key的后继节点 Node&lt;K,V&gt; n = b.next; for (;;) &#123; // 新建节点(对应是“要被插入的键值对”) Node&lt;K,V&gt; z = new Node&lt;K,V&gt;(kkey, value, n); // 设置“b的后继节点”为z b.casNext(n, z); // 随机获取一个level // 然后在“第1层”到“第level层”的链表中都插入新建节点 int level = randomLevel(); if (level &gt; 0) insertIndex(z, level); return null; &#125; &#125;&#125; 理清主干之后，剩余的工作就相对简单了。主要是上面几步的对应算法的具体实现，以及多线程相关情况的处理！2. 删除下面以remove(Object key)为例，对ConcurrentSkipListMap的删除方法进行说明。123public V remove(Object key) &#123; return doRemove(key, null);&#125; 实际上，remove()是通过doRemove()将ConcurrentSkipListMap中的key对应的键值对删除的。doRemove()的源码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253final V doRemove(Object okey, Object value) &#123; Comparable&lt;? super K&gt; key = comparable(okey); for (;;) &#123; // 找到“key的前继节点” Node&lt;K,V&gt; b = findPredecessor(key); // 设置n为“b的后继节点”(即若key存在于“跳表中”，n就是key对应的节点) Node&lt;K,V&gt; n = b.next; for (;;) &#123; if (n == null) return null; // f是“当前节点n的后继节点” Node&lt;K,V&gt; f = n.next; // 如果两次读取到的“b的后继节点”不同(其它线程操作了该跳表)，则返回到“外层for循环”重新遍历。 if (n != b.next) // inconsistent read break; // 如果“当前节点n的值”变为null(其它线程操作了该跳表)，则返回到“外层for循环”重新遍历。 Object v = n.value; if (v == null) &#123; // n is deleted n.helpDelete(b, f); break; &#125; // 如果“前继节点b”被删除(其它线程操作了该跳表)，则返回到“外层for循环”重新遍历。 if (v == n || b.value == null) // b is deleted break; int c = key.compareTo(n.key); if (c &lt; 0) return null; if (c &gt; 0) &#123; b = n; n = f; continue; &#125; // 以下是c=0的情况 if (value != null &amp;&amp; !value.equals(v)) return null; // 设置“当前节点n”的值为null if (!n.casValue(v, null)) break; // 设置“b的后继节点”为f if (!n.appendMarker(f) || !b.casNext(n, f)) findNode(key); // Retry via findNode else &#123; // 清除“跳表”中每一层的key节点 findPredecessor(key); // Clean index // 如果“表头的右索引为空”，则将“跳表的层次”-1。 if (head.right == null) tryReduceLevel(); &#125; return (V)v; &#125; &#125;&#125; 说明：doRemove()的作用是删除跳表中的节点。和doPut()一样，我们重点看doRemove()的主干部分，了解主干部分之后，其余部分就非常容易理解了。下面是“单线程的情况下，删除跳表中键值对的步骤”：第1步：找到“被删除节点的位置”。即，找到“key的前继节点(b)”，“key所对应的节点(n)”，“n的后继节点f”；key是要删除节点的键。第2步：删除节点。即，将“key所对应的节点n”从跳表中移除 – 将“b的后继节点”设为“f”！第3步：更新跳表。即，遍历跳表，删除每一层的“key节点”(如果存在的话)。如果删除“key节点”之后，跳表的层次需要-1；则执行相应的操作！主干部分“对应的精简后的doRemove()的代码”如下(仅供参考)：12345678910111213141516171819202122232425final V doRemove(Object okey, Object value) &#123; Comparable&lt;? super K&gt; key = comparable(okey); for (;;) &#123; // 找到“key的前继节点” Node&lt;K,V&gt; b = findPredecessor(key); // 设置n为“b的后继节点”(即若key存在于“跳表中”，n就是key对应的节点) Node&lt;K,V&gt; n = b.next; for (;;) &#123; // f是“当前节点n的后继节点” Node&lt;K,V&gt; f = n.next; // 设置“当前节点n”的值为null n.casValue(v, null); // 设置“b的后继节点”为f b.casNext(n, f); // 清除“跳表”中每一层的key节点 findPredecessor(key); // 如果“表头的右索引为空”，则将“跳表的层次”-1。 if (head.right == null) tryReduceLevel(); return (V)v; &#125; &#125;&#125; 3. 获取下面以get(Object key)为例，对ConcurrentSkipListMap的获取方法进行说明123public V get(Object key) &#123; return doGet(key);&#125; doGet的源码如下：123456789101112private V doGet(Object okey) &#123; Comparable&lt;? super K&gt; key = comparable(okey); for (;;) &#123; // 找到“key对应的节点” Node&lt;K,V&gt; n = findNode(key); if (n == null) return null; Object v = n.value; if (v != null) return (V)v; &#125;&#125; 说明：doGet()是通过findNode()找到并返回节点的。1234567891011121314151617181920212223242526272829303132333435private Node&lt;K,V&gt; findNode(Comparable&lt;? super K&gt; key) &#123; for (;;) &#123; // 找到key的前继节点 Node&lt;K,V&gt; b = findPredecessor(key); // 设置n为“b的后继节点”(即若key存在于“跳表中”，n就是key对应的节点) Node&lt;K,V&gt; n = b.next; for (;;) &#123; // 如果“n为null”，则跳转中不存在key对应的节点，直接返回null。 if (n == null) return null; Node&lt;K,V&gt; f = n.next; // 如果两次读取到的“b的后继节点”不同(其它线程操作了该跳表)，则返回到“外层for循环”重新遍历。 if (n != b.next) // inconsistent read break; Object v = n.value; // 如果“当前节点n的值”变为null(其它线程操作了该跳表)，则返回到“外层for循环”重新遍历。 if (v == null) &#123; // n is deleted n.helpDelete(b, f); break; &#125; if (v == n || b.value == null) // b is deleted break; // 若n是当前节点，则返回n。 int c = key.compareTo(n.key); if (c == 0) return n; // 若“节点n的key”小于“key”，则说明跳表中不存在key对应的节点，返回null if (c &lt; 0) return null; // 若“节点n的key”大于“key”，则更新b和n，继续查找。 b = n; n = f; &#125; &#125;&#125; 说明：findNode(key)的作用是在返回跳表中key对应的节点；存在则返回节点，不存在则返回null。先弄清函数的主干部分，即抛开“多线程相关内容”，单纯的考虑单线程情况下，从跳表获取节点的算法。第1步：找到“被删除节点的位置”。根据findPredecessor()定位key所在的层次以及找到key的前继节点(b)，然后找到b的后继节点n。第2步：根据“key的前继节点(b)”和“key的前继节点的后继节点(n)”来定位“key对应的节点”。具体是通过比较“n的键值”和“key”的大小。如果相等，则n就是所要查找的键。 ConcurrentSkipListMap示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.*;import java.util.concurrent.*;/* * ConcurrentSkipListMap是“线程安全”的哈希表，而TreeMap是非线程安全的。 * * 下面是“多个线程同时操作并且遍历map”的示例 * (01) 当map是ConcurrentSkipListMap对象时，程序能正常运行。 * (02) 当map是TreeMap对象时，程序会产生ConcurrentModificationException异常。public class ConcurrentSkipListMapDemo1 &#123; // TODO: map是TreeMap对象时，程序会出错。 //private static Map&lt;String, String&gt; map = new TreeMap&lt;String, String&gt;(); private static Map&lt;String, String&gt; map = new ConcurrentSkipListMap&lt;String, String&gt;(); public static void main(String[] args) &#123; // 同时启动两个线程对map进行操作！ new MyThread(&quot;a&quot;).start(); new MyThread(&quot;b&quot;).start(); &#125; private static void printAll() &#123; String key, value; Iterator iter = map.entrySet().iterator(); while(iter.hasNext()) &#123; Map.Entry entry = (Map.Entry)iter.next(); key = (String)entry.getKey(); value = (String)entry.getValue(); System.out.print(&quot;(&quot;+key+&quot;, &quot;+value+&quot;), &quot;); &#125; System.out.println(); &#125; private static class MyThread extends Thread &#123; MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; int i = 0; while (i++ &lt; 6) &#123; // “线程名” + &quot;序号&quot; String val = Thread.currentThread().getName()+i; map.put(val, &quot;0&quot;); // 通过“Iterator”遍历map。 printAll(); &#125; &#125; &#125;&#125; 其中一次运行结果：123456789101112(a1, 0), (a1, 0), (b1, 0), (b1, 0),(a1, 0), (b1, 0), (b2, 0), (a1, 0), (a1, 0), (a2, 0), (a2, 0), (b1, 0), (b1, 0), (b2, 0), (b2, 0), (b3, 0), (b3, 0), (a1, 0), (a2, 0), (a3, 0), (a1, 0), (b1, 0), (a2, 0), (b2, 0), (a3, 0), (b3, 0), (b1, 0), (b4, 0), (b2, 0), (a1, 0), (b3, 0), (a2, 0), (b4, 0), (a3, 0), (a1, 0), (a4, 0), (a2, 0), (b1, 0), (a3, 0), (b2, 0), (a4, 0), (b3, 0), (b1, 0), (b4, 0), (b2, 0), (b5, 0), (b3, 0), (a1, 0), (b4, 0), (a2, 0), (b5, 0), (a3, 0), (a1, 0), (a4, 0), (a2, 0), (a5, 0), (a3, 0), (b1, 0), (a4, 0), (b2, 0), (a5, 0), (b3, 0), (b1, 0), (b4, 0), (b2, 0), (b5, 0), (b3, 0), (b6, 0), (b4, 0), (a1, 0), (b5, 0), (a2, 0), (b6, 0), (a3, 0), (a4, 0), (a5, 0), (a6, 0), (b1, 0), (b2, 0), (b3, 0), (b4, 0), (b5, 0), (b6, 0), 结果说明：示例程序中，启动两个线程(线程a和线程b)分别对ConcurrentSkipListMap进行操作。以线程a而言，它会先获取“线程名”+“序号”，然后将该字符串作为key，将“0”作为value，插入到ConcurrentSkipListMap中；接着，遍历并输出ConcurrentSkipListMap中的全部元素。 线程b的操作和线程a一样，只不过线程b的名字和线程a的名字不同。当map是ConcurrentSkipListMap对象时，程序能正常运行。如果将map改为TreeMap时，程序会产生ConcurrentModificationException异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>ConcurrentSkipListMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(ConcurrentSkipListSet)]]></title>
    <url>%2F2019%2F04%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(ConcurrentSkipListSet)%2F</url>
    <content type="text"><![CDATA[ConcurrentSkipListSet介绍ConcurrentSkipListSet是线程安全的有序的集合，适用于高并发的场景。ConcurrentSkipListSet和TreeSet，它们虽然都是有序的集合。 但是，第一，它们的线程安全机制不同，TreeSet是非线程安全的，而ConcurrentSkipListSet是线程安全的。第二，ConcurrentSkipListSet是通过ConcurrentSkipListMap实现的，而TreeSet是通过TreeMap实现的。 ConcurrentSkipListSet原理和数据结构ConcurrentSkipListSet的数据结构，如下图所示：.resources/1378EC09-5D7F-4544-AA4C-F43796D9A609.jpg)说明： (01) ConcurrentSkipListSet继承于AbstractSet。因此，它本质上是一个集合。(02) ConcurrentSkipListSet实现了NavigableSet接口。因此，ConcurrentSkipListSet是一个有序的集合。(03) ConcurrentSkipListSet是通过ConcurrentSkipListMap实现的。它包含一个ConcurrentNavigableMap对象m，而m对象实际上是ConcurrentNavigableMap的实现类ConcurrentSkipListMap的实例。ConcurrentSkipListMap中的元素是key-value键值对；而ConcurrentSkipListSet是集合，它只用到了ConcurrentSkipListMap中的key！ ConcurrentSkipListSet函数列表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 构造一个新的空 set，该 set 按照元素的自然顺序对其进行排序。ConcurrentSkipListSet()// 构造一个包含指定 collection 中元素的新 set，这个新 set 按照元素的自然顺序对其进行排序。ConcurrentSkipListSet(Collection&lt;? extends E&gt; c)// 构造一个新的空 set，该 set 按照指定的比较器对其元素进行排序。ConcurrentSkipListSet(Comparator&lt;? super E&gt; comparator)// 构造一个新 set，该 set 所包含的元素与指定的有序 set 包含的元素相同，使用的顺序也相同。ConcurrentSkipListSet(SortedSet&lt;E&gt; s)// 如果此 set 中不包含指定元素，则添加指定元素。boolean add(E e)// 返回此 set 中大于等于给定元素的最小元素；如果不存在这样的元素，则返回 null。E ceiling(E e)// 从此 set 中移除所有元素。void clear()// 返回此 ConcurrentSkipListSet 实例的浅表副本。ConcurrentSkipListSet&lt;E&gt; clone()// 返回对此 set 中的元素进行排序的比较器；如果此 set 使用其元素的自然顺序，则返回 null。Comparator&lt;? super E&gt; comparator()// 如果此 set 包含指定的元素，则返回 true。boolean contains(Object o)// 返回在此 set 的元素上以降序进行迭代的迭代器。Iterator&lt;E&gt; descendingIterator()// 返回此 set 中所包含元素的逆序视图。NavigableSet&lt;E&gt; descendingSet()// 比较指定对象与此 set 的相等性。boolean equals(Object o)// 返回此 set 中当前第一个（最低）元素。E first()// 返回此 set 中小于等于给定元素的最大元素；如果不存在这样的元素，则返回 null。E floor(E e)// 返回此 set 的部分视图，其元素严格小于 toElement。NavigableSet&lt;E&gt; headSet(E toElement)// 返回此 set 的部分视图，其元素小于（或等于，如果 inclusive 为 true）toElement。NavigableSet&lt;E&gt; headSet(E toElement, boolean inclusive)// 返回此 set 中严格大于给定元素的最小元素；如果不存在这样的元素，则返回 null。E higher(E e)// 如果此 set 不包含任何元素，则返回 true。boolean isEmpty()// 返回在此 set 的元素上以升序进行迭代的迭代器。Iterator&lt;E&gt; iterator()// 返回此 set 中当前最后一个（最高）元素。E last()// 返回此 set 中严格小于给定元素的最大元素；如果不存在这样的元素，则返回 null。E lower(E e)// 获取并移除第一个（最低）元素；如果此 set 为空，则返回 null。E pollFirst()// 获取并移除最后一个（最高）元素；如果此 set 为空，则返回 null。E pollLast()// 如果此 set 中存在指定的元素，则将其移除。boolean remove(Object o)// 从此 set 中移除包含在指定 collection 中的所有元素。boolean removeAll(Collection&lt;?&gt; c)// 返回此 set 中的元素数目。int size()// 返回此 set 的部分视图，其元素范围从 fromElement 到 toElement。NavigableSet&lt;E&gt; subSet(E fromElement, boolean fromInclusive, E toElement, boolean toInclusive)// 返回此 set 的部分视图，其元素从 fromElement（包括）到 toElement（不包括）。NavigableSet&lt;E&gt; subSet(E fromElement, E toElement)// 返回此 set 的部分视图，其元素大于等于 fromElement。NavigableSet&lt;E&gt; tailSet(E fromElement)// 返回此 set 的部分视图，其元素大于（或等于，如果 inclusive 为 true）fromElement。NavigableSet&lt;E&gt; tailSet(E fromElement, boolean inclusive) ConcurrentSkipListSet示例ConcurrentSkipListSet是通过ConcurrentSkipListMap实现的，它的接口基本上都是通过调用ConcurrentSkipListMap接口来实现的123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import java.util.*;import java.util.concurrent.*;/* * ConcurrentSkipListSet是“线程安全”的集合，而TreeSet是非线程安全的。 * * 下面是“多个线程同时操作并且遍历集合set”的示例 * (01) 当set是ConcurrentSkipListSet对象时，程序能正常运行。 * (02) 当set是TreeSet对象时，程序会产生ConcurrentModificationException异常。 * * @author skywang */public class ConcurrentSkipListSetDemo1 &#123; // TODO: set是TreeSet对象时，程序会出错。 //private static Set&lt;String&gt; set = new TreeSet&lt;String&gt;(); private static Set&lt;String&gt; set = new ConcurrentSkipListSet&lt;String&gt;(); public static void main(String[] args) &#123; // 同时启动两个线程对set进行操作！ new MyThread(&quot;a&quot;).start(); new MyThread(&quot;b&quot;).start(); &#125; private static void printAll() &#123; String value = null; Iterator iter = set.iterator(); while(iter.hasNext()) &#123; value = (String)iter.next(); System.out.print(value+&quot;, &quot;); &#125; System.out.println(); &#125; private static class MyThread extends Thread &#123; MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; int i = 0; while (i++ &lt; 10) &#123; // “线程名” + &quot;序号&quot; String val = Thread.currentThread().getName() + (i%6); set.add(val); // 通过“Iterator”遍历set。 printAll(); &#125; &#125; &#125;&#125; 其中一次运行结果：1234567891011121314151617181920a1, b1, a1, a1, a2, b1, b1, a1, a2, a3, b1,a1, a2, a3, a1, a4, b1, b2, a2, a1, a2, a3, a4, a5, b1, b2, a3, a0, a4, a5, a1, b1, a2, b2, a3, a0, a4, a1, a5, a2, b1, a3, b2, a4, b3, a5, a0, b1, a1, b2, a2, b3, a3, a0, a4, a1, a5, a2, b1, a3, b2, a4, b3, a5, b4, b1, a0, b2, a1, b3, a2, b4, a3, a0, a4, a1, a5, a2, b1, a3, b2, a4, b3, a5, b4, b1, b5, b2, a0, a1, a2, a3, a4, a5, b3, b1, b4, b2, b5, b3, a0, b4, a1, b5, a2, a0, a3, a1, a4, a2, a5, a3, b0, a4, b1, a5, b2, b0, b3, b1, b4, b2, b5, b3, b4, a0, b5, a1, a2, a3, a4, a5, b0, b1, b2, b3, b4, b5, a0, a1, a2, a3, a4, a5, b0, b1, b2, b3, b4, b5, a0, a1, a2, a3, a4, a5, b0, b1, b2, b3, b4, b5, a0, a1, a2, a3, a4, a5, b0, b1, b2, b3, b4, b5, 结果说明：示例程序中，启动两个线程(线程a和线程b)分别对ConcurrentSkipListSet进行操作。以线程a而言，它会先获取“线程名”+“序号”，然后将该字符串添加到ConcurrentSkipListSet集合中；接着，遍历并输出集合中的全部元素。 线程b的操作和线程a一样，只不过线程b的名字和线程a的名字不同。当set是ConcurrentSkipListSet对象时，程序能正常运行。如果将set改为TreeSet时，程序会产生ConcurrentModificationException异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>ConcurrentSkipListSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(CopyOnWriteArrayList)]]></title>
    <url>%2F2019%2F04%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(CopyOnWriteArrayList)%2F</url>
    <content type="text"><![CDATA[CopyOnWriteArrayList介绍它相当于线程安全的ArrayList。和ArrayList一样，它是个可变数组；但是和ArrayList不同的时，它具有以下特性： 它最适合于具有以下特征的应用程序：List 大小通常保持很小，只读操作远多于可变操作，需要在遍历期间防止线程间的冲突。 它是线程安全的。 因为通常需要复制整个基础数组，所以可变操作（add()、set() 和 remove() 等等）的开销很大。 迭代器支持hasNext(), next()等不可变操作，但不支持可变 remove()等操作。 使用迭代器进行遍历的速度很快,并且不会与其他线程发生冲突。在构造迭代器时,迭代器依赖于不变的数组快照。建议：在学习CopyOnWriteArraySet之前,先对ArrayList进行了解！ CopyOnWriteArrayList原理和数据结构CopyOnWriteArrayList的数据结构，如下图所示：.resources/D89A47F9-4C4F-4E57-BAE1-A314892BFFD8.jpg)说明：1.CopyOnWriteArrayList实现了List接口,因此它是一个队列。2.CopyOnWriteArrayList包含了成员lock。每一个CopyOnWriteArrayList都和一个互斥锁lock绑定,通过lock，实现了对CopyOnWriteArrayList的互斥访问。 CopyOnWriteArrayList包含了成员array数组,这说明CopyOnWriteArrayList本质上通过数组实现的。下面从“动态数组”和“线程安全”两个方面进一步对CopyOnWriteArrayList的原理进行说明。 CopyOnWriteArrayList的“动态数组”机制 – 它内部有个“volatile数组”(array)来保持数据。在“添加/修改/删除”数据时，都会新建一个数组，并将更新后的数据拷贝到新建的数组中，最后再将该数组赋值给“volatile数组”。这就是它叫做CopyOnWriteArrayList的原因！CopyOnWriteArrayList就是通过这种方式实现的动态数组；不过正由于它在“添加/修改/删除”数据时，都会新建数组，所以涉及到修改数据的操作，CopyOnWriteArrayList效率很低；但是单单只是进行遍历查找的话，效率比较高。 CopyOnWriteArrayList的“线程安全”机制 – 是通过volatile和互斥锁来实现的。(01) CopyOnWriteArrayList是通过”volatile数组”来保存数据的。一个线程读取volatile数组时，总能看到其它线程对该volatile变量最后的写入;就这样，通过volatile提供了”读取到的数据总是最新的”这个机制的保证。(02) CopyOnWriteArrayList通过互斥锁来保护数据。在”添加/修改/删除”数据时，会先”获取互斥锁”,再修改完毕之后，先将数据更新到“volatile数组”中，然后再”释放互斥锁”,这样,就达到了保护数据的目的。 CopyOnWriteArrayList函数列表1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// 创建一个空列表。CopyOnWriteArrayList()// 创建一个按 collection 的迭代器返回元素的顺序包含指定 collection 元素的列表。CopyOnWriteArrayList(Collection&amp;lt;? extends E&amp;gt; c)// CopyOnWriteArrayList(E[] toCopyIn)创建一个保存给定数组的副本的列表。// 将指定元素添加到此列表的尾部。boolean add(E e)// 在此列表的指定位置上插入指定元素。void add(int index, E element)// 按照指定 collection 的迭代器返回元素的顺序，将指定 collection 中的所有元素添加此列表的尾部。boolean addAll(Collection&amp;lt;? extends E&amp;gt; c)// 从指定位置开始，将指定 collection 的所有元素插入此列表。boolean addAll(int index, Collection&amp;lt;? extends E&amp;gt; c)// 按照指定 collection 的迭代器返回元素的顺序，将指定 collection 中尚未包含在此列表中的所有元素添加列表的尾部。int addAllAbsent(Collection&amp;lt;? extends E&amp;gt; c)// 添加元素（如果不存在）。boolean addIfAbsent(E e)// 从此列表移除所有元素。void clear()// 返回此列表的浅表副本。Object clone()// 如果此列表包含指定的元素，则返回 true。boolean contains(Object o)// 如果此列表包含指定 collection 的所有元素，则返回 true。boolean containsAll(Collection&amp;lt;?&amp;gt; c)// 比较指定对象与此列表的相等性。boolean equals(Object o)// 返回列表中指定位置的元素。E get(int index)// 返回此列表的哈希码值。int hashCode()// 返回第一次出现的指定元素在此列表中的索引，从 index 开始向前搜索，如果没有找到该元素，则返回 -1。int indexOf(E e, int index)// 返回此列表中第一次出现的指定元素的索引；如果此列表不包含该元素，则返回 -1。int indexOf(Object o)// 如果此列表不包含任何元素，则返回 true。boolean isEmpty()// 返回以恰当顺序在此列表元素上进行迭代的迭代器。Iterator&amp;lt;E&amp;gt; iterator()// 返回最后一次出现的指定元素在此列表中的索引，从 index 开始向后搜索，如果没有找到该元素，则返回 -1。int lastIndexOf(E e, int index)// 返回此列表中最后出现的指定元素的索引；如果列表不包含此元素，则返回 -1。int lastIndexOf(Object o)// 返回此列表元素的列表迭代器（按适当顺序）。ListIterator&amp;lt;E&amp;gt; listIterator()// 返回列表中元素的列表迭代器（按适当顺序），从列表的指定位置开始。ListIterator&amp;lt;E&amp;gt; listIterator(int index)// 移除此列表指定位置上的元素。E remove(int index)// 从此列表移除第一次出现的指定元素（如果存在）。boolean remove(Object o)// 从此列表移除所有包含在指定 collection 中的元素。boolean removeAll(Collection&amp;lt;?&amp;gt; c)// 只保留此列表中包含在指定 collection 中的元素。boolean retainAll(Collection&amp;lt;?&amp;gt; c)// 用指定的元素替代此列表指定位置上的元素。E set(int index, E element)// 返回此列表中的元素数。int size()// 返回此列表中 fromIndex（包括）和 toIndex（不包括）之间部分的视图。List&amp;lt;E&amp;gt; subList(int fromIndex, int toIndex)// 返回一个按恰当顺序（从第一个元素到最后一个元素）包含此列表中所有元素的数组。Object[] toArray()// 返回以恰当顺序（从第一个元素到最后一个元素）包含列表所有元素的数组；返回数组的运行时类型是指定数组的运行时类型。&amp;lt;T&amp;gt; T[] toArray(T[] a)// 返回此列表的字符串表示形式。String toString() CopyOnWriteArrayList源码分析下面我们从”创建,添加,删除,获取,遍历”这5个方面去分析CopyOnWriteArrayList的原理。1. 创建CopyOnWriteArrayList共3个构造函数。它们的源码如下：1234567891011121314public CopyOnWriteArrayList() &#123; setArray(new Object[0]);&#125;public CopyOnWriteArrayList(Collection&amp;lt;? extends E&amp;gt; c) &#123; Object[] elements = c.toArray(); if (elements.getClass() != Object[].class) elements = Arrays.copyOf(elements, elements.length, Object[].class); setArray(elements);&#125;public CopyOnWriteArrayList(E[] toCopyIn) &#123; setArray(Arrays.copyOf(toCopyIn, toCopyIn.length, Object[].class));&#125; 说明：这3个构造函数都调用了setArray()，setArray()的源码如下：1234567private volatile transient Object[] array;final Object[] getArray() &#123; return array;&#125;final void setArray(Object[] a) &#123; array = a;&#125; 说明：setArray()的作用是给array赋值；其中，array是volatile transient Object[]类型，即array是“volatile数组”。关于volatile关键字，我们知道“volatile能让变量变得可见”，即对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。正在由于这种特性，每次更新了“volatile数组”之后，其它线程都能看到对它所做的更新。关于transient关键字，它是在序列化中才起作用，transient变量不会被自动序列化。2. 添加以add(E e)为例，来对”CopyOnWriteArrayList”的添加操作进行说明。下面是add(E e)的代码:123456789101112131415161718192021public boolean add(E e) &#123; final ReentrantLock lock = this.lock; // 获取“锁” lock.lock(); try &#123; // 获取原始”volatile数组“中的数据和数据长度。 Object[] elements = getArray(); int len = elements.length; // 新建一个数组newElements，并将原始数据拷贝到newElements中； // newElements数组的长度=“原始数组的长度”+1 Object[] newElements = Arrays.copyOf(elements, len + 1); // 将“新增加的元素”保存到newElements中。 newElements[len] = e; // 将newElements赋值给”volatile数组“。 setArray(newElements); return true; &#125; finally &#123; // 释放“锁” lock.unlock(); &#125;&#125; 说明: add(E e)的作用就是将数据e添加到”volatile数组“中。它的实现方式是,新建一个数组,接着将原始的”volatile数组“的数据拷贝到新数组中，然后将新增数据也添加到新数组中:最后,将新数组赋值给”volatile数组“。在add(E e)中有两点需要关注。第一,在”添加操作“开始前，获取独占锁(lock)，若此时有需要线程要获取锁，则必须等待；在操作完毕后，释放独占锁(lock)，此时其它线程才能获取锁。通过独占锁，来防止多线程同时修改数据！lock的定义如下：12transient final ReentrantLock lock = new ReentrantLock(); 第二,操作完毕时，会通过setArray()来更新”volatile数组“。而且，前面我们提过”即对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入“；这样，每次添加元素之后，其它线程都能看到新添加的元素。3. 获取以get(int index)为例，来对“CopyOnWriteArrayList的删除操作”进行说明。下面是get(int index)的代码：.resources/465A838B-6367-4535-96B8-C9693E5D0B61.gif)public E get(int index) { return get(getArray(), index);} private E get(Object[] a, int index) { return (E) a[index];}.resources/465A838B-6367-4535-96B8-C9693E5D0B61.gif)说明：get(int index)的实现很简单，就是返回”volatile数组”中的第index个元素。4. 删除以remove(int index)为例，来对“CopyOnWriteArrayList的删除操作”进行说明。下面是remove(int index)的代码：12345678910111213141516171819202122232425262728public E remove(int index) &#123; final ReentrantLock lock = this.lock; // 获取“锁” lock.lock(); try &#123; // 获取原始”volatile数组“中的数据和数据长度。 Object[] elements = getArray(); int len = elements.length; // 获取elements数组中的第index个数据。 E oldValue = get(elements, index); int numMoved = len - index - 1; // 如果被删除的是最后一个元素，则直接通过Arrays.copyOf()进行处理，而不需要新建数组。 // 否则，新建数组，然后将”volatile数组中被删除元素之外的其它元素“拷贝到新数组中；最后，将新数组赋值给”volatile数组“。 if (numMoved == 0) setArray(Arrays.copyOf(elements, len - 1)); else &#123; Object[] newElements = new Object[len - 1]; System.arraycopy(elements, 0, newElements, 0, index); System.arraycopy(elements, index + 1, newElements, index, numMoved); setArray(newElements); &#125; return oldValue; &#125; finally &#123; // 释放“锁” lock.unlock(); &#125;&#125; 说明：remove(int index)的作用就是将”volatile数组“中第index个元素删除。它的实现方式是，如果被删除的是最后一个元素，则直接通过Arrays.copyOf()进行处理，而不需要新建数组。否则，新建数组，然后将”volatile数组中被删除元素之外的其它元素”拷贝到新数组中;最后,将新数组赋值给”volatile数组”。和add(E e)一样，remove(int index)也是”在操作之前，获取独占锁；操作完成之后，释放独占是“;并且”在操作完成时，会通过将数据更新到volatile数组中”。5. 遍历以iterator()为例，来对CopyOnWriteArrayList的遍历操作进行说明。下面是iterator()的代码：123public Iterator&lt;E&gt; iterator() &#123; return new COWIterator&lt;E&gt;(getArray(), 0);&#125; 说明：iterator()会返回COWIterator对象。COWIterator实现额ListIterator接口，它的源码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253private static class COWIterator&amp;lt;E&amp;gt; implements ListIterator&amp;lt;E&amp;gt; &#123; private final Object[] snapshot; private int cursor; private COWIterator(Object[] elements, int initialCursor) &#123; cursor = initialCursor; snapshot = elements; &#125; public boolean hasNext() &#123; return cursor &amp;lt; snapshot.length; &#125; public boolean hasPrevious() &#123; return cursor &amp;gt; 0; &#125; // 获取下一个元素 @SuppressWarnings(&quot;unchecked&quot;) public E next() &#123; if (! hasNext()) throw new NoSuchElementException(); return (E) snapshot[cursor++]; &#125; // 获取上一个元素 @SuppressWarnings(&quot;unchecked&quot;) public E previous() &#123; if (! hasPrevious()) throw new NoSuchElementException(); return (E) snapshot[--cursor]; &#125; public int nextIndex() &#123; return cursor; &#125; public int previousIndex() &#123; return cursor-1; &#125; public void remove() &#123; throw new UnsupportedOperationException(); &#125; public void set(E e) &#123; throw new UnsupportedOperationException(); &#125; public void add(E e) &#123; throw new UnsupportedOperationException(); &#125;&#125; 说明：COWIterator不支持修改元素的操作。例如，对于remove(),set(),add()等操作,COWIterator都会抛出异常！另外，需要提到的一点是，CopyOnWriteArrayList返回迭代器不会抛出ConcurrentModificationException异常，即它不是fail-fast机制的！ CopyOnWriteArrayList示例下面，我们通过一个例子去对比ArrayList和CopyOnWriteArrayList。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.*;import java.util.concurrent.*;/* * CopyOnWriteArrayList是“线程安全”的动态数组，而ArrayList是非线程安全的。 * * 下面是“多个线程同时操作并且遍历list”的示例 * (01) 当list是CopyOnWriteArrayList对象时，程序能正常运行。 * (02) 当list是ArrayList对象时，程序会产生ConcurrentModificationException异常。 * */public class CopyOnWriteArrayListTest1 &#123; // TODO: list是ArrayList对象时，程序会出错。 //private static List&lt;String&gt; list = new ArrayList&lt;String&gt;(); private static List&lt;String&gt; list = new CopyOnWriteArrayList&lt;String&gt;(); public static void main(String[] args) &#123; // 同时启动两个线程对list进行操作！ new MyThread(&quot;ta&quot;).start(); new MyThread(&quot;tb&quot;).start(); &#125; private static void printAll() &#123; String value = null; Iterator iter = list.iterator(); while(iter.hasNext()) &#123; value = (String)iter.next(); System.out.print(value+&quot;, &quot;); &#125; System.out.println(); &#125; private static class MyThread extends Thread &#123; MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; int i = 0; while (i++ &lt; 6) &#123; // “线程名” + &quot;-&quot; + &quot;序号&quot; String val = Thread.currentThread().getName()+&quot;-&quot;+i; list.add(val); // 通过“Iterator”遍历List。 printAll(); &#125; &#125; &#125;&#125; 其中一次运行结果：123456789101112ta-1, tb-1, ta-1, tb-1, ta-1, ta-1, tb-1, tb-1, tb-2, tb-2, ta-1, ta-2, tb-1, ta-1, tb-2, tb-1, ta-2, tb-2, tb-3, ta-2, ta-1, tb-3, tb-1, ta-3, tb-2, ta-1, ta-2, tb-1, tb-3, tb-2, ta-3, ta-2, tb-4, tb-3, ta-1, ta-3, tb-1, tb-4, tb-2, ta-4, ta-2, ta-1, tb-3, tb-1, ta-3, tb-2, tb-4, ta-2, ta-4, tb-3, tb-5, ta-3, ta-1, tb-4, tb-1, ta-4, tb-2, tb-5, ta-2, ta-5, tb-3, ta-1, ta-3, tb-1, tb-4, tb-2, ta-4, ta-2, tb-5, tb-3, ta-5, ta-3, tb-6, tb-4, ta-4, tb-5, ta-5, tb-6, ta-6, 结果说明：如果将源码中的list改成ArrayList对象时，程序会产生ConcurrentModificationException异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>CopyOnWriteArrayList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(CopyOnWriteArraySet)]]></title>
    <url>%2F2019%2F04%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(CopyOnWriteArraySet)%2F</url>
    <content type="text"><![CDATA[CopyOnWriteArraySet介绍它是线程安全的无序的集合，可以将它理解成线程安全的HashSet。有意思的是，CopyOnWriteArraySet和HashSet虽然都继承于共同的父类AbstractSet；但是，HashSet是通过“散列表(HashMap)”实现的，而CopyOnWriteArraySet则是通过“动态数组(CopyOnWriteArrayList)”实现的，并不是散列表。 和CopyOnWriteArrayList类似，CopyOnWriteArraySet具有以下特性： 它最适合于具有以下特征的应用程序：Set 大小通常保持很小，只读操作远多于可变操作，需要在遍历期间防止线程间的冲突。 它是线程安全的。 因为通常需要复制整个基础数组，所以可变操作（add()、set() 和 remove() 等等）的开销很大。 迭代器支持hasNext(), next()等不可变操作，但不支持可变 remove()等 操作。 使用迭代器进行遍历的速度很快，并且不会与其他线程发生冲突。在构造迭代器时，迭代器依赖于不变的数组快照。 建议：在学习CopyOnWriteArraySet之前,先对HashSet进行了解。 CopyOnWriteArraySet原理和数据结构CopyOnWriteArraySet的数据结构，如下图所示：.resources/F023A78F-7010-495C-B34F-FBCDE1801AF8.jpg)说明： CopyOnWriteArraySet继承于AbstractSet，这就意味着它是一个集合。 CopyOnWriteArraySet包含CopyOnWriteArrayList对象，它是通过CopyOnWriteArrayList实现的。而CopyOnWriteArrayList本质是个动态数组队列，所以CopyOnWriteArraySet相当于通过通过动态数组实现的“集合”！ CopyOnWriteArrayList中允许有重复的元素；但是,CopyOnWriteArraySet是一个集合,所以它不能有重复集合。因此,CopyOnWriteArrayList额外提供了addIfAbsent()和addAllAbsent()这两个添加元素的API,通过这些API来添加元素时，只有当元素不存在时才执行添加操作！至于CopyOnWriteArraySet的”线程安全”机制,和CopyOnWriteArrayList一样,是通过volatile和互斥锁来实现的。这个在前一章节介绍CopyOnWriteArrayList时数据结构时,已经进行了说明，这里就不再重复叙述了。 CopyOnWriteArraySet函数列表123456789101112131415161718192021222324252627282930313233// 创建一个空 set。CopyOnWriteArraySet()// 创建一个包含指定 collection 所有元素的 set。CopyOnWriteArraySet(Collection&lt;? extends E&gt; c)// 如果指定元素并不存在于此 set 中，则添加它。boolean add(E e)// 如果此 set 中没有指定 collection 中的所有元素，则将它们都添加到此 set 中。boolean addAll(Collection&lt;? extends E&gt; c)// 移除此 set 中的所有元素。void clear()// 如果此 set 包含指定元素，则返回 true。boolean contains(Object o)// 如果此 set 包含指定 collection 的所有元素，则返回 true。boolean containsAll(Collection&lt;?&gt; c)// 比较指定对象与此 set 的相等性。boolean equals(Object o)// 如果此 set 不包含任何元素，则返回 true。boolean isEmpty()// 返回按照元素添加顺序在此 set 中包含的元素上进行迭代的迭代器。Iterator&lt;E&gt; iterator()// 如果指定元素存在于此 set 中，则将其移除。boolean remove(Object o)// 移除此 set 中包含在指定 collection 中的所有元素。boolean removeAll(Collection&lt;?&gt; c)// 仅保留此 set 中那些包含在指定 collection 中的元素。boolean retainAll(Collection&lt;?&gt; c)// 返回此 set 中的元素数目。int size()// 返回一个包含此 set 所有元素的数组。Object[] toArray()// 返回一个包含此 set 所有元素的数组；返回数组的运行时类型是指定数组的类型。&lt;T&gt; T[] toArray(T[] a) CopyOnWriteArraySet示例CopyOnWriteArraySet是通过CopyOnWriteArrayList实现的，它的API基本上都是通过调用CopyOnWriteArrayList的API来实现的。相信对CopyOnWriteArrayList了解的话，对CopyOnWriteArraySet的了解是水到渠成的事。下面，我们通过一个例子去对比HashSet和CopyOnWriteArraySet。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.*;import java.util.concurrent.*;/* * CopyOnWriteArraySet是“线程安全”的集合，而HashSet是非线程安全的。 * * 下面是“多个线程同时操作并且遍历集合set”的示例 * (01) 当set是CopyOnWriteArraySet对象时，程序能正常运行。 * (02) 当set是HashSet对象时，程序会产生ConcurrentModificationException异常。 * */public class CopyOnWriteArraySetTest1 &#123; // TODO: set是HashSet对象时，程序会出错。 //private static Set&lt;String&gt; set = new HashSet&lt;String&gt;(); private static Set&lt;String&gt; set = new CopyOnWriteArraySet&lt;String&gt;(); public static void main(String[] args) &#123; // 同时启动两个线程对set进行操作！ new MyThread(&quot;ta&quot;).start(); new MyThread(&quot;tb&quot;).start(); &#125; private static void printAll() &#123; String value = null; Iterator iter = set.iterator(); while(iter.hasNext()) &#123; value = (String)iter.next(); System.out.print(value+&quot;, &quot;); &#125; System.out.println(); &#125; private static class MyThread extends Thread &#123; MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; int i = 0; while (i++ &lt; 10) &#123; // “线程名” + &quot;-&quot; + &quot;序号&quot; String val = Thread.currentThread().getName() + &quot;-&quot; + (i%6); set.add(val); // 通过“Iterator”遍历set。 printAll(); &#125; &#125; &#125;&#125; 其中一次运行结果：1234567891011121314151617181920ta-1, tb-1, ta-1, tb-1, ta-1, tb-1, ta-1, ta-2, tb-1, ta-1, ta-2, tb-1, tb-2, ta-2, ta-1, tb-2, tb-1, ta-3, ta-2, ta-1, tb-2, tb-1, ta-3, ta-2, tb-3, tb-2, ta-1, ta-3, tb-1, tb-3, ta-2, ta-4, tb-2, ta-1, ta-3, tb-1, tb-3, ta-2, ta-4, tb-2, tb-4, ta-3, ta-1, tb-3, tb-1, ta-4, ta-2, tb-4, tb-2, ta-5, ta-3, ta-1, tb-3, tb-1, ta-4, ta-2, tb-4, tb-2, ta-5, ta-3, tb-5, tb-3, ta-1, ta-4, tb-1, tb-4, ta-2, ta-5, tb-2, tb-5, ta-3, ta-0, tb-3, ta-1, ta-4, tb-1, tb-4, ta-2, ta-5, tb-2, tb-5, ta-3, ta-0, tb-3, tb-0, ta-4, ta-1, tb-4, tb-1, ta-5, ta-2, tb-5, tb-2, ta-0, ta-3, tb-0, tb-3, ta-1, ta-4, tb-1, tb-4, ta-2, ta-5, tb-5, ta-0, tb-0, ta-1, tb-2, tb-1, ta-3, ta-2, tb-3, tb-2, ta-4, ta-3, tb-4, tb-3, ta-5, ta-4, tb-5, tb-4, ta-0, ta-5, tb-0, tb-5, ta-1, ta-0, tb-1, tb-0, ta-2, ta-1, tb-2, tb-1, ta-3, ta-2, tb-3, tb-2, ta-4, ta-3, tb-4, tb-3, ta-5, tb-5, ta-0, tb-0, ta-4, ta-1, tb-4, tb-1, ta-5, ta-2, tb-5, tb-2, ta-0, ta-3, tb-0, tb-3, ta-1, ta-4, tb-1, tb-4, ta-2, ta-5, tb-2, tb-5, ta-3, ta-0, tb-3, tb-0, ta-4, tb-4, ta-5, tb-5, ta-0, tb-0, 结果说明：由于set是集合对象,因此它不会包含重复的元素。如果将源码中的set改成HashSet对象时,程序会产生ConcurrentModificationException异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>CopyOnWriteArraySet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(LinkedBlockingDeque)]]></title>
    <url>%2F2019%2F04%2F13%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(LinkedBlockingDeque)%2F</url>
    <content type="text"><![CDATA[LinkedBlockingDeque介绍LinkedBlockingDeque是双向链表实现的双向并发阻塞队列。该阻塞队列同时支持FIFO和FILO两种操作方式，即可以从队列的头和尾同时操作(插入/删除)；并且，该阻塞队列是支持线程安全。 此外，LinkedBlockingDeque还是可选容量的(防止过度膨胀)，即可以指定队列的容量。如果不指定，默认容量大小等于Integer.MAX_VALUE。 LinkedBlockingDeque原理和数据结构LinkedBlockingDeque的数据结构，如下图所示： .resources/385B998A-3394-4B44-AE6B-B66F7775E0A4.jpg)说明：1. LinkedBlockingDeque继承于AbstractQueue，它本质上是一个支持FIFO和FILO的双向的队列。 LinkedBlockingDeque实现了BlockingDeque接口，它支持多线程并发。当多线程竞争同一个资源时，某线程获取到该资源之后，其它线程需要阻塞等待。 LinkedBlockingDeque是通过双向链表实现的。3.1 first是双向链表的表头。3.2 last是双向链表的表尾。3.3 count是LinkedBlockingDeque的实际大小，即双向链表中当前节点个数。3.4 capacity是LinkedBlockingDeque的容量，它是在创建LinkedBlockingDeque时指定的。3.5 lock是控制对LinkedBlockingDeque的互斥锁，当多个线程竞争同时访问LinkedBlockingDeque时，某线程获取到了互斥锁lock，其它线程则需要阻塞等待，直到该线程释放lock，其它线程才有机会获取lock从而获取cpu执行权。3.6 notEmpty和notFull分别是“非空条件”和“未满条件”。通过它们能够更加细腻进行并发控制。LinkedBlockingDeque函数列表123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899// 创建一个容量为 Integer.MAX_VALUE 的 LinkedBlockingDeque。LinkedBlockingDeque()// 创建一个容量为 Integer.MAX_VALUE 的 LinkedBlockingDeque，最初包含给定 collection 的元素，以该 collection 迭代器的遍历顺序添加。LinkedBlockingDeque(Collection&lt;? extends E&gt; c)// 创建一个具有给定（固定）容量的 LinkedBlockingDeque。LinkedBlockingDeque(int capacity)// 在不违反容量限制的情况下，将指定的元素插入此双端队列的末尾。boolean add(E e)// 如果立即可行且不违反容量限制，则将指定的元素插入此双端队列的开头；如果当前没有空间可用，则抛出 IllegalStateException。void addFirst(E e)// 如果立即可行且不违反容量限制，则将指定的元素插入此双端队列的末尾；如果当前没有空间可用，则抛出 IllegalStateException。void addLast(E e)// 以原子方式 (atomically) 从此双端队列移除所有元素。void clear()// 如果此双端队列包含指定的元素，则返回 true。boolean contains(Object o)// 返回在此双端队列的元素上以逆向连续顺序进行迭代的迭代器。Iterator&lt;E&gt; descendingIterator()// 移除此队列中所有可用的元素，并将它们添加到给定 collection 中。int drainTo(Collection&lt;? super E&gt; c)// 最多从此队列中移除给定数量的可用元素，并将这些元素添加到给定 collection 中。int drainTo(Collection&lt;? super E&gt; c, int maxElements)// 获取但不移除此双端队列表示的队列的头部。E element()// 获取，但不移除此双端队列的第一个元素。E getFirst()// 获取，但不移除此双端队列的最后一个元素。E getLast()// 返回在此双端队列元素上以恰当顺序进行迭代的迭代器。Iterator&lt;E&gt; iterator()// 如果立即可行且不违反容量限制，则将指定的元素插入此双端队列表示的队列中（即此双端队列的尾部），并在成功时返回 true；如果当前没有空间可用，则返回 false。boolean offer(E e)// 将指定的元素插入此双端队列表示的队列中（即此双端队列的尾部），必要时将在指定的等待时间内一直等待可用空间。boolean offer(E e, long timeout, TimeUnit unit)// 如果立即可行且不违反容量限制，则将指定的元素插入此双端队列的开头，并在成功时返回 true；如果当前没有空间可用，则返回 false。boolean offerFirst(E e)// 将指定的元素插入此双端队列的开头，必要时将在指定的等待时间内等待可用空间。boolean offerFirst(E e, long timeout, TimeUnit unit)// 如果立即可行且不违反容量限制，则将指定的元素插入此双端队列的末尾，并在成功时返回 true；如果当前没有空间可用，则返回 false。boolean offerLast(E e)// 将指定的元素插入此双端队列的末尾，必要时将在指定的等待时间内等待可用空间。boolean offerLast(E e, long timeout, TimeUnit unit)// 获取但不移除此双端队列表示的队列的头部（即此双端队列的第一个元素）；如果此双端队列为空，则返回 null。E peek()// 获取，但不移除此双端队列的第一个元素；如果此双端队列为空，则返回 null。E peekFirst()// 获取，但不移除此双端队列的最后一个元素；如果此双端队列为空，则返回 null。E peekLast()// 获取并移除此双端队列表示的队列的头部（即此双端队列的第一个元素）；如果此双端队列为空，则返回 null。E poll()// 获取并移除此双端队列表示的队列的头部（即此双端队列的第一个元素），如有必要将在指定的等待时间内等待可用元素。E poll(long timeout, TimeUnit unit)// 获取并移除此双端队列的第一个元素；如果此双端队列为空，则返回 null。E pollFirst()// 获取并移除此双端队列的第一个元素，必要时将在指定的等待时间等待可用元素。E pollFirst(long timeout, TimeUnit unit)// 获取并移除此双端队列的最后一个元素；如果此双端队列为空，则返回 null。E pollLast()// 获取并移除此双端队列的最后一个元素，必要时将在指定的等待时间内等待可用元素。E pollLast(long timeout, TimeUnit unit)// 从此双端队列所表示的堆栈中弹出一个元素。E pop()// 将元素推入此双端队列表示的栈。void push(E e)// 将指定的元素插入此双端队列表示的队列中（即此双端队列的尾部），必要时将一直等待可用空间。void put(E e)// 将指定的元素插入此双端队列的开头，必要时将一直等待可用空间。void putFirst(E e)// 将指定的元素插入此双端队列的末尾，必要时将一直等待可用空间。void putLast(E e)// 返回理想情况下（没有内存和资源约束）此双端队列可不受阻塞地接受的额外元素数。int remainingCapacity()// 获取并移除此双端队列表示的队列的头部。E remove()// 从此双端队列移除第一次出现的指定元素。boolean remove(Object o)// 获取并移除此双端队列第一个元素。E removeFirst()// 从此双端队列移除第一次出现的指定元素。boolean removeFirstOccurrence(Object o)// 获取并移除此双端队列的最后一个元素。E removeLast()// 从此双端队列移除最后一次出现的指定元素。boolean removeLastOccurrence(Object o)// 返回此双端队列中的元素数。int size()// 获取并移除此双端队列表示的队列的头部（即此双端队列的第一个元素），必要时将一直等待可用元素。E take()// 获取并移除此双端队列的第一个元素，必要时将一直等待可用元素。E takeFirst()// 获取并移除此双端队列的最后一个元素，必要时将一直等待可用元素。E takeLast()// 返回以恰当顺序（从第一个元素到最后一个元素）包含此双端队列所有元素的数组。Object[] toArray()// 返回以恰当顺序包含此双端队列所有元素的数组；返回数组的运行时类型是指定数组的运行时类型。&lt;T&gt; T[] toArray(T[] a)// 返回此 collection 的字符串表示形式。String toString() LinkedBlockingDeque源码分析下面从ArrayBlockingQueue的创建，添加，取出，遍历这几个方面对LinkedBlockingDeque进行分析 1. 创建 下面以LinkedBlockingDeque(int capacity)来进行说明。1234public LinkedBlockingDeque(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity;&#125; 说明：capacity是“链式阻塞队列”的容量。 LinkedBlockingDeque中相关的数据结果定义如下：123456789101112// “双向队列”的表头transient Node&lt;E&gt; first;// “双向队列”的表尾transient Node&lt;E&gt; last;// 节点数量private transient int count;// 容量private final int capacity;// 互斥锁 , 互斥锁对应的“非空条件notEmpty”, 互斥锁对应的“未满条件notFull”final ReentrantLock lock = new ReentrantLock();private final Condition notEmpty = lock.newCondition();private final Condition notFull = lock.newCondition(); 说明：lock是互斥锁，用于控制多线程对LinkedBlockingDeque中元素的互斥访问；而notEmpty和notFull是与lock绑定的条件，它们用于实现对多线程更精确的控制。 双向链表的节点Node的定义如下：1234567static final class Node&lt;E&gt; &#123; E item; // 数据 Node&lt;E&gt; prev; // 前一节点 Node&lt;E&gt; next; // 后一节点 Node(E x) &#123; item = x; &#125;&#125; 添加 下面以offer(E e)为例，对LinkedBlockingDeque的添加方法进行说明。123public boolean offer(E e) &#123; return offerLast(e);&#125; offer()实际上是调用offerLast()将元素添加到队列的末尾。offerLast()的源码如下：123456789101112131415public boolean offerLast(E e) &#123; if (e == null) throw new NullPointerException(); // 新建节点 Node&lt;E&gt; node = new Node&lt;E&gt;(e); final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; // 将“新节点”添加到双向链表的末尾 return linkLast(node); &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 说明：offerLast()的作用，是新建节点并将该节点插入到双向链表的末尾。它在插入节点前，会获取锁；操作完毕，再释放锁。 linkLast()的源码如下：123456789101112131415161718private boolean linkLast(Node&lt;E&gt; node) &#123; // 如果“双向链表的节点数量” &gt; “容量”，则返回false，表示插入失败。 if (count &gt;= capacity) return false; // 将“node添加到链表末尾”，并设置node为新的尾节点 Node&lt;E&gt; l = last; node.prev = l; last = node; if (first == null) first = node; else l.next = node; // 将“节点数量”+1 ++count; // 插入节点之后，唤醒notEmpty上的等待线程。 notEmpty.signal(); return true;&#125; 说明：linkLast()的作用，是将节点插入到双向队列的末尾；插入节点之后，唤醒notEmpty上的等待线程。 3. 删除 下面以take()为例，对LinkedBlockingDeque的取出方法进行说明。123public E take() throws InterruptedException &#123; return takeFirst();&#125; take()实际上是调用takeFirst()队列的第一个元素。takeFirst()的源码如下：123456789101112131415public E takeFirst() throws InterruptedException &#123; final ReentrantLock lock = this.lock; // 获取锁 lock.lock(); try &#123; E x; // 若“队列为空”，则一直等待。否则，通过unlinkFirst()删除第一个节点。 while ( (x = unlinkFirst()) == null) notEmpty.await(); return x; &#125; finally &#123; // 释放锁 lock.unlock(); &#125;&#125; 说明：takeFirst()的作用，是删除双向链表的第一个节点，并返回节点对应的值。它在插入节点前，会获取锁；操作完毕，再释放锁。 unlinkFirst()的源码如下：123456789101112131415161718192021private E unlinkFirst() &#123; // assert lock.isHeldByCurrentThread(); Node&lt;E&gt; f = first; if (f == null) return null; // 删除并更新“第一个节点” Node&lt;E&gt; n = f.next; E item = f.item; f.item = null; f.next = f; // help GC first = n; if (n == null) last = null; else n.prev = null; // 将“节点数量”-1 --count; // 删除节点之后，唤醒notFull上的等待线程。 notFull.signal(); return item;&#125; 说明：unlinkFirst()的作用，是将双向队列的第一个节点删除；删除节点之后，唤醒notFull上的等待线程。 遍历下面对LinkedBlockingDeque的遍历方法进行说明。 123public Iterator&lt;E&gt; iterator() &#123; return new Itr();&#125; iterator()实际上是返回一个Iter对象。Itr类的定义如下：123456private class Itr extends AbstractItr &#123; // “双向队列”的表头 Node&lt;E&gt; firstNode() &#123; return first; &#125; // 获取“节点n的下一个节点” Node&lt;E&gt; nextNode(Node&lt;E&gt; n) &#123; return n.next; &#125;&#125; Itr继承于AbstractItr，而AbstractItr的定义如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788private abstract class AbstractItr implements Iterator&lt;E&gt; &#123; // next是下一次调用next()会返回的节点。 Node&lt;E&gt; next; // nextItem是next()返回节点对应的数据。 E nextItem; // 上一次next()返回的节点。 private Node&lt;E&gt; lastRet; // 返回第一个节点 abstract Node&lt;E&gt; firstNode(); // 返回下一个节点 abstract Node&lt;E&gt; nextNode(Node&lt;E&gt; n); AbstractItr() &#123; final ReentrantLock lock = LinkedBlockingDeque.this.lock; // 获取“LinkedBlockingDeque的互斥锁” lock.lock(); try &#123; // 获取“双向队列”的表头 next = firstNode(); // 获取表头对应的数据 nextItem = (next == null) ? null : next.item; &#125; finally &#123; // 释放“LinkedBlockingDeque的互斥锁” lock.unlock(); &#125; &#125; // 获取n的后继节点 private Node&lt;E&gt; succ(Node&lt;E&gt; n) &#123; // Chains of deleted nodes ending in null or self-links // are possible if multiple interior nodes are removed. for (;;) &#123; Node&lt;E&gt; s = nextNode(n); if (s == null) return null; else if (s.item != null) return s; else if (s == n) return firstNode(); else n = s; &#125; &#125; // 更新next和nextItem。 void advance() &#123; final ReentrantLock lock = LinkedBlockingDeque.this.lock; lock.lock(); try &#123; // assert next != null; next = succ(next); nextItem = (next == null) ? null : next.item; &#125; finally &#123; lock.unlock(); &#125; &#125; // 返回“下一个节点是否为null” public boolean hasNext() &#123; return next != null; &#125; // 返回下一个节点 public E next() &#123; if (next == null) throw new NoSuchElementException(); lastRet = next; E x = nextItem; advance(); return x; &#125; // 删除下一个节点 public void remove() &#123; Node&lt;E&gt; n = lastRet; if (n == null) throw new IllegalStateException(); lastRet = null; final ReentrantLock lock = LinkedBlockingDeque.this.lock; lock.lock(); try &#123; if (n.item != null) unlink(n); &#125; finally &#123; lock.unlock(); &#125; &#125;&#125; LinkedBlockingDeque示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.*;import java.util.concurrent.*;/* * LinkedBlockingDeque是“线程安全”的队列，而LinkedList是非线程安全的。 * * 下面是“多个线程同时操作并且遍历queue”的示例 * (01) 当queue是LinkedBlockingDeque对象时，程序能正常运行。 * (02) 当queue是LinkedList对象时，程序会产生ConcurrentModificationException异常。 * */public class LinkedBlockingDequeDemo1 &#123; // TODO: queue是LinkedList对象时，程序会出错。 //private static Queue&lt;String&gt; queue = new LinkedList&lt;String&gt;(); private static Queue&lt;String&gt; queue = new LinkedBlockingDeque&lt;String&gt;(); public static void main(String[] args) &#123; // 同时启动两个线程对queue进行操作！ new MyThread(&quot;ta&quot;).start(); new MyThread(&quot;tb&quot;).start(); &#125; private static void printAll() &#123; String value; Iterator iter = queue.iterator(); while(iter.hasNext()) &#123; value = (String)iter.next(); System.out.print(value+&quot;, &quot;); &#125; System.out.println(); &#125; private static class MyThread extends Thread &#123; MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; int i = 0; while (i++ &lt; 6) &#123; // “线程名” + &quot;-&quot; + &quot;序号&quot; String val = Thread.currentThread().getName()+i; queue.add(val); // 通过“Iterator”遍历queue。 printAll(); &#125; &#125; &#125;&#125; 其中一次运行结果：123456789101112ta1, ta1, tb1, tb1,ta1, ta1, tb1, tb1, tb2, tb2, ta2, ta2, ta1, ta1, tb1, tb1, tb2, tb2, ta2, ta2, tb3, tb3, ta3, ta3, ta1, tb1, ta1, tb2, tb1, ta2, tb2, tb3, ta2, ta3, tb3, tb4, ta3, ta4, tb4, ta1, ta4, tb1, tb5, tb2, ta1, ta2, tb1, tb3, tb2, ta3, ta2, tb4, tb3, ta4, ta3, tb5, tb4, ta5, ta4, ta1, tb5, tb1, ta5, tb2, tb6, ta2, ta1, tb3, tb1, ta3, tb2, tb4, ta2, ta4, tb3, tb5, ta3, ta5, tb4, tb6, ta4, ta6, tb5, ta5, tb6, ta6, 结果说明：示例程序中，启动两个线程(线程ta和线程tb)分别对LinkedBlockingDeque进行操作。以线程ta而言，它会先获取“线程名”+“序号”，然后将该字符串添加到LinkedBlockingDeque中；接着，遍历并输出LinkedBlockingDeque中的全部元素。 线程tb的操作和线程ta一样，只不过线程tb的名字和线程ta的名字不同。当queue是LinkedBlockingDeque对象时，程序能正常运行。如果将queue改为LinkedList时，程序会产生ConcurrentModificationException异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>LinkedBlockingDeque</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(LinkedBlockingQueue)]]></title>
    <url>%2F2019%2F04%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(LinkedBlockingQueue)%2F</url>
    <content type="text"><![CDATA[LinkedBlockingQueue介绍LinkedBlockingQueue是一个单向链表实现的阻塞队列。该队列按 FIFO（先进先出）排序元素，新元素插入到队列的尾部，并且队列获取操作会获得位于队列头部的元素。链接队列的吞吐量通常要高于基于数组的队列，但是在大多数并发应用程序中，其可预知的性能要低。 此外，LinkedBlockingQueue还是可选容量的(防止过度膨胀)，即可以指定队列的容量。如果不指定，默认容量大小等于Integer.MAX_VALUE。 LinkedBlockingQueue原理和数据结构LinkedBlockingQueue的数据结构，如下图所示：.resources/C0887B04-3B23-4874-B6D5-702B2FE76242.jpg)说明： LinkedBlockingQueue继承于AbstractQueue，它本质上是一个FIFO(先进先出)的队列。 LinkedBlockingQueue实现了BlockingQueue接口，它支持多线程并发。当多线程竞争同一个资源时，某线程获取到该资源之后，其它线程需要阻塞等待。 LinkedBlockingQueue是通过单链表实现的:(01) head是链表的表头。取出数据时，都是从表头head处插入。(02) last是链表的表尾。新增数据时，都是从表尾last处插入。(03) count是链表的实际大小，即当前链表中包含的节点个数。(04) capacity是列表的容量，它是在创建链表时指定的。(05) putLock是插入锁，takeLock是取出锁；notEmpty是“非空条件”，notFull是“未满条件”。通过它们对链表进行并发控制。LinkedBlockingQueue在实现“多线程对竞争资源的互斥访问”时，对于“插入”和“取出(删除)”操作分别使用了不同的锁。对于插入操作，通过“插入锁putLock”进行同步；对于取出操作，通过“取出锁takeLock”进行同步。此外，插入锁putLock和“非满条件notFull”相关联，取出锁takeLock和“非空条件notEmpty”相关联。通过notFull和notEmpty更细腻的控制锁。 12若某线程(线程A)要取出数据时，队列正好为空，则该线程会执行notEmpty.await()进行等待；当其它某个线程(线程B)向队列中插入了数据之后，会调用notEmpty.signal()唤醒“notEmpty上的等待线程”。此时，线程A会被唤醒从而得以继续运行。 此外，线程A在执行取操作前，会获取takeLock，在取操作执行完毕再释放takeLock。若某线程(线程H)要插入数据时，队列已满，则该线程会它执行notFull.await()进行等待；当其它某个线程(线程I)取出数据之后，会调用notFull.signal()唤醒“notFull上的等待线程”。此时，线程H就会被唤醒从而得以继续运行。 此外，线程H在执行插入操作前，会获取putLock，在插入操作执行完毕才释放putLock。 LinkedBlockingQueue函数列表1234567891011121314151617181920212223242526272829303132333435363738394041// 创建一个容量为 Integer.MAX_VALUE 的 LinkedBlockingQueue。LinkedBlockingQueue()// 创建一个容量是 Integer.MAX_VALUE 的 LinkedBlockingQueue，最初包含给定 collection 的元素，元素按该 collection 迭代器的遍历顺序添加。LinkedBlockingQueue(Collection&lt;? extends E&gt; c)// 创建一个具有给定（固定）容量的 LinkedBlockingQueue。LinkedBlockingQueue(int capacity)// 从队列彻底移除所有元素。void clear()// 移除此队列中所有可用的元素，并将它们添加到给定 collection 中。int drainTo(Collection&lt;? super E&gt; c)// 最多从此队列中移除给定数量的可用元素，并将这些元素添加到给定 collection 中。int drainTo(Collection&lt;? super E&gt; c, int maxElements)// 返回在队列中的元素上按适当顺序进行迭代的迭代器。Iterator&lt;E&gt; iterator()// 将指定元素插入到此队列的尾部（如果立即可行且不会超出此队列的容量），在成功时返回 true，如果此队列已满，则返回 false。boolean offer(E e)// 将指定元素插入到此队列的尾部，如有必要，则等待指定的时间以使空间变得可用。boolean offer(E e, long timeout, TimeUnit unit)// 获取但不移除此队列的头；如果此队列为空，则返回 null。E peek()// 获取并移除此队列的头，如果此队列为空，则返回 null。E poll()// 获取并移除此队列的头部，在指定的等待时间前等待可用的元素（如果有必要）。E poll(long timeout, TimeUnit unit)// 将指定元素插入到此队列的尾部，如有必要，则等待空间变得可用。void put(E e)// 返回理想情况下（没有内存和资源约束）此队列可接受并且不会被阻塞的附加元素数量。int remainingCapacity()// 从此队列移除指定元素的单个实例（如果存在）。boolean remove(Object o)// 返回队列中的元素个数。int size()// 获取并移除此队列的头部，在元素变得可用之前一直等待（如果有必要）。E take()// 返回按适当顺序包含此队列中所有元素的数组。Object[] toArray()// 返回按适当顺序包含此队列中所有元素的数组；返回数组的运行时类型是指定数组的运行时类型。&lt;T&gt; T[] toArray(T[] a)// 返回此 collection 的字符串表示形式。String toString() LinkedBlockingQueue源码分析下面从LinkedBlockingQueue的创建，添加，删除，遍历这几个方面对它进行分析。1. 创建下面以LinkedBlockingQueue(int capacity)来进行说明。12345public LinkedBlockingQueue(int capacity) &#123; if (capacity &lt;= 0) throw new IllegalArgumentException(); this.capacity = capacity; last = head = new Node&lt;E&gt;(null);&#125; 说明：(01) capacity是“链式阻塞队列”的容量。(02) head和last是“链式阻塞队列”的首节点和尾节点。它们在LinkedBlockingQueue中的声明如下：123456789101112// 容量private final int capacity;// 当前数量private final AtomicInteger count = new AtomicInteger(0);private transient Node&lt;E&gt; head; // 链表的表头private transient Node&lt;E&gt; last; // 链表的表尾// 用于控制“删除元素”的互斥锁takeLock 和 锁对应的“非空条件”notEmptyprivate final ReentrantLock takeLock = new ReentrantLock();private final Condition notEmpty = takeLock.newCondition();// 用于控制“添加元素”的互斥锁putLock 和 锁对应的“非满条件”notFullprivate final ReentrantLock putLock = new ReentrantLock();private final Condition notFull = putLock.newCondition(); 链表的节点定义如下：123456static class Node&lt;E&gt; &#123; E item; // 数据 Node&lt;E&gt; next; // 下一个节点的指针 Node(E x) &#123; item = x; &#125;&#125; 2. 添加 下面以offer(E e)为例，对LinkedBlockingQueue的添加方法进行说明。123456789101112131415161718192021222324252627282930313233public boolean offer(E e) &#123; if (e == null) throw new NullPointerException(); // 如果“队列已满”，则返回false，表示插入失败。 final AtomicInteger count = this.count; if (count.get() == capacity) return false; int c = -1; // 新建“节点e” Node&lt;E&gt; node = new Node(e); final ReentrantLock putLock = this.putLock; // 获取“插入锁putLock” putLock.lock(); try &#123; // 再次对“队列是不是满”的进行判断。 // 若“队列未满”，则插入节点。 if (count.get() &lt; capacity) &#123; // 插入节点 enqueue(node); // 将“当前节点数量”+1，并返回“原始的数量” c = count.getAndIncrement(); // 如果在插入元素之后，队列仍然未满，则唤醒notFull上的等待线程。 if (c + 1 &lt; capacity) notFull.signal(); &#125; &#125; finally &#123; // 释放“插入锁putLock” putLock.unlock(); &#125; // 如果在插入节点前，队列为空；则插入节点后，唤醒notEmpty上的等待线程 if (c == 0) signalNotEmpty(); return c &gt;= 0;&#125; 说明：offer()的作用很简单，就是将元素E添加到队列的末尾。enqueue()的源码如下：12345private void enqueue(Node&lt;E&gt; node) &#123; // assert putLock.isHeldByCurrentThread(); // assert last.next == null; last = last.next = node;&#125; enqueue()的作用是将node添加到队列末尾，并设置node为新的尾节点！signalNotEmpty()的源码如下：123456789private void signalNotEmpty() &#123; final ReentrantLock takeLock = this.takeLock; takeLock.lock(); try &#123; notEmpty.signal(); &#125; finally &#123; takeLock.unlock(); &#125;&#125; signalNotEmpty()的作用是唤醒notEmpty上的等待线程。3. 取出 下面以take()为例，对LinkedBlockingQueue的取出方法进行说明。123456789101112131415161718192021222324252627public E take() throws InterruptedException &#123; E x; int c = -1; final AtomicInteger count = this.count; final ReentrantLock takeLock = this.takeLock; // 获取“取出锁”，若当前线程是中断状态，则抛出InterruptedException异常 takeLock.lockInterruptibly(); try &#123; // 若“队列为空”，则一直等待。 while (count.get() == 0) &#123; notEmpty.await(); &#125; // 取出元素 x = dequeue(); // 取出元素之后，将“节点数量”-1；并返回“原始的节点数量”。 c = count.getAndDecrement(); if (c &gt; 1) notEmpty.signal(); &#125; finally &#123; // 释放“取出锁” takeLock.unlock(); &#125; // 如果在“取出元素之前”，队列是满的；则在取出元素之后，唤醒notFull上的等待线程。 if (c == capacity) signalNotFull(); return x;&#125; 说明：take()的作用是取出并返回队列的头。若队列为空，则一直等待。dequeue()的源码如下：1234567891011private E dequeue() &#123; // assert takeLock.isHeldByCurrentThread(); // assert head.item == null; Node&lt;E&gt; h = head; Node&lt;E&gt; first = h.next; h.next = h; // help GC head = first; E x = first.item; first.item = null; return x;&#125; dequeue()的作用就是删除队列的头节点，并将表头指向“原头节点的下一个节点”。signalNotFull()的源码如下：123456789private void signalNotFull() &#123; final ReentrantLock putLock = this.putLock; putLock.lock(); try &#123; notFull.signal(); &#125; finally &#123; putLock.unlock(); &#125;&#125; signalNotFull()的作用就是唤醒notFull上的等待线程。 4. 遍历下面对LinkedBlockingQueue的遍历方法进行说明。123public Iterator&lt;E&gt; iterator() &#123; return new Itr();&#125; iterator()实际上是返回一个Iter对象。Itr类的定义如下：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475private class Itr implements Iterator&lt;E&gt; &#123; // 当前节点 private Node&lt;E&gt; current; // 上一次返回的节点 private Node&lt;E&gt; lastRet; // 当前节点对应的值 private E currentElement; Itr() &#123; // 同时获取“插入锁putLock” 和 “取出锁takeLock” fullyLock(); try &#123; // 设置“当前元素”为“队列表头的下一节点”，即为队列的第一个有效节点 current = head.next; if (current != null) currentElement = current.item; &#125; finally &#123; // 释放“插入锁putLock” 和 “取出锁takeLock” fullyUnlock(); &#125; &#125; // 返回“下一个节点是否为null” public boolean hasNext() &#123; return current != null; &#125; private Node&lt;E&gt; nextNode(Node&lt;E&gt; p) &#123; for (;;) &#123; Node&lt;E&gt; s = p.next; if (s == p) return head.next; if (s == null || s.item != null) return s; p = s; &#125; &#125; // 返回下一个节点 public E next() &#123; fullyLock(); try &#123; if (current == null) throw new NoSuchElementException(); E x = currentElement; lastRet = current; current = nextNode(current); currentElement = (current == null) ? null : current.item; return x; &#125; finally &#123; fullyUnlock(); &#125; &#125; // 删除下一个节点 public void remove() &#123; if (lastRet == null) throw new IllegalStateException(); fullyLock(); try &#123; Node&lt;E&gt; node = lastRet; lastRet = null; for (Node&lt;E&gt; trail = head, p = trail.next; p != null; trail = p, p = p.next) &#123; if (p == node) &#123; unlink(p, trail); break; &#125; &#125; &#125; finally &#123; fullyUnlock(); &#125; &#125;&#125; LinkedBlockingQueue示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import java.util.*;import java.util.concurrent.*;/* * LinkedBlockingQueue是“线程安全”的队列，而LinkedList是非线程安全的。 * * 下面是“多个线程同时操作并且遍历queue”的示例 * (01) 当queue是LinkedBlockingQueue对象时，程序能正常运行。 * (02) 当queue是LinkedList对象时，程序会产生ConcurrentModificationException异常。 * */public class LinkedBlockingQueueDemo1 &#123; // TODO: queue是LinkedList对象时，程序会出错。 //private static Queue&lt;String&gt; queue = new LinkedList&lt;String&gt;(); private static Queue&lt;String&gt; queue = new LinkedBlockingQueue&lt;String&gt;(); public static void main(String[] args) &#123; // 同时启动两个线程对queue进行操作！ new MyThread(&quot;ta&quot;).start(); new MyThread(&quot;tb&quot;).start(); &#125; private static void printAll() &#123; String value; Iterator iter = queue.iterator(); while(iter.hasNext()) &#123; value = (String)iter.next(); System.out.print(value+&quot;, &quot;); &#125; System.out.println(); &#125; private static class MyThread extends Thread &#123; MyThread(String name) &#123; super(name); &#125; @Override public void run() &#123; int i = 0; while (i++ &lt; 6) &#123; // “线程名” + &quot;-&quot; + &quot;序号&quot; String val = Thread.currentThread().getName()+i; queue.add(val); // 通过“Iterator”遍历queue。 printAll(); &#125; &#125; &#125;&#125; 其中一次运行结果：123456789101112tb1, ta1, tb1, ta1, ta2, tb1, ta1, ta2, ta3, tb1, ta1, ta2, ta3, ta4, tb1, ta1, tb1, ta2, ta1, ta3, ta2, ta4, ta3, ta5, ta4, tb1, ta5, ta1, ta6, ta2, tb1, ta3, ta1, ta4, ta2, ta5, ta3, ta6, ta4, tb2, ta5, ta6, tb2, tb1, ta1, ta2, ta3, ta4, ta5, ta6, tb2, tb3, tb1, ta1, ta2, ta3, ta4, ta5, ta6, tb2, tb3, tb4, tb1, ta1, ta2, ta3, ta4, ta5, ta6, tb2, tb3, tb4, tb5, tb1, ta1, ta2, ta3, ta4, ta5, ta6, tb2, tb3, tb4, tb5, tb6, 结果说明：示例程序中，启动两个线程(线程ta和线程tb)分别对LinkedBlockingQueue进行操作。以线程ta而言，它会先获取“线程名”+“序号”，然后将该字符串添加到LinkedBlockingQueue中；接着，遍历并输出LinkedBlockingQueue中的全部元素。 线程tb的操作和线程ta一样，只不过线程tb的名字和线程ta的名字不同。当queue是LinkedBlockingQueue对象时，程序能正常运行。如果将queue改为LinkedList时，程序会产生ConcurrentModificationException异常。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>LinkedBlockingQueue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(并发容器)]]></title>
    <url>%2F2019%2F04%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(%E5%B9%B6%E5%8F%91%E5%AE%B9%E5%99%A8)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-并发容器List和SetJUC(java.util.concurrent)集合包中的List和Set实现类包括:**CopyOnWriteArrayList CopyOnWriteArraySetConcurrentSkipListSet**ConcurrentSkipListSet稍后在说明Map时再说明,CopyOnWriteArrayList和CopyOnWriteArraySet的框架如下图所示：.resources/9AB2EEAE-5F35-4C72-9953-E24DB769027D.jpg) CopyOnWriteArrayList相当于线程安全的ArrayList,它实现了List接口。CopyOnWriteArrayList是支持高并发的。CopyOnWriteArraySet相当于线程安全的HashSet,它继承于AbstractSet类。CopyOnWriteArraySet内部包含一个CopyOnWriteArrayList对象，它是通过CopyOnWriteArrayList实现的。 MapJUC集合包中Map的实现类包括: ConcurrentHashMap和ConcurrentSkipListMap。它们的框架如下图所示：.resources/95BA2E89-44BB-461D-A8FA-9C62C4E65FE3.jpg) ConcurrentHashMap是线程安全的哈希表(相当于线程安全的HashMap)；它继承于AbstractMap类，并且实现ConcurrentMap接口。ConcurrentHashMap是通过“锁分段”来实现的，它支持并发。 ConcurrentSkipListMap是线程安全的有序的哈希表(相当于线程安全的TreeMap); 它继承于AbstractMap类，并且实现ConcurrentNavigableMap接口。ConcurrentSkipListMap是通过“跳表”来实现的，它支持并发。 ConcurrentSkipListSet是线程安全的有序的集合(相当于线程安全的TreeSet)；它继承于AbstractSet，并实现了NavigableSet接口。ConcurrentSkipListSet是通过ConcurrentSkipListMap实现的，它也支持并发。 QueueJUC集合包中Queue的实现类包括: ArrayBlockingQueue, LinkedBlockingQueue, LinkedBlockingDeque, ConcurrentLinkedQueue和ConcurrentLinkedDeque。它们的框架如下图所示：.resources/E30FE17C-4587-43C9-9DF3-7C0891408761.jpg) ArrayBlockingQueue是数组实现的线程安全的有界的阻塞队列。 LinkedBlockingQueue是单向链表实现的(指定大小)阻塞队列，该队列按 FIFO（先进先出）排序元素。 LinkedBlockingDeque是双向链表实现的(指定大小)双向并发阻塞队列，该阻塞队列同时支持FIFO和FILO两种操作方式。 ConcurrentLinkedQueue是单向链表实现的无界队列，该队列按 FIFO（先进先出）排序元素。 ConcurrentLinkedDeque是双向链表实现的无界队列，该队列同时支持FIFO和FILO两种操作方式。 接下来Here We Go ~]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>并发容器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java NIO初始Path和Files]]></title>
    <url>%2F2019%2F04%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%20NIO%E5%88%9D%E5%A7%8BPath%E5%92%8CFiles%2F</url>
    <content type="text"><![CDATA[Java NIO之拥抱Path和Files文件I/O基石：PathJava7中文件IO发生了很大的变化，专门引入了很多新的类来取代原来的基于java.io.File的文件IO操作方式: 12345678910import java.nio.file.DirectoryStream;import java.nio.file.FileSystem;import java.nio.file.FileSystems;import java.nio.file.Files;import java.nio.file.Path;import java.nio.file.Paths;import java.nio.file.attribute.FileAttribute;import java.nio.file.attribute.PosixFilePermission;import java.nio.file.attribute.PosixFilePermissions;·...... 我们将从下面几个方面来学习Path类: 创建一个Path File和Path之间的转换，File和URI之间的转换 获取Path的相关信息 移除Path中的冗余项 1 创建一个Path创建Path实例可以通过 Paths工具类 的 get（）方法：1234//使用绝对路径 Path path= Paths.get(&quot;c:\\data\\myfile.txt&quot;);//使用相对路径Path path = Paths.get(&quot;/home/jakobjenkov/myfile.txt&quot;); 下面这种创建方式和上面等效：1Path path = FileSystems.getDefault().getPath(&quot;c:\\data\\myfile.txt&quot;); 2 File和Path之间的转换，File和URI之间的转换1234File file = new File(&quot;C:/my.ini&quot;);Path p1 = file.toPath();p1.toFile();file.toURI(); 3 获取Path的相关信息 12345678910//使用Paths工具类的get()方法创建Path path = Paths.get(&quot;D:\\XMind\\bcl-java.txt&quot;);System.out.println(&quot;文件名：&quot; + path.getFileName());System.out.println(&quot;名称元素的数量：&quot; + path.getNameCount());System.out.println(&quot;父路径：&quot; + path.getParent());System.out.println(&quot;根路径：&quot; + path.getRoot());System.out.println(&quot;是否是绝对路径：&quot; + path.isAbsolute());//startsWith()方法的参数既可以是字符串也可以是Path对象System.out.println(&quot;是否是以为给定的路径D:开始：&quot; + path.startsWith(&quot;D:\\&quot;) );System.out.println(&quot;该路径的字符串形式：&quot; + path.toString()); 结果：1234567文件名：bcl-java.txt名称元素的数量：2父路径：D:\XMind根路径：D:\是否是绝对路径：true是否是以为给定的路径D:开始：true该路径的字符串形式：D:\XMind\bcl-java.txt 4 移除冗余项某些时候在我们需要处理的Path路径中可能会有一个或两个点 .表示的是当前目录 ..表示父目录或者说是上一级目录： 下面通过实例来演示一下使用Path类的normalize()和toRealPath()方法把.和..去除。 normalize() : 返回一个路径，该路径是冗余名称元素的消除。 toRealPath() : 融合了toAbsolutePath()方法和normalize()方法12345678910111213//.表示的是当前目录Path currentDir = Paths.get(&quot;.&quot;);System.out.println(currentDir.toAbsolutePath());//输出C:\Users\Administrator\NIODemo\.Path currentDir2 = Paths.get(&quot;.\\NIODemo.iml&quot;);System.out.println(&quot;原始路径格式：&quot;+currentDir2.toAbsolutePath());System.out.println(&quot;执行normalize（）方法之后：&quot;+currentDir2.toAbsolutePath().normalize());System.out.println(&quot;执行toRealPath()方法之后：&quot;+currentDir2.toRealPath());//..表示父目录或者说是上一级目录：Path currentDir3 = Paths.get(&quot;..&quot;);System.out.println(&quot;原始路径格式：&quot;+currentDir3.toAbsolutePath());System.out.println(&quot;执行normalize（）方法之后：&quot;+currentDir3.toAbsolutePath().normalize());System.out.println(&quot;执行toRealPath()方法之后：&quot;+currentDir3.toRealPath()); 结果：1234567C:\Users\Administrator\NIODemo\.原始路径格式：C:\Users\Administrator\NIODemo\.\NIODemo.iml执行normalize（）方法之后：C:\Users\Administrator\NIODemo\NIODemo.iml执行toRealPath()方法之后：C:\Users\Administrator\NIODemo\NIODemo.iml原始路径格式：C:\Users\Administrator\NIODemo\..执行normalize（）方法之后：C:\Users\Administrator执行toRealPath()方法之后：C:\Users\Administrator 拥抱Files类Java NIO中的Files类（java.nio.file.Files）提供了多种操作文件系统中文件的方法。本节教程将覆盖大部分方法。Files类包含了很多方法，所以如果本文没有提到的你也可以直接查询JavaDoc文档。java.nio.file.Files类是和java.nio.file.Path相结合使用的 1 检查给定的Path在文件系统中是否存在通过 Files.exists() 检测文件路径是否存在：12345Path path = Paths.get(&quot;D:\\XMind\\bcl-java.txt&quot;); boolean pathExists = Files.exists(path, new LinkOption[]&#123;LinkOption.NOFOLLOW_LINKS&#125;); System.out.println(pathExists);//true 注意Files.exists()的的第二个参数。它是一个数组，这个参数直接影响到Files.exists()如何确定一个路径是否存在。在本例中，这个数组内包含了LinkOptions.NOFOLLOW_LINKS，表示检测时不包含符号链接文件。 2 创建文件/文件夹 创建文件：通过 Files.createFile() 创建文件:1234567Path target2 = Paths.get(&quot;C:\\mystuff.txt&quot;);try &#123; if(!Files.exists(target2)) Files.createFile(target2);&#125; catch (IOException e) &#123; e.printStackTrace();&#125; 创建文件夹： 通过 Files.createDirectory() 创建文件夹 通过 Files.createDirectories() 创建文件夹 Files.createDirectories()会首先创建所有不存在的父目录来创建目录，而Files.createDirectory()方法只是创建目录，如果它的上级目录不存在就会报错。比如下面的程序使用Files.createDirectory() 方法创建就会报错，这是因为我的D盘下没有data文件夹，加入存在data文件夹的话则没问题。123456789Path path = Paths.get(&quot;D://data//test&quot;);try &#123; Path newDir = Files.createDirectories(path);&#125; catch(FileAlreadyExistsException e)&#123; // the directory already exists.&#125; catch (IOException e) &#123; //something else went wrong e.printStackTrace();&#125; 3 删除文件或目录通过 Files.delete()方法 可以删除一个文件或目录：12345678Path path = Paths.get(&quot;data/subdir/logging-moved.properties&quot;);try &#123; Files.delete(path);&#125; catch (IOException e) &#123; //deleting file failed e.printStackTrace();&#125; 4 把一个文件从一个地址复制到另一个位置通过Files.copy()方法可以吧一个文件从一个地址复制到另一个位置1234567891011Path sourcePath = Paths.get(&quot;data/logging.properties&quot;);Path destinationPath = Paths.get(&quot;data/logging-copy.properties&quot;);try &#123; Files.copy(sourcePath, destinationPath);&#125; catch(FileAlreadyExistsException e) &#123; //destination file already exists&#125; catch (IOException e) &#123; //something else went wrong e.printStackTrace();&#125; copy操作还可可以强制覆盖已经存在的目标文件，只需要将上面的copy()方法改为如下格式：12Files.copy(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING); 5 获取文件属性123456Path path = Paths.get(&quot;D:\\XMind\\bcl-java.txt&quot;);System.out.println(Files.getLastModifiedTime(path));System.out.println(Files.size(path));System.out.println(Files.isSymbolicLink(path));System.out.println(Files.isDirectory(path));System.out.println(Files.readAttributes(path, &quot;*&quot;)); 结果：123452016-05-18T08:01:44Z18934falsefalse&#123;lastAccessTime=2017-04-12T01:42:21.149351Z, lastModifiedTime=2016-05-18T08:01:44Z, size=18934, creationTime=2017-04-12T01:42:21.149351Z, isSymbolicLink=false, isRegularFile=true, fil 6 遍历一个文件夹12345678Path dir = Paths.get(&quot;D:\\Java&quot;);try(DirectoryStream&lt;Path&gt; stream = Files.newDirectoryStream(dir))&#123; for(Path e : stream)&#123; System.out.println(e.getFileName()); &#125;&#125;catch(IOException e)&#123;&#125; 结果：12345678910apache-maven-3.5.0Eclipseintellij ideaJarJDKMarvenRespositoryMyEclipse 2017 CINodejsRedisDesktopManagersolr-7.2.1 上面是遍历单个目录，它不会遍历整个目录。遍历整个目录需要使用：Files.walkFileTree().Files.walkFileTree()方法具有递归遍历目录的功能。 7 遍历整个文件目录：walkFileTree接受一个Path和FileVisitor作为参数。Path对象是需要遍历的目录，FileVistor则会在每次遍历中被调用。FileVisitor需要调用方自行实现，然后作为参数传入walkFileTree().FileVisitor的每个方法会在遍历过程中被调用多次。如果不需要处理每个方法，那么可以继承它的默认实现类SimpleFileVisitor，它将所有的接口做了空实现。12345678910111213141516171819202122public class WorkFileTree &#123; public static void main(String[] args) throws IOException&#123; Path startingDir = Paths.get(&quot;D:\\apache-tomcat-9.0.0.M17&quot;); List&lt;Path&gt; result = new LinkedList&lt;Path&gt;(); Files.walkFileTree(startingDir, new FindJavaVisitor(result)); System.out.println(&quot;result.size()=&quot; + result.size()); &#125; private static class FindJavaVisitor extends SimpleFileVisitor&lt;Path&gt;&#123; private List&lt;Path&gt; result; public FindJavaVisitor(List&lt;Path&gt; result)&#123; this.result = result; &#125; @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs)&#123; if(file.toString().endsWith(&quot;.java&quot;))&#123; result.add(file.getFileName()); &#125; return FileVisitResult.CONTINUE; &#125; &#125;&#125; 上面这个例子输出了我的D:\apache-tomcat-9.0.0.M17也就是我的Tomcat安装目录下以.java结尾文件的数量。结果：1result.size()=4 Files类真的很强大，除了我讲的这些操作之外还有其他很多操作比如：读取和设置文件权限、更新文件所有者等等操作。 参考文档： 官方JDK相关文档 谷歌搜索排名第一的Java NIO教程 《Java程序员修炼之道》 《Java 8编程官方参考教程（第9版）》 Java7新特性之文件操作]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>NIO</tag>
        <tag>Path和Files</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java NIO选择器(Selector)]]></title>
    <url>%2F2019%2F04%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%20NIO%E9%80%89%E6%8B%A9%E5%99%A8(Selector)%2F</url>
    <content type="text"><![CDATA[Java NIO之Selector（选择器）Selector（选择器）介绍Selector一般称为选择器,当然你也可以翻译为多路复用器。它是Java NIO核心组件中的一个,用于检查一个或多个NIO Channel（通道）的状态是否处于可读、可写。如此可以实现单线程管理多个channels,也就是可以管理多个网络链接。 .resources/C32A7750-CD6E-469A-8E9E-BEC983989522.png)使用Selector的好处在于:使用更少的线程来就可以来处理通道了,相比使用多个线程,避免了线程上下文切换带来的开销。 Selector（选择器）的使用方法介绍1. Selector的创建通过调用Selector.open()方法创建一个Selector对象，如下：1Selector selector = Selector.open(); 2. 注册Channel到Selector12channel.configureBlocking(false);SelectionKey key = channel.register(selector, Selectionkey.OP_READ); Channel必须是非阻塞的。所以FileChannel不适用Selector，因为FileChannel不能切换为非阻塞模式，更准确的来说是因为FileChannel没有继承SelectableChannel。Socket channel可以正常使用。SelectableChannel抽象类 有一个 configureBlocking（） 方法用于使通道处于阻塞模式或非阻塞模式。1abstract SelectableChannel configureBlocking(boolean block) 注意：SelectableChannel抽象类的configureBlocking（） 方法是由 AbstractSelectableChannel抽象类实现的，SocketChannel、ServerSocketChannel、DatagramChannel都是直接继承了 AbstractSelectableChannel抽象类 。大家有兴趣可以看看NIO的源码，各种抽象类和抽象类上层的抽象类。我本人暂时不准备研究NIO源码，因为还有很多事情要做，需要研究的同学可以自行看看。register() 方法的第二个参数。这是一个“ interest集合 ”，意思是在通过Selector监听Channel时对什么事件感兴趣。可以监听四种不同类型的事件： Connect Accept Read Write 通道触发了一个事件意思是该事件已经就绪。比如某个Channel成功连接到另一个服务器称为”连接就绪”。一个Server Socket Channel准备好接收新进入的连接称为”接收就绪”。一个有数据可读的通道可以说是”读就绪”。等待写数据的通道可以说是”写就绪”。这四种事件用SelectionKey的四个常量来表示：SelectionKey.OP_CONNECTSelectionKey.OP_ACCEPTSelectionKey.OP_READSelectionKey.OP_WRITE 如果你对不止一种事件感兴趣，使用或运算符即可，如下：int interestSet = SelectionKey.OP_READ | SelectionKey.OP_WRITE; 3. SelectionKey介绍一个SelectionKey键表示了一个特定的通道对象和一个特定的选择器对象之间的注册关系。12345key.attachment(); //返回SelectionKey的attachment，attachment可以在注册channel的时候指定。key.channel(); // 返回该SelectionKey对应的channel。key.selector(); // 返回该SelectionKey对应的Selector。key.interestOps(); //返回代表需要Selector监控的IO操作的bit maskkey.readyOps(); // 返回一个bit mask，代表在相应channel上可以进行的IO操作。 key.interestOps(): 我们可以通过以下方法来判断Selector是否对Channel的某种事件感兴趣12345int interestSet = selectionKey.interestOps(); boolean isInterestedInAccept = (interestSet &amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT；boolean isInterestedInConnect = interestSet &amp; SelectionKey.OP_CONNECT;boolean isInterestedInRead = interestSet &amp; SelectionKey.OP_READ;boolean isInterestedInWrite = interestSet &amp; SelectionKey.OP_WRITE; key.readyOps()ready 集合是通道已经准备就绪的操作的集合。JAVA中定义以下几个方法用来检查这些操作是否就绪.1234567//创建ready集合的方法int readySet = selectionKey.readyOps();//检查这些操作是否就绪的方法key.isAcceptable();//是否可读，是返回 trueboolean isWritable()：//是否可写，是返回 trueboolean isConnectable()：//是否可连接，是返回 trueboolean isAcceptable()：//是否可接收，是返回 true 从SelectionKey访问Channel和Selector很简单。如下：123Channel channel = key.channel();Selector selector = key.selector();key.attachment(); 可以将一个对象或者更多信息附着到SelectionKey上，这样就能方便的识别某个给定的通道。例如，可以附加 与通道一起使用的Buffer，或是包含聚集数据的某个对象。使用方法如下：12key.attach(theObject);Object attachedObj = key.attachment(); 还可以在用register()方法向Selector注册Channel的时候附加对象。如：1SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); 4. 从Selector中选择channel(Selecting Channels via a Selector)选择器维护注册过的通道的集合，并且这种注册关系都被封装在SelectionKey当中. Selector维护的三种类型SelectionKey集合： 已注册的键的集合(Registered key set) 所有与选择器关联的通道所生成的键的集合称为已经注册的键的集合。并不是所有注册过的键都仍然有效。这个集合通过 keys() 方法返回，并且可能是空的。这个已注册的键的集合不是可以直接修改的；试图这么做的话将引发java.lang.UnsupportedOperationException。 已选择的键的集合(Selected key set) 所有与选择器关联的通道所生成的键的集合称为已经注册的键的集合。并不是所有注册过的键都仍然有效。这个集合通过 keys() 方法返回，并且可能是空的。这个已注册的键的集合不是可以直接修改的；试图这么做的话将引发java.lang.UnsupportedOperationException。 已取消的键的集合(Cancelled key set) 已注册的键的集合的子集，这个集合包含了 cancel() 方法被调用过的键(这个键已经被无效化)，但它们还没有被注销。这个集合是选择器对象的私有成员，因而无法直接访问。注意：当键被取消（ 可以通过isValid( ) 方法来判断）时，它将被放在相关的选择器的已取消的键的集合里。注册不会立即被取消，但键会立即失效。当再次调用 select( ) 方法时（或者一个正在进行的select()调用结束时），已取消的键的集合中的被取消的键将被清理掉，并且相应的注销也将完成。通道会被注销，而新的SelectionKey将被返回。当通道关闭时，所有相关的键会自动取消（记住，一个通道可以被注册到多个选择器上）。当选择器关闭时，所有被注册到该选择器的通道都将被注销，并且相关的键将立即被无效化（取消）。一旦键被无效化，调用它的与选择相关的方法就将抛出CancelledKeyException。 select()方法介绍： 在刚初始化的Selector对象中，这三个集合都是空的。 通过Selector的select（）方法可以选择已经准备就绪的通道 （这些通道包含你感兴趣的的事件）。比如你对读就绪的通道感兴趣，那么select（）方法就会返回读事件已经就绪的那些通道。下面是Selector几个重载的select()方法： int select()：阻塞到至少有一个通道在你注册的事件上就绪了。 int select(long timeout)：和select()一样，但最长阻塞时间为timeout毫秒。 int selectNow()：非阻塞，只要有通道就绪就立刻返回。 select()方法返回的int值表示有多少通道已经就绪,是自上次调用select()方法后有多少通道变成就绪状态。之前在select（）调用时进入就绪的通道不会在本次调用中被记入，而在前一次select（）调用进入就绪但现在已经不在处于就绪的通道也不会被记入。例如：首次调用select()方法，如果有一个通道变成就绪状态，返回了1，若再次调用select()方法，如果另一个通道就绪了，它会再次返回1。如果对第一个就绪的channel没有做任何操作，现在就有两个就绪的通道，但在每次select()方法调用之间，只有一个通道就绪了。一旦调用select()方法，并且返回值不为0时，则 可以通过调用Selector的selectedKeys()方法来访问已选择键集合 。如下：1234567891011121314151617Set selectedKeys=selector.selectedKeys();进而可以放到和某SelectionKey关联的Selector和Channel。如下所示：Set selectedKeys = selector.selectedKeys();Iterator keyIterator = selectedKeys.iterator();while(keyIterator.hasNext()) &#123; SelectionKey key = keyIterator.next(); if(key.isAcceptable()) &#123; // a connection was accepted by a ServerSocketChannel. &#125; else if (key.isConnectable()) &#123; // a connection was established with a remote server. &#125; else if (key.isReadable()) &#123; // a channel is ready for reading &#125; else if (key.isWritable()) &#123; // a channel is ready for writing &#125; keyIterator.remove();&#125; 5. 停止选择的方法选择器执行选择的过程，系统底层会依次询问每个通道是否已经就绪，这个过程可能会造成调用线程进入阻塞状态,那么我们有以下三种方式可以唤醒在select（）方法中阻塞的线程。 wakeup()方法 ：通过调用Selector对象的wakeup（）方法让处在阻塞状态的select()方法立刻返回该方法使得选择器上的第一个还没有返回的选择操作立即返回。如果当前没有进行中的选择操作，那么下一次对select()方法的一次调用将立即返回。 close()方法 ：通过close（）方法关闭Selector该方法使得任何一个在选择操作中阻塞的线程都被唤醒(类似wakeup()),同时使得注册到该Selector的所有Channel被注销，所有的键将被取消，但是Channel本身并不会关闭。 模板代码一个服务端的模板代码：有了模板代码我们在编写程序时，大多数时间都是在模板代码中添加相应的业务代码123456789101112131415161718192021222324252627282930ServerSocketChannel ssc = ServerSocketChannel.open();ssc.socket().bind(new InetSocketAddress(&quot;localhost&quot;, 8080));ssc.configureBlocking(false);Selector selector = Selector.open();ssc.register(selector, SelectionKey.OP_ACCEPT);while(true) &#123; int readyNum = selector.select(); if (readyNum == 0) &#123; continue; &#125; Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = selectedKeys.iterator(); while(it.hasNext()) &#123; SelectionKey key = it.next(); if(key.isAcceptable()) &#123; // 接受连接 &#125; else if (key.isReadable()) &#123; // 通道可读 &#125; else if (key.isWritable()) &#123; // 通道可写 &#125; it.remove(); &#125;&#125; 客户端与服务端简单交互实例服务端：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package selector;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SelectionKey;import java.nio.channels.Selector;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;import java.util.Iterator;import java.util.Set;public class WebServer &#123; public static void main(String[] args) &#123; try &#123; ServerSocketChannel ssc = ServerSocketChannel.open(); ssc.socket().bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000)); ssc.configureBlocking(false); Selector selector = Selector.open(); // 注册 channel，并且指定感兴趣的事件是 Accept ssc.register(selector, SelectionKey.OP_ACCEPT); ByteBuffer readBuff = ByteBuffer.allocate(1024); ByteBuffer writeBuff = ByteBuffer.allocate(128); writeBuff.put(&quot;received&quot;.getBytes()); writeBuff.flip(); while (true) &#123; int nReady = selector.select(); Set&lt;SelectionKey&gt; keys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; it = keys.iterator(); while (it.hasNext()) &#123; SelectionKey key = it.next(); it.remove(); if (key.isAcceptable()) &#123; // 创建新的连接，并且把连接注册到selector上，而且， // 声明这个channel只对读操作感兴趣。 SocketChannel socketChannel = ssc.accept(); socketChannel.configureBlocking(false); socketChannel.register(selector, SelectionKey.OP_READ); &#125; else if (key.isReadable()) &#123; SocketChannel socketChannel = (SocketChannel) key.channel(); readBuff.clear(); socketChannel.read(readBuff); readBuff.flip(); System.out.println(&quot;received : &quot; + new String(readBuff.array())); key.interestOps(SelectionKey.OP_WRITE); &#125; else if (key.isWritable()) &#123; writeBuff.rewind(); SocketChannel socketChannel = (SocketChannel) key.channel(); socketChannel.write(writeBuff); key.interestOps(SelectionKey.OP_READ); &#125; &#125; &#125; &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 客户端：12345678910111213141516171819202122232425262728package selector;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SocketChannel;public class WebClient &#123; public static void main(String[] args) throws IOException &#123; try &#123; SocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 8000)); ByteBuffer writeBuffer = ByteBuffer.allocate(32); ByteBuffer readBuffer = ByteBuffer.allocate(32); writeBuffer.put(&quot;hello&quot;.getBytes()); writeBuffer.flip(); while (true) &#123; writeBuffer.rewind(); socketChannel.write(writeBuffer); readBuffer.clear(); socketChannel.read(readBuffer); &#125; &#125; catch (IOException e) &#123; &#125; &#125;&#125; 运行结果：先运行服务端，再运行客户端，服务端会不断收到客户端发送过来的消息。 .resources/3B614359-3026-4B01-938C-605FA70D1FCD.png) 参考文档： 官方JDK相关文档 谷歌搜索排名第一的Java NIO教程 《Java程序员修炼之道》 ByteBuffer常用方法详解 JavaNIO易百教程 https://www.jianshu.com/nb/18340870]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>NIO</tag>
        <tag>Selector</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java NIO通道(Channel)]]></title>
    <url>%2F2019%2F04%2F08%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%20NIO%E9%80%9A%E9%81%93(Channel)%2F</url>
    <content type="text"><![CDATA[Java NIO之Channel（通道）Buffer(缓冲区)介绍通常来说NIO中的所有IO都是从 Channel（通道） 开始的。 从通道进行数据读取 ：创建一个缓冲区，然后请求通道读取数据。 从通道进行数据写入 ：创建一个缓冲区，填充数据，并要求通道写入数据。 数据读取和写入操作图示：.resources/2958433B-EEAF-4D8B-98A2-39941C7C1733.png) Java NIO Channel通道和流非常相似，主要有以下几点区别： 通道可以读也可以写，流一般来说是单向的（只能读或者写，所以之前我们用流进行IO操作的时候需要分别创建一个输入流和一个输出流）。通道可以异步读写。通道总是基于缓冲区Buffer来读写。 Java NIO中最重要的几个Channel的实现： FileChannel： 用于文件的数据读写 DatagramChannel： 用于UDP的数据读写 SocketChannel： 用于TCP的数据读写，一般是客户端实现 ServerSocketChannel: 允许我们监听TCP链接请求，每个请求会创建会一个SocketChannel，一般是服务器实现 类层次结构：下面的UML图使用Idea生成的。.resources/3A2E73E4-2445-4B90-93F0-0EB34EB8C82B.png) FileChannel的使用使用FileChannel读取数据到Buffer（缓冲区）以及利用Buffer（缓冲区）写入数据到FileChannel：12345678910111213141516171819202122232425262728293031323334353637383940package filechannel;import java.io.IOException;import java.io.RandomAccessFile;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;public class FileChannelTxt &#123; public static void main(String args[]) throws IOException &#123; //1.创建一个RandomAccessFile（随机访问文件）对象， RandomAccessFile raf=new RandomAccessFile(&quot;D:\\niodata.txt&quot;, &quot;rw&quot;); //通过RandomAccessFile对象的getChannel()方法。FileChannel是抽象类。 FileChannel inChannel=raf.getChannel(); //2.创建一个读数据缓冲区对象 ByteBuffer buf=ByteBuffer.allocate(48); //3.从通道中读取数据 int bytesRead = inChannel.read(buf); //创建一个写数据缓冲区对象 ByteBuffer buf2=ByteBuffer.allocate(48); //写入数据 buf2.put(&quot;filechannel test&quot;.getBytes()); buf2.flip(); inChannel.write(buf); while (bytesRead != -1) &#123; System.out.println(&quot;Read &quot; + bytesRead); //Buffer有两种模式，写模式和读模式。在写模式下调用flip()之后，Buffer从写模式变成读模式。 buf.flip(); //如果还有未读内容 while (buf.hasRemaining()) &#123; System.out.print((char) buf.get()); &#125; //清空缓存区 buf.clear(); bytesRead = inChannel.read(buf); &#125; //关闭RandomAccessFile（随机访问文件）对象 raf.close(); &#125;&#125; 运行效果：.resources/0CC9E605-79FB-455E-AF3F-1CD41832B4A6.png)通过上述实例代码，我们可以大概总结出FileChannel的一般使用规则： 1. 开启FileChannel 使用之前，FileChannel必须被打开 ，但是你无法直接打开FileChannel（FileChannel是抽象类）。需要通过 InputStream ， OutputStream 或 RandomAccessFile 获取FileChannel。我们上面的例子是通过RandomAccessFile打开FileChannel的：1234//1.创建一个RandomAccessFile（随机访问文件）对象， RandomAccessFile raf=new RandomAccessFile(&quot;D:\\niodata.txt&quot;, &quot;rw&quot;); //通过RandomAccessFile对象的getChannel()方法。FileChannel是抽象类。 FileChannel inChannel=raf.getChannel(); 2. 从FileChannel读取数据/写入数据从FileChannel中读取数据/写入数据之前首先要创建一个Buffer（缓冲区）对象，Buffer（缓冲区）对象的使用我们在上一篇文章中已经详细说明了，如果不了解的话可以看我的上一篇关于Buffer的文章。 使用FileChannel的read()方法读取数据：1234//2.创建一个读数据缓冲区对象 ByteBuffer buf=ByteBuffer.allocate(48);//3.从通道中读取数据 int bytesRead = inChannel.read(buf); 使用FileChannel的write()方法写入数据：123456//创建一个写数据缓冲区对象 ByteBuffer buf2=ByteBuffer.allocate(48);//写入数据 buf2.put(&quot;filechannel test&quot;.getBytes()); buf2.flip(); inChannel.write(buf); 3. 关闭FileChannel 完成使用后，FileChannel您必须关闭它。1channel.close(); SocketChannel和ServerSocketChannel的使用利用SocketChannel和ServerSocketChannel实现客户端与服务器端简单通信：SocketChannel 用于创建基于tcp协议的客户端对象，因为SocketChannel中不存在accept()方法，所以，它不能成为一个服务端程序。通过 connect()方法 ，SocketChannel对象可以连接到其他tcp服务器程序。客户端:12345678910111213141516171819202122232425262728293031323334package socketchannel;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.SocketChannel;public class WebClient &#123; public static void main(String[] args) throws IOException &#123; //1.通过SocketChannel的open()方法创建一个SocketChannel对象 SocketChannel socketChannel = SocketChannel.open(); //2.连接到远程服务器（连接此通道的socket） socketChannel.connect(new InetSocketAddress(&quot;127.0.0.1&quot;, 3333)); // 3.创建写数据缓存区对象 ByteBuffer writeBuffer = ByteBuffer.allocate(128); writeBuffer.put(&quot;hello WebServer this is from WebClient&quot;.getBytes()); writeBuffer.flip(); socketChannel.write(writeBuffer); //创建读数据缓存区对象 ByteBuffer readBuffer = ByteBuffer.allocate(128); socketChannel.read(readBuffer); //String 字符串常量，不可变；StringBuffer 字符串变量（线程安全），可变；StringBuilder 字符串变量（非线程安全），可变 StringBuilder stringBuffer=new StringBuilder(); //4.将Buffer从写模式变为可读模式 readBuffer.flip(); while (readBuffer.hasRemaining()) &#123; stringBuffer.append((char) readBuffer.get()); &#125; System.out.println(&quot;从服务端接收到的数据：&quot;+stringBuffer); socketChannel.close(); &#125;&#125; ServerSocketChannel 允许我们监听TCP链接请求，通过ServerSocketChannelImpl的 accept()方法 可以创建一个SocketChannel对象用户从客户端读/写数据。 服务端：12345678910111213141516171819202122232425262728293031323334353637383940package socketchannel;import java.io.IOException;import java.net.InetSocketAddress;import java.nio.ByteBuffer;import java.nio.channels.ServerSocketChannel;import java.nio.channels.SocketChannel;public class WebServer &#123; public static void main(String args[]) throws IOException &#123; try &#123; //1.通过ServerSocketChannel 的open()方法创建一个ServerSocketChannel对象，open方法的作用：打开套接字通道 ServerSocketChannel ssc = ServerSocketChannel.open(); //2.通过ServerSocketChannel绑定ip地址和port(端口号) ssc.socket().bind(new InetSocketAddress(&quot;127.0.0.1&quot;, 3333)); //通过ServerSocketChannelImpl的accept()方法创建一个SocketChannel对象用户从客户端读/写数据 SocketChannel socketChannel = ssc.accept(); //3.创建写数据的缓存区对象 ByteBuffer writeBuffer = ByteBuffer.allocate(128); writeBuffer.put(&quot;hello WebClient this is from WebServer&quot;.getBytes()); writeBuffer.flip(); socketChannel.write(writeBuffer); //创建读数据的缓存区对象 ByteBuffer readBuffer = ByteBuffer.allocate(128); //读取缓存区数据 socketChannel.read(readBuffer); StringBuilder stringBuffer=new StringBuilder(); //4.将Buffer从写模式变为可读模式 readBuffer.flip(); while (readBuffer.hasRemaining()) &#123; stringBuffer.append((char) readBuffer.get()); &#125; System.out.println(&quot;从客户端接收到的数据：&quot;+stringBuffer); socketChannel.close(); ssc.close(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行效果客户端：.resources/6AF85EF7-83C7-48B6-A6AB-C70AD22A91D4.png)服务端：.resources/821A61BD-80DF-493F-99D8-4F5330211339.png)通过上述实例代码，我们可以大概总结出SocketChannel和ServerSocketChannel的使用的一般使用规则：考虑到篇幅问题，下面只给出大致步骤，不贴代码，可以结合上述实例理解。客户端1.通过SocketChannel连接到远程服务器2.创建读数据/写数据缓冲区对象来读取服务端数据或向服务端发送数据3.关闭SocketChannel服务端1.通过ServerSocketChannel 绑定ip地址和端口号2.通过ServerSocketChannelImpl的accept()方法创建一个SocketChannel对象用户从客户端读/写数据3.创建读数据/写数据缓冲区对象来读取客户端数据或向客户端发送数据 关闭SocketChannel和ServerSocketChannel DatagramChannel的使用DataGramChannel，类似于java 网络编程的DatagramSocket类；使用UDP进行网络传输， UDP是无连接，面向数据报文段的协议，对传输的数据不保证安全与完整 ；和上面介绍的SocketChannel和ServerSocketChannel的使用方法类似，所以这里就简单介绍一下如何使用。1.获取DataGramChannel1234//1.通过DatagramChannel的open()方法创建一个DatagramChannel对象 DatagramChannel datagramChannel = DatagramChannel.open(); //绑定一个port（端口） datagramChannel.bind(new InetSocketAddress(1234)); 上面代码表示程序可以在1234端口接收数据报。 2.接收/发送消息接收消息：先创建一个缓存区对象，然后通过receive方法接收消息，这个方法返回一个SocketAddress对象，表示发送消息方的地址：123ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();channel.receive(buf); 发送消息：由于UDP下，服务端和客户端通信并不需要建立连接，只需要知道对方地址即可发出消息，但是是否发送成功或者成功被接收到是没有保证的;发送消息通过send方法发出，改方法返回一个int值，表示成功发送的字节数：12345ByteBuffer buf = ByteBuffer.allocate(48);buf.clear();buf.put(&quot;datagramchannel&quot;.getBytes());buf.flip();int send = channel.send(buffer, new InetSocketAddress(&quot;localhost&quot;,1234)); 这个例子发送一串字符：“datagramchannel”到主机名为”localhost”服务器的端口1234上。 Scatter/GatherChannel 提供了一种被称为 Scatter/Gather 的新功能，也称为本地矢量 I/O。Scatter/Gather 是指在多个缓冲区上实现一个简单的 I/O 操作。正确使用 Scatter / Gather可以明显提高性能。大多数现代操作系统都支持本地矢量I/O（native vectored I/O）操作。当您在一个通道上请求一个Scatter/Gather操作时，该请求会被翻译为适当的本地调用来直接填充或抽取缓冲区，减少或避免了缓冲区拷贝和系统调用；Scatter/Gather应该使用直接的ByteBuffers以从本地I/O获取最大性能优势。Scatter/Gather功能是通道(Channel)提供的 并不是Buffer。 Scatter: 从一个Channel读取的信息分散到N个缓冲区中(Buufer). Gather: 将N个Buffer里面内容按照顺序发送到一个Channel. Scattering Reads“scattering read”是把数据从单个Channel写入到多个buffer,如下图所示：.resources/D2633F82-0A59-488A-AEC6-AB443A3125F4.png)示例代码:1234ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);ByteBuffer[] bufferArray = &#123; header, body &#125;;channel.read(bufferArray); read()方法内部会负责把数据按顺序写进传入的buffer数组内。一个buffer写满后，接着写到下一个buffer中。举个例子，假如通道中有200个字节数据，那么header会被写入128个字节数据，body会被写入72个字节数据；注意：无论是scatter还是gather操作，都是按照buffer在数组中的顺序来依次读取或写入的；Gathering Writes“gathering write”把多个buffer的数据写入到同一个channel中，下面是示意图.resources/19060EA5-78B2-49F1-A706-0C99F3BC51A5.png)示例代码：12345ByteBuffer header = ByteBuffer.allocate(128);ByteBuffer body = ByteBuffer.allocate(1024);//write data into buffersByteBuffer[] bufferArray = &#123; header, body &#125;;channel.write(bufferArray); write()方法内部会负责把数据按顺序写入到channel中。注意：并不是所有数据都写入到通道，写入的数据要根据position和limit的值来判断，只有position和limit之间的数据才会被写入；举个例子，假如以上header缓冲区中有128个字节数据，但此时position=0，limit=58；那么只有下标索引为0-57的数据才会被写入到通道中. 通道之间的数据传输在Java NIO中如果一个channel是FileChannel类型的，那么他可以直接把数据传输到另一个channel。 transferFrom(): transferFrom方法把数据从通道源传输到FileChannel transferTo(): transferTo方法把FileChannel数据传输到另一个channel 参考文档： 官方JDK相关文档 谷歌搜索排名第一的Java NIO教程 《Java程序员修炼之道》 ByteBuffer常用方法详解 JavaNIO易百教程 参考文章：《Netty官网》 https://www.jianshu.com/nb/18340870]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>NIO</tag>
        <tag>Channel</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java NIO缓冲区(Buffer)]]></title>
    <url>%2F2019%2F04%2F08%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%20NIO%E7%BC%93%E5%86%B2%E5%8C%BA(Buffer)%2F</url>
    <content type="text"><![CDATA[Java NIO之Buffer(缓冲区)Buffer(缓冲区)介绍Java NIO Buffers用于和NIO Channel交互。 我们从Channel中读取数据到buffers里，从Buffer把数据写入到Channels. Buffer本质上就是一块内存区，可以用来写入数据，并在稍后读取出来。这块内存被NIO Buffer包裹起来，对外提供一系列的读写方便开发的接口。 在Java NIO中使用的核心缓冲区如下（覆盖了通过I/O发送的基本数据类型：byte, char、short, int, long, float, double ，long）： ByteBuffer CharBuffer ShortBuffer IntBuffer FloatBuffer DoubleBuffer LongBuffer.resources/1E718F2D-CAEB-4378-8FDB-780BE9803BF5.png)利用Buffer读写数据，通常遵循四个步骤： 把数据写入buffer 调用flip 从Buffer中读取数据 调用buffer.clear()或者buffer.compact() 当写入数据到buffer中时，buffer会记录已经写入的数据大小。当需要读数据时，通过 flip() 方法把buffer从写模式调整为读模式；在读模式下，可以读取所有已经写入的数据。当读取完数据后，需要清空buffer，以满足后续写入操作。清空buffer有两种方式：调用 clear() 或 compact() 方法。clear会清空整个buffer，compact则只清空已读取的数据，未被读取的数据会被移动到buffer的开始位置，写入位置则近跟着未读数据之后。 Buffer的容量，位置，上限（Buffer Capacity, Position and Limit）Buffer缓冲区实质上就是一块内存，用于写入数据，也供后续再次读取数据。这块内存被NIO Buffer管理，并提供一系列的方法用于更简单的操作这块内存。一个Buffer有三个属性是必须掌握的，分别是： capacity容量 position位置 limit限制 position和limit的具体含义取决于当前buffer的模式。capacity在两种模式下都表示容量。下面有张示例图，描诉了读写模式下position和limit的含义： .resources/CDDCF910-B3A2-41C1-AB22-6EAFAAD9BE35.png) 容量（Capacity）作为一块内存，buffer有一个固定的大小，叫做capacit（容量）。也就是最多只能写入容量值得字节，整形等数据。一旦buffer写满了就需要清空已读数据以便下次继续写入新的数据. 位置（Position）当写入数据到Buffer的时候需要从一个确定的位置开始，默认初始化时这个位置position为0，一旦写入了数据比如一个字节，整形数据，那么position的值就会指向数据之后的一个单元，position最大可以到capacity-1. 当从Buffer读取数据时，也需要从一个确定的位置开始。buffer从写入模式变为读取模式时，position会归零，每次读取后，position向后移动。 上限（Limit）在写模式，limit的含义是我们所能写入的最大数据量，它等同于buffer的容量。 一旦切换到读模式，limit则代表我们所能读取的最大数据量，他的值等同于写模式下position的位置。换句话说，您可以读取与写入数量相同的字节数（限制设置为写入的字节数，由位置标记） Buffer的常见方法.resources/03F3F860-14A4-4D45-A998-313304B775E1.png) Buffer的使用方式/方法介绍分配缓冲区（Allocating a Buffer） 为了获得缓冲区对象，我们必须首先分配一个缓冲区。在每个Buffer类中，allocate()方法用于分配缓冲区。下面来看看ByteBuffer分配容量为28字节的例子:1ByteBuffer buf = ByteBuffer.allocate(28); 下面来看看另一个示例：CharBuffer分配空间大小为2048个字符1CharBuffer buf = CharBuffer.allocate(2048); 写入数据到缓冲区（Writing Data to a Buffer） 写数据到Buffer有两种方法： 从Channel中写数据到Buffer手动写数据到Buffer，调用put方法下面是一个实例，演示从Channel写数据到Buffer：1int bytesRead = inChannel.read(buf); //read into buffer. 通过put写数据：1buf.put(127); put方法有很多不同版本，对应不同的写数据方法。例如把数据写到特定的位置，或者把一个字节数据写入buffer。看考JavaDoc文档可以查阅的更多数据。 翻转(flip()) flip()方法可以吧Buffer从写模式切换到读模式。调用flip方法会把position归零，并设置limit为之前的position的值。 也就是说，现在position代表的是读取位置，limit标示的是已写入的数据位置。 从Buffer读取数据（Reading Data from a Buffer） 从Buffer读数据也有两种方式 从buffer读数据到channel 从buffer直接读取数据，调用get方法 读取数据到channel的例子：1int bytesWritten = inChannel.write(buf); 调用get读取数据的例子：1byte aByte = buf.get(); get也有诸多版本，对应了不同的读取方式。 rewind() Buffer.rewind()方法将position置为0，这样我们可以重复读取buffer中的数据。limit保持不变。 clear() and compact() 一旦我们从buffer中读取完数据，需要复用buffer为下次写数据做准备。只需要调用clear（）或compact（）方法。如果调用的是clear()方法，position将被设回0，limit被设置成 capacity的值。换句话说，Buffer 被清空了。Buffer中的数据并未清除，只是这些标记告诉我们可以从哪里开始往Buffer里写数据。如果Buffer还有一些数据没有读取完，调用clear就会导致这部分数据被“遗忘”，因为我们没有标记这部分数据未读。针对这种情况，如果需要保留未读数据，那么可以使用compact。 因此 compact() 和 clear() 的区别就在于: 对未读数据的处理，是保留这部分数据还是一起清空 。 mark()与reset()方法 通过调用Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用Buffer.reset()方法恢复到这个position。例如：123buffer.mark();//call buffer.get() a couple of times, e.g. during parsing.buffer.reset(); //set position back to mark. equals() and compareTo() 可以用eqauls和compareTo比较两个bufferequals(): 判断两个buffer相对，需满足： 类型相同 buffer中剩余字节数相同 所有剩余字节相等 从上面的三个条件可以看出，equals只比较buffer中的部分内容，并不会去比较每一个元素。compareTo():compareTo也是比较buffer中的剩余元素，只不过这个方法适用于比较排序的： Buffer常用方法测试这里以ByteBuffer为例子说明抽象类Buffer的实现类的一些常见方法的使用：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687package channel;import java.nio.ByteBuffer;public class ByteBufferMethods &#123; public static void main(String args[])&#123; //分配缓冲区（Allocating a Buffer） ByteBuffer buffer = ByteBuffer.allocate(33); System.out.println(&quot;-------------Test reset-------------&quot;); //clear()方法，position将被设回0，limit被设置成 capacity的值 buffer.clear(); // 设置这个缓冲区的位置 buffer.position(5); //将此缓冲区的标记设置在其位置。没有buffer.mark();这句话会报错 buffer.mark(); buffer.position(10); System.out.println(&quot;before reset: &quot; + buffer); //将此缓冲区的位置重置为先前标记的位置。（buffer.position(5)） buffer.reset(); System.out.println(&quot;after reset: &quot; + buffer); System.out.println(&quot;-------------Test rewind-------------&quot;); buffer.clear(); buffer.position(10); //返回此缓冲区的限制。 buffer.limit(15); System.out.println(&quot;before rewind: &quot; + buffer); //把position设为0，mark设为-1，不改变limit的值 buffer.rewind(); System.out.println(&quot;before rewind: &quot; + buffer); System.out.println(&quot;-------------Test compact-------------&quot;); buffer.clear(); buffer.put(&quot;abcd&quot;.getBytes()); System.out.println(&quot;before compact: &quot; + buffer); System.out.println(new String(buffer.array())); //limit = position;position = 0;mark = -1; 翻转，也就是让flip之后的position到limit这块区域变成之前的0到position这块， //翻转就是将一个处于存数据状态的缓冲区变为一个处于准备取数据的状态 buffer.flip(); System.out.println(&quot;after flip: &quot; + buffer); //get()方法：相对读，从position位置读取一个byte，并将position+1，为下次读写作准备 System.out.println((char) buffer.get()); System.out.println((char) buffer.get()); System.out.println((char) buffer.get()); System.out.println(&quot;after three gets: &quot; + buffer); System.out.println(&quot;\t&quot; + new String(buffer.array())); //把从position到limit中的内容移到0到limit-position的区域内，position和limit的取值也分别变成limit-position、capacity。 // 如果先将positon设置到limit，再compact，那么相当于clear() buffer.compact(); System.out.println(&quot;after compact: &quot; + buffer); System.out.println(&quot;\t&quot; + new String(buffer.array())); System.out.println(&quot;-------------Test get-------------&quot;); buffer = ByteBuffer.allocate(32); buffer.put((byte) &apos;a&apos;).put((byte) &apos;b&apos;).put((byte) &apos;c&apos;).put((byte) &apos;d&apos;) .put((byte) &apos;e&apos;).put((byte) &apos;f&apos;); System.out.println(&quot;before flip(): &quot; + buffer); // 转换为读取模式 buffer.flip(); System.out.println(&quot;before get(): &quot; + buffer); System.out.println((char) buffer.get()); System.out.println(&quot;after get(): &quot; + buffer); // get(index)不影响position的值 System.out.println((char) buffer.get(2)); System.out.println(&quot;after get(index): &quot; + buffer); byte[] dst = new byte[10]; buffer.get(dst, 0, 2); System.out.println(&quot;after get(dst, 0, 2): &quot; + buffer); System.out.println(&quot;\t dst:&quot; + new String(dst)); System.out.println(&quot;buffer now is: &quot; + buffer); System.out.println(&quot;\t&quot; + new String(buffer.array())); System.out.println(&quot;-------------Test put-------------&quot;); ByteBuffer bb = ByteBuffer.allocate(32); System.out.println(&quot;before put(byte): &quot; + bb); System.out.println(&quot;after put(byte): &quot; + bb.put((byte) &apos;z&apos;)); System.out.println(&quot;\t&quot; + bb.put(2, (byte) &apos;c&apos;)); // put(2,(byte) &apos;c&apos;)不改变position的位置 System.out.println(&quot;after put(2,(byte) &apos;c&apos;): &quot; + bb); System.out.println(&quot;\t&quot; + new String(bb.array())); // 这里的buffer是 abcdef[pos=3 lim=6 cap=32] bb.put(buffer); System.out.println(&quot;after put(buffer): &quot; + bb); System.out.println(&quot;\t&quot; + new String(bb.array())); &#125;&#125; 参考文档： 官方JDK相关文档 谷歌搜索排名第一的Java NIO教程 《Java程序员修炼之道》 ByteBuffer常用方法详解 JavaNIO易百教程 参考文章：《Netty官网》 https://www.jianshu.com/nb/18340870]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>NIO</tag>
        <tag>Buffer</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java NIO概览]]></title>
    <url>%2F2019%2F04%2F08%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%20NIO%E6%A6%82%E8%A7%88%2F</url>
    <content type="text"><![CDATA[**Java高级特性增强-NIONIO概览从Java IO入手先看一张网上流传的http://java.io包的类结构图： 当你看到这幅图的时候，我相信，你跟我一样内心是崩溃的。有些人不怕枯燥，不怕寂寞，硬着头皮看源码，但是，能坚持下去全部看完的又有几个呢！然而，就算源码全部看完看懂，过不了几天，脑子里也会变成一团浆糊。因为这里的类实在太多了。可能我们反复看，反复记，也很难做到清晰明白。他就像是一块超级硬的骨头，怎么啃都啃不烂。面对这样的做法，要坚决对他说，NO。 我的做法是找出他们的共性，给他们分类，只记典型，触类旁通。上面的图虽然有分类，但是还不够细，而且没有总结出方便记忆的规律，所以我们要重新整理和归类。这篇文章中，使用了两种分时给他们分组，目的是更全面的了解共性，帮助记忆。 分类一：按操作方式（类结构）字节流和字符流:字节流：以字节为单位，每次次读入或读出是8位数据。可以读任何类型数据。字符流：以字符为单位，每次次读入或读出是16位数据。其只能读取字符类型数据。输出流和输入流:输出流：从内存读出到文件。只能进行写操作。输入流：从文件读入到内存。只能进行读操作。注意： 这里的出和入，都是相对于系统内存而言的。节点流和处理流:节点流：直接与数据源相连，读入或读出。处理流：与节点流一块使用，在节点流的基础上，再套接一层，套接在节点流上的就是处理流。为什么要有处理流？直接使用节点流，读写不方便，为了更快的读写文件，才有了处理流。根据以上分类，以及jdk的说明，我们可以画出更详细的类结构图，如下:分类说明：1） 输入字节流InputStream: ByteArrayInputStream、StringBufferInputStream、FileInputStream 是三种基本的介质流，它们分别从Byte 数组、StringBuffer、和本地文件中读取数据。 PipedInputStream 是从与其它线程共用的管道中读取数据。PipedInputStream的一个实例要和PipedOutputStream的一个实例共同使用，共同完成管道的读取写入操作。主要用于线程操作。 DataInputStream： 将基础数据类型读取出来 ObjectInputStream 和所有 FilterInputStream 的子类都是装饰流（装饰器模式的主角）。 2）输出字节流OutputStream: ByteArrayOutputStream、FileOutputStream： 是两种基本的介质流，它们分别向- Byte 数组、和本地文件中写入数据。 PipedOutputStream 是向与其它线程共用的管道中写入数据。 DataOutputStream 将基础数据类型写入到文件中 ObjectOutputStream 和所有 FilterOutputStream 的子类都是装饰流。 节流的输入和输出类结构图：3）字符输入流Reader：： FileReader、CharReader、StringReader 是三种基本的介质流，它们分在本地文件、Char 数组、String中读取数据。 PipedReader：是从与其它线程共用的管道中读取数据 BufferedReader ：加缓冲功能，避免频繁读写硬盘 InputStreamReader： 是一个连接字节流和字符流的桥梁，它将字节流转变为字符流。 4）字符输出流Writer： StringWriter:向String 中写入数据。 CharArrayWriter：实现一个可用作字符输入流的字符缓冲区 PipedWriter:是向与其它线程共用的管道中写入数据 BufferedWriter ： 增加缓冲功能，避免频繁读写硬盘。 PrintWriter 和PrintStream 将对象的格式表示打印到文本输出流。 极其类似，功能和使用也非常相似 OutputStreamWriter： 是OutputStream 到Writer 转换的桥梁，它的子类FileWriter 其实就是一个实现此功能的具体类（具体可以研究一SourceCode）。功能和使用和OutputStream 极其类似，后面会有它们的对应图。 字符流的输入和输出类结构图： 分类二：按操作对象分类说明：对文件进行操作（节点流）： FileInputStream（字节输入流） FileOutputStream（字节输出流） FileReader（字符输入流） FileWriter（字符输出流） 对管道进行操作（节点流）： PipedInputStream（字节输入流） PipedOutStream（字节输出流） PipedReader（字符输入流） PipedWriter（字符输出流） PipedInputStream的一个实例要和PipedOutputStream的一个实例共同使用，共同完成管道的读取写入操作。主要用于线程操作。 字节/字符数组流（节点流）： ByteArrayInputStream ByteArrayOutputStream CharArrayReader CharArrayWriter 除了上述三种是节点流，其他都是处理流，需要跟节点流配合使用。 Buffered缓冲流（处理流）：带缓冲区的处理流，缓冲区的作用的主要目的是：避免每次和硬盘打交道，提高数据访问的效率。 BufferedInputStream BufferedOutputStream BufferedReader BufferedWriter 转化流（处理流）： InputStreamReader：把字节转化成字符； OutputStreamWriter：把字节转化成字符。 基本类型数据流（处理流）：用于操作基本数据类型值。因为平时若是我们输出一个8个字节的long类型或4个字节的float类型，那怎么办呢？可以一个字节一个字节输出，也可以把转换成字符串输出，但是这样转换费时间，若是直接输出该多好啊，因此这个数据流就解决了我们输出数据类型的困难。数据流可以直接输出float类型或long类型，提高了数据读写的效率。 DataInputStream DataOutputStream 打印流（处理流）： 一般是打印到控制台，可以进行控制打印的地方。 PrintStream PrintWriter 对象流（处理流）： 把封装的对象直接输出，而不是一个个在转换成字符串再输出。 ObjectInputStream，对象反序列化 ObjectOutputStream，对象序列化 合并流（处理流）： SequenceInputStream：可以认为是一个工具类，将两个或者多个输入流当成一个输入流依次读取 其他类：FileFile类是对文件系统中文件以及文件夹进行封装的对象，可以通过对象的思想来操作文件和文件夹。 File类保存文件或目录的各种元数据信息，包括文件名、文件长度、最后修改时间、是否可读、获取当前文件的路径名，判断指定文件是否存在、获得当前目录中的文件列表，创建、删除文件和目录等方法。 其他类：RandomAccessFile该对象并不是流体系中的一员，其封装了字节流，同时还封装了一个缓冲区（字符数组），通过内部的指针来操作字符数组中的数据。 该对象特点：该对象只能操作文件，所以构造函数接收两种类型的参数：a.字符串文件路径；b.File对象。该对象既可以对文件进行读操作，也能进行写操作，在进行对象实例化时可指定操作模式(r,rw)。注意:IO中的很多内容都可以使用NIO完成，这些知识点大家知道就好，使用的话还是尽量使用NIO/AIO。 参考文章：《Netty官网》 https://www.jianshu.com/nb/18340870]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>NIO</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(LinkedHashMap)]]></title>
    <url>%2F2019%2F04%2F06%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(LinkedHashMap)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-集合框架(LinkedHashMap)LinkedHashMap底层分析众所周知HashMap是一个无序的Map,因为每次根据key的hashcode映射到Entry数组上,所以遍历出来的顺序并不是写入的顺序。 因此JDK推出一个基于HashMap但具有顺序的LinkedHashMap来解决有排序需求的场景。它的底层是继承于HashMap实现的,由一个双向链表所构成。LinkedHashMap的排序方式有两种：根据写入顺序排序。根据访问顺序排序。其中根据访问顺序排序时,每次get都会将访问的值移动到链表末尾,这样重复操作就能得到一个按照访问顺序排序的链表。 数据结构1234567891011@Test public void test()&#123; Map&lt;String, Integer&gt; map = new LinkedHashMap&lt;String, Integer&gt;(); map.put(&quot;1&quot;,1) ; map.put(&quot;2&quot;,2) ; map.put(&quot;3&quot;,3) ; map.put(&quot;4&quot;,4) ; map.put(&quot;5&quot;,5) ; System.out.println(map.toString()); &#125; 调试可以看到 map 的组成：.resources/2BC36CA6-D029-4249-A984-86F29FE10381.jpg)打开源码可以看到：123456789101112131415161718192021/** * The head of the doubly linked list. */private transient Entry&lt;K,V&gt; header;/** * The iteration ordering method for this linked hash map: &lt;tt&gt;true&lt;/tt&gt; * for access-order, &lt;tt&gt;false&lt;/tt&gt; for insertion-order. * * @serial */private final boolean accessOrder;private static class Entry&lt;K,V&gt; extends HashMap.Entry&lt;K,V&gt; &#123; // These fields comprise the doubly linked list used for iteration. Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, HashMap.Entry&lt;K,V&gt; next) &#123; super(hash, key, value, next); &#125;&#125; 其中 Entry 继承于 HashMap 的 Entry，并新增了上下节点的指针，也就形成了双向链表。还有一个 header 的成员变量，是这个双向链表的头结点。上边的 demo 总结成一张图如下：.resources/A9332FA3-2758-40CD-95DC-2A2BAC724F73.jpg)第一个类似于 HashMap 的结构，利用 Entry 中的 next 指针进行关联。 下边则是 LinkedHashMap 如何达到有序的关键。 就是利用了头节点和其余的各个节点之间通过 Entry 中的 after 和 before 指针进行关联。 其中还有一个 accessOrder 成员变量，默认是 false，默认按照插入顺序排序，为 true 时按照访问顺序排序，也可以调用:123456public LinkedHashMap(int initialCapacity, float loadFactor, boolean accessOrder) &#123; super(initialCapacity, loadFactor); this.accessOrder = accessOrder;&#125; 这个构造方法可以显示的传入 accessOrder。 构造方法LinkedHashMap 的构造方法:1234 public LinkedHashMap() &#123; super(); accessOrder = false;&#125; 其实就是调用的 HashMap 的构造方法:HashMap 实现:123456789101112131415public HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; threshold = initialCapacity; //HashMap 只是定义了改方法，具体实现交给了 LinkedHashMap init(); &#125; 可以看到里面有一个空的 init(), 具体是由 LinkedHashMap 来实现的:12345@Override void init() &#123; header = new Entry&lt;&gt;(-1, null, null, null); header.before = header.after = header; &#125; 其实也就是对 header 进行了初始化。 put() 方法看 LinkedHashMap 的 put() 方法之前先看看 HashMap 的 put 方法:123456789101112131415161718192021222324252627282930313233343536373839404142public V put(K key, V value) &#123; if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; //空实现，交给 LinkedHashMap 自己实现 e.recordAccess(this); return oldValue; &#125; &#125; modCount++; // LinkedHashMap 对其重写 addEntry(hash, key, value, i); return null; &#125; // LinkedHashMap 对其重写 void addEntry(int hash, K key, V value, int bucketIndex) &#123; if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex); &#125; // LinkedHashMap 对其重写 void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++; &#125; 主体的实现都是借助于 HashMap 来完成的，只是对其中的 recordAccess(), addEntry(), createEntry() 进行了重写。LinkedHashMap 的实现：1234567891011121314151617181920212223242526272829303132333435363738//就是判断是否是根据访问顺序排序，如果是则需要将当前这个 Entry 移动到链表的末尾 void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; if (lm.accessOrder) &#123; lm.modCount++; remove(); addBefore(lm.header); &#125; &#125; //调用了 HashMap 的实现，并判断是否需要删除最少使用的 Entry(默认不删除) void addEntry(int hash, K key, V value, int bucketIndex) &#123; super.addEntry(hash, key, value, bucketIndex); // Remove eldest entry if instructed Entry&lt;K,V&gt; eldest = header.after; if (removeEldestEntry(eldest)) &#123; removeEntryForKey(eldest.key); &#125; &#125; void createEntry(int hash, K key, V value, int bucketIndex) &#123; HashMap.Entry&lt;K,V&gt; old = table[bucketIndex]; Entry&lt;K,V&gt; e = new Entry&lt;&gt;(hash, key, value, old); //就多了这一步，将新增的 Entry 加入到 header 双向链表中 table[bucketIndex] = e; e.addBefore(header); size++; &#125; //写入到双向链表中 private void addBefore(Entry&lt;K,V&gt; existingEntry) &#123; after = existingEntry; before = existingEntry.before; before.after = this; after.before = this; &#125; get方法LinkedHashMap 的 get() 方法也重写了:123456789101112131415161718192021public V get(Object key) &#123; Entry&lt;K,V&gt; e = (Entry&lt;K,V&gt;)getEntry(key); if (e == null) return null; //多了一个判断是否是按照访问顺序排序，是则将当前的 Entry 移动到链表头部。 e.recordAccess(this); return e.value; &#125; void recordAccess(HashMap&lt;K,V&gt; m) &#123; LinkedHashMap&lt;K,V&gt; lm = (LinkedHashMap&lt;K,V&gt;)m; if (lm.accessOrder) &#123; lm.modCount++; //删除 remove(); //添加到头部 addBefore(lm.header); &#125; &#125; clear() 清空就要比较简单了：12345//只需要把指针都指向自己即可，原本那些 Entry 没有引用之后就会被 JVM 自动回收。 public void clear() &#123; super.clear(); header.before = header.after = header; &#125; 总的来说 LinkedHashMap 其实就是对 HashMap 进行了拓展，使用了双向链表来保证了顺序性。因为是继承与 HashMap 的，所以一些 HashMap 存在的问题 LinkedHashMap 也会存在，比如不支持并发等。 参考书籍：《Effective Java》https://www.jianshu.com/p/eeffc764f231https://www.jianshu.com/p/83648fa22c4chttps://crossoverjie.top/JCSprout/#/collections/LinkedHashMap]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>LinkedHashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(HashSet)]]></title>
    <url>%2F2019%2F04%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(HashSet)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-集合框架(HashSet)HashSet简介HashSet 是一个不允许存储重复元素的集合，它的实现比较简单，只要理解了 HashMap，HashSet就水到渠成了。 .resources/8C932B6E-3C26-40E7-B797-EAAE2194E5BF.jpg)从图中可以看出： HashSet继承于AbstractSet，并且实现了Set接口。 HashSet的本质是一个”没有重复元素”的集合，它是通过HashMap实现的。HashSet中含有一个”HashMap类型的成员变量”map，HashSet的操作函数，实际上都是通过map实现的。 成员变量首先了解下 HashSet 的成员变量:1234private transient HashMap&lt;E,Object&gt; map; // Dummy value to associate with an Object in the backing Map private static final Object PRESENT = new Object(); 发现主要就两个变量: map: 用于存放最终数据的。PRESENT: 是所有写入 map 的 value 值。 构造函数1234567public HashSet() &#123; map = new HashMap&lt;&gt;();&#125;public HashSet(int initialCapacity, float loadFactor) &#123; map = new HashMap&lt;&gt;(initialCapacity, loadFactor);&#125; 构造函数很简单，利用了HashMap初始化了map。 add123public boolean add(E e) &#123; return map.put(e, PRESENT)==null; &#125; 比较关键的就是这个add()方法。可以看出它是将存放的对象当做了HashMap 的健,value都是相同的PRESENT。由于HashMap的key是不能重复的,所以每当有重复的值写入到HashSet时,value会被覆盖,但key不会受到影响,这样就保证了HashSet中只能存放不重复的元素。HashSet的原理比较简单,几乎全部借助于HashMap来实现的。 参考书籍：《Effective Java》]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>HashSet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(HashMap)]]></title>
    <url>%2F2019%2F04%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(HashMap)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-集合框架(HashMap)HashMap简介HashMap 主要用来存放键值对，它基于哈希表的Map接口实现，是常用的Java集合之一。JDK1.8 之前 HashMap 由 数组+链表 组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在的（“拉链法”解决冲突）.JDK1.8 以后在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为 8）时，将链表转化为红黑树，以减少搜索时间。 底层数据结构分析JDK1.8 之前 HashMap 底层是 数组和链表 结合在一起使用也就是 链表散列。HashMap 通过 key 的 hashCode 经过扰动函数处理过后得到 hash 值，然后通过 (n - 1) &amp; hash 判断当前元素存放的位置（这里的 n 指的是数组的长度），如果当前位置存在元素的话，就判断该元素与要存入的元素的 hash 值以及 key 是否相同，如果相同的话，直接覆盖，不相同就通过拉链法解决冲突。所谓扰动函数指的就是 HashMap 的 hash 方法。使用 hash 方法也就是扰动函数是为了防止一些实现比较差的 hashCode() 方法 换句话说使用扰动函数之后可以减少碰撞。JDK 1.8 HashMap 的 hash 方法源码:JDK 1.8 的 hash方法 相比于 JDK 1.7 hash 方法更加简化，但是原理不变。1234567static final int hash(Object key) &#123; int h; // key.hashCode()：返回散列值也就是hashcode // ^ ：按位异或 // &gt;&gt;&gt;:无符号右移，忽略符号位，空位都以0补齐 return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); &#125; 对比一下 JDK1.7的 HashMap 的 hash 方法源码.12345678static int hash(int h) &#123; // This function ensures that hashCodes that differ only by // constant multiples at each bit position have a bounded // number of collisions (approximately 8 at default load factor). h ^= (h &gt;&gt;&gt; 20) ^ (h &gt;&gt;&gt; 12); return h ^ (h &gt;&gt;&gt; 7) ^ (h &gt;&gt;&gt; 4);&#125; 相比于 JDK1.8 的 hash 方法 ，JDK 1.7 的 hash 方法的性能会稍差一点点，因为毕竟扰动了 4 次。 所谓 “拉链法” 就是：将链表和数组相结合。也就是说创建一个链表数组，数组中每一格就是一个链表。若遇到哈希冲突，则将冲突的值加到链表中即可。 .resources/6EBF1755-C6EA-48A3-A99F-55D598EDDFD2.png)JDK1.8之后相比于之前的版本，jdk1.8在解决哈希冲突时有了较大的变化，当链表长度大于阈值（默认为8）时，将链表转化为红黑树，以减少搜索时间。.resources/435C0F08-CE65-413F-8D3A-EE5B20EDCA0D.jpg) 类的属性：12345678910111213141516171819202122232425262728public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; // 序列号 private static final long serialVersionUID = 362498820763181265L; // 默认的初始容量是16 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // 最大容量 static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 默认的填充因子 static final float DEFAULT_LOAD_FACTOR = 0.75f; // 当桶(bucket)上的结点数大于这个值时会转成红黑树 static final int TREEIFY_THRESHOLD = 8; // 当桶(bucket)上的结点数小于这个值时树转链表 static final int UNTREEIFY_THRESHOLD = 6; // 桶中结构转化为红黑树对应的table的最小大小 static final int MIN_TREEIFY_CAPACITY = 64; // 存储元素的数组，总是2的幂次倍 transient Node&lt;k,v&gt;[] table; // 存放具体元素的集 transient Set&lt;map.entry&lt;k,v&gt;&gt; entrySet; // 存放元素的个数，注意这个不等于数组的长度。 transient int size; // 每次扩容和更改map结构的计数器 transient int modCount; // 临界值 当实际大小(容量*填充因子)超过临界值时，会进行扩容 int threshold; // 填充因子 final float loadFactor;&#125; loadFactor加载因子 loadFactor加载因子是控制数组存放数据的疏密程度，loadFactor越趋近于1，那么 数组中存放的数据(entry)也就越多，也就越密，也就是会让链表的长度增加，load Factor越小，也就是趋近于0， loadFactor太大导致查找元素效率低，太小导致数组的利用率低，存放的数据会很分散。loadFactor的默认值为0.75f是官方给出的一个比较好的临界值。 给定的默认容量为 16，负载因子为 0.75。Map 在使用过程中不断的往里面存放数据，当数量达到了 16 * 0.75 = 12 就需要将当前 16 的容量进行扩容，而扩容这个过程涉及到 rehash、复制数据等操作，所以非常消耗性能。 threshold threshold = capacity * loadFactor，当Size&gt;=threshold的时候，那么就要考虑对数组的扩增了，也就是说，这个的意思就是 衡量数组是否需要扩增的一个标准。 Node节点类源码:123456789101112131415161718192021222324252627282930313233343536373839// 继承自 Map.Entry&lt;K,V&gt;static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash;// 哈希值，存放元素到hashmap中时用来与其他元素hash值比较 final K key;//键 V value;//值 // 指向下一个节点 Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + &quot;=&quot; + value; &#125; // 重写hashCode()方法 public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 重写 equals() 方法 public final boolean equals(Object o) &#123; if (o == this) return true; if (o instanceof Map.Entry) &#123; Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 树节点类源码:12345678910111213141516static final class TreeNode&lt;K,V&gt; extends LinkedHashMap.Entry&lt;K,V&gt; &#123; TreeNode&lt;K,V&gt; parent; // 父 TreeNode&lt;K,V&gt; left; // 左 TreeNode&lt;K,V&gt; right; // 右 TreeNode&lt;K,V&gt; prev; // needed to unlink next upon deletion boolean red; // 判断颜色 TreeNode(int hash, K key, V val, Node&lt;K,V&gt; next) &#123; super(hash, key, val, next); &#125; // 返回根节点 final TreeNode&lt;K,V&gt; root() &#123; for (TreeNode&lt;K,V&gt; r = this, p;;) &#123; if ((p = r.parent) == null) return r; r = p; &#125; HashMap源码分析构造方法.resources/D02AB301-FD17-40DF-9B99-7B6C6911D4F9.jpg) 123456789101112131415161718192021222324252627// 默认构造函数。 public More ...HashMap() &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; // all other fields defaulted &#125; // 包含另一个“Map”的构造函数 public More ...HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this.loadFactor = DEFAULT_LOAD_FACTOR; putMapEntries(m, false);//下面会分析到这个方法 &#125; // 指定“容量大小”的构造函数 public More ...HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR); &#125; // 指定“容量大小”和“加载因子”的构造函数 public More ...HashMap(int initialCapacity, float loadFactor) &#123; if (initialCapacity &lt; 0) throw new IllegalArgumentException(&quot;Illegal initial capacity: &quot; + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException(&quot;Illegal load factor: &quot; + loadFactor); this.loadFactor = loadFactor; this.threshold = tableSizeFor(initialCapacity); &#125; putMapEntries方法：123456789101112131415161718192021222324final void putMapEntries(Map&lt;? extends K, ? extends V&gt; m, boolean evict) &#123; int s = m.size(); if (s &gt; 0) &#123; // 判断table是否已经初始化 if (table == null) &#123; // pre-size // 未初始化，s为m的实际元素个数 float ft = ((float)s / loadFactor) + 1.0F; int t = ((ft &lt; (float)MAXIMUM_CAPACITY) ? (int)ft : MAXIMUM_CAPACITY); // 计算得到的t大于阈值，则初始化阈值 if (t &gt; threshold) threshold = tableSizeFor(t); &#125; // 已初始化，并且m元素个数大于阈值，进行扩容处理 else if (s &gt; threshold) resize(); // 将m中的所有元素添加至HashMap中 for (Map.Entry&lt;? extends K, ? extends V&gt; e : m.entrySet()) &#123; K key = e.getKey(); V value = e.getValue(); putVal(hash(key), key, value, false, evict); &#125; &#125;&#125; put方法HashMap只提供了put用于添加元素，putVal方法只是给put方法调用的一个方法，并没有提供给用户使用。 对putVal方法添加元素的分析如下： ①如果定位到的数组位置没有元素 就直接插入。②如果定位到的数组位置有元素就和要插入的key比较，如果key相同就直接覆盖，如果key不相同，就判断p是否是一个树节点，如果是就调用e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value)将元素添加进入。如果不是就遍历链表插入。.resources/97F816CA-9593-4F26-A189-38BEEF2FEE24.png) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125;final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // table未初始化或者长度为0，进行扩容 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (n - 1) &amp; hash 确定元素存放在哪个桶中，桶为空，新生成结点放入桶中(此时，这个结点是放在数组中) if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); // 桶中已经存在元素 else &#123; Node&lt;K,V&gt; e; K k; // 比较桶中第一个元素(数组中的结点)的hash值相等，key相等 if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) // 将第一个元素赋值给e，用e来记录 e = p; // hash值不相等，即key不相等；为红黑树结点 else if (p instanceof TreeNode) // 放入树中 e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 为链表结点 else &#123; // 在链表最末插入结点 for (int binCount = 0; ; ++binCount) &#123; // 到达链表的尾部 if ((e = p.next) == null) &#123; // 在尾部插入新结点 p.next = newNode(hash, key, value, null); // 结点数量达到阈值，转化为红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); // 跳出循环 break; &#125; // 判断链表中结点的key值与插入的元素的key值是否相等 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) // 相等，跳出循环 break; // 用于遍历桶中的链表，与前面的e = p.next组合，可以遍历链表 p = e; &#125; &#125; // 表示在桶中找到key值、hash值与插入元素相等的结点 if (e != null) &#123; // 记录e的value V oldValue = e.value; // onlyIfAbsent为false或者旧值为null if (!onlyIfAbsent || oldValue == null) //用新值替换旧值 e.value = value; // 访问后回调 afterNodeAccess(e); // 返回旧值 return oldValue; &#125; &#125; // 结构性修改 ++modCount; // 实际大小大于阈值则扩容 if (++size &gt; threshold) resize(); // 插入后回调 afterNodeInsertion(evict); return null;&#125; 我们再来对比一下 JDK1.7 put方法的代码 对于put方法的分析如下： ①如果定位到的数组位置没有元素 就直接插入。②如果定位到的数组位置有元素，遍历以这个元素为头结点的链表，依次和插入的key比较，如果key相同就直接覆盖，不同就采用头插法插入元素。 12345678910111213141516171819202122public V put(K key, V value) if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; if (key == null) return putForNullKey(value); int hash = hash(key); int i = indexFor(hash, table.length); for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; // 先遍历 Object k; if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; modCount++; addEntry(hash, key, value, i); // 再插入 return null;&#125; get方法12345678910111213141516171819202122232425262728public V get(Object key) &#123; Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;&#125;final Node&lt;K,V&gt; getNode(int hash, Object key) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) &#123; // 数组元素相等 if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; // 桶中不止一个节点 if ((e = first.next) != null) &#123; // 在树中get if (first instanceof TreeNode) return ((TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); // 在链表中get do &#123; if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; while ((e = e.next) != null); &#125; &#125; return null;&#125; resize方法进行扩容，会伴随着一次重新hash分配，并且会遍历hash表中所有的元素，是非常耗时的。在编写程序中，要尽量避免resize。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充了，就只好随你碰撞去吧 if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 没超过最大值，就扩充为原来的2倍 else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; // double threshold &#125; else if (oldThr &gt; 0) // initial capacity was placed in threshold newCap = oldThr; else &#123; signifies using defaults newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 计算新的resize上限 if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; threshold = newThr; @SuppressWarnings(&#123;&quot;rawtypes&quot;,&quot;unchecked&quot;&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; if (oldTab != null) &#123; // 把每个bucket都移动到新的buckets中 for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; oldTab[j] = null; if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; Node&lt;K,V&gt; loHead = null, loTail = null; Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; do &#123; next = e.next; // 原索引 if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引+oldCap else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 原索引放到bucket里 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; // 原索引+oldCap放到bucket里 if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap常用方法测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import java.util.Collection;import java.util.HashMap;import java.util.Set;public class HashMapDemo &#123; public static void main(String[] args) &#123; HashMap&lt;String, String&gt; map = new HashMap&lt;String, String&gt;(); // 键不能重复，值可以重复 map.put(&quot;san&quot;, &quot;张三&quot;); map.put(&quot;si&quot;, &quot;李四&quot;); map.put(&quot;wu&quot;, &quot;王五&quot;); map.put(&quot;wang&quot;, &quot;老王&quot;); map.put(&quot;wang&quot;, &quot;老王2&quot;);// 老王被覆盖 map.put(&quot;lao&quot;, &quot;老王&quot;); System.out.println(&quot;-------直接输出hashmap:-------&quot;); System.out.println(map); /** * 遍历HashMap */ // 1.获取Map中的所有键 System.out.println(&quot;-------foreach获取Map中所有的键:------&quot;); Set&lt;String&gt; keys = map.keySet(); for (String key : keys) &#123; System.out.print(key+&quot; &quot;); &#125; System.out.println();//换行 // 2.获取Map中所有值 System.out.println(&quot;-------foreach获取Map中所有的值:------&quot;); Collection&lt;String&gt; values = map.values(); for (String value : values) &#123; System.out.print(value+&quot; &quot;); &#125; System.out.println();//换行 // 3.得到key的值的同时得到key所对应的值 System.out.println(&quot;-------得到key的值的同时得到key所对应的值:-------&quot;); Set&lt;String&gt; keys2 = map.keySet(); for (String key : keys2) &#123; System.out.print(key + &quot;：&quot; + map.get(key)+&quot; &quot;); &#125; /** * 另外一种不常用的遍历方式 */ // 当我调用put(key,value)方法的时候，首先会把key和value封装到 // Entry这个静态内部类对象中，把Entry对象再添加到数组中，所以我们想获取 // map中的所有键值对，我们只要获取数组中的所有Entry对象，接下来 // 调用Entry对象中的getKey()和getValue()方法就能获取键值对了 Set&lt;java.util.Map.Entry&lt;String, String&gt;&gt; entrys = map.entrySet(); for (java.util.Map.Entry&lt;String, String&gt; entry : entrys) &#123; System.out.println(entry.getKey() + &quot;--&quot; + entry.getValue()); &#125; /** * HashMap其他常用方法 */ System.out.println(&quot;after map.size()：&quot;+map.size()); System.out.println(&quot;after map.isEmpty()：&quot;+map.isEmpty()); System.out.println(map.remove(&quot;san&quot;)); System.out.println(&quot;after map.remove()：&quot;+map); System.out.println(&quot;after map.get(si)：&quot;+map.get(&quot;si&quot;)); System.out.println(&quot;after map.containsKey(si)：&quot;+map.containsKey(&quot;si&quot;)); System.out.println(&quot;after containsValue(李四)：&quot;+map.containsValue(&quot;李四&quot;)); System.out.println(map.replace(&quot;si&quot;, &quot;李四2&quot;)); System.out.println(&quot;after map.replace(si, 李四2):&quot;+map); &#125;&#125; 参考文章和书籍：《Effective Java》感谢以下作者：https://www.cnblogs.com/skywang12345/p/3308556.htmlhttps://crossoverjie.top/JCSprout/#/collections/ArrayListhttps://github.com/Snailclimb/JavaGuide/blob/master/Java%E7%9B%B8%E5%85%B3/ArrayList.mdhttps://blog.csdn.net/qq_34337272/article/details/79680771https://www.jianshu.com/p/a5f99f25329ahttps://www.jianshu.com/p/506c1e38a922]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(LinkedList)]]></title>
    <url>%2F2019%2F03%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(LinkedList)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-集合框架(LinkedList)LinkedList 定义LinkedList 是一个用链表实现的集合，元素有序且可以重复。 123public class LinkedList&lt;E&gt; extends AbstractSequentialList&lt;E&gt; implements List&lt;E&gt;, Deque&lt;E&gt;, Cloneable, java.io.Serializable 和 ArrayList 集合一样，LinkedList 集合也实现了Cloneable接口和Serializable接口，分别用来支持克隆以及支持序列化。List 接口也不用多说，定义了一套 List 集合类型的方法规范。 注意，相对于 ArrayList 集合，LinkedList 集合多实现了一个 Deque 接口，这是一个双向队列接口，双向队列就是两端都可以进行增加和删除操作。 字段属性123456789101112//链表元素（节点）的个数 transient int size = 0; /** *指向第一个节点的指针 */ transient Node&lt;E&gt; first; /** *指向最后一个节点的指针 */ transient Node&lt;E&gt; last; 注意这里出现了一个 Node 类，这是 LinkedList 类中的一个内部类，其中每一个元素就代表一个 Node 类对象，LinkedList 集合就是由许多个 Node 对象类似于手拉着手构成。123456789101112private static class Node&lt;E&gt; &#123; E item;//实际存储的元素 Node&lt;E&gt; next;//指向上一个节点的引用 Node&lt;E&gt; prev;//指向下一个节点的引用 //构造函数 Node(Node&lt;E&gt; prev, E element, Node&lt;E&gt; next) &#123; this.item = element; this.next = next; this.prev = prev; &#125; &#125; 如下图所示：上图的 LinkedList 是有四个元素，也就是由 4 个 Node 对象组成，size=4，head 指向第一个elementA,tail指向最后一个节点elementD。 构造函数123456public LinkedList() &#123; &#125; public LinkedList(Collection&lt;? extends E&gt; c) &#123; this(); addAll(c); &#125; LinkedList 有两个构造函数，第一个是默认的空的构造函数，第二个是将已有元素的集合Collection 的实例添加到 LinkedList 中，调用的是 addAll() 方法，这个方法下面我们会介绍。 注意：LinkedList 是没有初始化链表大小的构造函数，因为链表不像数组，一个定义好的数组是必须要有确定的大小，然后去分配内存空间，而链表不一样，它没有确定的大小，通过指针的移动来指向下一个内存地址的分配。 添加元素addFirst(E e)将指定元素添加到链表头123456789101112131415//将指定的元素附加到链表头节点 public void addFirst(E e) &#123; linkFirst(e); &#125; private void linkFirst(E e) &#123; final Node&lt;E&gt; f = first;//将头节点赋值给 f final Node&lt;E&gt; newNode = new Node&lt;&gt;(null, e, f);//将指定元素构造成一个新节点，此节点的指向下一个节点的引用为头节点 first = newNode;//将新节点设为头节点，那么原先的头节点 f 变为第二个节点 if (f == null)//如果第二个节点为空，也就是原先链表是空 last = newNode;//将这个新节点也设为尾节点（前面已经设为头节点了） else f.prev = newNode;//将原先的头节点的上一个节点指向新节点 size++;//节点数加1 modCount++;//和ArrayList中一样，iterator和listIterator方法返回的迭代器和列表迭代器实现使用。 &#125; addLast(E e)和add(E e)将指定元素添加到链表尾1234567891011121314151617181920//将元素添加到链表末尾 public void addLast(E e) &#123; linkLast(e); &#125; //将元素添加到链表末尾 public boolean add(E e) &#123; linkLast(e); return true; &#125; void linkLast(E e) &#123; final Node&lt;E&gt; l = last;//将l设为尾节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null);//构造一个新节点，节点上一个节点引用指向尾节点l last = newNode;//将尾节点设为创建的新节点 if (l == null)//如果尾节点为空，表示原先链表为空 first = newNode;//将头节点设为新创建的节点（尾节点也是新创建的节点） else l.next = newNode;//将原来尾节点下一个节点的引用指向新节点 size++;//节点数加1 modCount++;//和ArrayList中一样，iterator和listIterator方法返回的迭代器和列表迭代器实现使用。 &#125; add(int index, E element)将指定的元素插入此列表中的指定位置123456789101112131415161718192021222324252627282930313233343536373839404142434445//将指定的元素插入此列表中的指定位置 public void add(int index, E element) &#123; //判断索引 index &gt;= 0 &amp;&amp; index &lt;= size中时抛出IndexOutOfBoundsException异常 checkPositionIndex(index); if (index == size)//如果索引值等于链表大小 linkLast(element);//将节点插入到尾节点 else linkBefore(element, node(index)); &#125; void linkLast(E e) &#123; final Node&lt;E&gt; l = last;//将l设为尾节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(l, e, null);//构造一个新节点，节点上一个节点引用指向尾节点l last = newNode;//将尾节点设为创建的新节点 if (l == null)//如果尾节点为空，表示原先链表为空 first = newNode;//将头节点设为新创建的节点（尾节点也是新创建的节点） else l.next = newNode;//将原来尾节点下一个节点的引用指向新节点 size++;//节点数加1 modCount++;//和ArrayList中一样，iterator和listIterator方法返回的迭代器和列表迭代器实现使用。 &#125; Node&lt;E&gt; node(int index) &#123; if (index &lt; (size &gt;&gt; 1)) &#123;//如果插入的索引在前半部分 Node&lt;E&gt; x = first;//设x为头节点 for (int i = 0; i &lt; index; i++)//从开始节点到插入节点索引之间的所有节点向后移动一位 x = x.next; return x; &#125; else &#123;//如果插入节点位置在后半部分 Node&lt;E&gt; x = last;//将x设为最后一个节点 for (int i = size - 1; i &gt; index; i--)//从最后节点到插入节点的索引位置之间的所有节点向前移动一位 x = x.prev; return x; &#125; &#125; void linkBefore(E e, Node&lt;E&gt; succ) &#123; final Node&lt;E&gt; pred = succ.prev;//将pred设为插入节点的上一个节点 final Node&lt;E&gt; newNode = new Node&lt;&gt;(pred, e, succ);//将新节点的上引用设为pred,下引用设为succ succ.prev = newNode;//succ的上一个节点的引用设为新节点 if (pred == null)//如果插入节点的上一个节点引用为空 first = newNode;//新节点就是头节点 else pred.next = newNode;//插入节点的下一个节点引用设为新节点 size++; modCount++; &#125; addAll(Collection&lt;? extends E&gt; c) 按照指定集合的迭代器返回的顺序，将指定集合中的所有元素追加到此列表的末尾 此方法还有一个 addAll(int index, Collection&lt;? extends E&gt; c)，将集合 c 中所有元素插入到指定索引的位置。其实addAll(Collection&lt;? extends E&gt; c) == addAll(size, Collection&lt;? extends E&gt; c) 删除元素删除元素和添加元素一样，也是通过更改指向上一个节点和指向下一个节点的引用即可.remove()和removeFirst() 从此列表中移除并返回第一个元素removeLast() 从该列表中删除并返回最后一个元素remove(int index) 删除此列表中指定位置的元素remove(Object o) 如果存在，则从该列表中删除指定元素的第一次出现 此方法本质上和 remove(int index) 没多大区别，通过循环判断元素进行删除，需要注意的是，是删除第一次出现的元素，不是所有的。 修改元素通过调用 set(int index, E element) 方法，用指定的元素替换此列表中指定位置的元素。12345678public E set(int index, E element) &#123; //判断索引 index &gt;= 0 &amp;&amp; index &lt;= size中时抛出IndexOutOfBoundsException异常 checkElementIndex(index); Node&lt;E&gt; x = node(index);//获取指定索引处的元素 E oldVal = x.item; x.item = element;//将指定位置的元素替换成要修改的元素 return oldVal;//返回指定索引位置原来的元素 &#125; 这里主要是通过 node(index) 方法获取指定索引位置的节点，然后修改此节点位置的元素即可。 查找元素getFirst() 返回此列表中的第一个元素getLast() 返回此列表中的最后一个元素get(int index) 返回指定索引处的元素indexOf(Object o) 返回此列表中指定元素第一次出现的索引，如果此列表不包含元素，则返回-1。 遍历集合普通for循环12345678LinkedList&lt;String&gt; linkedList = new LinkedList&lt;&gt;();linkedList.add(&quot;A&quot;);linkedList.add(&quot;B&quot;);linkedList.add(&quot;C&quot;);linkedList.add(&quot;D&quot;);for(int i = 0 ; i &lt; linkedList.size() ; i++)&#123; System.out.print(linkedList.get(i)+&quot; &quot;);//A B C D&#125; 代码很简单，我们就利用 LinkedList 的 get(int index) 方法，遍历出所有的元素。 但是需要注意的是， get(int index) 方法每次都要遍历该索引之前的所有元素，这句话这么理解： 比如上面的一个 LinkedList 集合，我放入了 A,B,C,D是个元素。总共需要四次遍历： 第一次遍历打印 A：只需遍历一次。 第二次遍历打印 B：需要先找到 A，然后再找到 B 打印。 第三次遍历打印 C：需要先找到 A，然后找到 B，最后找到 C 打印。 第四次遍历打印 D：需要先找到 A，然后找到 B，然后找到 C，最后找到 D。 这样如果集合元素很多，越查找到后面（当然此处的get方法进行了优化，查找前半部分从前面开始遍历，查找后半部分从后面开始遍历，但是需要的时间还是很多）花费的时间越多。那么如何改进呢？ 迭代器1234567891011121314151617LinkedList&lt;String&gt; linkedList = new LinkedList&lt;&gt;();linkedList.add(&quot;A&quot;);linkedList.add(&quot;B&quot;);linkedList.add(&quot;C&quot;);linkedList.add(&quot;D&quot;);Iterator&lt;String&gt; listIt = linkedList.listIterator();while(listIt.hasNext())&#123; System.out.print(listIt.next()+&quot; &quot;);//A B C D&#125;//通过适配器模式实现的接口，作用是倒叙打印链表Iterator&lt;String&gt; it = linkedList.descendingIterator();while(it.hasNext())&#123; System.out.print(it.next()+&quot; &quot;);//D C B A&#125; 在 LinkedList 集合中也有一个内部类 ListItr，方法实现大体上也差不多，通过移动游标指向每一次要遍历的元素，不用在遍历某个元素之前都要从头开始。其方法实现也比较简单：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103public ListIterator&lt;E&gt; listIterator(int index) &#123; checkPositionIndex(index); return new ListItr(index); &#125; private class ListItr implements ListIterator&lt;E&gt; &#123; private Node&lt;E&gt; lastReturned; private Node&lt;E&gt; next; private int nextIndex; private int expectedModCount = modCount; ListItr(int index) &#123; // assert isPositionIndex(index); next = (index == size) ? null : node(index); nextIndex = index; &#125; public boolean hasNext() &#123; return nextIndex &lt; size; &#125; public E next() &#123; checkForComodification(); if (!hasNext()) throw new NoSuchElementException(); lastReturned = next; next = next.next; nextIndex++; return lastReturned.item; &#125; public boolean hasPrevious() &#123; return nextIndex &gt; 0; &#125; public E previous() &#123; checkForComodification(); if (!hasPrevious()) throw new NoSuchElementException(); lastReturned = next = (next == null) ? last : next.prev; nextIndex--; return lastReturned.item; &#125; public int nextIndex() &#123; return nextIndex; &#125; public int previousIndex() &#123; return nextIndex - 1; &#125; public void remove() &#123; checkForComodification(); if (lastReturned == null) throw new IllegalStateException(); Node&lt;E&gt; lastNext = lastReturned.next; unlink(lastReturned); if (next == lastReturned) next = lastNext; else nextIndex--; lastReturned = null; expectedModCount++; &#125; public void set(E e) &#123; if (lastReturned == null) throw new IllegalStateException(); checkForComodification(); lastReturned.item = e; &#125; public void add(E e) &#123; checkForComodification(); lastReturned = null; if (next == null) linkLast(e); else linkBefore(e, next); nextIndex++; expectedModCount++; &#125; public void forEachRemaining(Consumer&lt;? super E&gt; action) &#123; Objects.requireNonNull(action); while (modCount == expectedModCount &amp;&amp; nextIndex &lt; size) &#123; action.accept(next.item); lastReturned = next; next = next.next; nextIndex++; &#125; checkForComodification(); &#125; final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125; &#125; 这里需要重点注意的是 modCount 字段，前面我们在增加和删除元素的时候，都会进行自增操作 modCount，这是因为如果想一边迭代，一边用集合自带的方法进行删除或者新增操作，都会抛出异常。（使用迭代器的增删方法不会抛异常）1234final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125; 比如：12345678910111213LinkedList&lt;String&gt; linkedList = new LinkedList&lt;&gt;();linkedList.add(&quot;A&quot;);linkedList.add(&quot;B&quot;);linkedList.add(&quot;C&quot;);linkedList.add(&quot;D&quot;);Iterator&lt;String&gt; listIt = linkedList.listIterator();while(listIt.hasNext())&#123; System.out.print(listIt.next()+&quot; &quot;);//A B C D //linkedList.remove();//此处会抛出异常 listIt.remove();//这样可以进行删除操作&#125; 迭代器的另一种形式就是使用 foreach 循环，底层实现也是使用的迭代器.12345678LinkedList&lt;String&gt; linkedList = new LinkedList&lt;&gt;();linkedList.add(&quot;A&quot;);linkedList.add(&quot;B&quot;);linkedList.add(&quot;C&quot;);linkedList.add(&quot;D&quot;);for(String str : linkedList)&#123; System.out.print(str + &quot;&quot;);&#125; 参考文章和书籍：《Effective Java》感谢以下作者：https://www.cnblogs.com/skywang12345/p/3308556.htmlhttps://crossoverjie.top/JCSprout/#/collections/ArrayListhttps://github.com/Snailclimb/JavaGuide/blob/master/Java%E7%9B%B8%E5%85%B3/ArrayList.mdhttps://blog.csdn.net/qq_34337272/article/details/79680771https://www.jianshu.com/p/a5f99f25329ahttps://www.jianshu.com/p/506c1e38a922]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>LinkedList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(ArrayList:Vector)]]></title>
    <url>%2F2019%2F03%2F27%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(ArrayList%3AVector)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-集合框架(ArrayList/Vector)ArrayList简介 ArrayList 的底层是数组队列，相当于动态数组。与 Java 中的数组相比，它的容量能动态增长。在添加大量元素前，应用程序可以使用ensureCapacity操作来增加 ArrayList 实例的容量。这可以减少递增式再分配的数量。 它继承于 AbstractList，实现了 List, RandomAccess, Cloneable, java.io.Serializable 这些接口。 在我们学数据结构的时候就知道了线性表的顺序存储，插入删除元素的时间复杂度为O（n）,求表长以及增加元素，取第 i 元素的时间复杂度为O（1） ArrayList 继承了AbstractList，实现了List。它是一个数组队列，提供了相关的添加、删除、修改、遍历等功能。 ArrayList 实现了RandomAccess 接口，即提供了随机访问功能。RandomAccess 是 Java 中用来被 List 实现，为 List 提供快速访问功能的。在 ArrayList 中，我们即可以通过元素的序号快速获取元素对象，这就是快速随机访问。 ArrayList 实现了Cloneable 接口，即覆盖了函数 clone()，能被克隆。 ArrayList 实现java.io.Serializable 接口，这意味着ArrayList支持序列化，能通过序列化去传输。 和Vector 不同，ArrayList 中的操作不是线程安全的！所以，建议在单线程中才使用 ArrayList，而在多线程中可以选择 Vector 或者 CopyOnWriteArrayList。 ArrayList核心源码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498package java.util;import java.util.function.Consumer;import java.util.function.Predicate;import java.util.function.UnaryOperator;public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable&#123; private static final long serialVersionUID = 8683452581122892189L; /** * 默认初始容量大小 */ private static final int DEFAULT_CAPACITY = 10; /** * 空数组（用于空实例）。 */ private static final Object[] EMPTY_ELEMENTDATA = &#123;&#125;; //用于默认大小空实例的共享空数组实例。 //我们把它从EMPTY_ELEMENTDATA数组中区分出来，以知道在添加第一个元素时容量需要增加多少。 private static final Object[] DEFAULTCAPACITY_EMPTY_ELEMENTDATA = &#123;&#125;; /** * 保存ArrayList数据的数组 */ transient Object[] elementData; // non-private to simplify nested class access /** * ArrayList 所包含的元素个数 */ private int size; /** * 带初始容量参数的构造函数。（用户自己指定容量） */ public ArrayList(int initialCapacity) &#123; if (initialCapacity &gt; 0) &#123; //创建initialCapacity大小的数组 this.elementData = new Object[initialCapacity]; &#125; else if (initialCapacity == 0) &#123; //创建空数组 this.elementData = EMPTY_ELEMENTDATA; &#125; else &#123; throw new IllegalArgumentException(&quot;Illegal Capacity: &quot;+ initialCapacity); &#125; &#125; /** *默认构造函数，DEFAULTCAPACITY_EMPTY_ELEMENTDATA 为0.初始化为10，也就是说初始其实是空数组 当添加第一个元素的时候数组容量才变成10 */ public ArrayList() &#123; this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA; &#125; /** * 构造一个包含指定集合的元素的列表，按照它们由集合的迭代器返回的顺序。 */ public ArrayList(Collection&lt;? extends E&gt; c) &#123; // elementData = c.toArray(); //如果指定集合元素个数不为0 if ((size = elementData.length) != 0) &#123; // c.toArray 可能返回的不是Object类型的数组所以加上下面的语句用于判断， //这里用到了反射里面的getClass()方法 if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); &#125; else &#123; // 用空数组代替 this.elementData = EMPTY_ELEMENTDATA; &#125; &#125; /** * 修改这个ArrayList实例的容量是列表的当前大小。 应用程序可以使用此操作来最小化ArrayList实例的存储。 */ public void trimToSize() &#123; modCount++; if (size &lt; elementData.length) &#123; elementData = (size == 0) ? EMPTY_ELEMENTDATA : Arrays.copyOf(elementData, size); &#125; &#125;//下面是ArrayList的扩容机制//ArrayList的扩容机制提高了性能，如果每次只扩充一个，//那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It&apos;s already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; //判断是否需要扩容 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // overflow-conscious code if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; /** * 要分配的最大数组大小 */ private static final int MAX_ARRAY_SIZE = Integer.MAX_VALUE - 8; /** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) &#123; // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为ArrayList定义的最大容量，否则，新容量大小则为 minCapacity。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; //比较minCapacity和 MAX_ARRAY_SIZE private static int hugeCapacity(int minCapacity) &#123; if (minCapacity &lt; 0) // overflow throw new OutOfMemoryError(); return (minCapacity &gt; MAX_ARRAY_SIZE) ? Integer.MAX_VALUE : MAX_ARRAY_SIZE; &#125; /** *返回此列表中的元素数。 */ public int size() &#123; return size; &#125; /** * 如果此列表不包含元素，则返回 true 。 */ public boolean isEmpty() &#123; //注意=和==的区别 return size == 0; &#125; /** * 如果此列表包含指定的元素，则返回true 。 */ public boolean contains(Object o) &#123; //indexOf()方法：返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 return indexOf(o) &gt;= 0; &#125; /** *返回此列表中指定元素的首次出现的索引，如果此列表不包含此元素，则为-1 */ public int indexOf(Object o) &#123; if (o == null) &#123; for (int i = 0; i &lt; size; i++) if (elementData[i]==null) return i; &#125; else &#123; for (int i = 0; i &lt; size; i++) //equals()方法比较 if (o.equals(elementData[i])) return i; &#125; return -1; &#125; /** * 返回此列表中指定元素的最后一次出现的索引，如果此列表不包含元素，则返回-1。. */ public int lastIndexOf(Object o) &#123; if (o == null) &#123; for (int i = size-1; i &gt;= 0; i--) if (elementData[i]==null) return i; &#125; else &#123; for (int i = size-1; i &gt;= 0; i--) if (o.equals(elementData[i])) return i; &#125; return -1; &#125; /** * 返回此ArrayList实例的浅拷贝。 （元素本身不被复制。） */ public Object clone() &#123; try &#123; ArrayList&lt;?&gt; v = (ArrayList&lt;?&gt;) super.clone(); //Arrays.copyOf功能是实现数组的复制，返回复制后的数组。参数是被复制的数组和复制的长度 v.elementData = Arrays.copyOf(elementData, size); v.modCount = 0; return v; &#125; catch (CloneNotSupportedException e) &#123; // 这不应该发生，因为我们是可以克隆的 throw new InternalError(e); &#125; &#125; /** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() &#123; return Arrays.copyOf(elementData, size); &#125; /** * 以正确的顺序返回一个包含此列表中所有元素的数组（从第一个到最后一个元素）; *返回的数组的运行时类型是指定数组的运行时类型。 如果列表适合指定的数组，则返回其中。 *否则，将为指定数组的运行时类型和此列表的大小分配一个新数组。 *如果列表适用于指定的数组，其余空间（即数组的列表数量多于此元素），则紧跟在集合结束后的数组中的元素设置为null 。 *（这仅在调用者知道列表不包含任何空元素的情况下才能确定列表的长度。） */ @SuppressWarnings(&quot;unchecked&quot;) public &lt;T&gt; T[] toArray(T[] a) &#123; if (a.length &lt; size) // 新建一个运行时类型的数组，但是ArrayList数组的内容 return (T[]) Arrays.copyOf(elementData, size, a.getClass()); //调用System提供的arraycopy()方法实现数组之间的复制 System.arraycopy(elementData, 0, a, 0, size); if (a.length &gt; size) a[size] = null; return a; &#125; // Positional Access Operations @SuppressWarnings(&quot;unchecked&quot;) E elementData(int index) &#123; return (E) elementData[index]; &#125; /** * 返回此列表中指定位置的元素。 */ public E get(int index) &#123; rangeCheck(index); return elementData(index); &#125; /** * 用指定的元素替换此列表中指定位置的元素。 */ public E set(int index, E element) &#123; //对index进行界限检查 rangeCheck(index); E oldValue = elementData(index); elementData[index] = element; //返回原来在这个位置的元素 return oldValue; &#125; /** * 将指定的元素追加到此列表的末尾。 */ public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! //这里看到ArrayList添加元素的实质就相当于为数组赋值 elementData[size++] = e; return true; &#125; /** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */ public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()这个实现数组之间复制的方法一定要看一下，下面就用到了arraycopy()方法实现数组自己复制自己 System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++; &#125; /** * 删除该列表中指定位置的元素。 将任何后续元素移动到左侧（从其索引中减去一个元素）。 */ public E remove(int index) &#123; rangeCheck(index); modCount++; E oldValue = elementData(index); int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work //从列表中删除的元素 return oldValue; &#125; /** * 从列表中删除指定元素的第一个出现（如果存在）。 如果列表不包含该元素，则它不会更改。 *返回true，如果此列表包含指定的元素 */ public boolean remove(Object o) &#123; if (o == null) &#123; for (int index = 0; index &lt; size; index++) if (elementData[index] == null) &#123; fastRemove(index); return true; &#125; &#125; else &#123; for (int index = 0; index &lt; size; index++) if (o.equals(elementData[index])) &#123; fastRemove(index); return true; &#125; &#125; return false; &#125; /* * Private remove method that skips bounds checking and does not * return the value removed. */ private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work &#125; /** * 从列表中删除所有元素。 */ public void clear() &#123; modCount++; // 把数组中所有的元素的值设为null for (int i = 0; i &lt; size; i++) elementData[i] = null; size = 0; &#125; /** * 按指定集合的Iterator返回的顺序将指定集合中的所有元素追加到此列表的末尾。 */ public boolean addAll(Collection&lt;? extends E&gt; c) &#123; Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount System.arraycopy(a, 0, elementData, size, numNew); size += numNew; return numNew != 0; &#125; /** * 将指定集合中的所有元素插入到此列表中，从指定的位置开始。 */ public boolean addAll(int index, Collection&lt;? extends E&gt; c) &#123; rangeCheckForAdd(index); Object[] a = c.toArray(); int numNew = a.length; ensureCapacityInternal(size + numNew); // Increments modCount int numMoved = size - index; if (numMoved &gt; 0) System.arraycopy(elementData, index, elementData, index + numNew, numMoved); System.arraycopy(a, 0, elementData, index, numNew); size += numNew; return numNew != 0; &#125; /** * 从此列表中删除所有索引为fromIndex （含）和toIndex之间的元素。 *将任何后续元素移动到左侧（减少其索引）。 */ protected void removeRange(int fromIndex, int toIndex) &#123; modCount++; int numMoved = size - toIndex; System.arraycopy(elementData, toIndex, elementData, fromIndex, numMoved); // clear to let GC do its work int newSize = size - (toIndex-fromIndex); for (int i = newSize; i &lt; size; i++) &#123; elementData[i] = null; &#125; size = newSize; &#125; /** * 检查给定的索引是否在范围内。 */ private void rangeCheck(int index) &#123; if (index &gt;= size) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; /** * add和addAll使用的rangeCheck的一个版本 */ private void rangeCheckForAdd(int index) &#123; if (index &gt; size || index &lt; 0) throw new IndexOutOfBoundsException(outOfBoundsMsg(index)); &#125; /** * 返回IndexOutOfBoundsException细节信息 */ private String outOfBoundsMsg(int index) &#123; return &quot;Index: &quot;+index+&quot;, Size: &quot;+size; &#125; /** * 从此列表中删除指定集合中包含的所有元素。 */ public boolean removeAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); //如果此列表被修改则返回true return batchRemove(c, false); &#125; /** * 仅保留此列表中包含在指定集合中的元素。 *换句话说，从此列表中删除其中不包含在指定集合中的所有元素。 */ public boolean retainAll(Collection&lt;?&gt; c) &#123; Objects.requireNonNull(c); return batchRemove(c, true); &#125; /** * 从列表中的指定位置开始，返回列表中的元素（按正确顺序）的列表迭代器。 *指定的索引表示初始调用将返回的第一个元素为next 。 初始调用previous将返回指定索引减1的元素。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator(int index) &#123; if (index &lt; 0 || index &gt; size) throw new IndexOutOfBoundsException(&quot;Index: &quot;+index); return new ListItr(index); &#125; /** *返回列表中的列表迭代器（按适当的顺序）。 *返回的列表迭代器是fail-fast 。 */ public ListIterator&lt;E&gt; listIterator() &#123; return new ListItr(0); &#125; /** *以正确的顺序返回该列表中的元素的迭代器。 *返回的迭代器是fail-fast 。 */ public Iterator&lt;E&gt; iterator() &#123; return new Itr(); &#125; ArrayList源码分析 System.arraycopy()和Arrays.copyOf()方法 通过上面源码我们发现这两个实现数组复制的方法被广泛使用而且很多地方都特别巧妙。比如下面add(int index, E element)方法就很巧妙的用到了arraycopy()方法让数组自己复制自己实现让index开始之后的所有成员后移一个位置:123456789101112131415/** * 在此列表中的指定位置插入指定的元素。 *先调用 rangeCheckForAdd 对index进行界限检查；然后调用 ensureCapacityInternal 方法保证capacity足够大； *再将从index开始之后的所有成员后移一个位置；将element插入index位置；最后size加1。 */public void add(int index, E element) &#123; rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // Increments modCount!! //arraycopy()方法实现数组自己复制自己 //elementData:源数组;index:源数组中的起始位置;elementData：目标数组；index + 1：目标数组中的起始位置； size - index：要复制的数组元素的数量； System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = element; size++;&#125; 又如toArray()方法中用到了copyOf()方法123456789/** *以正确的顺序（从第一个到最后一个元素）返回一个包含此列表中所有元素的数组。 *返回的数组将是“安全的”，因为该列表不保留对它的引用。 （换句话说，这个方法必须分配一个新的数组）。 *因此，调用者可以自由地修改返回的数组。 此方法充当基于阵列和基于集合的API之间的桥梁。 */ public Object[] toArray() &#123; //elementData：要复制的数组；size：要复制的长度 return Arrays.copyOf(elementData, size); &#125; 两者联系与区别联系:看两者源代码可以发现copyOf()内部调用了System.arraycopy()方法 区别:arraycopy()需要目标数组，将原数组拷贝到你自己定义的数组里，而且可以选择拷贝的起点和长度以及放入新数组中的位置;copyOf()是系统自动在内部新建一个数组，并返回该数组。 ArrayList 核心扩容技术1234567891011121314151617181920212223242526272829303132333435363738//下面是ArrayList的扩容机制//ArrayList的扩容机制提高了性能，如果每次只扩充一个，//那么频繁的插入会导致频繁的拷贝，降低性能，而ArrayList的扩容机制避免了这种情况。 /** * 如有必要，增加此ArrayList实例的容量，以确保它至少能容纳元素的数量 * @param minCapacity 所需的最小容量 */ public void ensureCapacity(int minCapacity) &#123; int minExpand = (elementData != DEFAULTCAPACITY_EMPTY_ELEMENTDATA) // any size if not default element table ? 0 // larger than default for default empty table. It&apos;s already // supposed to be at default size. : DEFAULT_CAPACITY; if (minCapacity &gt; minExpand) &#123; ensureExplicitCapacity(minCapacity); &#125; &#125; //得到最小扩容量 private void ensureCapacityInternal(int minCapacity) &#123; if (elementData == DEFAULTCAPACITY_EMPTY_ELEMENTDATA) &#123; // 获取默认的容量和传入参数的较大值 minCapacity = Math.max(DEFAULT_CAPACITY, minCapacity); &#125; ensureExplicitCapacity(minCapacity); &#125; //判断是否需要扩容,上面两个方法都要调用 private void ensureExplicitCapacity(int minCapacity) &#123; modCount++; // 如果说minCapacity也就是所需的最小容量大于保存ArrayList数据的数组的长度的话，就需要调用grow(minCapacity)方法扩容。 //这个minCapacity到底为多少呢？举个例子在添加元素(add)方法中这个minCapacity的大小就为现在数组的长度加1 if (minCapacity - elementData.length &gt; 0) //调用grow方法进行扩容，调用此方法代表已经开始扩容了 grow(minCapacity); &#125; 12345678910111213141516171819202122/** * ArrayList扩容的核心方法。 */ private void grow(int minCapacity) &#123; //elementData为保存ArrayList数据的数组 ///elementData.length求数组长度elementData.size是求数组中的元素个数 // oldCapacity为旧容量，newCapacity为新容量 int oldCapacity = elementData.length; //将oldCapacity 右移一位，其效果相当于oldCapacity /2， //我们知道位运算的速度远远快于整除运算，整句运算式的结果就是将新容量更新为旧容量的1.5倍， int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); //然后检查新容量是否大于最小需要容量，若还是小于最小需要容量，那么就把最小需要容量当作数组的新容量， if (newCapacity - minCapacity &lt; 0) newCapacity = minCapacity; //再检查新容量是否超出了ArrayList所定义的最大容量， //若超出了，则调用hugeCapacity()来比较minCapacity和 MAX_ARRAY_SIZE， //如果minCapacity大于最大容量，则新容量则为ArrayList定义的最大容量，否则，新容量大小则为 minCapacity。 if (newCapacity - MAX_ARRAY_SIZE &gt; 0) newCapacity = hugeCapacity(minCapacity); // minCapacity is usually close to size, so this is a win: elementData = Arrays.copyOf(elementData, newCapacity); &#125; 扩容机制代码已经做了详细的解释。另外值得注意的是大家很容易忽略的一个运算符：移位运算符 简介：移位运算符就是在二进制的基础上对数字进行平移。按照平移的方向和填充数字的规则分为三种:&lt;&lt;(左移)、&gt;&gt;(带符号右移)和&gt;&gt;&gt;(无符号右移)。 作用：对于大数据的2进制运算,位移运算符比那些普通运算符的运算要快很多,因为程序仅仅移动一下而已,不去计算,这样提高了效率,节省了资源 比如这里：int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1); 右移一位相当于除2，右移n位相当于除以 2 的 n 次方。这里 oldCapacity 明显右移了1位所以相当于oldCapacity /2。 另外需要注意的是： java 中的length 属性是针对数组说的,比如说你声明了一个数组,想知道这个数组的长度则用到了 length 这个属性. java 中的length()方法是针对字 符串String说的,如果想看这个字符串的长度则用到 length()这个方法. .java 中的size()方法是针对泛型集合说的,如果想看这个泛型有多少个元素,就调用此方法来查看! 内部类1234(1)private class Itr implements Iterator&lt;E&gt; (2)private class ListItr extends Itr implements ListIterator&lt;E&gt; (3)private class SubList extends AbstractList&lt;E&gt; implements RandomAccess (4)static final class ArrayListSpliterator&lt;E&gt; implements Spliterator&lt;E&gt; ArrayList有四个内部类，其中的Itr是实现了Iterator接口，同时重写了里面的hasNext()，next()，remove()等方法；其中的ListItr继承Itr，实现了ListIterator接口，同时重写了hasPrevious()，nextIndex()，previousIndex()，previous()，set(E e)，add(E e)等方法，所以这也可以看出了Iterator和ListIterator的区别:ListIterator在Iterator的基础上增加了添加对象，修改对象，逆向遍历等方法，这些是Iterator不能实现的。 ArrayList经典Demo1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import java.util.ArrayList;import java.util.Iterator;public class ArrayListDemo &#123; public static void main(String[] srgs)&#123; ArrayList&lt;Integer&gt; arrayList = new ArrayList&lt;Integer&gt;(); System.out.printf(&quot;Before add:arrayList.size() = %d\n&quot;,arrayList.size()); arrayList.add(1); arrayList.add(3); arrayList.add(5); arrayList.add(7); arrayList.add(9); System.out.printf(&quot;After add:arrayList.size() = %d\n&quot;,arrayList.size()); System.out.println(&quot;Printing elements of arrayList&quot;); // 三种遍历方式打印元素 // 第一种：通过迭代器遍历 System.out.print(&quot;通过迭代器遍历:&quot;); Iterator&lt;Integer&gt; it = arrayList.iterator(); while(it.hasNext())&#123; System.out.print(it.next() + &quot; &quot;); &#125; System.out.println(); // 第二种：通过索引值遍历 System.out.print(&quot;通过索引值遍历:&quot;); for(int i = 0; i &lt; arrayList.size(); i++)&#123; System.out.print(arrayList.get(i) + &quot; &quot;); &#125; System.out.println(); // 第三种：for循环遍历 System.out.print(&quot;for循环遍历:&quot;); for(Integer number : arrayList)&#123; System.out.print(number + &quot; &quot;); &#125; // toArray用法 // 第一种方式(最常用) Integer[] integer = arrayList.toArray(new Integer[0]); // 第二种方式(容易理解) Integer[] integer1 = new Integer[arrayList.size()]; arrayList.toArray(integer1); // 抛出异常，java不支持向下转型 //Integer[] integer2 = new Integer[arrayList.size()]; //integer2 = arrayList.toArray(); System.out.println(); // 在指定位置添加元素 arrayList.add(2,2); // 删除指定位置上的元素 arrayList.remove(2); // 删除指定元素 arrayList.remove((Object)3); // 判断arrayList是否包含5 System.out.println(&quot;ArrayList contains 5 is: &quot; + arrayList.contains(5)); // 清空ArrayList arrayList.clear(); // 判断ArrayList是否为空 System.out.println(&quot;ArrayList is empty: &quot; + arrayList.isEmpty()); &#125;&#125; VectorVector 也是实现于 List 接口，底层数据结构和 ArrayList 类似,也是一个动态数组存放数据。不过是在 add() 方法的时候使用 synchronized 进行同步写数据，但是开销较大，所以 Vector 是一个同步容器并不是一个并发容器。以下是 add() 方法：123456public synchronized boolean add(E e) &#123; modCount++; ensureCapacityHelper(elementCount + 1); elementData[elementCount++] = e; return true; &#125; 以及指定位置插入数据:1234567891011121314public void add(int index, E element) &#123; insertElementAt(element, index); &#125; public synchronized void insertElementAt(E obj, int index) &#123; modCount++; if (index &gt; elementCount) &#123; throw new ArrayIndexOutOfBoundsException(index + &quot; &gt; &quot; + elementCount); &#125; ensureCapacityHelper(elementCount + 1); System.arraycopy(elementData, index, elementData, index + 1, elementCount - index); elementData[index] = obj; elementCount++; &#125; 参考文章和书籍：《Effective Java》感谢以下作者：https://www.cnblogs.com/skywang12345/p/3308556.htmlhttps://crossoverjie.top/JCSprout/#/collections/ArrayListhttps://github.com/Snailclimb/JavaGuide/blob/master/Java%E7%9B%B8%E5%85%B3/ArrayList.mdhttps://blog.csdn.net/qq_34337272/article/details/79680771https://www.jianshu.com/p/a5f99f25329ahttps://www.jianshu.com/p/506c1e38a922]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>ArrayList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(锁机制)]]></title>
    <url>%2F2019%2F03%2F23%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(%E9%94%81%E6%9C%BA%E5%88%B6)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-锁Java中的锁分类在读很多并发文章中，会提及各种各样锁如公平锁，乐观锁等等，这篇文章介绍就是各种锁。介绍的内容如下：公平锁/非公平锁可重入锁独享锁/共享锁互斥锁/读写锁乐观锁/悲观锁分段锁偏向锁/轻量级锁/重量级锁自旋锁 上面是很多锁的名词，这些分类并不是全是指锁的状态，有的指锁的特性，有的指锁的设计，下面总结的内容是对每个锁的名词进行一定的解释。公平锁/非公平锁公平锁是指多个线程按照申请锁的顺序来获取锁。非公平锁是指多个线程获取锁的顺序并不是按照申请锁的顺序，有可能后申请的线程比先申请的线程优先获取锁。有可能，会造成优先级反转或者饥饿现象。对于Java ReentrantLock而言，通过构造函数指定该锁是否是公平锁，默认是非公平锁。非公平锁的优点在于吞吐量比公平锁大。对于Synchronized而言，也是一种非公平锁。由于其并不像ReentrantLock是通过AQS的来实现线程调度，所以并没有任何办法使其变成公平锁。 可重入锁可重入锁又名递归锁，是指在同一个线程在外层方法获取锁的时候，在进入内层方法会自动获取锁。说的有点抽象，下面会有一个代码的示例。对于Java ReentrantLock而言, 他的名字就可以看出是一个可重入锁，其名字是Re entrant Lock重新进入锁。对于Synchronized而言,也是一个可重入锁。可重入锁的一个好处是可一定程度避免死锁。12345678synchronized void setA() throws Exception&#123; Thread.sleep(1000); setB();&#125;synchronized void setB() throws Exception&#123; Thread.sleep(1000);&#125; 上面的代码就是一个可重入锁的一个特点，如果不是可重入锁的话，setB可能不会被当前线程执行，可能造成死锁。 独享锁/共享锁独享锁是指该锁一次只能被一个线程所持有。共享锁是指该锁可被多个线程所持有。 对于Java ReentrantLock而言，其是独享锁。但是对于Lock的另一个实现类ReadWriteLock，其读锁是共享锁，其写锁是独享锁。读锁的共享锁可保证并发读是非常高效的，读写，写读 ，写写的过程是互斥的。独享锁与共享锁也是通过AQS来实现的，通过实现不同的方法，来实现独享或者共享。对于Synchronized而言，当然是独享锁。 互斥锁/读写锁上面讲的独享锁/共享锁就是一种广义的说法，互斥锁/读写锁就是具体的实现。互斥锁在Java中的具体实现就是ReentrantLock读写锁在Java中的具体实现就是ReadWriteLock 乐观锁/悲观锁乐观锁与悲观锁不是指具体的什么类型的锁，而是指看待并发同步的角度。悲观锁认为对于同一个数据的并发操作，一定是会发生修改的，哪怕没有修改，也会认为修改。因此对于同一个数据的并发操作，悲观锁采取加锁的形式。悲观的认为，不加锁的并发操作一定会出问题。乐观锁则认为对于同一个数据的并发操作，是不会发生修改的。在更新数据的时候，会采用尝试更新，不断重新的方式更新数据。乐观的认为，不加锁的并发操作是没有事情的。 从上面的描述我们可以看出，悲观锁适合写操作非常多的场景，乐观锁适合读操作非常多的场景，不加锁会带来大量的性能提升。悲观锁在Java中的使用，就是利用各种锁。乐观锁在Java中的使用，是无锁编程，常常采用的是CAS算法，典型的例子就是原子类，通过CAS自旋实现原子操作的更新。 分段锁分段锁其实是一种锁的设计，并不是具体的一种锁，对于ConcurrentHashMap而言，其并发的实现就是通过分段锁的形式来实现高效的并发操作。我们以ConcurrentHashMap来说一下分段锁的含义以及设计思想，ConcurrentHashMap中的分段锁称为Segment，它即类似于HashMap（JDK7与JDK8中HashMap的实现）的结构，即内部拥有一个Entry数组，数组中的每个元素又是一个链表；同时又是一个ReentrantLock（Segment继承了ReentrantLock)。当需要put元素的时候，并不是对整个hashmap进行加锁，而是先通过hashcode来知道他要放在那一个分段中，然后对这个分段进行加锁，所以当多线程put的时候，只要不是放在一个分段中，就实现了真正的并行的插入。但是，在统计size的时候，可就是获取hashmap全局信息的时候，就需要获取所有的分段锁才能统计。分段锁的设计目的是细化锁的粒度，当操作不需要更新整个数组的时候，就仅仅针对数组中的一项进行加锁操作。 偏向锁/轻量级锁/重量级锁这三种锁是指锁的状态，并且是针对Synchronized。在Java 5通过引入锁升级的机制来实现高效Synchronized。这三种锁的状态是通过对象监视器在对象头中的字段来表明的。偏向锁是指一段同步代码一直被一个线程所访问，那么该线程会自动获取锁。降低获取锁的代价。轻量级锁是指当锁是偏向锁的时候，被另一个线程所访问，偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，提高性能。重量级锁是指当锁为轻量级锁的时候，另一个线程虽然是自旋，但自旋不会一直持续下去，当自旋一定次数的时候，还没有获取到锁，就会进入阻塞，该锁膨胀为重量级锁。重量级锁会让其他申请的线程进入阻塞，性能降低。 自旋锁在Java中，自旋锁是指尝试获取锁的线程不会立即阻塞，而是采用循环的方式去尝试获取锁，这样的好处是减少线程上下文切换的消耗，缺点是循环会消耗CPU。 Lock接口在Lock接口出现之前，Java程序是靠synchronized关键字实现锁功能的。JDK1.5之后并发包中新增了Lock接口以及相关实现类来实现锁功能。 虽然synchronized方法和语句的范围机制使得使用监视器锁更容易编程，并且有助于避免涉及锁的许多常见编程错误，但是有时您需要以更灵活的方式处理锁。例如，用于遍历并发访问的数据结构的一些算法需要使用“手动”或“链锁定”：您获取节点A的锁定，然后获取节点B，然后释放A并获取C，然后释放B并获得D等。在这种场景中synchronized关键字就不那么容易实现了，使用Lock接口容易很多。 Lock接口的实现类：ReentrantLock ， ReentrantReadWriteLock.ReadLock ， ReentrantReadWriteLock.WriteLock AbstractQueuedSynchronizer当你查看源码时你会惊讶的发现ReentrantLock并没有多少代码，另外有一个很明显的特点是：基本上所有的方法的实现实际上都是调用了其静态内存类Sync中的方法，而Sync类继承了AbstractQueuedSynchronizer（AQS）。可以看出要想理解ReentrantLock关键核心在于对队列同步器AbstractQueuedSynchronizer（简称同步器）的理解。 在同步组件的实现中，AQS是核心部分，同步组件的实现者通过使用AQS提供的模板方法实现同步组件语义，AQS则实现了对同步状态的管理，以及对阻塞线程进行排队，等待通知等等一些底层的实现处理。AQS的核心也包括了这些方面:同步队列，独占式锁的获取和释放，共享锁的获取和释放以及可中断锁，超时等待锁获取这些特性的实现，而这些实际上则是AQS提供出来的模板方法，归纳整理如下：独占式锁：12345678void acquire(int arg):独占式获取同步状态，如果获取失败则插入同步队列进行等待；void acquireInterruptibly(int arg):与acquire方法相同，但在同步队列中进行等待的时候可以检测中断；boolean tryAcquireNanos(int arg, long nanosTimeout):在acquireInterruptibly基础上增加了超时等待功能，在超时时间内没有获得同步状态返回false;boolean release(int arg):释放同步状态，该方法会唤醒在同步队列中的下一个节点 共享式锁：1234567void acquireShared(int arg):共享式获取同步状态，与独占式的区别在于同一时刻有多个线程获取同步状态void acquireSharedInterruptibly(int arg):在acquireShared方法基础上增加了能响应中断的功能boolean tryAcquireSharedNanos(int arg, long nanosTimeout):在acquireSharedInterruptibly基础上增加了超时等待的功能boolean releaseShared(int arg):共享式释放同步状态 ReentrantLockReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞。在java关键字synchronized隐式支持重入性,synchronized通过获取自增，释放自减的方式实现重入。与此同时，ReentrantLock还支持公平锁和非公平锁两种方式。那么，要想完完全全的弄懂ReentrantLock的话，主要也就是ReentrantLock同步语义的学习：1. 重入性的实现原理；2. 公平锁和非公平锁。 重入性的实现原理要想支持重入性，就要解决两个问题：1. 在线程获取锁的时候，如果已经获取锁的线程是当前线程的话则直接再次获取成功；2. 由于锁会被获取n次，那么只有锁在被释放同样的n次之后，该锁才算是完全释放成功。通过这篇文章，我们知道，同步组件主要是通过重写AQS的几个protected方法来表达自己的同步语义。针对第一个问题，我们来看看ReentrantLock是怎样实现的，以非公平锁为例，判断当前线程能否获得锁为例，核心方法为nonfairTryAcquire： 123456789101112131415161718192021final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); //1. 如果该锁未被任何线程占有，该锁能被当前线程获取 if (c == 0) &#123; if (compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; //2.若被占有，检查占有线程是否是当前线程 else if (current == getExclusiveOwnerThread()) &#123; // 3. 再次获取，计数加一 int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false;&#125; 这段代码的逻辑也很简单，具体请看注释。为了支持重入性，在第二步增加了处理逻辑，如果该锁已经被线程所占有了，会继续检查占有线程是否为当前线程，如果是的话，同步状态加1返回true，表示可以再次获取成功。每次重新获取都会对同步状态进行加一的操作，那么释放的时候处理思路是怎样的了？（依然还是以非公平锁为例）核心方法为tryRelease：123456789101112131415protected final boolean tryRelease(int releases) &#123; //1. 同步状态减1 int c = getState() - releases; if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) &#123; //2. 只有当同步状态为0时，锁成功被释放，返回true free = true; setExclusiveOwnerThread(null); &#125; // 3. 锁未被完全释放，返回false setState(c); return free;&#125; 代码的逻辑请看注释，需要注意的是，重入锁的释放必须得等到同步状态为0时锁才算成功释放，否则锁仍未释放。如果锁被获取n次，释放了n-1次，该锁未完全释放返回false，只有被释放n次才算成功释放，返回true。到现在我们可以理清ReentrantLock重入性的实现了，也就是理解了同步语义的第一条. 公平锁与非公平锁ReentrantLock支持两种锁：公平锁和非公平锁。何谓公平性，是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求上的绝对时间顺序，满足FIFO。ReentrantLock的构造方法无参时是构造非公平锁，源码为：123public ReentrantLock() &#123; sync = new NonfairSync();&#125; 另外还提供了另外一种方式，可传入一个boolean值，true时为公平锁，false时为非公平锁，源码为：123public ReentrantLock(boolean fair) &#123; sync = fair ? new FairSync() : new NonfairSync();&#125; 在上面非公平锁获取时（nonfairTryAcquire方法）只是简单的获取了一下当前状态做了一些逻辑处理，并没有考虑到当前同步队列中线程等待的情况。我们来看看公平锁的处理逻辑是怎样的，核心方法为： 1234567891011121314151617181920protected final boolean tryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); int c = getState(); if (c == 0) &#123; if (!hasQueuedPredecessors() &amp;&amp; compareAndSetState(0, acquires)) &#123; setExclusiveOwnerThread(current); return true; &#125; &#125; else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) throw new Error(&quot;Maximum lock count exceeded&quot;); setState(nextc); return true; &#125; return false; &#125;&#125; 这段代码的逻辑与nonfairTryAcquire基本上一直，唯一的不同在于增加了hasQueuedPredecessors的逻辑判断，方法名就可知道该方法用来判断当前节点在同步队列中是否有前驱节点的判断，如果有前驱节点说明有线程比当前线程更早的请求资源，根据公平性，当前线程请求资源失败。如果当前节点没有前驱节点的话，再才有做后面的逻辑判断的必要性。公平锁每次都是从同步队列中的第一个节点获取到锁，而非公平性锁则不一定，有可能刚释放锁的线程能再次获取到锁。 公平锁 VS 非公平锁 公平锁每次获取到锁为同步队列中的第一个节点，保证请求资源时间上的绝对顺序，而非公平锁有可能刚释放锁的线程下次继续获取该锁，则有可能导致其他线程永远无法获取到锁，造成“饥饿”现象。 公平锁为了保证时间上的绝对顺序，需要频繁的上下文切换，而非公平锁会降低一定的上下文切换，降低性能开销。因此，ReentrantLock默认选择的是非公平锁，则是为了减少一部分上下文切换，保证了系统更大的吞吐量。 ReentrantReadWriteLock在并发场景中用于解决线程安全的问题，我们几乎会高频率的使用到独占式锁，通常使用java提供的关键字synchronized或者concurrents包中实现了Lock接口的ReentrantLock。它们都是独占式获取锁，也就是在同一时刻只有一个线程能够获取锁。而在一些业务场景中，大部分只是读数据，写数据很少，如果仅仅是读数据的话并不会影响数据正确性（出现脏读），而如果在这种业务场景下，依然使用独占锁的话，很显然这将是出现性能瓶颈的地方。针对这种读多写少的情况，java还提供了另外一个实现Lock接口的ReentrantReadWriteLock(读写锁)。读写所允许同一时刻被多个读线程访问，但是在写线程访问时，所有的读线程和其他的写线程都会被阻塞。在分析WirteLock和ReadLock的互斥性时可以按照WriteLock与WriteLock之间，WriteLock与ReadLock之间以及ReadLock与ReadLock之间进行分析。这里做一个归纳总结： 公平性选择：支持非公平性（默认）和公平的锁获取方式，吞吐量还是非公平优于公平；重入性：支持重入，读锁获取后能再次获取，写锁获取之后能够再次获取写锁，同时也能够获取读锁；锁降级：遵循获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁 要想能够彻底的理解读写锁必须能够理解这样几个问题：1. 读写锁是怎样实现分别记录读写状态的？2. 写锁是怎样获取和释放的？3.读锁是怎样获取和释放的？我们带着这样的三个问题，再去了解下读写锁。 写锁详解写锁的获取 同步组件的实现聚合了同步器（AQS），并通过重写重写同步器（AQS）中的方法实现同步组件的同步语义。因此，写锁的实现依然也是采用这种方式。在同一时刻写锁是不能被多个线程所获取，很显然写锁是独占式锁，而实现写锁的同步语义是通过重写AQS中的tryAcquire方法实现的。源码为:12345678910111213141516171819202122232425262728293031323334353637protected final boolean tryAcquire(int acquires) &#123; /* * Walkthrough: * 1. If read count nonzero or write count nonzero * and owner is a different thread, fail. * 2. If count would saturate, fail. (This can only * happen if count is already nonzero.) * 3. Otherwise, this thread is eligible for lock if * it is either a reentrant acquire or * queue policy allows it. If so, update state * and set owner. */ Thread current = Thread.currentThread(); // 1. 获取写锁当前的同步状态 int c = getState(); // 2. 获取写锁获取的次数 int w = exclusiveCount(c); if (c != 0) &#123; // (Note: if c != 0 and w == 0 then shared count != 0) // 3.1 当读锁已被读线程获取或者当前线程不是已经获取写锁的线程的话 // 当前线程获取写锁失败 if (w == 0 || current != getExclusiveOwnerThread()) return false; if (w + exclusiveCount(acquires) &gt; MAX_COUNT) throw new Error(&quot;Maximum lock count exceeded&quot;); // Reentrant acquire // 3.2 当前线程获取写锁，支持可重复加锁 setState(c + acquires); return true; &#125; // 3.3 写锁未被任何线程获取，当前线程可获取写锁 if (writerShouldBlock() || !compareAndSetState(c, c + acquires)) return false; setExclusiveOwnerThread(current); return true;&#125; 这段代码的逻辑请看注释，这里有一个地方需要重点关注，exclusiveCount(c)方法，该方法源码为：123static int exclusiveCount(int c) &#123; return c &amp; EXCLUSIVE_MASK; &#125; 其中EXCLUSIVE_MASK为: static final int EXCLUSIVE_MASK = (1 &lt;&lt; SHARED_SHIFT) - 1;EXCLUSIVE_MASK为1左移16位然后减1，即为0x0000FFFF。而exclusiveCount方法是将同步状态（state为int类型）与0x0000FFFF相与，即取同步状态的低16位。那么低16位代表什么呢？根据exclusiveCount方法的注释为独占式获取的次数即写锁被获取的次数，现在就可以得出来一个结论同步状态的低16位用来表示写锁的获取次数。同时还有一个方法值得我们注意：1static int sharedCount(int c) &#123; return c &gt;&gt;&gt; SHARED_SHIFT; &#125; 该方法是获取读锁被获取的次数，是将同步状态（int c）右移16次，即取同步状态的高16位，现在我们可以得出另外一个结论同步状态的高16位用来表示读锁被获取的次数。现在还记得我们开篇说的需要弄懂的第一个问题吗？读写锁是怎样实现分别记录读锁和写锁的状态的，现在这个问题的答案就已经被我们弄清楚了，其示意图如下图所示：.resources/46AF9BE6-A1C5-418C-9836-CDE377311CC0.png)现在我们回过头来看写锁获取方法tryAcquire，其主要逻辑为：当读锁已经被读线程获取或者写锁已经被其他写线程获取，则写锁获取失败；否则，获取成功并支持重入，增加写状态。 写锁的释放写锁释放通过重写AQS的tryRelease方法，源码为：12345678910111213protected final boolean tryRelease(int releases) &#123; if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //1. 同步状态减去写状态 int nextc = getState() - releases; //2. 当前写状态是否为0，为0则释放写锁 boolean free = exclusiveCount(nextc) == 0; if (free) setExclusiveOwnerThread(null); //3. 不为0则更新同步状态 setState(nextc); return free;&#125; 源码的实现逻辑请看注释，不难理解与ReentrantLock基本一致，这里需要注意的是，减少写状态int nextc = getState() - releases;只需要用当前同步状态直接减去写状态的原因正是我们刚才所说的写状态是由同步状态的低16位表示的。 读锁详解读锁的获取看完了写锁，现在来看看读锁，读锁不是独占式锁，即同一时刻该锁可以被多个读线程获取也就是一种共享式锁。按照之前对AQS介绍，实现共享式同步组件的同步语义需要通过重写AQS的tryAcquireShared方法和tryReleaseShared方法。读锁的获取实现方法为： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748protected final int tryAcquireShared(int unused) &#123; /* * Walkthrough: * 1. If write lock held by another thread, fail. * 2. Otherwise, this thread is eligible for * lock wrt state, so ask if it should block * because of queue policy. If not, try * to grant by CASing state and updating count. * Note that step does not check for reentrant * acquires, which is postponed to full version * to avoid having to check hold count in * the more typical non-reentrant case. * 3. If step 2 fails either because thread * apparently not eligible or CAS fails or count * saturated, chain to version with full retry loop. */ Thread current = Thread.currentThread(); int c = getState(); //1. 如果写锁已经被获取并且获取写锁的线程不是当前线程的话，当前 // 线程获取读锁失败返回-1 if (exclusiveCount(c) != 0 &amp;&amp; getExclusiveOwnerThread() != current) return -1; int r = sharedCount(c); if (!readerShouldBlock() &amp;&amp; r &lt; MAX_COUNT &amp;&amp; //2. 当前线程获取读锁 compareAndSetState(c, c + SHARED_UNIT)) &#123; //3. 下面的代码主要是新增的一些功能，比如getReadHoldCount()方法 //返回当前获取读锁的次数 if (r == 0) &#123; firstReader = current; firstReaderHoldCount = 1; &#125; else if (firstReader == current) &#123; firstReaderHoldCount++; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) cachedHoldCounter = rh = readHolds.get(); else if (rh.count == 0) readHolds.set(rh); rh.count++; &#125; return 1; &#125; //4. 处理在第二步中CAS操作失败的自旋已经实现重入性 return fullTryAcquireShared(current);&#125; 代码的逻辑请看注释，需要注意的是 当写锁被其他线程获取后，读锁获取失败，否则获取成功利用CAS更新同步状态。另外，当前同步状态需要加上SHARED_UNIT（(1 &lt;&lt; SHARED_SHIFT)即0x00010000）的原因这是我们在上面所说的同步状态的高16位用来表示读锁被获取的次数。如果CAS失败或者已经获取读锁的线程再次获取读锁时，是靠fullTryAcquireShared方法实现的，有兴趣可以看看。 读锁的释放读锁释放的实现主要通过方法tryReleaseShared，源码如下，主要逻辑请看注释：1234567891011121314151617181920212223242526272829303132protected final boolean tryReleaseShared(int unused) &#123; Thread current = Thread.currentThread(); // 前面还是为了实现getReadHoldCount等新功能 if (firstReader == current) &#123; // assert firstReaderHoldCount &gt; 0; if (firstReaderHoldCount == 1) firstReader = null; else firstReaderHoldCount--; &#125; else &#123; HoldCounter rh = cachedHoldCounter; if (rh == null || rh.tid != getThreadId(current)) rh = readHolds.get(); int count = rh.count; if (count &lt;= 1) &#123; readHolds.remove(); if (count &lt;= 0) throw unmatchedUnlockException(); &#125; --rh.count; &#125; for (;;) &#123; int c = getState(); // 读锁释放 将同步状态减去读状态即可 int nextc = c - SHARED_UNIT; if (compareAndSetState(c, nextc)) // Releasing the read lock has no effect on readers, // but it may allow waiting writers to proceed if // both read and write locks are now free. return nextc == 0; &#125;&#125; 锁降级读写锁支持锁降级，遵循按照获取写锁，获取读锁再释放写锁的次序，写锁能够降级成为读锁，不支持锁升级，关于锁降级下面的示例代码摘自ReentrantWriteReadLock源码中：123456789101112131415161718192021222324252627void processCachedData() &#123; rwl.readLock().lock(); if (!cacheValid) &#123; // Must release read lock before acquiring write lock rwl.readLock().unlock(); rwl.writeLock().lock(); try &#123; // Recheck state because another thread might have // acquired write lock and changed state before we did. if (!cacheValid) &#123; data = ... cacheValid = true; &#125; // Downgrade by acquiring read lock before releasing write lock rwl.readLock().lock(); &#125; finally &#123; rwl.writeLock().unlock(); // Unlock write, still hold read &#125; &#125; try &#123; use(data); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125;&#125; 参考文章和书籍： 《Java并发编程的艺术》《实战Java高并发程序设计》https://blog.csdn.net/qq_34337272/article/details/79680771https://www.jianshu.com/p/a5f99f25329ahttps://www.jianshu.com/p/506c1e38a922]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>锁机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(volatile关键字)]]></title>
    <url>%2F2019%2F03%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(volatile%E5%85%B3%E9%94%AE%E5%AD%97)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-Volatilevolatile特性volatile就可以说是java虚拟机提供的最轻量级的同步机制。但它同时不容易被正确理解，也至于在并发编程中很多程序员遇到线程安全的问题就会使用synchronized。Java内存模型告诉我们，各个线程会将共享变量从主内存中拷贝到工作内存，然后执行引擎会基于工作内存中的数据进行操作处理。线程在工作内存进行操作后何时会写到主内存中？这个时机对普通变量是没有规定的，而针对volatile修饰的变量给java虚拟机特殊的约定，线程对volatile变量的修改会立刻被其他线程所感知，即不会出现数据脏读的现象，从而保证数据的“可见性”。 通俗来说就是，线程A对一个volatile变量的修改，对于其它线程来说是可见的，即线程每次获取volatile变量的值都是最新的。 volatile的实现原理在生成汇编代码时会在volatile修饰的共享变量进行写操作的时候会多出Lock前缀的指令。我们想这个Lock指令肯定有神奇的地方，那么Lock前缀的指令在多核处理器下会发现什么事情了？主要有这两个方面的影响： 将当前处理器缓存行的数据写回系统内存；这个写回内存的操作会使得其他CPU里缓存了该内存地址的数据无效 为了提高处理速度，处理器不直接和内存进行通信，而是先将系统内存的数据读到内部缓存（L1，L2或其他）后再进行操作，但操作完不知道何时会写到内存。如果对声明了volatile的变量进行写操作，JVM就会向处理器发送一条Lock前缀的指令，将这个变量所在缓存行的数据写回到系统内存。但是，就算写回到内存，如果其他处理器缓存的值还是旧的，再执行计算操作就会有问题。所以，在多处理器下，为了保证各个处理器的缓存是一致的，就会实现缓存一致性协议，每个处理器通过嗅探在总线上传播的数据来检查自己缓存的值是不是过期了，当处理器发现自己缓存行对应的内存地址被修改，就会将当前处理器的缓存行设置成无效状态，当处理器对这个数据进行修改操作的时候，会重新从系统内存中把数据读到处理器缓存里。因此，经过分析我们可以得出如下结论： Lock前缀的指令会引起处理器缓存写回内存；一个处理器的缓存回写到内存会导致其他处理器的缓存失效；当处理器发现本地缓存失效后，就会从内存中重读该变量数据，即可以获取当前最新值。 这样针对volatile变量通过这样的机制就使得每个线程都能获得该变量的最新值。 我们在项目中如何使用？1、状态标记量在高并发的场景中，通过一个boolean类型的变量isopen，控制代码是否走促销逻辑，该如何实现？12345678910111213public class ServerHandler &#123; private volatile isopen; public void run() &#123; if (isopen) &#123; //isopen=true逻辑 &#125; else &#123; //其他逻辑 &#125; &#125; public void setIsopen(boolean isopen) &#123; this.isopen = isopen &#125;&#125; 场景细节无需过分纠结，这里只是举个例子说明volatile的使用方法，用户的请求线程执行run方法，如果需要开启促销活动，可以通过后台设置，具体实现可以发送一个请求，调用setIsopen方法并设置isopen为true，由于isopen是volatile修饰的，所以一经修改，其他线程都可以拿到isopen的最新值，用户请求就可以执行isopen=true的逻辑。 2、double check单例模式的一种实现方式，但很多人会忽略volatile关键字，因为没有该关键字，程序也可以很好的运行，只不过代码的稳定性总不是100%，说不定在未来的某个时刻，隐藏的bug就出来了。12345678910111213class Singleton &#123; private volatile static Singleton instance; public static Singleton getInstance() &#123; if (instance == null) &#123; syschronized(Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 不过在众多单例模式的实现中，我比较推荐懒加载的优雅写法Initialization on Demand Holder（IODH）。123456789public class Singleton &#123; static class SingletonHolder &#123; static Singleton instance = new Singleton(); &#125; public static Singleton getInstance()&#123; return SingletonHolder.instance; &#125; &#125; 如何保证内存可见性在java虚拟机的内存模型中，有主内存和工作内存的概念，每个线程对应一个工作内存，并共享主内存的数据，下面看看操作普通变量和volatile变量有什么不同：1、对于普通变量：读操作会优先读取工作内存的数据，如果工作内存中不存在，则从主内存中拷贝一份数据到工作内存中；写操作只会修改工作内存的副本数据，这种情况下，其它线程就无法读取变量的最新值。2、对于volatile变量，读操作时JMM会把工作内存中对应的值设为无效，要求线程从主内存中读取数据；写操作时JMM会把工作内存中对应的数据刷新到主内存中，这种情况下，其它线程就可以读取变量的最新值。volatile变量的内存可见性是基于内存屏障(Memory Barrier)实现的，什么是内存屏障？内存屏障，又称内存栅栏，是一个CPU指令。在程序运行时，为了提高执行性能，编译器和处理器会对指令进行重排序，JMM为了保证在不同的编译器和CPU上有相同的结果，通过插入特定类型的内存屏障来禁止特定类型的编译器重排序和处理器重排序，插入一条内存屏障会告诉编译器和CPU：不管什么指令都不能和这条Memory Barrier指令重排序。 举例如下：12345678910111213141516171819class Singleton &#123; private volatile static Singleton instance; private int a; private int b; private int b; public static Singleton getInstance() &#123; if (instance == null) &#123; syschronized(Singleton.class) &#123; if (instance == null) &#123; a = 1; // 1 b = 2; // 2 instance = new Singleton(); // 3 c = a + b; // 4 &#125; &#125; &#125; return instance; &#125; &#125; 1、如果变量instance没有volatile修饰，语句1、2、3可以随意的进行重排序执行，即指令执行过程可能是3214或1324。2、如果是volatile修饰的变量instance，会在语句3的前后各插入一个内存屏障。通过观察volatile变量和普通变量所生成的汇编代码可以发现，操作volatile变量会多出一个lock前缀指令：123456Java代码：instance = new Singleton();汇编代码：0x01a3de1d: movb $0x0,0x1104800(%esi);0x01a3de24: **lock** addl $0x0,(%esp); 这个lock前缀指令相当于上述的内存屏障，提供了以下保证：1、将当前CPU缓存行的数据写回到主内存；2、这个写回内存的操作会导致在其它CPU里缓存了该内存地址的数据无效。CPU为了提高处理性能，并不直接和内存进行通信，而是将内存的数据读取到内部缓存（L1，L2）再进行操作，但操作完并不能确定何时写回到内存，如果对volatile变量进行写操作，当CPU执行到Lock前缀指令时，会将这个变量所在缓存行的数据写回到内存，不过还是存在一个问题，就算内存的数据是最新的，其它CPU缓存的还是旧值，所以为了保证各个CPU的缓存一致性，每个CPU通过嗅探在总线上传播的数据来检查自己缓存的数据有效性，当发现自己缓存行对应的内存地址的数据被修改，就会将该缓存行设置成无效状态，当CPU读取该变量时，发现所在的缓存行被设置为无效，就会重新从内存中读取数据到缓存中。这也是我们之前讲的原理部分的解释~ volatile的happens-before关系volatile变量可以通过缓存一致性协议保证每个线程都能获得最新值，即满足数据的“可见性”。我们继续延续上一篇分析问题的方式（我一直认为思考问题的方式是属于自己，也才是最重要的，也在不断培养这方面的能力），我一直将并发分析的切入点分为两个核心，三大性质。两大核心：JMM内存模型（主内存和工作内存）以及happens-before；三条性质：原子性，可见性，有序性（关于三大性质的总结在以后得文章会和大家共同探讨）。废话不多说，先来看两个核心之一：volatile的happens-before关系。在六条happens-before规则中有一条是：volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。下面我们结合具体的代码，我们利用这条规则推导下：12345678910111213public class VolatileExample &#123; private int a = 0; private volatile boolean flag = false; public void writer()&#123; a = 1; //1 flag = true; //2 &#125; public void reader()&#123; if(flag)&#123; //3 int i = a; //4 &#125; &#125;&#125; 上面的实例代码对应的happens-before关系如下图所示：.resources/14BF4468-D1E0-4FBF-B503-A888E309418D.png)加锁线程A先执行writer方法，然后线程B执行reader方法图中每一个箭头两个节点就代码一个happens-before关系，黑色的代表根据程序顺序规则推导出来，红色的是根据volatile变量的写happens-before 于任意后续对volatile变量的读，而蓝色的就是根据传递性规则推导出来的。这里的2 happen-before 3，同样根据happens-before规则定义：如果A happens-before B,则A的执行结果对B可见，并且A的执行顺序先于B的执行顺序，我们可以知道操作2执行结果对操作3来说是可见的，也就是说当线程A将volatile变量 flag更改为true后线程B就能够迅速感知。 参考文章和书籍： 《Java并发编程的艺术》《实战Java高并发程序设计》 https://blog.csdn.net/qq_34337272/article/details/79680771 https://www.jianshu.com/p/a5f99f25329a https://www.jianshu.com/p/506c1e38a922]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>volatile</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(Synchronized关键字)]]></title>
    <url>%2F2019%2F03%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(Synchronized%E5%85%B3%E9%94%AE%E5%AD%97)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-Synchronized简介Java并发编程这个领域中synchronized关键字一直都是元老级的角色，很久之前很多人都会称它为“重量级锁”。但是，在JavaSE 1.6之后进行了主要包括为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁以及其它各种优化之后变得在某些情况下并不是那么重了。 变量安全性“非线程安全”问题存在于“实例变量”中，如果是方法内部的私有变量，则不存在“非线程安全”问题，所得结果也就是“线程安全”的了。 如果两个线程同时操作对象中的实例变量，则会出现“非线程安全”，解决办法就是在方法前加上synchronized关键字即可。 Synchronized的使用修饰代码块12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * 同步线程 */class SyncThread implements Runnable &#123; private static int count; public SyncThread() &#123; count = 0; &#125; public void run() &#123; synchronized(this) &#123; for (int i = 0; i &lt; 5; i++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; &#125; &#125; public int getCount() &#123; return count; &#125;&#125;SyncThread的调用：SyncThread syncThread = new SyncThread();Thread thread1 = new Thread(syncThread, &quot;SyncThread1&quot;);Thread thread2 = new Thread(syncThread, &quot;SyncThread2&quot;);thread1.start();thread2.start();结果如下：SyncThread1:0SyncThread1:1SyncThread1:2SyncThread1:3SyncThread1:4SyncThread2:5SyncThread2:6SyncThread2:7SyncThread2:8SyncThread2:9 当两个并发线程(thread1和thread2)访问同一个对象(syncThread)中的synchronized代码块时，在同一时刻只能有一个线程得到执行，另一个线程受阻塞，必须等待当前线程执行完这个代码块以后才能执行该代码块。Thread1和thread2是互斥的，因为在执行synchronized代码块时会锁定当前的对象，只有执行完该代码块才能释放该对象锁，下一个线程才能执行并锁定该对象。我们再把SyncThread的调用稍微改一下：1234Thread thread1 = new Thread(new SyncThread(), &quot;SyncThread1&quot;);Thread thread2 = new Thread(new SyncThread(), &quot;SyncThread2&quot;);thread1.start();thread2.start(); 结果如下：12345678910SyncThread1:0SyncThread2:1SyncThread1:2SyncThread2:3SyncThread1:4SyncThread2:5SyncThread2:6SyncThread1:7SyncThread1:8SyncThread2:9 不是说一个线程执行synchronized代码块时其它的线程受阻塞吗？为什么上面的例子中thread1和thread2同时在执行。这是因为synchronized只锁定对象，每个对象只有一个锁（lock）与之相关联，而上面的代码等同于下面这段代码：123456SyncThread syncThread1 = new SyncThread();SyncThread syncThread2 = new SyncThread();Thread thread1 = new Thread(syncThread1, &quot;SyncThread1&quot;);Thread thread2 = new Thread(syncThread2, &quot;SyncThread2&quot;);thread1.start();thread2.start(); 这时创建了两个SyncThread的对象syncThread1和syncThread2，线程thread1执行的是syncThread1对象中的synchronized代码(run)，而线程thread2执行的是syncThread2对象中的synchronized代码(run)；我们知道synchronized锁定的是对象，这时会有两把锁分别锁定syncThread1对象和syncThread2对象，而这两把锁是互不干扰的，不形成互斥，所以两个线程可以同时执行。 修饰一个方法Synchronized修饰一个方法很简单，就是在方法的前面加synchronized，public synchronized void method(){//todo}; synchronized修饰方法和修饰一个代码块类似，只是作用范围不一样，修饰代码块是大括号括起来的范围，而修饰方法范围是整个函数。123456789public synchronized void run() &#123; for (int i = 0; i &lt; 5; i ++) &#123; try &#123; System.out.println(Thread.currentThread().getName() + &quot;:&quot; + (count++)); Thread.sleep(100); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; 修饰一个静态的方法Synchronized也可修饰一个静态方法，用法如下：123public synchronized static void method() &#123; // todo&#125; 我们知道静态方法是属于类的而不属于对象的。同样的，synchronized修饰的静态方法锁定的是这个类的所有对象. 修饰一个类Synchronized还可作用于一个类，用法如下：1234567class ClassName &#123; public void method() &#123; synchronized(ClassName.class) &#123; // todo &#125; &#125;&#125; 总结：.resources/07D5A65F-74BE-4357-8309-B0D71C4D45B4.png)A. 无论synchronized关键字加在方法上还是对象上，如果它作用的对象是非静态的，则它取得的锁是对象；如果synchronized作用的对象是一个静态方法或一个类，则它取得的锁是对类，该类所有的对象同一把锁。B. 每个对象只有一个锁（lock）与之相关联，谁拿到这个锁谁就可以运行它所控制的那段代码。C. 实现同步是要很大的系统开销作为代价的，甚至可能造成死锁，所以尽量避免无谓的同步控制。 Synchronized的原理对象锁（monitor）机制现在我们来看看synchronized的具体底层实现。先写一个简单的demo:12345678910public class SynchronizedDemo &#123; public static void main(String[] args) &#123; synchronized (SynchronizedDemo.class) &#123; &#125; method(); &#125; private static void method() &#123; &#125;&#125; 上面的代码中有一个同步代码块，锁住的是类对象，并且还有一个同步静态方法，锁住的依然是该类的类对象。编译之后，切换到SynchronizedDemo.class的同级目录之后，然后用javap -v SynchronizedDemo.class查看字节码文件:.resources/57E615FE-9961-40F9-832C-FE2313570D85.png)synchronized关键字基于上述两个指令实现了锁的获取和释放过程，解释器执行monitorenter时会进入到InterpreterRuntime.cpp的InterpreterRuntime::monitorenter函数，具体实现如下：.resources/6C874101-939A-42B6-A2F8-4A502472DC6D.png)执行同步代码块后首先要先执行monitorenter指令，退出的时候monitorexit指令。通过分析之后可以看出，使用Synchronized进行同步，其关键就是必须要对对象的监视器monitor进行获取，当线程获取monitor后才能继续往下执行，否则就只能等待。而这个获取的过程是互斥的，即同一时刻只有一个线程能够获取到monitor。上面的demo中在执行完同步代码块之后紧接着再会去执行一个静态同步方法，而这个方法锁的对象依然就这个类对象，那么这个正在执行的线程还需要获取该锁吗？答案是不必的，从上图中就可以看出来，执行静态同步方法的时候就只有一条monitorexit指令，并没有monitorenter获取锁的指令。这就是锁的重入性，即在同一锁程中，线程不需要再次获取同一把锁。Synchronized先天具有重入性。每个对象拥有一个计数器，当线程获取该对象锁后，计数器就会加一，释放锁后就会将计数器减一。 synchronized的happens-before关系什么是happens-before概念happens-before的概念最初由Leslie Lamport在其一篇影响深远的论文（《Time，Clocks and the Ordering of Events in a Distributed System》）中提出，有兴趣的可以google一下。JSR-133使用happens-before的概念来指定两个操作之间的执行顺序。由于这两个操作可以在一个线程之内，也可以是在不同线程之间。因此，JMM可以通过happens-before关系向程序员提供跨线程的内存可见性保证（如果A线程的写操作a与B线程的读操作b之间存在happens-before关系，尽管a操作和b操作在不同的线程中执行，但JMM向程序员保证a操作将对b操作可见）。具体的定义为：1）如果一个操作happens-before另一个操作，那么第一个操作的执行结果将对第二个操作可见，而且第一个操作的执行顺序排在第二个操作之前。2）两个操作之间存在happens-before关系，并不意味着Java平台的具体实现必须要按照happens-before关系指定的顺序来执行。如果重排序之后的执行结果，与按happens-before关系来执行的结果一致，那么这种重排序并不非法（也就是说，JMM允许这种重排序）。上面的1）是JMM对程序员的承诺。从程序员的角度来说，可以这样理解happens-before关系：如果A happens-before B，那么Java内存模型将向程序员保证——A操作的结果将对B可见，且A的执行顺序排在B之前。注意，这只是Java内存模型向程序员做出的保证！上面的2）是JMM对编译器和处理器重排序的约束原则。正如前面所言，JMM其实是在遵循一个基本原则：只要不改变程序的执行结果（指的是单线程程序和正确同步的多线程程序），编译器和处理器怎么优化都行。JMM这么做的原因是：程序员对于这两个操作是否真的被重排序并不关心，程序员关心的是程序执行时的语义不能被改变（即执行结果不能被改变）。因此，happens-before关系本质上和as-if-serial语义是一回事。 具体规则 具体规则如下： 程序顺序规则：一个线程中的每个操作，happens-before于该线程中的任意后续操作。 监视器锁规则：对一个锁的解锁，happens-before于随后对这个锁的加锁。 volatile变量规则：对一个volatile域的写，happens-before于任意后续对这个volatile域的读。 传递性：如果A happens-before B，且B happens-before C，那么A happens-before C。 start()规则：如果线程A执行操作ThreadB.start()（启动线程B），那么A线程的ThreadB.start()操作happens-before于线程B中的任意操作。 join()规则：如果线程A执行操作ThreadB.join()并成功返回，那么线程B中的任意操作happens-before于线程A从ThreadB.join()操作成功返回。 程序中断规则：对线程interrupted()方法的调用先行于被中断线程的代码检测到中断时间的发生。 对象finalize规则：一个对象的初始化完成（构造函数执行结束）先行于发生它的finalize()方法的开始。 synchronized的happens-before关系Synchronized的happens-before规则，即监视器锁规则：对同一个监视器的解锁，happens-before于对该监视器的加锁。继续来看代码：1234567891011public class MonitorDemo &#123; private int a = 0; public synchronized void writer() &#123; // 1 a++; // 2 &#125; // 3 public synchronized void reader() &#123; // 4 int i = a; // 5 &#125; // 6&#125; 该代码的happens-before关系如图所示：.resources/B079BB99-25B3-4365-8938-A75246E6237E.png)在图中每一个箭头连接的两个节点就代表之间的happens-before关系，黑色的是通过程序顺序规则推导出来，红色的为监视器锁规则推导而出：线程A释放锁happens-before线程B加锁，蓝色的则是通过程序顺序规则和监视器锁规则推测出来happens-befor关系，通过传递性规则进一步推导的happens-before关系。现在我们来重点关注2 happens-before 5，通过这个关系我们可以得出什么？根据happens-before的定义中的一条:如果A happens-before B，则A的执行结果对B可见，并且A的执行顺序先于B。线程A先对共享变量A进行加一，由2 happens-before 5关系可知线程A的执行结果对线程B可见即线程B所读取到的a的值为1。 synchronized的优化通过上面的讨论现在我们对Synchronized应该有所印象了，它最大的特征就是在同一时刻只有一个线程能够获得对象的监视器（monitor），从而进入到同步代码块或者同步方法之中，即表现为互斥性（排它性）。这种方式肯定效率低下，每次只能通过一个线程，既然每次只能通过一个，这种形式不能改变的话，那么我们能不能让每次通过的速度变快一点了。打个比方，去收银台付款，之前的方式是，大家都去排队，然后去纸币付款收银员找零，有的时候付款的时候在包里拿出钱包再去拿出钱，这个过程是比较耗时的，然后，支付宝解放了大家去钱包找钱的过程，现在只需要扫描下就可以完成付款了，也省去了收银员跟你找零的时间的了。同样是需要排队，但整个付款的时间大大缩短，是不是整体的效率变高速率变快了？这种优化方式同样可以引申到锁优化上，缩短获取锁的时间。 CAS操作这里做一个介绍，CAS为后续锁的章节做一个铺垫O(∩_∩)O~ 推荐文章：https://www.jianshu.com/p/24ffe531e9ee什么是CAS?使用锁时，线程获取锁是一种悲观锁策略，即假设每一次执行临界区代码都会产生冲突，所以当前线程获取到锁的时候同时也会阻塞其他线程获取该锁。而CAS操作（又称为无锁操作）是一种乐观锁策略，它假设所有线程访问共享资源的时候不会出现冲突，既然不会出现冲突自然而然就不会阻塞其他线程的操作。因此，线程就不会出现阻塞停顿的状态。那么，如果出现冲突了怎么办？无锁操作是使用CAS(compare and swap)又叫做比较交换来鉴别线程是否出现冲突，出现冲突就重试当前操作直到没有冲突为止。 CAS的操作过程CAS比较交换的过程可以通俗的理解为CAS(V,O,N)，包含三个值分别为：V 内存地址存放的实际值；O 预期的值（旧值）；N 更新的新值。当V和O相同时，也就是说旧值和内存中实际的值相同表明该值没有被其他线程更改过，即该旧值O就是目前来说最新的值了，自然而然可以将新值N赋值给V。反之，V和O不相同，表明该值已经被其他线程改过了则该旧值O不是最新版本的值了，所以不能将新值N赋给V，返回V即可。当多个线程使用CAS操作一个变量是，只有一个线程会成功，并成功更新，其余会失败。失败的线程会重新尝试，当然也可以选择挂起线程CAS的实现需要硬件指令集的支撑，在JDK1.5后虚拟机才可以使用处理器提供的CMPXCHG指令实现。CAS的应用场景在J.U.C包中利用CAS实现类有很多，可以说是支撑起整个concurrency包的实现，在Lock实现中会有CAS改变state变量，在atomic包中的实现类也几乎都是用CAS实现，关于这些具体的实现场景在之后会详细聊聊，现在有个印象就好了（微笑脸）。 CAS的问题 ABA问题因为CAS会检查旧值有没有变化，这里存在这样一个有意思的问题。比如一个旧值A变为了成B，然后再变成A，刚好在做CAS时检查发现旧值并没有变化依然为A，但是实际上的确发生了变化。解决方案可以沿袭数据库中常用的乐观锁方式，添加一个版本号可以解决。原来的变化路径A-&gt;B-&gt;A就变成了1A-&gt;2B-&gt;3C。java这么优秀的语言，当然在java 1.5后的atomic包中提供了AtomicStampedReference来解决ABA问题，解决思路就是这样的。 自旋时间过长使用CAS时非阻塞同步，也就是说不会将线程挂起，会自旋（无非就是一个死循环）进行下一次尝试，如果这里自旋时间过长对性能是很大的消耗。如果JVM能支持处理器提供的pause指令，那么在效率上会有一定的提升。 只能保证一个共享变量的原子操作当对一个共享变量执行操作时CAS能保证其原子性，如果对多个共享变量进行操作,CAS就不能保证其原子性。有一个解决方案是利用对象整合多个共享变量，即一个类中的成员变量就是这几个共享变量。然后将这个对象做CAS操作就可以保证其原子性。atomic中提供了AtomicReference来保证引用对象之间的原子性。 参考文章目录：感谢各位大大的劳动成果~深表敬意~ https://blog.csdn.net/qq_34337272/article/details/79655194 https://blog.csdn.net/qq_34337272/article/details/79670775 https://www.jianshu.com/p/d53bf830fa09 https://www.jianshu.com/p/c5058b6fe8e5]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>Synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Java高级特性(多线程)]]></title>
    <url>%2F2019%2F03%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BJava%E9%AB%98%E7%BA%A7%E7%89%B9%E6%80%A7(%E5%A4%9A%E7%BA%BF%E7%A8%8B)%2F</url>
    <content type="text"><![CDATA[Java高级特性增强-多线程本部分网络上有大量的资源可以参考，在这里做了部分整理，感谢前辈的付出，每节文章末尾有引用列表，源码推荐看JDK1.8以后的版本，注意甄别~ ####多线程 ###集合框架 ###NIO ###Java并发容器 多线程.resources/F18CB21B-41D4-4D8D-890D-4B632F69F96A.jpg)参考资料列表：java并发编程指南https://blog.csdn.net/qq_34337272/column/info/20860死磕系列：http://cmsblogs.com/?p=2611面试题系列：https://blog.csdn.net/linzhiqiang0316/article/details/80473906简书：https://www.jianshu.com/nb/4893857以上几个博客足够了，着重推荐一下死磕系列和简书的文章，比较深入 进程和多线程简介进程和线程进程和线程的对比这一知识点由于过于基础，所以在面试中很少碰到，但是极有可能会在笔试题中碰到。常见的提问形式是这样的：“什么是线程和进程?，请简要描述线程与进程的关系、区别及优缺点？ ”。 何为进程？进程是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。或者我们可以这样说：进程，是程序的一次执行过程，是系统运行程序的基本单位，因此进程是动态的。系统运行一个程序即是一个进程从创建，运行到消亡的过程。简单来说，一个进程就是一个执行中的程序，它在计算机中一个指令接着一个指令地执行着，同时，每个进程还占有某些系统资源如CPU时间，内存空间，文件，文件，输入输出设备的使用权等等。换句话说，当程序在执行时，将会被操作系统载入内存中。 何为线程？线程与进程相似，但线程是一个比进程更小的执行单位。一个进程在其执行的过程中可以产生多个线程。与进程不同的是同类的多个线程共享同一块内存空间和一组系统资源，所以系统在产生一个线程，或是在各个线程之间作切换工作时，负担要比进程小得多，也正因为如此，线程也被称为轻量级进程。 何为多线程多线程就是多个线程同时运行或交替运行。单核CPU的话是顺序执行，也就是交替运行。多核CPU的话，因为每个CPU有自己的运算器，所以在多个CPU中可以同时运行。 为什么多线程是必要的个人觉得可以用一句话概括：开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。 为什么提倡多线程而不是多进程线程就是轻量级进程，是程序执行的最小单位。使用多线程而不是用多进程去进行并发程序的设计，是因为线程间的切换和调度的成本远远小于进程。 线程有什么优缺点1）好处使用多线程可以把程序中占据时间长的任务放到后台去处理，如图片、视屏的下载。发挥多核处理器的优势，并发执行让系统运行的更快、更流畅，用户体验更好。2）坏处大量的线程降低代码的可读性。更多的线程需要更多的内存空间。当多个线程对同一个资源出现争夺时候要注意线程安全的问题。 多线程中重要的概念同步和异步同步和异步通常用来形容一次方法调用。同步方法调用一旦开始，调用者必须等到方法调用返回后，才能继续后续的行为。异步方法调用更像一个消息传递，一旦开始，方法调用就会立即返回，调用者可以继续后续的操作。 关于异步目前比较经典以及常用的实现方式就是消息队列：在不使用消息队列服务器的时候，用户的请求数据直接写入数据库，在高并发的情况下数据库压力剧增，使得响应速度变慢。但是在使用消息队列之后，用户的请求数据发送给消息队列之后立即 返回，再由消息队列的消费者进程从消息队列中获取数据，异步写入数据库。由于消息队列服务器处理速度快于数据库（消息队列也比数据库有更好的伸缩性），因此响应速度得到大幅改善。 并发(Concurrency)和并行(Parallelism)并发和并行是两个非常容易被混淆的概念。它们都可以表示两个或者多个任务一起执行，但是偏重点有些不同。并发偏重于多个任务交替执行，而多个任务之间有可能还是串行的。而并行是真正意义上的“同时执行”。 多线程在单核CPU的话是顺序执行，也就是交替运行（并发）。多核CPU的话，因为每个CPU有自己的运算器，所以在多个CPU中可以同时运行（并行）。 高并发高并发（High Concurrency）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计保证系统能够同时并行处理很多请求。 高并发相关常用的一些指标有响应时间（Response Time），吞吐量（Throughput），每秒查询率QPS（Query Per Second），并发用户数等。 临界区临界区用来表示一种公共资源或者说是共享数据，可以被多个线程使用。但是每一次，只能有一个线程使用它，一旦临界区资源被占用，其他线程要想使用这个资源，就必须等待。在并行程序中，临界区资源是保护的对象。 阻塞和非阻塞非阻塞指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回，而阻塞与之相反。 多线程的创建方式继承Thread类1234567public class MyThread extends Thread &#123; @Override public void run() &#123; super.run(); System.out.println(&quot;MyThread&quot;); &#125;&#125; 实现Runnable接口123456public class MyRunnable implements Runnable &#123; @Override public void run() &#123; System.out.println(&quot;MyRunnable&quot;); &#125;&#125; 线程池 《阿里巴巴Java开发手册》在第一章第六节并发处理这一部分也强调到“线程资源必须通过线程池提供，不允许在应用中自行显示创建线程”。我们在实际开发环境中，建议使用线程池的方式创建线程。1234567891011121314151617181920212223242526public class ThreadPool&#123; private static int POOL_NUM = 10; public static void main(String[] args) &#123;ExecutorService executorService = Executors.newFixedThreadPool(5); for(int i = 0; i&lt;POOL_NUM; i++) &#123; RunnableThread thread = new RunnableThread(); executorService.execute(thread); &#125; &#125;&#125; class RunnableThread implements Runnable&#123; private int THREAD_NUM = 10; public void run() &#123; for(int i = 0; i&lt;THREAD_NUM; i++) &#123; System.out.println(&quot;线程&quot; + Thread.currentThread() + &quot; &quot; + i); &#125; &#125;&#125; 线程的生命周期线程一共有五个状态，分别如下：新建(new)：当创建Thread类的一个实例（对象）时，此线程进入新建状态（未被启动）。例如：Thread t1 = new Thread() 。 可运行(runnable)：线程对象创建后，其他线程(比如 main 线程）调用了该对象的 start 方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取 cpu 的使用权。例如：t1.start() 。 运行(running)：线程获得 CPU 资源正在执行任务（#run() 方法），此时除非此线程自动放弃 CPU 资源或者有优先级更高的线程进入，线程将一直运行到结束。死亡(dead)：当线程执行完毕或被其它线程杀死，线程就进入死亡状态，这时线程不可能再进入就绪状态等待执行。自然终止：正常运行完 #run()方法，终止。异常终止：调用 #stop() 方法，让一个线程终止运行。堵塞(blocked)：由于某种原因导致正在运行的线程让出 CPU 并暂停自己的执行，即进入堵塞状态。直到线程进入可运行(runnable)状态，才有机会再次获得 CPU 资源，转到运行(running)状态。阻塞的情况有三种：正在睡眠：调用 #sleep(long t) 方法，可使线程进入睡眠方式。一个睡眠着的线程在指定的时间过去可进入可运行(runnable)状态。正在等待：调用 #wait() 方法。调用 notify() 方法，回到就绪状态。被另一个线程所阻塞：调用 #suspend() 方法。调用 #resume() 方法，就可以恢复。 见下图：.resources/6AC11272-0DE3-44D7-9533-2647D0A652BF.png) 线程的优先级每个线程都具有各自的优先级，线程的优先级可以在程序中表明该线程的重要性，如果有很多线程处于就绪状态，系统会根据优先级来决定首先使哪个线程进入运行状态。但这个并不意味着低。优先级的线程得不到运行，而只是它运行的几率比较小，如垃圾回收机制线程的优先级就比较低。所以很多垃圾得不到及时的回收处理。 线程优先级具有继承特性比如A线程启动B线程，则B线程的优先级和A是一样的。 线程优先级具有随机性也就是说线程优先级高的不一定每一次都先执行完。 Thread类中包含的成员变量代表了线程的某些优先级。如Thread.MIN_PRIORITY（常数1），Thread.NORM_PRIORITY（常数5）,Thread.MAX_PRIORITY（常数10）。其中每个线程的优先级都在Thread.MIN_PRIORITY（常数1） 到Thread.MAX_PRIORITY（常数10） 之间，在默认情况下优先级都是Thread.NORM_PRIORITY（常数5）。 学过操作系统这门课程的话，我们可以发现多线程优先级或多或少借鉴了操作系统对进程的管理 线程的终止interrupt()方法注意：interrupt()方法的使用效果并不像for+break语句那样，马上就停止循环。调用interrupt方法是在当前线程中打了一个停止标志，并不是真的停止线程。123456789101112131415161718192021public class MyThread extends Thread &#123; public void run()&#123; super.run(); for(int i=0; i&lt;500000; i++)&#123; System.out.println(&quot;i=&quot;+(i+1)); &#125; &#125;&#125;public class Run &#123; public static void main(String args[])&#123; Thread thread = new MyThread(); thread.start(); try &#123; Thread.sleep(2000); thread.interrupt(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出结果：12345678...i=499994i=499995i=499996i=499997i=499998i=499999i=500000 判断线程是否停止状态Thread.java类中提供了两种方法： this.interrupted(): 测试当前线程是否已经中断；this.isInterrupted(): 测试线程是否已经中断；那么这两个方法有什么图区别呢？我们先来看看this.interrupted()方法的解释：测试当前线程是否已经中断，当前线程是指运行this.interrupted()方法的线程。 123456789101112131415161718192021222324public class MyThread extends Thread &#123; public void run()&#123; super.run(); for(int i=0; i&lt;500000; i++)&#123; i++; &#125; &#125;&#125;public class Run &#123; public static void main(String args[])&#123; Thread thread = new MyThread(); thread.start(); try &#123; Thread.sleep(2000); thread.interrupt(); System.out.println(&quot;stop 1-&gt;&quot; + thread.interrupted()); System.out.println(&quot;stop 2-&gt;&quot; + thread.interrupted()); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果：12stop 1-&gt;falsestop 2-&gt;false 类Run.java中虽然是在thread对象上调用以下代码：thread.interrupt(), 后面又使用 12System.out.println(&quot;stop 1-&gt;&quot; + thread.interrupted());System.out.println(&quot;stop 2-&gt;&quot; + thread.interrupted()); 来判断thread对象所代表的线程是否停止，但从控制台打印的结果来看，线程并未停止，这也证明了interrupted()方法的解释，测试当前线程是否已经中断。这个当前线程是main，它从未中断过，所以打印的结果是两个false. 如何使main线程产生中断效果呢？123456789public class Run2 &#123; public static void main(String args[])&#123; Thread.currentThread().interrupt(); System.out.println(&quot;stop 1-&gt;&quot; + Thread.interrupted()); System.out.println(&quot;stop 2-&gt;&quot; + Thread.interrupted()); System.out.println(&quot;End&quot;); &#125;&#125; 运行结果为：123stop 1-&gt;truestop 2-&gt;falseEnd 方法interrupted()的确判断出当前线程是否是停止状态。但为什么第2个布尔值是false呢？ 官方帮助文档中对interrupted方法的解释：测试当前线程是否已经中断。线程的中断状态由该方法清除。 换句话说，如果连续两次调用该方法，则第二次调用返回false。 下面来看一下inInterrupted()方法。123456789public class Run3 &#123; public static void main(String args[])&#123; Thread thread = new MyThread(); thread.start(); thread.interrupt(); System.out.println(&quot;stop 1-&gt;&quot; + thread.isInterrupted()); System.out.println(&quot;stop 2-&gt;&quot; + thread.isInterrupted()); &#125;&#125; 运行结果： 12stop 1-&gt;truestop 2-&gt;true isInterrupted()并为清除状态，所以打印了两个true。 能停止的线程–异常法有了前面学习过的知识点，就可以在线程中用for语句来判断一下线程是否是停止状态，如果是停止状态，则后面的代码不再运行即可： 12345678910111213141516171819202122232425public class MyThread extends Thread &#123; public void run()&#123; super.run(); for(int i=0; i&lt;500000; i++)&#123; if(this.interrupted()) &#123; System.out.println(&quot;线程已经终止， for循环不再执行&quot;); break; &#125; System.out.println(&quot;i=&quot;+(i+1)); &#125; &#125;&#125;public class Run &#123; public static void main(String args[])&#123; Thread thread = new MyThread(); thread.start(); try &#123; Thread.sleep(2000); thread.interrupt(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 运行结果：123456...i=202053i=202054i=202055i=202056线程已经终止， for循环不再执行 上面的示例虽然停止了线程，但如果for语句下面还有语句，还是会继续运行的。看下面的例子：1234567891011121314public class MyThread extends Thread &#123; public void run()&#123; super.run(); for(int i=0; i&lt;500000; i++)&#123; if(this.interrupted()) &#123; System.out.println(&quot;线程已经终止， for循环不再执行&quot;); break; &#125; System.out.println(&quot;i=&quot;+(i+1)); &#125; System.out.println(&quot;这是for循环外面的语句，也会被执行&quot;); &#125;&#125; 使用Run.java执行的结果是：1234567...i=180136i=180137i=180138i=180139线程已经终止， for循环不再执行这是for循环外面的语句，也会被执行 如何解决语句继续运行的问题呢？ 看一下更新后的代码：12345678910111213141516171819public class MyThread extends Thread &#123; public void run()&#123; super.run(); try &#123; for(int i=0; i&lt;500000; i++)&#123; if(this.interrupted()) &#123; System.out.println(&quot;线程已经终止， for循环不再执行&quot;); throw new InterruptedException(); &#125; System.out.println(&quot;i=&quot;+(i+1)); &#125; System.out.println(&quot;这是for循环外面的语句，也会被执行&quot;); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;进入MyThread.java类中的catch了。。。&quot;); e.printStackTrace(); &#125; &#125;&#125; 使用Run.java运行的结果如下：12345678...i=203798i=203799i=203800线程已经终止， for循环不再执行进入MyThread.java类中的catch了。。。java.lang.InterruptedException at thread.MyThread.run(MyThread.java:13) 在沉睡中停止如果线程在sleep()状态下停止线程，会是什么效果呢？123456789101112131415public class MyThread extends Thread &#123; public void run()&#123; super.run(); try &#123; System.out.println(&quot;线程开始。。。&quot;); Thread.sleep(200000); System.out.println(&quot;线程结束。&quot;); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;在沉睡中被停止, 进入catch， 调用isInterrupted()方法的结果是：&quot; + this.isInterrupted()); e.printStackTrace(); &#125; &#125;&#125; 使用Run.java运行的结果是：12345线程开始。。。在沉睡中被停止, 进入catch， 调用isInterrupted()方法的结果是：falsejava.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at thread.MyThread.run(MyThread.java:12) 从打印的结果来看， 如果在sleep状态下停止某一线程，会进入catch语句，并且清除停止状态值，使之变为false。 前一个实验是先sleep然后再用interrupt()停止，与之相反的操作在学习过程中也要注意：12345678910111213141516171819202122232425public class MyThread extends Thread &#123; public void run()&#123; super.run(); try &#123; System.out.println(&quot;线程开始。。。&quot;); for(int i=0; i&lt;10000; i++)&#123; System.out.println(&quot;i=&quot; + i); &#125; Thread.sleep(200000); System.out.println(&quot;线程结束。&quot;); &#125; catch (InterruptedException e) &#123; System.out.println(&quot;先停止，再遇到sleep，进入catch异常&quot;); e.printStackTrace(); &#125; &#125;&#125;public class Run &#123; public static void main(String args[])&#123; Thread thread = new MyThread(); thread.start(); thread.interrupt(); &#125;&#125; 运行结果：123456i=9998i=9999先停止，再遇到sleep，进入catch异常java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at thread.MyThread.run(MyThread.java:15) 能停止的线程—暴力停止使用stop()方法停止线程则是非常暴力的。123456789101112131415161718192021222324public class MyThread extends Thread &#123; private int i = 0; public void run()&#123; super.run(); try &#123; while (true)&#123; System.out.println(&quot;i=&quot; + i); i++; Thread.sleep(200); &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125;public class Run &#123; public static void main(String args[]) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); Thread.sleep(2000); thread.stop(); &#125;&#125; 运行结果：123456789101112i=0i=1i=2i=3i=4i=5i=6i=7i=8i=9Process finished with exit code 0 方法stop()与java.lang.ThreadDeath异常 调用stop()方法时会抛出java.lang.ThreadDeath异常，但是通常情况下，此异常不需要显示地捕捉。12345678910111213141516171819public class MyThread extends Thread &#123; private int i = 0; public void run()&#123; super.run(); try &#123; this.stop(); &#125; catch (ThreadDeath e) &#123; System.out.println(&quot;进入异常catch&quot;); e.printStackTrace(); &#125; &#125;&#125;public class Run &#123; public static void main(String args[]) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); &#125;&#125; stop()方法以及作废，因为如果强制让线程停止有可能使一些清理性的工作得不到完成。另外一个情况就是对锁定的对象进行了解锁，导致数据得不到同步的处理，出现数据不一致的问题。 释放锁的不良后果 使用stop()释放锁将会给数据造成不一致性的结果。如果出现这样的情况，程序处理的数据就有可能遭到破坏，最终导致程序执行的流程错误，一定要特别注意：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class SynchronizedObject &#123; private String name = &quot;a&quot;; private String password = &quot;aa&quot;; public synchronized void printString(String name, String password)&#123; try &#123; this.name = name; Thread.sleep(100000); this.password = password; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getPassword() &#123; return password; &#125; public void setPassword(String password) &#123; this.password = password; &#125;&#125;public class MyThread extends Thread &#123; private SynchronizedObject synchronizedObject; public MyThread(SynchronizedObject synchronizedObject)&#123; this.synchronizedObject = synchronizedObject; &#125; public void run()&#123; synchronizedObject.printString(&quot;b&quot;, &quot;bb&quot;); &#125;&#125;public class Run &#123; public static void main(String args[]) throws InterruptedException &#123; SynchronizedObject synchronizedObject = new SynchronizedObject(); Thread thread = new MyThread(synchronizedObject); thread.start(); Thread.sleep(500); thread.stop(); System.out.println(synchronizedObject.getName() + &quot; &quot; + synchronizedObject.getPassword()); &#125;&#125; 输出结果：1b aa 由于stop()方法以及在JDK中被标明为“过期/作废”的方法，显然它在功能上具有缺陷，所以不建议在程序张使用stop()方法。 使用return停止线程 将方法interrupt()与return结合使用也能实现停止线程的效果：1234567891011121314151617181920 public class MyThread extends Thread &#123; public void run()&#123; while (true)&#123; if(this.isInterrupted())&#123; System.out.println(&quot;线程被停止了！&quot;); return; &#125; System.out.println(&quot;Time: &quot; + System.currentTimeMillis()); &#125; &#125;&#125;public class Run &#123; public static void main(String args[]) throws InterruptedException &#123; Thread thread = new MyThread(); thread.start(); Thread.sleep(2000); thread.interrupt(); &#125;&#125; 输出结果：12345...Time: 1467072288503Time: 1467072288503Time: 1467072288503线程被停止了！ 笔者花了巨大篇幅介绍线程的终止，因为这是在实际开发中最容易犯的错误，千万注意哦~]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Java</tag>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据学习路线]]></title>
    <url>%2F2019%2F03%2F12%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[大数据学习路线一、大数据处理流程1.1 数据收集1.2 数据存储1.3 数据分析1.4 数据应用1.5 其他框架二、学习路线2.1 语言基础2.2 Linux 基础2.3 构建工具2.4 框架学习三、开发工具四、结语 一、大数据处理流程 上图是一个简化的大数据处理流程图，大数据处理的主要流程包括数据收集、数据存储、数据处理、数据应用等主要环节。下面我们逐一对各个环节所需要的技术栈进行讲解：### 1.1 数据收集大数据处理的第一步是数据的收集。现在的中大型项目通常采用微服务架构进行分布式部署，所以数据的采集需要在多台服务器上进行，且采集过程不能影响正常业务的开展。基于这种需求，就衍生了多种日志收集工具，如 Flume 、Logstash、Kibana 等，它们都能通过简单的配置完成复杂的数据收集和数据聚合。### 1.2 数据存储收集到数据后，下一个问题就是：数据该如何进行存储？通常大家最为熟知是 MySQL、Oracle 等传统的关系型数据库，它们的优点是能够快速存储结构化的数据，并支持随机访问。但大数据的数据结构通常是半结构化（如日志数据）、甚至是非结构化的（如视频、音频数据），为了解决海量半结构化和非结构化数据的存储，衍生了 Hadoop HDFS 、KFS、GFS 等分布式文件系统，它们都能够支持结构化、半结构和非结构化数据的存储，并可以通过增加机器进行横向扩展。分布式文件系统完美地解决了海量数据存储的问题，但是一个优秀的数据存储系统需要同时考虑数据存储和访问两方面的问题，比如你希望能够对数据进行随机访问，这是传统的关系型数据库所擅长的，但却不是分布式文件系统所擅长的，那么有没有一种存储方案能够同时兼具分布式文件系统和关系型数据库的优点，基于这种需求，就产生了 HBase、MongoDB。### 1.3 数据分析大数据处理最重要的环节就是数据分析，数据分析通常分为两种：批处理和流处理。+ 批处理：对一段时间内海量的离线数据进行统一的处理，对应的处理框架有 Hadoop MapReduce、Spark、Flink 等；+ 流处理：对运动中的数据进行处理，即在接收数据的同时就对其进行处理，对应的处理框架有 Storm、Spark Streaming、Flink Streaming 等。批处理和流处理各有其适用的场景，时间不敏感或者硬件资源有限，可以采用批处理；时间敏感和及时性要求高就可以采用流处理。随着服务器硬件的价格越来越低和大家对及时性的要求越来越高，流处理越来越普遍，如股票价格预测和电商运营数据分析等。上面的框架都是需要通过编程来进行数据分析，那么如果你不是一个后台工程师，是不是就不能进行数据的分析了？当然不是，大数据是一个非常完善的生态圈，有需求就有解决方案。为了能够让熟悉 SQL 的人员也能够进行数据的分析，查询分析框架应运而生，常用的有 Hive 、Spark SQL 、Flink SQL、 Pig、Phoenix 等。这些框架都能够使用标准的 SQL 或者 类 SQL 语法灵活地进行数据的查询分析。这些 SQL 经过解析优化后转换为对应的作业程序来运行，如 Hive 本质上就是将 SQL 转换为 MapReduce 作业，Spark SQL 将 SQL 转换为一系列的 RDDs 和转换关系（transformations），Phoenix 将 SQL 查询转换为一个或多个 HBase Scan。### 1.4 数据应用数据分析完成后，接下来就是数据应用的范畴，这取决于你实际的业务需求。比如你可以将数据进行可视化展现，或者将数据用于优化你的推荐算法，这种运用现在很普遍，比如短视频个性化推荐、电商商品推荐、头条新闻推荐等。当然你也可以将数据用于训练你的机器学习模型，这些都属于其他领域的范畴，都有着对应的框架和技术栈进行处理，这里就不一一赘述。### 1.5 其他框架上面是一个标准的大数据处理流程所用到的技术框架。但是实际的大数据处理流程比上面复杂很多，针对大数据处理中的各种复杂问题分别衍生了各类框架：+ 单机的处理能力都是存在瓶颈的，所以大数据框架都是采用集群模式进行部署，为了更方便的进行集群的部署、监控和管理，衍生了 Ambari、Cloudera Manager 等集群管理工具；+ 想要保证集群高可用，需要用到 ZooKeeper ，ZooKeeper 是最常用的分布式协调服务，它能够解决大多数集群问题，包括首领选举、失败恢复、元数据存储及其一致性保证。同时针对集群资源管理的需求，又衍生了 Hadoop YARN ;+ 复杂大数据处理的另外一个显著的问题是，如何调度多个复杂的并且彼此之间存在依赖关系的作业？基于这种需求，产生了 Azkaban 和 Oozie 等工作流调度框架；+ 大数据流处理中使用的比较多的另外一个框架是 Kafka，它可以用于消峰，避免在秒杀等场景下并发数据对流处理程序造成冲击；+ 另一个常用的框架是 Sqoop ，主要是解决了数据迁移的问题，它能够通过简单的命令将关系型数据库中的数据导入到 HDFS 、Hive 或 HBase 中，或者从 HDFS 、Hive 导出到关系型数据库上。## 二、学习路线介绍完大数据框架，接着就可以介绍其对应的学习路线了，主要分为以下几个方面：### 2.1 语言基础#### 1. Java大数据框架大多采用 Java 语言进行开发，并且几乎全部的框架都会提供 Java API 。Java 是目前比较主流的后台开发语言，所以网上免费的学习资源也比较多。如果你习惯通过书本进行学习，这里推荐以下入门书籍：+ 《Java 编程的逻辑》：这里一本国人编写的系统入门 Java 的书籍，深入浅出，内容全面；+ 《Java 核心技术》：目前最新的是第 10 版，有卷一 和卷二 两册，卷二可以选择性阅读，因为其中很多章节的内容在实际开发中很少用到。目前大多数框架要求 Java 版本至少是 1.8，这是由于 Java 1.8 提供了函数式编程，使得可以用更精简的代码来实现之前同样的功能，比如你调用 Spark API，使用 1.8 可能比 1.7 少数倍的代码，所以这里额外推荐阅读 《Java 8 实战》 这本书籍。#### 2. ScalaScala 是一门综合了面向对象和函数式编程概念的静态类型的编程语言，它运行在 Java 虚拟机上，可以与所有的 Java 类库无缝协作，著名的 Kafka 就是采用 Scala 语言进行开发的。为什么需要学习 Scala 语言 ？ 这是因为当前最火的计算框架 Flink 和 Spark 都提供了 Scala 语言的接口，使用它进行开发，比使用 Java 8 所需要的代码更少，且 Spark 就是使用 Scala 语言进行编写的，学习 Scala 可以帮助你更深入的理解 Spark。同样的，对于习惯书本学习的小伙伴，这里推荐两本入门书籍：- 《快学 Scala(第 2 版)》- 《Scala 编程 (第 3 版)》&gt; 这里说明一下，如果你的时间有限，不一定要学完 Scala 才去学习大数据框架。Scala 确实足够的精简和灵活，但其在语言复杂度上略大于 Java，例如隐式转换和隐式参数等概念在初次涉及时会比较难以理解，所以你可以在了解 Spark 后再去学习 Scala，因为类似隐式转换等概念在 Spark 源码中有大量的运用。### 2.2 Linux 基础通常大数据框架都部署在 Linux 服务器上，所以需要具备一定的 Linux 知识。Linux 书籍当中比较著名的是 《鸟哥私房菜》系列，这个系列很全面也很经典。但如果你希望能够快速地入门，这里推荐《Linux 就该这么学》，其网站上有免费的电子书版本。### 2.3 构建工具这里需要掌握的自动化构建工具主要是 Maven。Maven 在大数据场景中使用比较普遍，主要在以下三个方面：+ 管理项目 JAR 包，帮助你快速构建大数据应用程序；+ 不论你的项目是使用 Java 语言还是 Scala 语言进行开发，提交到集群环境运行时，都需要使用 Maven 进行编译打包；+ 大部分大数据框架使用 Maven 进行源码管理，当你需要从其源码编译出安装包时，就需要使用到 Maven。### 2.4 框架学习#### 1. 框架分类上面我们介绍了很多大数据框架，这里进行一下分类总结：日志收集框架：Flume 、Logstash、Kibana分布式文件存储系统：Hadoop HDFS数据库系统：Mongodb、HBase分布式计算框架：+ 批处理框架：Hadoop MapReduce+ 流处理框架：Storm+ 混合处理框架：Spark、Flink查询分析框架：Hive 、Spark SQL 、Flink SQL、 Pig、Phoenix集群资源管理器：Hadoop YARN分布式协调服务：Zookeeper数据迁移工具：Sqoop任务调度框架：Azkaban、Oozie集群部署和监控：Ambari、Cloudera Manager上面列出的都是比较主流的大数据框架，社区都很活跃，学习资源也比较丰富。建议从 Hadoop 开始入门学习，因为它是整个大数据生态圈的基石，其它框架都直接或者间接依赖于 Hadoop 。接着就可以学习计算框架，Spark 和 Flink 都是比较主流的混合处理框架，Spark 出现得较早，所以其应用也比较广泛。 Flink 是当下最火热的新一代的混合处理框架，其凭借众多优异的特性得到了众多公司的青睐。两者可以按照你个人喜好或者实际工作需要进行学习。 图片引用自 ：https://www.edureka.co/blog/hadoop-ecosystem 至于其它框架，在学习上并没有特定的先后顺序，如果你的学习时间有限，建议初次学习时候，同一类型的框架掌握一种即可，比如日志收集框架就有很多种，初次学习时候只需要掌握一种，能够完成日志收集的任务即可，之后工作上有需要可以再进行针对性地学习。 2. 学习资料大数据最权威和最全面的学习资料就是官方文档。热门的大数据框架社区都比较活跃、版本更新迭代也比较快，所以其出版物都明显滞后于其实际版本，基于这个原因采用书本学习不是一个最好的方案。比较庆幸的是，大数据框架的官方文档都写的比较好，内容完善，重点突出，同时都采用了大量配图进行辅助讲解。当然也有一些优秀的书籍历经时间的检验，至今依然很经典，这里列出部分个人阅读过的经典书籍： 《hadoop 权威指南 (第四版)》 2017 年 《Kafka 权威指南》 2017 年 《从 Paxos 到 Zookeeper 分布式一致性原理与实践》 2015 年 《Spark 技术内幕 深入解析 Spark 内核架构设计与实现原理》 2015 年 《Spark.The.Definitive.Guide》 2018 年 《HBase 权威指南》 2012 年 《Hive 编程指南》 2013 年 3. 视频学习资料上面我推荐的都是书籍学习资料，很少推荐视频学习资料，这里说明一下原因：因为书籍历经时间的考验，能够再版的或者豆瓣等平台评价高的证明都是被大众所认可的，从概率的角度上来说，其必然更加优秀，不容易浪费大家的学习时间和精力，所以我个人更倾向于官方文档或者书本的学习方式，而不是视频。因为视频学习资料，缺少一个公共的评价平台和完善的评价机制，所以其质量良莠不齐。但是视频任然有其不可替代的好处，学习起来更直观、印象也更深刻，所以对于习惯视频学习的小伙伴，这里我各推荐一个免费的和付费的视频学习资源，大家按需选择： 免费学习资源：尚硅谷大数据学习路线 —— 下载链接 \ 在线观看链接 付费学习资源：慕课网 Michael PK 的系列课程 三、开发工具这里推荐一些大数据常用的开发工具： Java IDE：IDEA 和 Eclipse 都可以。从个人使用习惯而言，更倾向于 IDEA ; VirtualBox：在学习过程中，你可能经常要在虚拟机上搭建服务和集群。VirtualBox 是一款开源、免费的虚拟机管理软件，虽然是轻量级软件，但功能很丰富，基本能够满足日常的使用需求； MobaXterm：大数据的框架通常都部署在服务器上，这里推荐使用 MobaXterm 进行连接。同样是免费开源的，支持多种连接协议，支持拖拽上传文件，支持使用插件扩展； Translate Man：一款浏览器上免费的翻译插件 (谷歌和火狐均支持)。它采用谷歌的翻译接口，准确性非常高，支持划词翻译，可以辅助进行官方文档的阅读。 四、结语以上就是个人关于大数据的学习心得和路线推荐。本片文章对大数据技术栈做了比较狭义的限定，随着学习的深入，大家也可以把 Python 语言、推荐系统、机器学习等逐步加入到自己的大数据技术栈中。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>学习路线</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据技术栈思维导图]]></title>
    <url>%2F2019%2F03%2F10%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8A%80%E6%9C%AF%E6%A0%88%E6%80%9D%E7%BB%B4%E5%AF%BC%E5%9B%BE%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>思维导图</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之虚拟机静态IP及多IP配置]]></title>
    <url>%2F2019%2F03%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8B%E8%99%9A%E6%8B%9F%E6%9C%BA%E9%9D%99%E6%80%81IP%E5%8F%8A%E5%A4%9AIP%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[虚拟机静态IP及多IP配置一、虚拟机静态IP配置1. 编辑网络配置文件2. 重启网络服务二、虚拟机多个静态IP配置1. 配置多网卡2. 查看网卡名称3. 配置第二块网卡4. 重启网络服务器5. 使用说明 一、虚拟机静态IP配置1. 编辑网络配置文件1# vim /etc/sysconfig/network-scripts/ifcfg-enp0s3 添加如下网络配置： IPADDR 需要和宿主机同一个网段； GATEWAY 保持和宿主机一致； 123456BOOTPROTO=staticIPADDR=192.168.0.107NETMASK=255.255.255.0GATEWAY=192.168.0.1DNS1=192.168.0.1ONBOOT=yes 我的主机配置： 修改后完整配置如下： 1234567891011121314151617181920TYPE=EthernetPROXY_METHOD=noneBROWSER_ONLY=noBOOTPROTO=staticIPADDR=192.168.0.107NETMASK=255.255.255.0GATEWAY=192.168.0.1BROADCAST=192.168.0.255DNS1=192.168.0.1DEFROUTE=yesIPV4_FAILURE_FATAL=noIPV6INIT=yesIPV6_AUTOCONF=yesIPV6_DEFROUTE=yesIPV6_FAILURE_FATAL=noIPV6_ADDR_GEN_MODE=stable-privacyNAME=enp0s3UUID=03d45df1-8514-4774-9b47-fddd6b9d9fcaDEVICE=enp0s3ONBOOT=yes 2. 重启网络服务1# systemctl restart network 二、虚拟机多个静态IP配置如果一台虚拟机需要经常在不同网络环境使用，可以配置多个静态 IP。 1. 配置多网卡这里我是用的虚拟机是 virtualBox，开启多网卡配置方式如下： 2. 查看网卡名称使用 ifconfig，查看第二块网卡名称，这里我的名称为 enp0s8： 3. 配置第二块网卡开启多网卡后并不会自动生成配置文件，需要拷贝 ifcfg-enp0s3 进行修改： 1# cp ifcfg-enp0s3 ifcfg-enp0s8 静态 IP 配置方法如上，这里不再赘述。除了静态 IP 参数外，以下三个参数还需要修改，UUID 必须与 ifcfg-enp0s3 中的不一样： 123NAME=enp0s8UUID=03d45df1-8514-4774-9b47-fddd6b9d9fcbDEVICE=enp0s8 4. 重启网络服务器1# systemctl restart network 5. 使用说明使用时只需要根据所处的网络环境，勾选对应的网卡即可，不使用的网卡尽量不要勾选启动。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>静态IP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Linux下Python安装]]></title>
    <url>%2F2019%2F03%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BLinux%E4%B8%8BPython%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Linux下Python安装 系统环境：centos 7.6 Python 版本：Python-3.6.8 1. 环境依赖Python3.x 的安装需要依赖这四个组件：gcc， zlib，zlib-devel，openssl-devel；所以需要预先安装，命令如下： 1234yum install gcc -yyum install zlib -yyum install zlib-devel -yyum install openssl-devel -y 2. 下载编译Python 源码包下载地址： https://www.python.org/downloads/ 1# wget https://www.python.org/ftp/python/3.6.8/Python-3.6.8.tgz 3. 解压编译1# tar -zxvf Python-3.6.8.tgz 进入根目录进行编译，可以指定编译安装的路径，这里我们指定为 /usr/app/python3.6 ： 123# cd Python-3.6.8# ./configure --prefix=/usr/app/python3.6# make &amp;&amp; make install 4. 环境变量配置1vim /etc/profile 12export PYTHON_HOME=/usr/app/python3.6export PATH=$&#123;PYTHON_HOME&#125;/bin:$PATH 使得配置的环境变量立即生效： 1source /etc/profile 5. 验证安装是否成功输入 python3 命令，如果能进入 python 交互环境，则代表安装成功： 12345678[root@hadoop001 app]# python3Python 3.6.8 (default, Mar 29 2019, 10:17:41)[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linuxType "help", "copyright", "credits" or "license" for more information.&gt;&gt;&gt; 1+12&gt;&gt;&gt; exit()[root@hadoop001 app]#]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Python安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据之Linux下JDK安装]]></title>
    <url>%2F2019%2F03%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%B9%8BLinux%E4%B8%8BJDK%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[Linux下JDK的安装 系统环境：centos 7.6 JDK 版本：jdk 1.8.0_20 1. 下载并解压在官网 下载所需版本的 JDK，这里我下载的版本为JDK 1.8 ,下载后进行解压： 1[root@ java]# tar -zxvf jdk-8u201-linux-x64.tar.gz 2. 设置环境变量1[root@ java]# vi /etc/profile 添加如下配置： 1234export JAVA_HOME=/usr/java/jdk1.8.0_201 export JRE_HOME=$&#123;JAVA_HOME&#125;/jre export CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/lib export PATH=$&#123;JAVA_HOME&#125;/bin:$PATH 执行 source 命令，使得配置立即生效： 1[root@ java]# source /etc/profile 3. 检查是否安装成功1[root@ java]# java -version 显示出对应的版本信息则代表安装成功。 123java version "1.8.0_201"Java(TM) SE Runtime Environment (build 1.8.0_201-b09)Java HotSpot(TM) 64-Bit Server VM (build 25.201-b09, mixed mode)]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>JDK安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Zookeeper系列——Zookeeper面试题]]></title>
    <url>%2F2018%2F06%2F30%2F%E5%A4%A7%E6%95%B0%E6%8D%AEZookeeper%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Zookeeper%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Zookeeper面试题总结1、请简述Zookeeper的选举机制假设有五台服务器组成的zookeeper集群，它们的id从1-5，同时它们都是最新启动的，也就是没有历史数据，在存放数据量这一点上，都是一样的。假设这些服务器依序启动，来看看会发生什么。 （1）服务器1启动，此时只有它一台服务器启动了，它发出去的报没有任何响应，所以它的选举状态一直是LOOKING状态。（2）服务器2启动，它与最开始启动的服务器1进行通信，互相交换自己的选举结果，由于两者都没有历史数据，所以id值较大的服务器2胜出，但是由于没有达到超过半数以上的服务器都同意选举它(这个例子中的半数以上是3)，所以服务器1、2还是继续保持LOOKING状态。（3）服务器3启动，根据前面的理论分析，服务器3成为服务器1、2、3中的Leader，而与上面不同的是，此时有三台服务器选举了它，所以它成为了这次选举的Leader。（4）服务器4启动，根据前面的分析，理论上服务器4应该是服务器1、2、3、4中最大的，但是由于前面已经有半数以上的服务器选举了服务器3，所以它成为Follower。（5）服务器5启动，同4一样成为Follower。注意，如果按照5,4,3,2,1的顺序启动，那么5将成为Leader，因为在满足半数条件后，ZooKeeper集群启动，5的Id最大，被选举为Leader。 2、客户端如何正确处理CONNECTIONLOSS(连接断开) 和 SESSIONEXPIRED(Session 过期)两类连接异常？在ZooKeeper中，服务器和客户端之间维持的是一个长连接，在 SESSION_TIMEOUT 时间内，服务器会确定客户端是否正常连接(客户端会定时向服务器发送heart_beat),服务器重置下次SESSION_TIMEOUT时间。因此，在正常情况下，Session一直有效，并且zk集群所有机器上都保存这个Session信息。在出现问题的情况下，客户端与服务器之间连接断了（客户端所连接的那台zk机器挂了，或是其它原因的网络闪断），这个时候客户端会主动在地址列表（初始化的时候传入构造方法的那个参数connectString）中选择新的地址进行连接。以上即为服务器与客户端之间维持长连接的过程，在这个过程中，用户可能会看到两类异常CONNECTIONLOSS(连接断开) 和SESSIONEXPIRED(Session 过期)。发生CONNECTIONLOSS后，此时用户不需要关心我的会话是否可用，应用所要做的就是等待客户端帮我们自动连接上新的zk机器，一旦成功连接上新的zk机器后，确认之前的操作是否执行成功了。 3、一个客户端修改了某个节点的数据，其他客户端能够马上获取到这个最新数据吗？ZooKeeper不能确保任何客户端能够获取（即Read Request）到一样的数据，除非客户端自己要求，方法是客户端在获取数据之前调用org.apache.zookeeper.AsyncCallbac k.VoidCallback, java.lang.Object) sync。通常情况下（这里所说的通常情况满足：1. 对获取的数据是否是最新版本不敏感，2. 一个客户端修改了数据，其它客户端是否需要立即能够获取最新数据），可以不关心这点。在其它情况下，最清晰的场景是这样：ZK客户端A对 /my_test 的内容从 v1-&gt;v2, 但是ZK客户端B对 /my_test 的内容获取，依然得到的是 v1. 请注意，这个是实际存在的现象，当然延时很短。解决的方法是客户端B先调用 sync(), 再调用 getData()。 4、ZooKeeper对节点的watch监听是永久的吗？为什么？不是。官方声明：一个Watch事件是一个一次性的触发器，当被设置了Watch的数据发生了改变的时候，则服务器将这个改变发送给设置了Watch的客户端，以便通知它们。为什么不是永久的，举个例子，如果服务端变动频繁，而监听的客户端很多情况下，每次变动都要通知到所有的客户端，这太消耗性能了。一般是客户端执行getData(“/节点A”,true)，如果节点A发生了变更或删除，客户端会得到它的watch事件，但是在之后节点A又发生了变更，而客户端又没有设置watch事件，就不再给客户端发送。在实际应用中，很多情况下，我们的客户端不需要知道服务端的每一次变动，我只要最新的数据即可。 5、ZooKeeper中使用watch的注意事项有哪些？使用watch需要注意的几点：1）Watches通知是一次性的，必须重复注册。2）发生CONNECTIONLOSS之后，只要在session_timeout之内再次连接上（即不发生SESSIONEXPIRED），那么这个连接注册的watches依然在。3）节点数据的版本变化会触发NodeDataChanged，注意，这里特意说明了是版本变化。存在这样的情况，只要成功执行了setData()方法，无论内容是否和之前一致，都会触发NodeDataChanged。4）对某个节点注册了watch，但是节点被删除了，那么注册在这个节点上的watches都会被移除。5）同一个zk客户端对某一个节点注册相同的watch，只会收到一次通知。6）Watcher对象只会保存在客户端，不会传递到服务端。 6、能否收到每次节点变化的通知？如果节点数据的更新频率很高的话，不能。原因在于：当一次数据修改，通知客户端，客户端再次注册watch，在这个过程中，可能数据已经发生了许多次数据修改，因此，千万不要做这样的测试：”数据被修改了n次，一定会收到n次通知”来测试server是否正常工作。 7、能否为临时节点创建子节点？ZooKeeper中不能为临时节点创建子节点，如果需要创建子节点，应该将要创建子节点的节点创建为永久性节点。 8、是否可以拒绝单个IP对ZooKeeper的访问？如何实现？ZK本身不提供这样的功能，它仅仅提供了对单个IP的连接数的限制。你可以通过修改iptables来实现对单个ip的限制。 9、创建的临时节点什么时候会被删除，是连接一断就删除吗？延时是多少？连接断了之后，ZK不会马上移除临时数据，只有当SESSIONEXPIRED之后，才会把这个会话建立的临时数据移除。因此，用户需要谨慎设置Session_TimeOut。 10、ZooKeeper是否支持动态进行机器扩容？如果目前不支持，那么要如何扩容呢？ZooKeeper中的动态扩容其实就是水平扩容，Zookeeper对这方面的支持不太好，目前有两种方式：全部重启：关闭所有Zookeeper服务，修改配置之后启动，不影响之前客户端的会话。逐个重启：这是比较常用的方式。 11、ZooKeeper集群中服务器之间是怎样通信的？Leader服务器会和每一个Follower/Observer服务器都建立TCP连接，同时为每个F/O都创建一个叫做LearnerHandler的实体。LearnerHandler主要负责Leader和F/O之间的网络通讯，包括数据同步，请求转发和Proposal提议的投票等。Leader服务器保存了所有F/O的LearnerHandler。 12、ZooKeeper是否会自动进行日志清理？如何进行日志清理？zk自己不会进行日志清理，需要运维人员进行日志清理。 13、谈谈你对ZooKeeper的理解？Zookeeper 作为一个分布式的服务框架，主要用来解决分布式集群中应用系统的一致性问题。ZooKeeper提供的服务包括：分布式消息同步和协调机制、服务器节点动态上下线、统一配置管理、负载均衡、集群管理等。ZooKeeper提供基于类似于Linux文件系统的目录节点树方式的数据存储，即分层命名空间。Zookeeper 并不是用来专门存储数据的，它的作用主要是用来维护和监控你存储的数据的状态变化，通过监控这些数据状态的变化，从而可以达到基于数据的集群管理，ZooKeeper节点的数据上限是1MB。我们可以认为Zookeeper=文件系统+通知机制，对于ZooKeeper的数据结构，每个子目录项如 NameService 都被称作为 znode，这个 znode 是被它所在的路径唯一标识，如 Server1 这个 znode 的标识为 /NameService/Server1；znode 可以有子节点目录，并且每个 znode 可以存储数据，注意 EPHEMERAL 类型的目录节点不能有子节点目录(因为它是临时节点)；znode 是有版本的，每个 znode 中存储的数据可以有多个版本，也就是一个访问路径中可以存储多份数据；znode 可以是临时节点，一旦创建这个 znode 的客户端与服务器失去联系，这个 znode 也将自动删除，Zookeeper 的客户端和服务器通信采用长连接方式，每个客户端和服务器通过心跳来保持连接，这个连接状态称为 session，如果 znode 是临时节点，这个 session 失效，znode 也就删除了；znode 的目录名可以自动编号，如 App1 已经存在，再创建的话，将会自动命名为 App2；znode 可以被监控，包括这个目录节点中存储的数据的修改，子节点目录的变化等，一旦变化可以通知设置监控的客户端，这个是 Zookeeper 的核心特性，Zookeeper 的很多功能都是基于这个特性实现的，后面在典型的应用场景中会有实例介绍。 14、ZooKeeper节点类型？1）Znode有两种类型：短暂（ephemeral）：客户端和服务器端断开连接后，创建的节点自己删除。持久（persistent）：客户端和服务器端断开连接后，创建的节点不删除。2）Znode有四种形式的目录节点（默认是persistent ）（1）持久化目录节点（PERSISTENT） 客户端与zookeeper断开连接后，该节点依旧存在。（2）持久化顺序编号目录节点（PERSISTENT_SEQUENTIAL） 客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。（3）临时目录节点（EPHEMERAL） 客户端与zookeeper断开连接后，该节点被删除。（4）临时顺序编号目录节点（EPHEMERAL_SEQUENTIAL）客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 15、请说明ZooKeeper的通知机制？ZooKeeper选择了基于通知（notification）的机制，即：客户端向ZooKeeper注册需要接受通知的znode，通过znode设置监控点（watch）来接受通知。监视点是一个单次触发的操作，意即监视点会触发一个通知。为了接收多个通知，客户端必须在每次通知后设置一个新的监视点。在下图阐述的情况下，当节点/task发生变化时，客户端会受到一个通知，并从ZooKeeper读取一个新值。 16、ZooKeeper的监听原理是什么？在应用程序中，mian()方法首先会创建zkClient，创建zkClient的同时就会产生两个进程，即Listener进程（监听进程）和connect进程（网络连接/传输进程），当zkClient调用getChildren()等方法注册监视器时，connect进程向ZooKeeper注册监听器，注册后的监听器位于ZooKeeper的监听器列表中，监听器列表中记录了zkClient的IP，端口号以及要监控的路径，一旦目标文件发生变化，ZooKeeper就会把这条消息发送给对应的zkClient的Listener()进程，Listener进程接收到后，就会执行process()方法，在process()方法中针对发生的事件进行处理。 17、请说明ZooKeeper使用到的各个端口的作用？2888：Follower与Leader交换信息的端口。3888：万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 18、ZooKeeper的部署方式有哪几种？集群中的角色有哪些？集群最少需要几台机器？ZooKeeper的部署方式有单机模式和集群模式，集群中的角色有Leader和Follower，集群最少3（2N+1）台，根据选举算法，应保证奇数。 19、ZooKeeper集群如果有3台机器，挂掉一台是否还能工作？挂掉两台呢？对于ZooKeeper集群，过半存活即可使用。 20、ZooKeeper使用的ZAB协议与Paxo算法的异同？Paxos算法是分布式选举算法，Zookeeper使用的 ZAB协议（Zookeeper原子广播），两者的异同如下：1）相同之处：比如都有一个Leader，用来协调N个Follower的运行；Leader要等待超半数的Follower做出正确反馈之后才进行提案；二者都有一个值来代表Leader的周期。2）不同之处：ZAB用来构建高可用的分布式数据主备系统（Zookeeper），Paxos是用来构建分布式一致性状态机系统。 21、请谈谈对ZooKeeper对事务性的支持？ZooKeeper对于事务性的支持主要依赖于四个函数，zoo_create_op_init， zoo_delete_op_init， zoo_set_op_init以及zoo_check_op_init。每一个函数都会在客户端初始化一个operation，客户端程序有义务保留这些operations。当准备好一个事务中的所有操作后，可以使用zoo_multi来提交所有的操作，由zookeeper服务来保证这一系列操作的原子性。也就是说只要其中有一个操作失败了，相当于此次提交的任何一个操作都没有对服务端的数据造成影响。Zoo_multi的返回值是第一个失败操作的状态信号。]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Zookeeper系列——Zookeeper简介]]></title>
    <url>%2F2018%2F06%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AEZookeeper%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Zookeeper%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Zookeeper介绍Zookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。 1、工作机制Zookeeper从设计模式角度来理解：是一个基于观察者模式设计的分布式服务管理框架，它负责存储和管理大家都关心的数据，然后接受观察者的注册，一旦这些数据的状态发生变化，Zookeeper就将负责通知已经在Zookeeper上注册的那些观察者做出相应的反应，从而实现集群中类似Master/Slave管理模式Zookeeper=文件系统（配置文件保存）+通知机制（在ZK注册多个节点的共同配置文件保存在ZK，当配置文件被修改后，ZK会通知每个节点做出反应） 2、特点 1）Zookeeper：一个领导者（leader），多个跟随者（follower）组成的集群。2）Leader：负责进行投票的发起和决议，更新系统状态。3）Follower：用于接收客户请求并向客户端返回结果，在选举Leader过程中参与投票。4）集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。5）全局数据一致：每个server保存一份相同的数据副本，client无论连接到哪个server，数据都是一致的。6）更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。7）数据更新原子性，一次数据更新要么成功，要么失败。8）实时性，在一定时间范围内，client能读到最新数据。 3、数据结构ZooKeeper数据模型的结构与Unix文件系统很类似，整体上可以看作是一棵树，每个节点称做一个ZNode。每一个znode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。 4、应用场景提供的服务包括：统一命名服务、统一配置管理、统一集群管理、服务器节点动态上下线、软负载均衡等。 1）统一命名服务在分布式环境下，经常需要对应用/服务进行统一命名，便于识别不同服务。（1）类似于域名与ip之间对应关系，ip不容易记住，而域名容易记住。（2）通过名称来获取资源或服务的地址，提供者等信息。 2）统一配置管理分布式环境下，配置文件管理和同步是一个常见问题。（1）一个集群中，所有节点的配置信息是一致的，比如 Hadoop 集群。（2）对配置文件修改后，希望能够快速同步到各个节点上。配置管理可交由ZooKeeper实现。（1）可将配置信息写入ZooKeeper上的一个Znode。（2）各个节点监听这个Znode。（3）一旦Znode中的数据被修改，ZooKeeper将通知各个节点。 3）统一集群管理分布式环境中，实时掌握每个节点的状态是必要的。（1）可根据节点实时状态做出一些调整。可交由ZooKeeper实现。（1）可将节点信息写入ZooKeeper上的一个Znode。（2）监听这个Znode可获取它的实时状态变化。典型应用（1）HBase中Master状态监控与选举。 4）服务器动态上下线客户端能实时洞察到服务器上下线的变化 5）软负载均衡在Zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求]]></content>
      <categories>
        <category>Zookeeper</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Kafka系列——Kafka面试题（二）]]></title>
    <url>%2F2018%2F06%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AEKafka%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Kafka%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Kafka面试题整理（二）1、请说明什么是Apache Kafka？Apache Kafka是由Apache开发的一种发布订阅消息系统，它是一个分布式的、分区的和重复的日志服务。 2、请说明什么是传统的消息传递方法？传统的消息传递方法包括两种：队列：在队列中，一组用户可以从服务器中读取消息，每条消息都发送给其中一个人。发布-订阅：在这个模型中，消息被广播给所有的用户。 3、请说明Kafka相对于传统的消息传递方法有什么优势？高性能：单一的Kafka代理可以处理成千上万的客户端，每秒处理数兆字节的读写操作，Kafka性能远超过传统的ActiveMQ、RabbitMQ等，而且Kafka支持Batch操作；可扩展：Kafka集群可以透明的扩展，增加新的服务器进集群；容错性： Kafka每个Partition数据会复制到几台服务器，当某个Broker失效时，Zookeeper将通知生产者和消费者从而使用其他的Broker。 4、在Kafka中broker的意义是什么？在Kafka集群中，broker指Kafka服务器。术语解析： 5、Kafka服务器能接收到的最大信息是多少？Kafka服务器可以接收到的消息的最大大小是1000000字节。 6、Kafka中的ZooKeeper是什么？Kafka是否可以脱离ZooKeeper独立运行？Zookeeper是一个开放源码的、高性能的协调服务，它用于Kafka的分布式应用。不可以，不可能越过Zookeeper直接联系Kafka broker，一旦Zookeeper停止工作，它就不能服务客户端请求。Zookeeper主要用于在集群中不同节点之间进行通信，在Kafka中，它被用于提交偏移量，因此如果节点在任何情况下都失败了，它都可以从之前提交的偏移量中获取，除此之外，它还执行其他活动，如: leader检测、分布式同步、配置管理、识别新节点何时离开或连接、集群、节点实时状态等等。 7、解释Kafka的用户如何消费信息？在Kafka中传递消息是通过使用sendfile API完成的。它支持将字节Socket转移到磁盘，通过内核空间保存副本，并在内核用户之间调用内核。 8、解释如何提高远程用户的吞吐量？如果用户位于与broker不同的数据中心，则可能需要调优Socket缓冲区大小，以对长网络延迟进行摊销。 9、解释一下，在数据制作过程中，你如何能从Kafka得到准确的信息？在数据中，为了精确地获得Kafka的消息，你必须遵循两件事：在数据消耗期间避免重复，在数据生产过程中避免重复。这里有两种方法，可以在数据生成时准确地获得一个语义:每个分区使用一个单独的写入器，每当你发现一个网络错误，检查该分区中的最后一条消息，以查看您的最后一次写入是否成功。在消息中包含一个主键(UUID或其他)，并在用户中进行反复制。 10、解释如何减少ISR中的扰动？broker什么时候离开ISR？（☆☆☆☆☆）ISR是一组与leaders完全同步的消息副本，也就是说ISR中包含了所有提交的消息。ISR应该总是包含所有的副本，直到出现真正的故障。如果一个副本从leader中脱离出来，将会从ISR中删除。 11、Kafka为什么需要复制？Kafka的信息复制确保了任何已发布的消息不会丢失，并且可以在机器错误、程序错误或更常见些的软件升级中使用。 12、如果副本在ISR中停留了很长时间表明什么？如果一个副本在ISR中保留了很长一段时间，那么它就表明，跟踪器无法像在leader收集数据那样快速地获取数据。 13、请说明如果首选的副本不在ISR中会发生什么？如果首选的副本不在ISR中，控制器将无法将leadership转移到首选的副本。 14、Kafka有可能在生产后发生消息偏移吗？在大多数队列系统中，作为生产者的类无法做到这一点，它的作用是触发并忘记消息。broker将完成剩下的工作，比如使用id进行适当的元数据处理、偏移量等。作为消息的用户，你可以从Kafka broker中获得补偿。如果你注视SimpleConsumer类，你会注意到它会获取包括偏移量作为列表的MultiFetchResponse对象。此外，当你对Kafka消息进行迭代时，你会拥有包括偏移量和消息发送的MessageAndOffset对象。 15、请说明Kafka 的消息投递保证（delivery guarantee）机制以及如何实现？（☆☆☆☆☆）Kafka支持三种消息投递语义：① At most once 消息可能会丢，但绝不会重复传递② At least one 消息绝不会丢，但可能会重复传递③ Exactly once 每条消息肯定会被传输一次且仅传输一次，很多时候这是用户想要的consumer在从broker读取消息后，可以选择commit，该操作会在Zookeeper中存下该consumer在该partition下读取的消息的offset，该consumer下一次再读该partition时会从下一条开始读取。如未commit，下一次读取的开始位置会跟上一次commit之后的开始位置相同。可以将consumer设置为autocommit，即consumer一旦读到数据立即自动commit。如果只讨论这一读取消息的过程，那Kafka是确保了Exactly once。但实际上实际使用中consumer并非读取完数据就结束了，而是要进行进一步处理，而数据处理与commit的顺序在很大程度上决定了消息从broker和consumer的delivery guarantee semantic。读完消息先commit再处理消息。这种模式下，如果consumer在commit后还没来得及处理消息就crash了，下次重新开始工作后就无法读到刚刚已提交而未处理的消息，这就对应于At most once。读完消息先处理再commit消费状态(保存offset)。这种模式下，如果在处理完消息之后commit之前Consumer crash了，下次重新开始工作时还会处理刚刚未commit的消息，实际上该消息已经被处理过了，这就对应于At least once。如果一定要做到Exactly once，就需要协调offset和实际操作的输出。经典的做法是引入两阶段提交，但由于许多输出系统不支持两阶段提交，更为通用的方式是将offset和操作输入存在同一个地方。比如，consumer拿到数据后可能把数据放到HDFS，如果把最新的offset和数据本身一起写到HDFS，那就可以保证数据的输出和offset的更新要么都完成，要么都不完成，间接实现Exactly once。（目前就high level API而言，offset是存于Zookeeper中的，无法存于HDFS，而low level API的offset是由自己去维护的，可以将之存于HDFS中）。总之，Kafka默认保证At least once，并且允许通过设置producer异步提交来实现At most once，而Exactly once要求与目标存储系统协作，Kafka提供的offset可以较为容易地实现这种方式。 16、如何保证Kafka的消息有序（☆☆☆☆☆）Kafka对于消息的重复、丢失、错误以及顺序没有严格的要求。Kafka只能保证一个partition中的消息被某个consumer消费时是顺序的，事实上，从Topic角度来说，当有多个partition时，消息仍然不是全局有序的。 17、kafka数据丢失问题,及如何保证？1）数据丢失：acks=1的时候(只保证写入leader成功)，如果刚好leader挂了。数据会丢失。acks=0的时候，使用异步模式的时候，该模式下kafka无法保证消息，有可能会丢。2）brocker如何保证不丢失：acks=all : 所有副本都写入成功并确认。retries = 一个合理值。min.insync.replicas=2 消息至少要被写入到这么多副本才算成功。unclean.leader.election.enable=false 关闭unclean leader选举，即不允许非ISR中的副本被选举为leader，以避免数据丢失。3）Consumer如何保证不丢失如果在消息处理完成前就提交了offset，那么就有可能造成数据的丢失。enabel.auto.commit=false关闭自动提交offset处理完数据之后手动提交。 18、kafka的balance是怎么做的？生产者将数据发布到他们选择的主题。生产者可以选择在主题中分配哪个分区的消息。这可以通过循环的方式来完成，只是为了平衡负载，或者可以根据一些语义分区功能（比如消息中的一些键）来完成。更多关于分区在一秒钟内的使用。 19、kafka的消费者方式？consumer采用pull（拉）模式从broker中读取数据。push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。对于Kafka而言，pull模式更合适，它可简化broker的设计，consumer可自主控制消费消息的速率，同时consumer可以自己控制消费方式——即可批量消费也可逐条消费，同时还能选择不同的提交方式从而实现不同的传输语义。pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直等待数据到达。为了避免这种情况，我们在我们的拉请求中有参数，允许消费者请求在等待数据到达的“长轮询”中进行阻塞。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Kafka系列——Kafka面试题（一）]]></title>
    <url>%2F2018%2F06%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AEKafka%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Kafka%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Kafka面试题总结（一）1、Kafka 都有哪些特点？高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic可以分多个partition, consumer group 对partition进行consume操作。 可扩展性：kafka集群支持热扩展持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）高并发：支持数千个客户端同时读写 2、请简述下你在哪些场景下会选择 Kafka？日志收集：一个公司可以用Kafka可以收集各种服务的log，通过kafka以统一接口服务的方式开放给各种consumer，例如hadoop、HBase、Solr等。消息系统：解耦和生产者和消费者、缓存消息等。用户活动跟踪：Kafka经常被用来记录web用户或者app用户的各种活动，如浏览网页、搜索、点击等活动，这些活动信息被各个服务器发布到kafka的topic中，然后订阅者通过订阅这些topic来做实时的监控分析，或者装载到hadoop、数据仓库中做离线分析和挖掘。运营指标：Kafka也经常用来记录运营监控数据。包括收集各种分布式应用的数据，生产各种操作的集中反馈，比如报警和报告。流式处理：比如spark streaming和 Flink 3、Kafka 的设计架构？简单架构如下： 详细架构如下： Kafka 架构分为以下几个部分：Producer：消息生产者，就是向 kafka broker 发消息的客户端。Consumer：消息消费者，向 kafka broker 取消息的客户端。Topic：可以理解为一个队列，一个 Topic 又分为一个或多个分区。Consumer Group：这是 kafka 用来实现一个 topic 消息的广播（发给所有的 consumer）和单播（发给任意一个 consumer）的手段。一个 topic 可以有多个 Consumer Group。Broker：一台 kafka 服务器就是一个 broker。一个集群由多个 broker 组成。一个 broker 可以容纳多个 topic。Partition：为了实现扩展性，一个非常大的 topic 可以分布到多个 broker上，每个 partition 是一个有序的队列。partition 中的每条消息都会被分配一个有序的id（offset）。将消息发给 consumer，kafka 只保证按一个 partition 中的消息的顺序，不保证一个 topic 的整体（多个 partition 间）的顺序。Offset：kafka 的存储文件都是按照 offset.kafka 来命名，用 offset 做名字的好处是方便查找。例如你想找位于 2049 的位置，只要找到 2048.kafka 的文件即可。当然 the first offset 就是 00000000000.kafka。 4、Kafka 分区的目的？分区对于 Kafka 集群的好处是：实现负载均衡。分区对于消费者来说，可以提高并发度，提高效率。 5、Kafka 是如何做到消息的有序性？kafka 中的每个 partition 中的消息在写入时都是有序的，而且单独一个 partition 只能由一个消费者去消费，可以在里面保证消息的顺序性。但是分区之间的消息是不保证有序的。 6、Kafka 的高可靠性是怎么实现的？可回答：Kafka 在什么情况下会出现消息丢失？1）数据可靠性（可回答 怎么尽可能保证 Kafka 的可靠性？）Kafka 作为一个商业级消息中间件，消息可靠性的重要性可想而知。本文从 Producter 往 Broker 发送消息、Topic 分区副本以及 Leader 选举几个角度介绍数据的可靠性。Topic分区副本在 Kafka 0.8.0 之前，Kafka 是没有副本的概念的，那时候人们只会用 Kafka 存储一些不重要的数据，因为没有副本，数据很可能会丢失。但是随着业务的发展，支持副本的功能越来越强烈，所以为了保证数据的可靠性，Kafka 从 0.8.0 版本开始引入了分区副本（详情请参见 KAFKA-50）。也就是说每个分区可以人为的配置几个副本（比如创建主题的时候指定 replication-factor，也可以在 Broker 级别进行配置 default.replication.factor），一般会设置为3。Kafka 可以保证单个分区里的事件是有序的，分区可以在线（可用），也可以离线（不可用）。在众多的分区副本里面有一个副本是 Leader，其余的副本是 follower，所有的读写操作都是经过 Leader 进行的，同时 follower 会定期地去 leader 上的复制数据。当 Leader 挂了的时候，其中一个 follower 会重新成为新的 Leader。通过分区副本，引入了数据冗余，同时也提供了 Kafka 的数据可靠性。Kafka 的分区多副本架构是 Kafka 可靠性保证的核心，把消息写入多个副本可以使 Kafka 在发生崩溃时仍能保证消息的持久性。Producer 往 Broker 发送消息如果我们要往 Kafka 对应的主题发送消息，我们需要通过 Producer 完成。前面我们讲过 Kafka 主题对应了多个分区，每个分区下面又对应了多个副本；为了让用户设置数据可靠性， Kafka 在 Producer 里面提供了消息确认机制。也就是说我们可以通过配置来决定消息发送到对应分区的几个副本才算消息发送成功。可以在定义 Producer 时通过 acks 参数指定（在 0.8.2.X 版本之前是通过 request.required.acks 参数设置的）。这个参数支持以下三种值：acks = 0：意味着如果生产者能够通过网络把消息发送出去，那么就认为消息已成功写入Kafka。在这种情况下还是有可能发生错误，比如发送的对象无能被序列化或者网卡发生故障，但如果是分区离线或整个集群长时间不可用，那就不会收到任何错误。在 acks=0 模式下的运行速度是非常快的（这就是为什么很多基准测试都是基于这个模式），你可以得到惊人的吞吐量和带宽利用率，不过如果选择了这种模式， 一定会丢失一些消息。acks = 1：意味若 Leader 在收到消息并把它写入到分区数据文件（不一定同步到磁盘上）时会返回确认或错误响应。在这个模式下，如果发生正常的 Leader 选举，生产者会在选举时收到一个 LeaderNotAvailableException 异常，如果生产者能恰当地处理这个错误，它会重试发送悄息，最终消息会安全到达新的 Leader 那里。不过在这个模式下仍然有可能丢失数据，比如消息已经成功写入 Leader，但在消息被复制到 follower 副本之前 Leader发生崩溃。acks = all（这个和 request.required.acks = -1 含义一样）：意味着 Leader 在返回确认或错误响应之前，会等待所有同步副本都收到悄息。如果和 min.insync.replicas 参数结合起来，就可以决定在返回确认前至少有多少个副本能够收到悄息，生产者会一直重试直到消息被成功提交。不过这也是最慢的做法，因为生产者在继续发送其他消息之前需要等待所有副本都收到当前的消息。根据实际的应用场景，我们设置不同的 acks，以此保证数据的可靠性。另外，Producer 发送消息还可以选择同步（默认，通过 producer.type=sync 配置） 或者异步（producer.type=async）模式。如果设置成异步，虽然会极大的提高消息发送的性能，但是这样会增加丢失数据的风险。如果需要确保消息的可靠性，必须将 producer.type 设置为 sync。Leader 选举在介绍 Leader 选举之前，让我们先来了解一下 ISR（in-sync replicas）列表。每个分区的 leader 会维护一个 ISR 列表，ISR 列表里面就是 follower 副本的 Borker 编号，只有跟得上 Leader 的 follower 副本才能加入到 ISR 里面，这个是通过 replica.lag.time.max.ms 参数配置的。只有 ISR 里的成员才有被选为 leader 的可能。2）数据一致性（可回答 Kafka数据一致性原理？）这里介绍的数据一致性主要是说不论是老的 Leader 还是新选举的 Leader，Consumer 都能读到一样的数据。那么 Kafka 是如何实现的呢？ 假设分区的副本为3，其中副本0是 Leader，副本1和副本2是 follower，并且在 ISR 列表里面。虽然副本0已经写入了 Message4，但是 Consumer 只能读取到 Message2。因为所有的 ISR 都同步了 Message2，只有 High Water Mark 以上的消息才支持 Consumer 读取，而 High Water Mark 取决于 ISR 列表里面偏移量最小的分区，对应于上图的副本2，这个很类似于木桶原理。这样做的原因是还没有被足够多副本复制的消息被认为是“不安全”的，如果 Leader 发生崩溃，另一个副本成为新 Leader，那么这些消息很可能丢失了。如果我们允许消费者读取这些消息，可能就会破坏一致性。试想，一个消费者从当前 Leader（副本0） 读取并处理了 Message4，这个时候 Leader 挂掉了，选举了副本1为新的 Leader，这时候另一个消费者再去从新的 Leader 读取消息，发现这个消息其实并不存在，这就导致了数据不一致性问题。当然，引入了 High Water Mark 机制，会导致 Broker 间的消息复制因为某些原因变慢，那么消息到达消费者的时间也会随之变长（因为我们会先等待消息复制完毕）。延迟时间可以通过参数 replica.lag.time.max.ms 参数配置，它指定了副本在复制消息时可被允许的最大延迟时间。 7、ISR、OSR、AR 是什么？ISR：In-Sync Replicas 副本同步队列OSR：Out-of-Sync ReplicasAR：Assigned Replicas 所有副本ISR是由leader维护，follower从leader同步数据有一些延迟（具体可以参见 图文了解 Kafka 的副本复制机制），超过相应的阈值会把 follower 剔除出 ISR, 存入OSR（Out-of-Sync Replicas ）列表，新加入的follower也会先存放在OSR中。AR=ISR+OSR。 8、LEO、HW、LSO、LW等分别代表什么？LEO：是 LogEndOffset 的简称，代表当前日志文件中下一条HW：水位或水印（watermark）一词，也可称为高水位(high watermark)，通常被用在流式处理领域（比如Apache Flink、Apache Spark等），以表征元素或事件在基于时间层面上的进度。在Kafka中，水位的概念反而与时间无关，而是与位置信息相关。严格来说，它表示的就是位置信息，即位移（offset）。取 partition 对应的 ISR中 最小的 LEO 作为 HW，consumer 最多只能消费到 HW 所在的位置上一条信息。LSO：是 LastStableOffset 的简称，对未完成的事务而言，LSO 的值等于事务中第一条消息的位置(firstUnstableOffset)，对已完成的事务而言，它的值同 HW 相同LW：Low Watermark 低水位, 代表 AR 集合中最小的 logStartOffset 值。 9、数据传输的事务有几种？数据传输的事务定义通常有以下三种级别：最多一次：消息不会被重复发送，最多被传输一次，但也有可能一次不传输最少一次：消息不会被漏发送，最少被传输一次，但也有可能被重复传输精确的一次（Exactly once）：不会漏传输也不会重复传输，每个消息都传输被接收 10、Kafka 消费者是否可以消费指定分区消息？Kafa consumer消费消息时，向broker发出fetch请求去消费特定分区的消息，consumer指定消息在日志中的偏移量（offset），就可以消费从这个位置开始的消息，customer拥有了offset的控制权，可以向后回滚去重新消费之前的消息，这是很有意义的。 11、Kafka消息是采用Pull模式，还是Push模式？Kafka最初考虑的问题是，customer应该从brokes拉取消息还是brokers将消息推送到consumer，也就是pull还push。在这方面，Kafka遵循了一种大部分消息系统共同的传统的设计：producer将消息推送到broker，consumer从broker拉取消息。一些消息系统比如Scribe和Apache Flume采用了push模式，将消息推送到下游的consumer。这样做有好处也有坏处：由broker决定消息推送的速率，对于不同消费速率的consumer就不太好处理了。消息系统都致力于让consumer以最大的速率最快速的消费消息，但不幸的是，push模式下，当broker推送的速率远大于consumer消费的速率时，consumer恐怕就要崩溃了。最终Kafka还是选取了传统的pull模式。Pull模式的另外一个好处是consumer可以自主决定是否批量的从broker拉取数据。Push模式必须在不知道下游consumer消费能力和消费策略的情况下决定是立即推送每条消息还是缓存之后批量推送。如果为了避免consumer崩溃而采用较低的推送速率，将可能导致一次只推送较少的消息而造成浪费。Pull模式下，consumer就可以根据自己的消费能力去决定这些策略。 Pull有个缺点是，如果broker没有可供消费的消息，将导致consumer不断在循环中轮询，直到新消息到t达。为了避免这点，Kafka有个参数可以让consumer阻塞知道新消息到达(当然也可以阻塞知道消息的数量达到某个特定的量这样就可以批量发送） 12、Kafka 高效文件存储设计特点？1）Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。2）通过索引信息可以快速定位message和确定response的最大大小。3）通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。4）通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 13、Kafka创建Topic时如何将分区放置到不同的Broker中？1）副本因子不能大于 Broker 的个数；2）第一个分区（编号为0）的第一个副本放置位置是随机从 brokerList 选择的；3）其他分区的第一个副本放置位置相对于第0个分区依次往后移。也就是如果我们有5个 Broker，5个分区，假设第一个分区放在第四个 Broker 上，那么第二个分区将会放在第五个 Broker 上；第三个分区将会放在第一个 Broker 上；第四个分区将会放在第二个 Broker 上，依次类推；4）剩余的副本相对于第一个副本放置位置其实是由 nextReplicaShift 决定的，而这个数也是随机产生的； 14、Kafka新建的分区会在哪个目录下创建？我们知道，在启动 Kafka 集群之前，我们需要配置好 log.dirs 参数，其值是 Kafka 数据的存放目录，这个参数可以配置多个目录，目录之间使用逗号分隔，通常这些目录是分布在不同的磁盘上用于提高读写性能。当然我们也可以配置 log.dir 参数，含义一样。只需要设置其中一个即可。如果 log.dirs 参数只配置了一个目录，那么分配到各个 Broker 上的分区肯定只能在这个目录下创建文件夹用于存放数据。但是如果 log.dirs 参数配置了多个目录，那么 Kafka 会在哪个文件夹中创建分区目录呢？答案是：Kafka 会在含有分区目录最少的文件夹中创建新的分区目录，分区目录名为 Topic名+分区ID。注意，是分区文件夹总数最少的目录，而不是磁盘使用量最少的目录！也就是说，如果你给 log.dirs 参数新增了一个新的磁盘，新的分区目录肯定是先在这个新的磁盘上创建直到这个新的磁盘目录拥有的分区目录不是最少为止。 15、谈一谈 Kafka 的再均衡在Kafka中，当有新消费者加入或者订阅的topic数发生变化时，会触发Rebalance(再均衡：在同一个消费者组当中，分区的所有权从一个消费者转移到另外一个消费者)机制，Rebalance顾名思义就是重新均衡消费者消费。Rebalance的过程如下：第一步：所有成员都向coordinator发送请求，请求入组。一旦所有成员都发送了请求，coordinator会从中选择一个consumer担任leader的角色，并把组成员信息以及订阅信息发给leader。第二步：leader开始分配消费方案，指明具体哪个consumer负责消费哪些topic的哪些partition。一旦完成分配，leader会将这个方案发给coordinator。coordinator接收到分配方案之后会把方案发给各个consumer，这样组内的所有成员就都知道自己应该消费哪些分区了。所以对于Rebalance来说，Coordinator起着至关重要的作用。 16、Kafka分区分配策略 在 Kafka 内部存在两种默认的分区分配策略：Range 和 RoundRobin。当以下事件发生时，Kafka 将会进行一次分区分配：1）同一个 Consumer Group 内新增消费者2）消费者离开当前所属的Consumer Group，包括shuts down 或 crashes3）订阅的主题新增分区将分区的所有权从一个消费者移到另一个消费者称为重新平衡（rebalance），如何rebalance就涉及到下面提到的分区分配策略。下面我们将详细介绍 Kafka 内置的两种分区分配策略。本文假设我们有个名为 T1 的主题，其包含了10个分区，然后我们有两个消费者（C1，C2）来消费这10个分区里面的数据，而且 C1 的 num.streams = 1，C2 的 num.streams = 2。Range strategyRange策略是对每个主题而言的，首先对同一个主题里面的分区按照序号进行排序，并对消费者按照字母顺序进行排序。在我们的例子里面，排完序的分区将会是0, 1, 2, 3, 4, 5, 6, 7, 8, 9；消费者线程排完序将会是C1-0, C2-0, C2-1。然后将partitions的个数除于消费者线程的总数来决定每个消费者线程消费几个分区。如果除不尽，那么前面几个消费者线程将会多消费一个分区。在我们的例子里面，我们有10个分区，3个消费者线程，10 / 3 = 3，而且除不尽，那么消费者线程 C1-0 将会多消费一个分区，所以最后分区分配的结果看起来是这样的：C1-0 将消费 0, 1, 2, 3 分区C2-0 将消费 4, 5, 6 分区C2-1 将消费 7, 8, 9 分区假如我们有11个分区，那么最后分区分配的结果看起来是这样的：C1-0 将消费 0, 1, 2, 3 分区C2-0 将消费 4, 5, 6, 7 分区C2-1 将消费 8, 9, 10 分区假如我们有2个主题(T1和T2)，分别有10个分区，那么最后分区分配的结果看起来是这样的：C1-0 将消费 T1主题的 0, 1, 2, 3 分区以及 T2主题的 0, 1, 2, 3分区C2-0 将消费 T1主题的 4, 5, 6 分区以及 T2主题的 4, 5, 6分区C2-1 将消费 T1主题的 7, 8, 9 分区以及 T2主题的 7, 8, 9分区可以看出，C1-0 消费者线程比其他消费者线程多消费了2个分区，这就是Range strategy的一个很明显的弊端。RoundRobin strategy使用RoundRobin策略有两个前提条件必须满足：同一个Consumer Group里面的所有消费者的num.streams必须相等；每个消费者订阅的主题必须相同。所以这里假设前面提到的2个消费者的num.streams = 2。RoundRobin策略的工作原理：将所有主题的分区组成 TopicAndPartition 列表，然后对 TopicAndPartition 列表按照 hashCode 进行排序，这里文字可能说不清，看下面的代码应该会明白： val allTopicPartitions = ctx.partitionsForTopic.flatMap { case(topic, partitions) =&gt; info("Consumer %s rebalancing the following partitions for topic %s: %s" .format(ctx.consumerId, topic, partitions)) partitions.map(partition =&gt; { TopicAndPartition(topic, partition) }) }.toSeq.sortWith((topicPartition1, topicPartition2) =&gt; { /* * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending * up on one consumer (if it has a high enough stream count). */ topicPartition1.toString.hashCode &lt; topicPartition2.toString.hashCode }) 最后按照round-robin风格将分区分别分配给不同的消费者线程。在我们的例子里面，假如按照 hashCode 排序完的topic-partitions组依次为T1-5, T1-3, T1-0, T1-8, T1-2, T1-1, T1-4, T1-7, T1-6, T1-9，我们的消费者线程排序为C1-0, C1-1, C2-0, C2-1，最后分区分配的结果为：C1-0 将消费 T1-5, T1-2, T1-6 分区；C1-1 将消费 T1-3, T1-1, T1-9 分区；C2-0 将消费 T1-0, T1-4 分区；C2-1 将消费 T1-8, T1-7 分区。多个主题的分区分配和单个主题类似。 17、Kafka 是如何实现高吞吐率的？Kafka是分布式消息系统，需要处理海量的消息，Kafka的设计是把所有的消息都写入速度低容量大的硬盘，以此来换取更强的存储能力，但实际上，使用硬盘并没有带来过多的性能损失。kafka主要使用了以下几个方式实现了超高的吞吐率：1）顺序读写2）零拷贝3）文件分段4）批量发送5）数据压缩 18、Kafka 缺点？1）由于是批量发送，数据并非真正的实时；2）对于mqtt协议不支持；3）不支持物联网传感数据直接接入；4）仅支持统一分区内消息有序，无法实现全局消息有序；5）监控不完善，需要安装插件；6）依赖zookeeper进行元数据管理。 19、Kafka 新旧消费者的区别？旧的 Kafka 消费者 API 主要包括：SimpleConsumer（简单消费者） 和 ZookeeperConsumerConnectir（高级消费者）。SimpleConsumer 名字看起来是简单消费者，但是其实用起来很不简单，可以使用它从特定的分区和偏移量开始读取消息。高级消费者和现在新的消费者有点像，有消费者群组，有分区再均衡，不过它使用 ZK 来管理消费者群组，并不具备偏移量和再均衡的可操控性。现在的消费者同时支持以上两种行为，所以为啥还用旧消费者 API 呢？ 20、Kafka 分区数可以增加或减少吗？为什么？我们可以使用 bin/kafka-topics.sh 命令对 Kafka 增加 Kafka 的分区数据，但是 Kafka 不支持减少分区数。 Kafka 分区数据不支持减少是由很多原因的，比如减少的分区其数据放到哪里去？是删除，还是保留？删除的话，那么这些没消费的消息不就丢了。如果保留这些消息如何放到其他分区里面？追加到其他分区后面的话那么就破坏了 Kafka 单个分区的有序性。如果要保证删除分区数据插入到其他分区保证有序性，那么实现起来逻辑就会非常复杂。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Kafka系列——Flume对接Kafka实践操作]]></title>
    <url>%2F2018%2F06%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AEKafka%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Flume%E5%AF%B9%E6%8E%A5Kafka%E5%AE%9E%E8%B7%B5%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[四、Flume对接Kafka一般实际应用中，会通过Flume+Kafka来对产生的数据进行采集，也可以在Kafka中对数据进行一个初步的处理，用于后续Spark或MapReduce的使用，这个案例主要是实现Flume和Kafka的对接， 1、配置flume(flume-kafka.conf)# define a1.sources = r1 a1.sinks = k1 a1.channels = c1 # source a1.sources.r1.type = exec a1.sources.r1.command = tail -F -c +0 /opt/module/datas/flume.log a1.sources.r1.shell = /bin/bash -c # sink a1.sinks.k1.type = org.apache.flume.sink.kafka.KafkaSink a1.sinks.k1.kafka.bootstrap.servers = hadoop102:9092,hadoop103:9092,hadoop104:9092 a1.sinks.k1.kafka.topic = first a1.sinks.k1.kafka.flumeBatchSize = 20 a1.sinks.k1.kafka.producer.acks = 1 a1.sinks.k1.kafka.producer.linger.ms = 1 # channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # bind a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 2、启动kafkaIDEA消费者（上一篇文章有）3、进入flume根目录下，启动flumebin/flume-ng agent -c conf/ -n a1 -f jobs/flume-kafka.conf 4、向/opt/module/datas/flume.log里追加数据，查看kafka消费者消费情况echo hello &gt;&gt; /opt/module/datas/flume.log]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>实战</tag>
        <tag>大数据</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Kafka系列——Kafka API操作实践]]></title>
    <url>%2F2018%2F06%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AEKafka%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Kafka%20API%E6%93%8D%E4%BD%9C%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[三、Kafka API操作实践 1、Producer API1）消息发送流程Kafka的Producer发送消息采用的是异步发送的方式。在消息发送的过程中，涉及到了两个线程——main线程和Sender线程，以及一个线程共享变量——RecordAccumulator。main线程将消息发送给RecordAccumulator，Sender线程不断从RecordAccumulator中拉取消息发送到Kafka broker。 batch.size：只有数据积累到batch.size之后，sender才会发送数据。linger.ms：如果数据迟迟未达到batch.size，sender等待linger.time之后就会发送数据。 2）异步发送API（1）导入依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt; &lt;/dependency&gt; （2）编写代码需要用到的类：KafkaProducer：需要创建一个生产者对象，用来发送数据ProducerConfig：获取所需的一系列配置参数ProducerRecord：每条数据都要封装成一个ProducerRecord对象 1.不带回调函数的API import org.apache.kafka.clients.producer.*; import java.util.Properties; import java.util.concurrent.ExecutionException; public class CustomProducer { public static void main(String[] args) throws ExecutionException, InterruptedException { Properties props = new Properties(); props.put("bootstrap.servers", "hadoop102:9092");//kafka集群，broker-list props.put("acks", "all"); props.put("retries", 1);//重试次数 props.put("batch.size", 16384);//批次大小 props.put("linger.ms", 1);//等待时间 props.put("buffer.memory", 33554432);//RecordAccumulator缓冲区大小 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) { producer.send(new ProducerRecord&lt;String, String&gt;("first", Integer.toString(i), Integer.toString(i))); } producer.close(); } } 2.带回调函数的API回调函数会在producer收到ack时调用，为异步调用，该方法有两个参数，分别是RecordMetadata和Exception，如果Exception为null，说明消息发送成功，如果Exception不为null，说明消息发送失败。注意：消息发送失败会自动重试，不需要我们在回调函数中手动重试。 import org.apache.kafka.clients.producer.*; import java.util.Properties; import java.util.concurrent.ExecutionException; public class CustomProducer { public static void main(String[] args) throws ExecutionException, InterruptedException { Properties props = new Properties(); props.put("bootstrap.servers", "hadoop102:9092");//kafka集群，broker-list props.put("acks", "all"); props.put("retries", 1);//重试次数 props.put("batch.size", 16384);//批次大小 props.put("linger.ms", 1);//等待时间 props.put("buffer.memory", 33554432);//RecordAccumulator缓冲区大小 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) { producer.send(new ProducerRecord&lt;String, String&gt;("first", Integer.toString(i), Integer.toString(i)), new Callback() { //回调函数，该方法会在Producer收到ack时调用，为异步调用 @Override public void onCompletion(RecordMetadata metadata, Exception exception) { if (exception == null) { System.out.println("success-&gt;" + metadata.offset()); } else { exception.printStackTrace(); } } }); } producer.close(); } } 3）同步发送API同步发送的意思就是，一条消息发送之后，会阻塞当前线程，直至返回ack。由于send方法返回的是一个Future对象，根据Futrue对象的特点，我们也可以实现同步发送的效果，只需在调用Future对象的get方发即可。 import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerRecord; import java.util.Properties; import java.util.concurrent.ExecutionException; public class CustomProducer { public static void main(String[] args) throws ExecutionException, InterruptedException { Properties props = new Properties(); props.put("bootstrap.servers", "hadoop102:9092");//kafka集群，broker-list props.put("acks", "all"); props.put("retries", 1);//重试次数 props.put("batch.size", 16384);//批次大小 props.put("linger.ms", 1);//等待时间 props.put("buffer.memory", 33554432);//RecordAccumulator缓冲区大小 props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for (int i = 0; i &lt; 100; i++) { producer.send(new ProducerRecord&lt;String, String&gt;("first", Integer.toString(i), Integer.toString(i))).get(); } producer.close(); } } 2、Consumer APIConsumer消费数据时的可靠性是很容易保证的，因为数据在Kafka中是持久化的，故不用担心数据丢失问题。由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。所以offset的维护是Consumer消费数据是必须考虑的问题。 1）手动提交offset（1）导入依赖 &lt;dependency&gt; &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt; &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt; &lt;version&gt;0.11.0.0&lt;/version&gt; &lt;/dependency&gt; （2）编写代码需要用到的类：KafkaConsumer：需要创建一个消费者对象，用来消费数据ConsumerConfig：获取所需的一系列配置参数ConsuemrRecord：每条数据都要封装成一个ConsumerRecord对象 import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.util.Arrays; import java.util.Properties; public class CustomConsumer { public static void main(String[] args) { Properties props = new Properties(); props.put("bootstrap.servers", "hadoop102:9092"); props.put("group.id", "test");//消费者组，只要group.id相同，就属于同一个消费者组 props.put("enable.auto.commit", "false");//自动提交offset props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("first")); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) { System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); } consumer.commitSync(); } } } （3）代码分析：手动提交offset的方法有两种：分别是commitSync（同步提交）和commitAsync（异步提交）。两者的相同点是，都会将本次poll的一批数据最高的偏移量提交；不同点是，commitSync会失败重试，一直到提交成功（如果由于不可恢复原因导致，也会提交失败）；而commitAsync则没有失败重试机制，故有可能提交失败。 （4）数据重复消费问题 2）自动提交offset为了使我们能够专注于自己的业务逻辑，Kafka提供了自动提交offset的功能。自动提交offset的相关参数：enable.auto.commit：是否开启自动提交offset功能auto.commit.interval.ms：自动提交offset的时间间隔 代码： import org.apache.kafka.clients.consumer.ConsumerRecord; import org.apache.kafka.clients.consumer.ConsumerRecords; import org.apache.kafka.clients.consumer.KafkaConsumer; import java.util.Arrays; import java.util.Properties; public class CustomConsumer { public static void main(String[] args) { Properties props = new Properties(); props.put("bootstrap.servers", "hadoop102:9092"); props.put("group.id", "test"); props.put("enable.auto.commit", "true"); props.put("auto.commit.interval.ms", "1000"); props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer"); KafkaConsumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(props); consumer.subscribe(Arrays.asList("first")); while (true) { ConsumerRecords&lt;String, String&gt; records = consumer.poll(100); for (ConsumerRecord&lt;String, String&gt; record : records) System.out.printf("offset = %d, key = %s, value = %s%n", record.offset(), record.key(), record.value()); } } } 3、自定义Interceptor1）拦截器原理Producer拦截器(interceptor)是在Kafka 0.10版本被引入的，主要用于实现clients端的定制化控制逻辑。对于producer而言，interceptor使得用户在消息发送前以及producer回调逻辑前有机会对消息做一些定制化需求，比如修改消息等。同时，producer允许用户指定多个interceptor按序作用于同一条消息从而形成一个拦截链(interceptor chain)。Intercetpor的实现接口是org.apache.kafka.clients.producer.ProducerInterceptor，其定义的方法包括：（1）configure(configs)获取配置信息和初始化数据时调用。（2）onSend(ProducerRecord)：该方法封装进KafkaProducer.send方法中，即它运行在用户主线程中。Producer确保在消息被序列化以及计算分区前调用该方法。用户可以在该方法中对消息做任何操作，但最好保证不要修改消息所属的topic和分区，否则会影响目标分区的计算。（3）onAcknowledgement(RecordMetadata, Exception)：该方法会在消息从RecordAccumulator成功发送到Kafka Broker之后，或者在发送过程中失败时调用。并且通常都是在producer回调逻辑触发之前。onAcknowledgement运行在producer的IO线程中，因此不要在该方法中放入很重的逻辑，否则会拖慢producer的消息发送效率。（4）close：关闭interceptor，主要用于执行一些资源清理工作如前所述，interceptor可能被运行在多个线程中，因此在具体实现时用户需要自行确保线程安全。另外倘若指定了多个interceptor，则producer将按照指定顺序调用它们，并仅仅是捕获每个interceptor可能抛出的异常记录到错误日志中而非在向上传递。这在使用过程中要特别留意。 2）拦截器案例（1）需求：实现一个简单的双interceptor组成的拦截链。第一个interceptor会在消息发送前将时间戳信息加到消息value的最前部；第二个interceptor会在消息发送后更新成功发送消息数或失败发送消息数。 （2）案例实践1.增加时间戳拦截器 import java.util.Map; import org.apache.kafka.clients.producer.ProducerInterceptor; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; public class TimeInterceptor implements ProducerInterceptor&lt;String, String&gt; { @Override public void configure(Map&lt;String, ?&gt; configs) { } @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) { // 创建一个新的record，把时间戳写入消息体的最前部 return new ProducerRecord(record.topic(), record.partition(), record.timestamp(), record.key(), System.currentTimeMillis() + "," + record.value().toString()); } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { } @Override public void close() { } } 2.统计发送消息成功和发送失败消息数，并在producer关闭时打印这两个计数器 import java.util.Map; import org.apache.kafka.clients.producer.ProducerInterceptor; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.clients.producer.RecordMetadata; public class CounterInterceptor implements ProducerInterceptor&lt;String, String&gt;{ private int errorCounter = 0; private int successCounter = 0; @Override public void configure(Map&lt;String, ?&gt; configs) { } @Override public ProducerRecord&lt;String, String&gt; onSend(ProducerRecord&lt;String, String&gt; record) { return record; } @Override public void onAcknowledgement(RecordMetadata metadata, Exception exception) { // 统计成功和失败的次数 if (exception == null) { successCounter++; } else { errorCounter++; } } @Override public void close() { // 保存结果 System.out.println("Successful sent: " + successCounter); System.out.println("Failed sent: " + errorCounter); } } 3.producer主程序 import java.util.ArrayList; import java.util.List; import java.util.Properties; import org.apache.kafka.clients.producer.KafkaProducer; import org.apache.kafka.clients.producer.Producer; import org.apache.kafka.clients.producer.ProducerConfig; import org.apache.kafka.clients.producer.ProducerRecord; public class InterceptorProducer { public static void main(String[] args) throws Exception { // 1 设置配置信息 Properties props = new Properties(); props.put("bootstrap.servers", "hadoop102:9092"); props.put("acks", "all"); props.put("retries", 0); props.put("batch.size", 16384); props.put("linger.ms", 1); props.put("buffer.memory", 33554432); props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer"); props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer"); // 2 构建拦截链 List&lt;String&gt; interceptors = new ArrayList&lt;&gt;(); interceptors.add("com.atguigu.kafka.interceptor.TimeInterceptor"); interceptors.add("com.atguigu.kafka.interceptor.CounterInterceptor"); props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, interceptors); String topic = "first"; Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); // 3 发送消息 for (int i = 0; i &lt; 10; i++) { ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;(topic, "message" + i); producer.send(record); } // 4 一定要关闭producer，这样才会调用interceptor的close方法 producer.close(); } } （3）测试在kafka上启动消费者，然后运行客户端java程序。 bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --from-beginning --topic first]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>实战</tag>
        <tag>大数据</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Kafka系列——Kafka深入解析]]></title>
    <url>%2F2018%2F06%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AEKafka%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Kafka%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[二、Kafka深入解析 1、Kafka工作流程及文件存储机制1）Kafka工作流程 Kafka中消息是以topic进行分类的，生产者生产消息，消费者消费消息，都是面向topic的。topic是逻辑上的概念，而partition是物理上的概念，每个partition对应于一个log文件，该log文件中存储的就是producer生产的数据。Producer生产的数据会被不断追加到该log文件末端，且每条数据都有自己的offset。消费者组中的每个消费者，都会实时记录自己消费到了哪个offset，以便出错恢复时，从上次的位置继续消费。 2）Kafka文件存储机制 由于生产者生产的消息会不断追加到log文件末尾，为防止log文件过大导致数据定位效率低下，Kafka采取了分片和索引机制，将每个partition分为多个segment。每个segment对应两个文件——“.index”文件和“.log”文件。这些文件位于一个文件夹下，该文件夹的命名规则为：topic名称+分区序号。例如，first这个topic有三个分区，则其对应的文件夹为first-0,first-1,first-2。 00000000000000000000.index 00000000000000000000.log 00000000000000170410.index 00000000000000170410.log 00000000000000239430.index 00000000000000239430.log index和log文件以当前segment的第一条消息的offset命名。下图为index文件和log文件的结构示意图。 “.index”文件存储大量的索引信息，“.log”文件存储大量的数据，索引文件中的元数据指向对应数据文件中message的物理偏移地址。 2、Kafka生产者1）分区策略（1）分区的原因① 方便在集群中扩展，每个Partition可以通过调整以适应它所在的机器，而一个topic又可以有多个Partition组成，因此整个集群就可以适应任意大小的数据了；② 可以提高并发，因为可以以Partition为单位读写了。（2）分区的原则我们需要将producer发送的数据封装成一个ProducerRecord对象。 ① 指明partition的情况下，直接将指明的值直接作为partiton值；② 没有指明partition值但有key的情况下，将key的hash值与topic的partition数进行取余得到partition值；③ 既没有partition值又没有key值的情况下，第一次调用时随机生成一个整数（后面每次调用在这个整数上自增），将这个值与topic可用的partition总数取余得到partition值，也就是常说的round-robin算法。 2）数据可靠性保证为保证producer发送的数据，能可靠的发送到指定的topic，topic的每个partition收到producer发送的数据后，都需要向producer发送ack（acknowledgement确认收到），如果producer收到ack，就会进行下一轮的发送，否则重新发送数据。 （1）副本同步策略： Kafka选择了第二种方案，原因如下：1）同样为了容忍n台节点的故障，第一种方案需要2n+1个副本，而第二种方案只需要n+1个副本，而Kafka的每个分区都有大量的数据，第一种方案会造成大量数据的冗余。2）虽然第二种方案的网络延迟会比较高，但网络延迟对Kafka的影响较小。 （2）ISR采用第二种方案之后，设想以下情景：leader收到数据，所有follower都开始同步数据，但有一个follower，因为某种故障，迟迟不能与leader进行同步，那leader就要一直等下去，直到它完成同步，才能发送ack。这个问题怎么解决呢？Leader维护了一个动态的in-sync replica set (ISR)，意为和leader保持同步的follower集合。当ISR中的follower完成数据的同步之后，leader就会给follower发送ack。如果follower长时间未向leader同步数据，则该follower将被踢出ISR，该时间阈值由replica.lag.time.max.ms参数设定。Leader发生故障之后，就会从ISR中选举新的leader。 （3）ack应答机制对于某些不太重要的数据，对数据的可靠性要求不是很高，能够容忍数据的少量丢失，所以没必要等ISR中的follower全部接收成功。所以Kafka为用户提供了三种可靠性级别，用户根据对可靠性和延迟的要求进行权衡，选择以下的配置。acks参数配置：acks：0：producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；1：producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，那么将会丢失数据； -1（all）：producer等待broker的ack，partition的leader和follower全部落盘成功后才返回ack。但是如果在follower同步完成后，broker发送ack之前，leader发生故障，那么会造成数据重复。 （4）故障处理细节 ① follower故障follower发生故障后会被临时踢出ISR，待该follower恢复后，follower会读取本地磁盘记录的上次的HW，并将log文件高于HW的部分截取掉，从HW开始向leader进行同步。等该follower的LEO大于等于该Partition的HW，即follower追上leader之后，就可以重新加入ISR了。② leader故障leader发生故障之后，会从ISR中选出一个新的leader，之后，为保证多个副本之间的数据一致性，其余的follower会先将各自的log文件高于HW的部分截掉，然后从新的leader同步数据。注意：这只能保证副本之间的数据一致性，并不能保证数据不丢失或者不重复。 3、Kafka消费者1）消费方式consumer采用pull（拉）模式从broker中读取数据。push（推）模式很难适应消费速率不同的消费者，因为消息发送速率是由broker决定的。它的目标是尽可能以最快速度传递消息，但是这样很容易造成consumer来不及处理消息，典型的表现就是拒绝服务以及网络拥塞。而pull模式则可以根据consumer的消费能力以适当的速率消费消息。pull模式不足之处是，如果kafka没有数据，消费者可能会陷入循环中，一直返回空数据。针对这一点，Kafka的消费者在消费数据时会传入一个时长参数timeout，如果当前没有数据可供消费，consumer会等待一段时间之后再返回，这段时长即为timeout。2）分区分配策略一个consumer group中有多个consumer，一个 topic有多个partition，所以必然会涉及到partition的分配问题，即确定那个partition由哪个consumer来消费。Kafka有两种分配策略：RoundRobin和Range。（1）RoundRobin （2）Range 3）offset维护由于consumer在消费过程中可能会出现断电宕机等故障，consumer恢复后，需要从故障前的位置的继续消费，所以consumer需要实时记录自己消费到了哪个offset，以便故障恢复后继续消费。Kafka 0.9版本之前，consumer默认将offset保存在Zookeeper中，从0.9版本开始，consumer默认将offset保存在Kafka一个内置的topic中，该topic为__consumer_offsets。 4、Kafka高效读写数据1）顺序写磁盘Kafka的producer生产数据，要写入到log文件中，写的过程是一直追加到文件末端，为顺序写。官网有数据表明，同样的磁盘，顺序写能到到600M/s，而随机写只有100k/s。这与磁盘的机械机构有关，顺序写之所以快，是因为其省去了大量磁头寻址的时间。2）零复制技术 5、Zookeeper在Kafka中的作用Kafka集群中有一个broker会被选举为Controller，负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。Controller的管理工作都是依赖于Zookeeper的。以下为partition的leader选举过程：]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>原理</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Kafka系列——Kafka概述]]></title>
    <url>%2F2018%2F06%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AEKafka%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Kafka%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[一、Kafka概述Kafka是一个分布式的基于发布/订阅模式的消息队列，主要应用于大数据实时处理领域。 1、消息队列 消息队列模式：1）点对点模式（一对一，消费者主动拉取数据，消息收到后消息清除） 点对点模型通常是一个基于拉取或者轮询的消息传送模型，这种模型从队列中请求信息，而不是将消息推送到客户端。这个模型的特点是发送到队列的消息被一个且只有一个接收者接收处理，即使有多个消息监听者也是如此。（缺点：需要时刻监控队列的消息变化，拉取数据，任务需要时刻开启，浪费资源） 2）发布/订阅模式（一对多，数据生产后，推送给所有订阅者） 发布订阅模型则是一个基于推送的消息传送模型。发布订阅模型可以有多种不同的订阅者，临时订阅者只在主动监听主题时才接收消息，而持久订阅者则监听主题的所有消息，即使当前订阅者不可用，处于离线状态。（缺点：推送消息的速度等没有办法适应所有的订阅者，因为订阅者的性能是不一样的） 2、为什么需要消息队列1）解耦：允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。2）冗余：消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。3）扩展性：因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。4）灵活性 &amp; 峰值处理能力：在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。5）可恢复性：系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。6）顺序保证：在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka保证一个Partition内的消息的有序性）7）缓冲：有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。8）异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 3、Kafka架构整体架构图： Kafka详细架构图： 1）Producer：消息生产者，就是向kafka broker发消息的客户端；2）Consumer：消息消费者，向kafka broker取消息的客户端；3）Consumer Group（CG）：消费者组，由多个consumer组成。消费者组内每个消费者负责消费不同分区的数据，一个分区只能由一个消费者消费；消费者组之间互不影响。所有的消费者都属于某个消费者组，即消费者组是逻辑上的一个订阅者。4）Broker：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic。5）Topic：可以理解为一个队列，生产者和消费者面向的都是一个topic；6）Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列；7）Replication：副本，为保证集群中的某个节点发生故障时，该节点上的partition数据不丢失，且kafka仍然能够继续工作，kafka提供了副本机制，一个topic的每个分区都有若干个副本，一个leader和若干个follower。8）leader：每个分区多个副本的“主”，生产者发送数据的对象，以及消费者消费数据的对象都是leader。9）follower：每个分区多个副本中的“从”，实时从leader中同步数据，保持和leader数据的同步。leader发生故障时，某个follower会成为新的follower。]]></content>
      <categories>
        <category>Kafka</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Flume系列——Flume面试题]]></title>
    <url>%2F2018%2F06%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AEFlume%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Flume%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Flume面试题整理（一）1、Flume使用场景（☆☆☆☆☆）线上数据一般主要是落地（存储到磁盘）或者通过socket传输给另外一个系统，这种情况下，你很难推动线上应用或服务去修改接口，实现直接向kafka里写数据，这时候你可能就需要flume这样的系统帮你去做传输。 2、Flume丢包问题（☆☆☆☆☆）单机upd的flume source的配置，100+M/s数据量，10w qps flume就开始大量丢包，因此很多公司在搭建系统时，抛弃了Flume，自己研发传输系统，但是往往会参考Flume的Source-Channel-Sink模式。一些公司在Flume工作过程中，会对业务日志进行监控，例如Flume agent中有多少条日志，Flume到Kafka后有多少条日志等等，如果数据丢失保持在1%左右是没有问题的，当数据丢失达到5%左右时就必须采取相应措施。 3、Flume与Kafka的选取采集层主要可以使用Flume、Kafka两种技术。Flume：Flume 是管道流方式，提供了很多的默认实现，让用户通过参数部署，及扩展API。Kafka：Kafka是一个可持久化的分布式的消息队列。Kafka 是一个非常通用的系统。你可以有许多生产者和很多的消费者共享多个主题Topics。相比之下，Flume是一个专用工具被设计为旨在往HDFS，HBase发送数据。它对HDFS有特殊的优化，并且集成了Hadoop的安全特性。所以，Cloudera 建议如果数据被多个系统消费的话，使用kafka；如果数据被设计给Hadoop使用，使用Flume。正如你们所知Flume内置很多的source和sink组件。然而，Kafka明显有一个更小的生产消费者生态系统，并且Kafka的社区支持不好。希望将来这种情况会得到改善，但是目前：使用Kafka意味着你准备好了编写你自己的生产者和消费者代码。如果已经存在的Flume Sources和Sinks满足你的需求，并且你更喜欢不需要任何开发的系统，请使用Flume。Flume可以使用拦截器实时处理数据。这些对数据屏蔽或者过量是很有用的。Kafka需要外部的流处理系统才能做到。Kafka和Flume都是可靠的系统，通过适当的配置能保证零数据丢失。然而，Flume不支持副本事件。于是，如果Flume代理的一个节点奔溃了，即使使用了可靠的文件管道方式，你也将丢失这些事件直到你恢复这些磁盘。如果你需要一个高可靠性的管道，那么使用Kafka是个更好的选择。Flume和Kafka可以很好地结合起来使用。如果你的设计需要从Kafka到Hadoop的流数据，使用Flume代理并配置Kafka的Source读取数据也是可行的：你没有必要实现自己的消费者。你可以直接利用Flume与HDFS及HBase的结合的所有好处。你可以使用Cloudera Manager对消费者的监控，并且你甚至可以添加拦截器进行一些流处理。 4、数据怎么采集到Kafka，实现方式使用官方提供的flumeKafka插件，插件的实现方式是自定义了flume的sink，将数据从channle中取出，通过kafka的producer写入到kafka中，可以自定义分区等。 5、flume管道内存，flume宕机了数据丢失怎么解决1）Flume的channel分为很多种，可以将数据写入到文件。2）防止非首个agent宕机的方法数可以做集群或者主备。 6、flume配置方式，flume集群（详细讲解下）Flume的配置围绕着source、channel、sink叙述，flume的集群是做在agent上的，而非机器上。 7、flume不采集Nginx日志，通过Logger4j采集日志，优缺点是什么？优点：Nginx的日志格式是固定的，但是缺少sessionid，通过logger4j采集的日志是带有sessionid的，而session可以通过redis共享，保证了集群日志中的同一session落到不同的tomcat时，sessionId还是一样的，而且logger4j的方式比较稳定，不会宕机。缺点：不够灵活，logger4j的方式和项目结合过于紧密，而flume的方式比较灵活，拔插式比较好，不会影响项目性能。 8、flume和kafka采集日志区别，采集日志时中间停了，怎么记录之前的日志？Flume采集日志是通过流的方式直接将日志收集到存储层，而kafka是将缓存在kafka集群，待后期可以采集到存储层。Flume采集中间停了，可以采用文件的方式记录之前的日志，而kafka是采用offset的方式记录之前的日志。 9、flume有哪些组件，flume的source、channel、sink具体是做什么的（☆☆☆☆☆） 1）source：用于采集数据，Source是产生数据流的地方，同时Source会将产生的数据流传输到Channel，这个有点类似于Java IO部分的Channel。2）channel：用于桥接Sources和Sinks，类似于一个队列。3）sink：从Channel收集数据，将数据写到目标源(可以是下一个Source，也可以是HDFS或者HBase)。注意：要熟悉source、channel、sink的类型]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flume</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Flume系列——Flume案例]]></title>
    <url>%2F2018%2F05%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AEFlume%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Flume%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[四、实践案例——自定义MySQLSource案例 1、概述实时监控MySQL，从MySQL中获取数据传输到HDFS或者其他存储框架，所以此时需要我们自己实现MySQLSource。官方也提供了自定义source的接口：https://flume.apache.org/FlumeDeveloperGuide.html#source 2、自定义MySQLSource组成 3、自定义MySQLSource步骤根据官方说明自定义mysqlsource需要继承AbstractSource类并实现Configurable和PollableSource接口。实现相应方法： getBackOffSleepIncrement() //暂不用 getMaxBackOffSleepInterval() //暂不用 configure(Context context) //初始化context process() //获取数据（从mysql获取数据，业务处理比较复杂，所以定义一个专门的类——SQLSourceHelper来处理跟mysql的交互），封装成event并写入channel，这个方法被循环调用 stop() //关闭相关的资源 PollableSource：从source中提取数据，将其发送到channel。Configurable：实现了Configurable的任何类都含有一个context，使用context获取配置信息。 4、代码实现1）导入pom依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.27&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2）添加配置信息在classpath下添加jdbc.properties和log4j.propertiesjdbc.properties: dbDriver=com.mysql.jdbc.Driver dbUrl=jdbc:mysql://hadoop102:3306/mysqlsource?useUnicode=true&amp;characterEncoding=utf-8 dbUser=root dbPassword=000000 log4j.properties: #--------console----------- log4j.rootLogger=info,myconsole,myfile log4j.appender.myconsole=org.apache.log4j.ConsoleAppender log4j.appender.myconsole.layout=org.apache.log4j.SimpleLayout #log4j.appender.myconsole.layout.ConversionPattern =%d [%t] %-5p [%c] - %m%n #log4j.rootLogger=error,myfile log4j.appender.myfile=org.apache.log4j.DailyRollingFileAppender log4j.appender.myfile.File=/tmp/flume.log log4j.appender.myfile.layout=org.apache.log4j.PatternLayout log4j.appender.myfile.layout.ConversionPattern =%d [%t] %-5p [%c] - %m%n 3）SQLSourceHelper属性说明： 方法说明： 代码实现： package com.source; import org.apache.flume.Context; import org.apache.flume.conf.ConfigurationException; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.io.IOException; import java.sql.*; import java.text.ParseException; import java.util.ArrayList; import java.util.List; import java.util.Properties; public class SQLSourceHelper { private static final Logger LOG = LoggerFactory.getLogger(SQLSourceHelper.class); private int runQueryDelay, //两次查询的时间间隔 startFrom, //开始id currentIndex, //当前id recordSixe = 0, //每次查询返回结果的条数 maxRow; //每次查询的最大条数 private String table, //要操作的表 columnsToSelect, //用户传入的查询的列 customQuery, //用户传入的查询语句 query, //构建的查询语句 defaultCharsetResultSet;//编码集 //上下文，用来获取配置文件 private Context context; //为定义的变量赋值（默认值），可在flume任务的配置文件中修改 private static final int DEFAULT_QUERY_DELAY = 10000; private static final int DEFAULT_START_VALUE = 0; private static final int DEFAULT_MAX_ROWS = 2000; private static final String DEFAULT_COLUMNS_SELECT = "*"; private static final String DEFAULT_CHARSET_RESULTSET = "UTF-8"; private static Connection conn = null; private static PreparedStatement ps = null; private static String connectionURL, connectionUserName, connectionPassword; //加载静态资源 static { Properties p = new Properties(); try { p.load(SQLSourceHelper.class.getClassLoader().getResourceAsStream("jdbc.properties")); connectionURL = p.getProperty("dbUrl"); connectionUserName = p.getProperty("dbUser"); connectionPassword = p.getProperty("dbPassword"); Class.forName(p.getProperty("dbDriver")); } catch (IOException | ClassNotFoundException e) { LOG.error(e.toString()); } } //获取JDBC连接 private static Connection InitConnection(String url, String user, String pw) { try { Connection conn = DriverManager.getConnection(url, user, pw); if (conn == null) throw new SQLException(); return conn; } catch (SQLException e) { e.printStackTrace(); } return null; } //构造方法 SQLSourceHelper(Context context) throws ParseException { //初始化上下文 this.context = context; //有默认值参数：获取flume任务配置文件中的参数，读不到的采用默认值 this.columnsToSelect = context.getString("columns.to.select", DEFAULT_COLUMNS_SELECT); this.runQueryDelay = context.getInteger("run.query.delay", DEFAULT_QUERY_DELAY); this.startFrom = context.getInteger("start.from", DEFAULT_START_VALUE); this.defaultCharsetResultSet = context.getString("default.charset.resultset", DEFAULT_CHARSET_RESULTSET); //无默认值参数：获取flume任务配置文件中的参数 this.table = context.getString("table"); this.customQuery = context.getString("custom.query"); connectionURL = context.getString("connection.url"); connectionUserName = context.getString("connection.user"); connectionPassword = context.getString("connection.password"); conn = InitConnection(connectionURL, connectionUserName, connectionPassword); //校验相应的配置信息，如果没有默认值的参数也没赋值，抛出异常 checkMandatoryProperties(); //获取当前的id currentIndex = getStatusDBIndex(startFrom); //构建查询语句 query = buildQuery(); } //校验相应的配置信息（表，查询语句以及数据库连接的参数） private void checkMandatoryProperties() { if (table == null) { throw new ConfigurationException("property table not set"); } if (connectionURL == null) { throw new ConfigurationException("connection.url property not set"); } if (connectionUserName == null) { throw new ConfigurationException("connection.user property not set"); } if (connectionPassword == null) { throw new ConfigurationException("connection.password property not set"); } } //构建sql语句 private String buildQuery() { String sql = ""; //获取当前id currentIndex = getStatusDBIndex(startFrom); LOG.info(currentIndex + ""); if (customQuery == null) { sql = "SELECT " + columnsToSelect + " FROM " + table; } else { sql = customQuery; } StringBuilder execSql = new StringBuilder(sql); //以id作为offset if (!sql.contains("where")) { execSql.append(" where "); execSql.append("id").append("&gt;").append(currentIndex); return execSql.toString(); } else { int length = execSql.toString().length(); return execSql.toString().substring(0, length - String.valueOf(currentIndex).length()) + currentIndex; } } //执行查询 List&lt;List&lt;Object&gt;&gt; executeQuery() { try { //每次执行查询时都要重新生成sql，因为id不同 customQuery = buildQuery(); //存放结果的集合 List&lt;List&lt;Object&gt;&gt; results = new ArrayList&lt;&gt;(); if (ps == null) { // ps = conn.prepareStatement(customQuery); } ResultSet result = ps.executeQuery(customQuery); while (result.next()) { //存放一条数据的集合（多个列） List&lt;Object&gt; row = new ArrayList&lt;&gt;(); //将返回结果放入集合 for (int i = 1; i &lt;= result.getMetaData().getColumnCount(); i++) { row.add(result.getObject(i)); } results.add(row); } LOG.info("execSql:" + customQuery + "\nresultSize:" + results.size()); return results; } catch (SQLException e) { LOG.error(e.toString()); // 重新连接 conn = InitConnection(connectionURL, connectionUserName, connectionPassword); } return null; } //将结果集转化为字符串，每一条数据是一个list集合，将每一个小的list集合转化为字符串 List&lt;String&gt; getAllRows(List&lt;List&lt;Object&gt;&gt; queryResult) { List&lt;String&gt; allRows = new ArrayList&lt;&gt;(); if (queryResult == null || queryResult.isEmpty()) return allRows; StringBuilder row = new StringBuilder(); for (List&lt;Object&gt; rawRow : queryResult) { Object value = null; for (Object aRawRow : rawRow) { value = aRawRow; if (value == null) { row.append(","); } else { row.append(aRawRow.toString()).append(","); } } allRows.add(row.toString()); row = new StringBuilder(); } return allRows; } //更新offset元数据状态，每次返回结果集后调用。必须记录每次查询的offset值，为程序中断续跑数据时使用，以id为offset void updateOffset2DB(int size) { //以source_tab做为KEY，如果不存在则插入，存在则更新（每个源表对应一条记录） String sql = "insert into flume_meta(source_tab,currentIndex) VALUES('" + this.table + "','" + (recordSixe += size) + "') on DUPLICATE key update source_tab=values(source_tab),currentIndex=values(currentIndex)"; LOG.info("updateStatus Sql:" + sql); execSql(sql); } //执行sql语句 private void execSql(String sql) { try { ps = conn.prepareStatement(sql); LOG.info("exec::" + sql); ps.execute(); } catch (SQLException e) { e.printStackTrace(); } } //获取当前id的offset private Integer getStatusDBIndex(int startFrom) { //从flume_meta表中查询出当前的id是多少 String dbIndex = queryOne("select currentIndex from flume_meta where source_tab='" + table + "'"); if (dbIndex != null) { return Integer.parseInt(dbIndex); } //如果没有数据，则说明是第一次查询或者数据表中还没有存入数据，返回最初传入的值 return startFrom; } //查询一条数据的执行语句(当前id) private String queryOne(String sql) { ResultSet result = null; try { ps = conn.prepareStatement(sql); result = ps.executeQuery(); while (result.next()) { return result.getString(1); } } catch (SQLException e) { e.printStackTrace(); } return null; } //关闭相关资源 void close() { try { ps.close(); conn.close(); } catch (SQLException e) { e.printStackTrace(); } } int getCurrentIndex() { return currentIndex; } void setCurrentIndex(int newValue) { currentIndex = newValue; } int getRunQueryDelay() { return runQueryDelay; } String getQuery() { return query; } String getConnectionURL() { return connectionURL; } private boolean isCustomQuerySet() { return (customQuery != null); } Context getContext() { return context; } public String getConnectionUserName() { return connectionUserName; } public String getConnectionPassword() { return connectionPassword; } String getDefaultCharsetResultSet() { return defaultCharsetResultSet; } } 4）MySQLSource package com.source; import org.apache.flume.Context; import org.apache.flume.Event; import org.apache.flume.EventDeliveryException; import org.apache.flume.PollableSource; import org.apache.flume.conf.Configurable; import org.apache.flume.event.SimpleEvent; import org.apache.flume.source.AbstractSource; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import java.text.ParseException; import java.util.ArrayList; import java.util.HashMap; import java.util.List; public class SQLSource extends AbstractSource implements Configurable, PollableSource { //打印日志 private static final Logger LOG = LoggerFactory.getLogger(SQLSource.class); //定义sqlHelper private SQLSourceHelper sqlSourceHelper; @Override public long getBackOffSleepIncrement() { return 0; } @Override public long getMaxBackOffSleepInterval() { return 0; } @Override public void configure(Context context) { try { //初始化 sqlSourceHelper = new SQLSourceHelper(context); } catch (ParseException e) { e.printStackTrace(); } } @Override public Status process() throws EventDeliveryException { try { //查询数据表 List&lt;List&lt;Object&gt;&gt; result = sqlSourceHelper.executeQuery(); //存放event的集合 List&lt;Event&gt; events = new ArrayList&lt;&gt;(); //存放event头集合 HashMap&lt;String, String&gt; header = new HashMap&lt;&gt;(); //如果有返回数据，则将数据封装为event if (!result.isEmpty()) { List&lt;String&gt; allRows = sqlSourceHelper.getAllRows(result); Event event = null; for (String row : allRows) { event = new SimpleEvent(); event.setBody(row.getBytes()); event.setHeaders(header); events.add(event); } //将event写入channel this.getChannelProcessor().processEventBatch(events); //更新数据表中的offset信息 sqlSourceHelper.updateOffset2DB(result.size()); } //等待时长 Thread.sleep(sqlSourceHelper.getRunQueryDelay()); return Status.READY; } catch (InterruptedException e) { LOG.error("Error procesing row", e); return Status.BACKOFF; } } @Override public synchronized void stop() { LOG.info("Stopping sql source {} ...", getName()); try { //关闭资源 sqlSourceHelper.close(); } finally { super.stop(); } } } 5、测试1）jar包准备（1）将mysql驱动包放入flume的lib目录下（2）打包项目并将jar包放入flume的lib目录下 2）配置文件准备创建配置文件mysql.canf # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = com.atguigu.source.SQLSource a1.sources.r1.connection.url = jdbc:mysql://192.168.1.102:3306/mysqlsource a1.sources.r1.connection.user = root a1.sources.r1.connection.password = 000000 a1.sources.r1.table = student a1.sources.r1.columns.to.select = * a1.sources.r1.run.query.delay=5000 # Describe the sink a1.sinks.k1.type = logger # Describe the channel a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 3）MySQL表准备（1）创建mysqlsource数据库 CREATE DATABASE mysqlsource； （2）在mysqlsource数据库下创建数据表student和元数据表flume_meta CREATE TABLE `student` ( `id` int(11) NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL, PRIMARY KEY (`id`) ); CREATE TABLE `flume_meta` ( `source_tab` varchar(255) NOT NULL, `currentIndex` varchar(255) NOT NULL, PRIMARY KEY (`source_tab`) ); （3）向数据表中添加数据 1 zhangsan 2 lisi 3 wangwu 4 zhaoliu （4）测试 bin/flume-ng agent --conf conf/ --name a1 --conf-file job/mysql.conf -Dflume.root.logger=INFO,console]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>实战</tag>
        <tag>大数据</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Flume系列——Flume实践操作]]></title>
    <url>%2F2018%2F05%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AEFlume%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Flume%E5%AE%9E%E8%B7%B5%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[二、自定义Source 1、概述Source是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。官方提供的source类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些source。 官方也提供了自定义source的接口：https://flume.apache.org/FlumeDeveloperGuide.html#source 根据官方说明自定义MySource需要继承AbstractSource类并实现Configurable和PollableSource接口。实现相应方法： getBackOffSleepIncrement() //暂不用 getMaxBackOffSleepInterval() //暂不用 configure(Context context) //初始化context（读取配置文件内容） process() //获取数据封装成event并写入channel，**这个方法将被循环调用**。 使用场景：读取MySQL数据或者其他文件系统。 2、需求使用flume接收数据，并给每条数据添加前缀，输出到控制台。前缀可从flume配置文件中配置。自定义Source需求： 自定义Source需求分析： 3、编码1）导入pom依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.flume&lt;/groupId&gt; &lt;artifactId&gt;flume-ng-core&lt;/artifactId&gt; &lt;version&gt;1.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 2）代码 package com; import org.apache.flume.Context; import org.apache.flume.EventDeliveryException; import org.apache.flume.PollableSource; import org.apache.flume.conf.Configurable; import org.apache.flume.event.SimpleEvent; import org.apache.flume.source.AbstractSource; import java.util.HashMap; public class MySource extends AbstractSource implements Configurable, PollableSource { //定义配置文件将来要读取的字段 private Long delay; private String field; //初始化配置信息 @Override public void configure(Context context) { delay = context.getLong("delay"); field = context.getString("field", "Hello!"); } @Override public Status process() throws EventDeliveryException { try { //创建事件头信息 HashMap&lt;String, String&gt; hearderMap = new HashMap&lt;&gt;(); //创建事件 SimpleEvent event = new SimpleEvent(); //循环封装事件 for (int i = 0; i &lt; 5; i++) { //给事件设置头信息 event.setHeaders(hearderMap); //给事件设置内容 event.setBody((field + i).getBytes()); //将事件写入channel getChannelProcessor().processEvent(event); Thread.sleep(delay); } } catch (Exception e) { e.printStackTrace(); return Status.BACKOFF; } return Status.READY; } @Override public long getBackOffSleepIncrement() { return 0; } @Override public long getMaxBackOffSleepInterval() { return 0; } } 3）测试（1）打包将写好的代码打包，并放到flume的lib目录（/opt/module/flume）下。（2）配置文件（mysource.conf） # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = com.MySource a1.sources.r1.delay = 1000 # Describe the sink a1.sinks.k1.type = logger # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 （3）开启任务在/opt/module/flume输入命令： bin/flume-ng agent -c conf/ -f job/mysource.conf -n a1 -Dflume.root.logger=INFO,console 三、自定义Sink 1、概述Sink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。官方提供的Sink类型已经很多，但是有时候并不能满足实际开发当中的需求，此时我们就需要根据实际需求自定义某些Sink。官方也提供了自定义source的接口：https://flume.apache.org/FlumeDeveloperGuide.html#sink 根据官方说明自定义MySink需要继承AbstractSink类并实现Configurable接口。实现相应方法： configure(Context context) //初始化context（读取配置文件内容） process() //从Channel读取获取数据（event），这个方法将被循环调用。 使用场景：读取Channel数据写入MySQL或者其他文件系统。 2、需求使用flume接收数据，并在Sink端给每条数据添加前缀和后缀，输出到控制台。前后缀可在flume任务配置文件中配置。流程分析： 3、编码1）代码 package com; import org.apache.flume.*; import org.apache.flume.conf.Configurable; import org.apache.flume.sink.AbstractSink; import org.slf4j.Logger; import org.slf4j.LoggerFactory; public class MySink extends AbstractSink implements Configurable { //创建Logger对象 private static final Logger LOG = LoggerFactory.getLogger(AbstractSink.class); private String prefix; private String suffix; @Override public Status process() throws EventDeliveryException { //声明返回值状态信息 Status status; //获取当前Sink绑定的Channel Channel ch = getChannel(); //获取事务 Transaction txn = ch.getTransaction(); //声明事件 Event event; //开启事务 txn.begin(); //读取Channel中的事件，直到读取到事件结束循环 while (true) { event = ch.take(); if (event != null) { break; } } try { //处理事件（打印） LOG.info(prefix + new String(event.getBody()) + suffix); //事务提交 txn.commit(); status = Status.READY; } catch (Exception e) { //遇到异常，事务回滚 txn.rollback(); status = Status.BACKOFF; } finally { //关闭事务 txn.close(); } return status; } @Override public void configure(Context context) { //读取配置文件内容，有默认值 prefix = context.getString("prefix", "hello:"); //读取配置文件内容，无默认值 suffix = context.getString("suffix"); } } 2）测试（1）打包将写好的代码打包，并放到flume的lib目录（/opt/module/flume）下。（2）配置文件（mysink.conf） # Name the components on this agent a1.sources = r1 a1.sinks = k1 a1.channels = c1 # Describe/configure the source a1.sources.r1.type = netcat a1.sources.r1.bind = localhost a1.sources.r1.port = 44444 # Describe the sink a1.sinks.k1.type = com.MySink #a1.sinks.k1.prefix = atguigu: a1.sinks.k1.suffix = :atguigu # Use a channel which buffers events in memory a1.channels.c1.type = memory a1.channels.c1.capacity = 1000 a1.channels.c1.transactionCapacity = 100 # Bind the source and sink to the channel a1.sources.r1.channels = c1 a1.sinks.k1.channel = c1 （3）开启任务在/opt/module/flume输入命令： bin/flume-ng agent -c conf/ -f job/mysink.conf -n a1 -Dflume.root.logger=INFO,console]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>实战</tag>
        <tag>大数据</tag>
        <tag>Flume</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Flume系列——Flume概述]]></title>
    <url>%2F2018%2F05%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AEFlume%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Flume%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[一、Flume概述 1、FlumeFlume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume基于流式架构，灵活简单。 主要作用：实时读取服务器本地磁盘的数据，将数据写入到HDFS。 2、Flume优点1）可以和任意存储进程集成。2）输入的的数据速率大于写入目的存储的速率，flume会进行缓冲，减小hdfs的压力。3）flume中的事务基于channel，使用了两个事务模型（sender + receiver），确保消息被可靠发送。Flume使用两个独立的事务分别负责从soucrce到channel，以及从channel到sink的事件传递。一旦事务中所有的数据全部成功提交到channel，那么source才认为该数据读取完成。同理，只有成功被sink写出去的数据，才会从channel中移除。 3、Flume架构 Source数据输入端的类型：avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy等。但是目前在企业中使用最广泛的就是日志文件。Channel位于Source和Sink之间的缓冲区。Flume自带两种Channel：Memory Channel和File Channel。Memory Channel是基于内存缓存，在不需要关心数据丢失的情景下适用。File Channel是Flume的持久化Channel。系统宕机不会丢失数据。Sink组件目的地包括hdfs、kafka、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。但是目前在企业中使用最广泛的是HDFS和Kafka。 1）AgentAgent是一个JVM进程，它以事件的形式将数据从源头送至目的，是Flume数据传输的基本单元。Agent主要有3个部分组成，Source、Channel、Sink。2）SourceSource是负责接收数据到Flume Agent的组件。Source组件可以处理各种类型、各种格式的日志数据，包括avro、thrift、exec、jms、spooling directory、netcat、sequence generator、syslog、http、legacy。3）ChannelChannel是位于Source和Sink之间的缓冲区。因此，Channel允许Source和Sink运作在不同的速率上。Channel是线程安全的，可以同时处理几个Source的写入操作和几个Sink的读取操作。Flume自带两种Channel：Memory Channel和File Channel。Memory Channel是内存中的队列。Memory Channel在不需要关心数据丢失的情景下适用。如果需要关心数据丢失，那么Memory Channel就不应该使用，因为程序死亡、机器宕机或者重启都会导致数据丢失。File Channel将所有事件写到磁盘。因此在程序关闭或机器宕机的情况下不会丢失数据。4）SinkSink不断地轮询Channel中的事件且批量地移除它们，并将这些事件批量写入到存储或索引系统、或者被发送到另一个Flume Agent。（sink从channel中拉取数据，然后推给下一个组件）Sink是完全事务性的。在从Channel批量删除数据之前，每个Sink用Channel启动一个事务。批量事件一旦成功写出到存储系统或下一个Flume Agent，Sink就利用Channel提交事务。事务一旦被提交，该Channel从自己的内部缓冲区删除事件。Sink组件目的地包括hdfs、logger、avro、thrift、ipc、file、null、HBase、solr、自定义。5）Event传输单元，Flume数据传输的基本单元，以事件的形式将数据从源头送至目的地。 4、Flume拓扑结构1）Flume Agent连接 该模式是将多个flume给顺序连接起来了，从最初的source开始到最终sink传送的目的存储系统。此模式不建议桥接过多的flume数量， flume数量过多不仅会影响传输速率，而且一旦传输过程中某个节点flume宕机，会影响整个传输系统。 2）单source，多channel、sink Flume支持将事件流向一个或者多个目的地。这种模式将数据源复制到多个channel中，每个channel都有相同的数据，sink可以选择传送的不同的目的地。 3）负载均衡 Flume支持使用将多个sink逻辑上分到一个sink组，flume将数据发送到不同的sink，主要解决负载均衡和故障转移问题。 4）Flume Agent聚合 这种模式是最常见的，也非常实用，日常web应用通常分布在上百个服务器，大者甚至上千个、上万个服务器。产生的日志，处理起来也非常麻烦。用flume的这种组合方式能很好的解决这一问题，每台服务器部署一个flume采集日志，传送到一个集中收集日志的flume，再由此flume上传到hdfs、hive、hbase、jms等，进行日志分析。 5、Flume Agent内部原理 Channel Selectors由两种类型：Replicating Channel Selector（默认）和Multiplexing Channel Selector。Replicating会将source过来的events发往所有channel，而Multiplexing可以配置发往哪些Channel。]]></content>
      <categories>
        <category>Flume</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Flume</tag>
        <tag>简介</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据HBase系列——HBase面试题]]></title>
    <url>%2F2018%2F05%2F19%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHBase%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94HBase%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[HBase面试题整理（一）1、 HBase的特点是什么？1）大：一个表可以有数十亿行，上百万列；2）无模式：每行都有一个可排序的主键和任意多的列，列可以根据需要动态的增加，同一张表中不同的行可以有截然不同的列； 3）面向列：面向列（族）的存储和权限控制，列（族）独立检索；4）稀疏：空（null）列并不占用存储空间，表可以设计的非常稀疏；5）数据多版本：每个单元中的数据可以有多个版本，默认情况下版本号自动分配，是单元格插入时的时间戳；6）数据类型单一：Hbase中的数据都是字符串，没有类型。 2、HBase和Hive的区别？ Hive和Hbase是两种基于Hadoop的不同技术–Hive是一种类SQL的引擎，并且运行MapReduce任务，Hbase是一种在Hadoop之上的NoSQL 的Key/vale数据库。 当然，这两种工具是可以同时使用的。就像用Google来搜索，用FaceBook进行社交一样，Hive可以用来进行统计查询，HBase可以用来进行实时查询， 数据也可以从Hive写到Hbase，设置再从Hbase写回Hive。 3、HBase适用于怎样的情景？① 半结构化或非结构化数据对于数据结构字段不够确定或杂乱无章很难按一个概念去进行抽取的数据适合用HBase。以上面的例子为例，当业务发展需要存储author的email，phone，address信息时RDBMS需要停机维护，而HBase支持动态增加。② 记录非常稀疏RDBMS的行有多少列是固定的，为null的列浪费了存储空间。而如上文提到的，HBase为null的Column不会被存储，这样既节省了空间又提高了读性能。③ 多版本数据如上文提到的根据Row key和Column key定位到的Value可以有任意数量的版本值，因此对于需要存储变动历史记录的数据，用HBase就非常方便了。比如上例中的author的Address是会变动的，业务上一般只需要最新的值，但有时可能需要查询到历史值。④ 超大数据量当数据量越来越大，RDBMS数据库撑不住了，就出现了读写分离策略，通过一个Master专门负责写操作，多个Slave负责读操作，服务器成本倍增。随着压力增加，Master撑不住了，这时就要分库了，把关联不大的数据分开部署，一些join查询不能用了，需要借助中间层。随着数据量的进一步增加，一个表的记录越来越大，查询就变得很慢，于是又得搞分表，比如按ID取模分成多个表以减少单个表的记录数。经历过这些事的人都知道过程是多么的折腾。采用HBase就简单了，只需要加机器即可，HBase会自动水平切分扩展，跟Hadoop的无缝集成保障了其数据可靠性（HDFS）和海量数据分析的高性能（MapReduce）。 4、描述HBase的rowKey的设计原则？（☆☆☆☆☆）（1）Rowkey长度原则Rowkey 是一个二进制码流，Rowkey 的长度被很多开发者建议说设计在10~100 个字节，不过建议是越短越好，不要超过16 个字节。原因如下：① 数据的持久化文件HFile 中是按照KeyValue 存储的，如果Rowkey 过长比如100 个字节，1000 万列数据光Rowkey 就要占用100*1000 万=10 亿个字节，将近1G 数据，这会极大影响HFile 的存储效率；② MemStore 将缓存部分数据到内存，如果Rowkey 字段过长内存的有效利用率会降低，系统将无法缓存更多的数据，这会降低检索效率。因此Rowkey 的字节长度越短越好。③ 目前操作系统是都是64 位系统，内存8 字节对齐。控制在16 个字节，8 字节的整数倍利用操作系统的最佳特性。（2）Rowkey散列原则如果Rowkey是按时间戳的方式递增，不要将时间放在二进制码的前面，建议将Rowkey的高位作为散列字段，由程序循环生成，低位放时间字段，这样将提高数据均衡分布在每个Regionserver 实现负载均衡的几率。如果没有散列字段，首字段直接是时间信息将产生所有新数据都在一个 RegionServer 上堆积的热点现象，这样在做数据检索的时候负载将会集中在个别RegionServer，降低查询效率。（3）Rowkey唯一原则必须在设计上保证其唯一性。 5、描述HBase中scan和get的功能以及实现的异同？（☆☆☆☆☆）HBase的查询实现只提供两种方式：1）按指定RowKey 获取唯一一条记录，get方法（org.apache.hadoop.hbase.client.Get） Get 的方法处理分两种 : 设置了ClosestRowBefore 和没有设置ClosestRowBefore的rowlock。主要是用来保证行的事务性，即每个get 是以一个row 来标记的。一个row中可以有很多family 和column。2）按指定的条件获取一批记录，scan方法(org.apache.Hadoop.hbase.client.Scan）实现条件查询功能使用的就是scan 方式。（1）scan 可以通过setCaching 与setBatch 方法提高速度(以空间换时间)；（2）scan 可以通过setStartRow 与setEndRow 来限定范围([start，end)start 是闭区间，end 是开区间)。范围越小，性能越高。（3）scan 可以通过setFilter 方法添加过滤器，这也是分页、多条件查询的基础。 6、请描述HBase中scan对象的setCache和setBatch方法的使用？（☆☆☆☆☆）setCache用于设置缓存，即设置一次RPC请求可以获取多行数据。对于缓存操作，如果行的数据量非常大，多行数据有可能超过客户端进程的内存容量，由此引入批量处理这一解决方案。setBatch 用于设置批量处理，批量可以让用户选择每一次ResultScanner实例的next操作要取回多少列，例如，在扫描中设置setBatch(5)，则一次next()返回的Result实例会包括5列。如果一行包括的列数超过了批量中设置的值，则可以将这一行分片，每次next操作返回一片，当一行的列数不能被批量中设置的值整除时，最后一次返回的Result实例会包含比较少的列，如，一行17列，batch设置为5，则一共返回4个Result实例，这4个实例中包括的列数分别为5、5、5、2。组合使用扫描器缓存和批量大小，可以让用户方便地控制扫描一个范围内的行键所需要的RPC调用次数。Cache设置了服务器一次返回的行数，而Batch设置了服务器一次返回的列数。假如我们建立了一张有两个列族的表，添加了10行数据，每个行的每个列族下有10列，这意味着整个表一共有200列（或单元格，因为每个列只有一个版本），其中每行有20列。 ① Batch参数决定了一行数据分为几个Result，它只针对一行数据，Batch再大，也只能将一行的数据放入一个Result中。所以当一行数据有10列，而Batch为100时，也只能将一行的所有列都放入一个Result，不会混合其他行；② 缓存值决定一次RPC返回几个Result，根据Batch划分的Result个数除以缓存个数可以得到RPC消息个数（之前定义缓存值决定一次返回的行数，这是不准确的，准确来说是决定一次RPC返回的Result个数，由于在引入Batch之前，一行封装为一个Result，因此定义缓存值决定一次返回的行数，但引入Batch后，更准确的说法是缓存值决定了一次RPC返回的Result个数）；RPC请求次数 = （行数 * 每行列数） / Min（每行的列数，批量大小） / 扫描器缓存下图展示了缓存和批量两个参数如何联动，下图中有一个包含9行数据的表，每行都包含一些列。使用了一个缓存为6、批量大小为3的扫描器，需要三次RPC请求来传送数据： 7、请详细描述HBase中一个cell的结构？HBase中通过row和columns确定的为一个存贮单元称为cell。Cell：由{row key, column(= + ), version}唯一确定的单元。cell 中的数据是没有类型的，全部是字节码形式存贮。 8、简述HBase中compact用途是什么，什么时候触发，分为哪两种，有什么区别，有哪些相关配置参数？（☆☆☆☆☆）在hbase中每当有memstore数据flush到磁盘之后，就形成一个storefile，当storeFile的数量达到一定程度后，就需要将 storefile 文件来进行 compaction 操作。Compact 的作用：① 合并文件② 清除过期，多余版本的数据③ 提高读写数据的效率HBase 中实现了两种 compaction 的方式：minor and major. 这两种 compaction 方式的区别是：1）Minor 操作只用来做部分文件的合并操作以及包括 minVersion=0 并且设置 ttl 的过期版本清理，不做任何删除数据、多版本数据的清理工作。2）Major 操作是对 Region 下的HStore下的所有StoreFile执行合并操作，最终的结果是整理合并出一个文件。 9、每天百亿数据存入HBase，如何保证数据的存储正确和在规定的时间里全部录入完毕，不残留数据？（☆☆☆☆☆）需求分析：1）百亿数据：证明数据量非常大；2）存入HBase：证明是跟HBase的写入数据有关；3）保证数据的正确：要设计正确的数据结构保证正确性；4）在规定时间内完成：对存入速度是有要求的。解决思路：1）数据量百亿条，什么概念呢？假设一整天60x60x24 = 86400秒都在写入数据，那么每秒的写入条数高达100万条，HBase当然是支持不了每秒百万条数据的，所以这百亿条数据可能不是通过实时地写入，而是批量地导入。批量导入推荐使用BulkLoad方式（推荐阅读：Spark之读写HBase），性能是普通写入方式几倍以上；2）存入HBase：普通写入是用JavaAPI put来实现，批量导入推荐使用BulkLoad；3）保证数据的正确：这里需要考虑RowKey的设计、预建分区和列族设计等问题；4）在规定时间内完成也就是存入速度不能过慢，并且当然是越快越好，使用BulkLoad。 10、请列举几个HBase优化方法？（☆☆☆☆☆）1）减少调整减少调整这个如何理解呢？HBase中有几个内容会动态调整，如region（分区）、HFile，所以通过一些方法来减少这些会带来I/O开销的调整。① Region如果没有预建分区的话，那么随着region中条数的增加，region会进行分裂，这将增加I/O开销，所以解决方法就是根据你的RowKey设计来进行预建分区，减少region的动态分裂。② HFileHFile是数据底层存储文件，在每个memstore进行刷新时会生成一个HFile，当HFile增加到一定程度时，会将属于一个region的HFile进行合并，这个步骤会带来开销但不可避免，但是合并后HFile大小如果大于设定的值，那么HFile会重新分裂。为了减少这样的无谓的I/O开销，建议估计项目数据量大小，给HFile设定一个合适的值。2）减少启停数据库事务机制就是为了更好地实现批量写入，较少数据库的开启关闭带来的开销，那么HBase中也存在频繁开启关闭带来的问题。① 关闭Compaction，在闲时进行手动Compaction。因为HBase中存在Minor Compaction和Major Compaction，也就是对HFile进行合并，所谓合并就是I/O读写，大量的HFile进行肯定会带来I/O开销，甚至是I/O风暴，所以为了避免这种不受控制的意外发生，建议关闭自动Compaction，在闲时进行compaction。② 批量数据写入时采用BulkLoad。如果通过HBase-Shell或者JavaAPI的put来实现大量数据的写入，那么性能差是肯定并且还可能带来一些意想不到的问题，所以当需要写入大量离线数据时建议使用BulkLoad。3）减少数据量虽然我们是在进行大数据开发，但是如果可以通过某些方式在保证数据准确性同时减少数据量，何乐而不为呢？① 开启过滤，提高查询速度开启BloomFilter，BloomFilter是列族级别的过滤，在生成一个StoreFile同时会生成一个MetaBlock，用于查询时过滤数据② 使用压缩一般推荐使用Snappy和LZO压缩4）合理设计在一张HBase表格中RowKey和ColumnFamily的设计是非常重要，好的设计能够提高性能和保证数据的准确性① RowKey设计：应该具备以下几个属性散列性：散列性能够保证相同相似的rowkey聚合，相异的rowkey分散，有利于查询。简短性：rowkey作为key的一部分存储在HFile中，如果为了可读性将rowKey设计得过长，那么将会增加存储压力。唯一性：rowKey必须具备明显的区别性。业务性：举例来说：假如我的查询条件比较多，而且不是针对列的条件，那么rowKey的设计就应该支持多条件查询。如果我的查询要求是最近插入的数据优先，那么rowKey则可以采用叫上Long.Max-时间戳的方式，这样rowKey就是递减排列。② 列族的设计：列族的设计需要看应用场景优势：HBase中数据时按列进行存储的，那么查询某一列族的某一列时就不需要全盘扫描，只需要扫描某一列族，减少了读I/O；其实多列族设计对减少的作用不是很明显，适用于读多写少的场景劣势：降低了写的I/O性能。原因如下：数据写到store以后是先缓存在memstore中，同一个region中存在多个列族则存在多个store，每个store都一个memstore，当其实memstore进行flush时，属于同一个region的store中的memstore都会进行flush，增加I/O开销。 11、Region如何预建分区？预分区的目的主要是在创建表的时候指定分区数，提前规划表有多个分区，以及每个分区的区间范围，这样在存储的时候rowkey按照分区的区间存储，可以避免region热点问题。通常有两种方案：方案1：shell 方法create ‘tb_splits’, {NAME =&gt; ‘cf’,VERSIONS=&gt; 3},{SPLITS =&gt; [‘10’,’20’,’30’]}方案2：JAVA程序控制① 取样，先随机生成一定数量的rowkey,将取样数据按升序排序放到一个集合里；② 根据预分区的region个数，对整个集合平均分割，即是相关的splitKeys；③ HBaseAdmin.createTable(HTableDescriptor tableDescriptor,byte[][]splitkeys)可以指定预分区的splitKey，即是指定region间的rowkey临界值。 12、HRegionServer宕机如何处理？（☆☆☆☆☆）1）ZooKeeper会监控HRegionServer的上下线情况，当ZK发现某个HRegionServer宕机之后会通知HMaster进行失效备援；2）该HRegionServer会停止对外提供服务，就是它所负责的region暂时停止对外提供服务；3）HMaster会将该HRegionServer所负责的region转移到其他HRegionServer上，并且会对HRegionServer上存在memstore中还未持久化到磁盘中的数据进行恢复；4）这个恢复的工作是由WAL重播来完成，这个过程如下：① wal实际上就是一个文件，存在/hbase/WAL/对应RegionServer路径下。② 宕机发生时，读取该RegionServer所对应的路径下的wal文件，然后根据不同的region切分成不同的临时文件recover.edits。③ 当region被分配到新的RegionServer中，RegionServer读取region时会进行是否存在recover.edits，如果有则进行恢复。 13、HBase读写流程？（☆☆☆☆☆）读：① HRegionServer保存着meta表以及表数据，要访问表数据，首先Client先去访问zookeeper，从zookeeper里面获取meta表所在的位置信息，即找到这个meta表在哪个HRegionServer上保存着。② 接着Client通过刚才获取到的HRegionServer的IP来访问Meta表所在的HRegionServer，从而读取到Meta，进而获取到Meta表中存放的元数据。③ Client通过元数据中存储的信息，访问对应的HRegionServer，然后扫描所在HRegionServer的Memstore和Storefile来查询数据。④ 最后HRegionServer把查询到的数据响应给Client。写：① Client先访问zookeeper，找到Meta表，并获取Meta表元数据。② 确定当前将要写入的数据所对应的HRegion和HRegionServer服务器。③ Client向该HRegionServer服务器发起写入数据请求，然后HRegionServer收到请求并响应。④ Client先把数据写入到HLog，以防止数据丢失。⑤ 然后将数据写入到Memstore。⑥ 如果HLog和Memstore均写入成功，则这条数据写入成功。⑦ 如果Memstore达到阈值，会把Memstore中的数据flush到Storefile中。⑧ 当Storefile越来越多，会触发Compact合并操作，把过多的Storefile合并成一个大的Storefile。⑨ 当Storefile越来越大，Region也会越来越大，达到阈值后，会触发Split操作，将Region一分为二。 14、HBase内部机制是什么？Hbase是一个能适应联机业务的数据库系统物理存储：hbase的持久化数据是将数据存储在HDFS上。存储管理：一个表是划分为很多region的，这些region分布式地存放在很多regionserver上Region内部还可以划分为store，store内部有memstore和storefile。版本管理：hbase中的数据更新本质上是不断追加新的版本，通过compact操作来做版本间的文件合并Region的split。集群管理：ZooKeeper + HMaster + HRegionServer。 15、Hbase中的memstore是用来做什么的？hbase为了保证随机读取的性能，所以hfile里面的rowkey是有序的。当客户端的请求在到达regionserver之后，为了保证写入rowkey的有序性，所以不能将数据立刻写入到hfile中，而是将每个变更操作保存在内存中，也就是memstore中。memstore能够很方便的支持操作的随机插入，并保证所有的操作在内存中是有序的。当memstore达到一定的量之后，会将memstore里面的数据flush到hfile中，这样能充分利用hadoop写入大文件的性能优势，提高写入性能。由于memstore是存放在内存中，如果regionserver因为某种原因死了，会导致内存中数据丢失。所有为了保证数据不丢失，hbase将更新操作在写入memstore之前会写入到一个write ahead log(WAL)中。WAL文件是追加、顺序写入的，WAL每个regionserver只有一个，同一个regionserver上所有region写入同一个的WAL文件。这样当某个regionserver失败时，可以通过WAL文件，将所有的操作顺序重新加载到memstore中。 16、HBase在进行模型设计时重点在什么地方？一张表中定义多少个Column Family最合适？为什么？（☆☆☆☆☆）Column Family的个数具体看表的数据，一般来说划分标准是根据数据访问频度，如一张表里有些列访问相对频繁，而另一些列访问很少，这时可以把这张表划分成两个列族，分开存储，提高访问效率。 17、如何提高HBase客户端的读写性能？请举例说明（☆☆☆☆☆）① 开启bloomfilter过滤器，开启bloomfilter比没开启要快3、4倍② Hbase对于内存有特别的需求，在硬件允许的情况下配足够多的内存给它③ 通过修改hbase-env.sh中的 export HBASE_HEAPSIZE=3000 #这里默认为1000m④ 增大RPC数量通过修改hbase-site.xml中的hbase.regionserver.handler.count属性，可以适当的放大RPC数量，默认值为10有点小。 18、HBase集群安装注意事项？① HBase需要HDFS的支持，因此安装HBase前确保Hadoop集群安装完成；② HBase需要ZooKeeper集群的支持，因此安装HBase前确保ZooKeeper集群安装完成；③ 注意HBase与Hadoop的版本兼容性；④ 注意hbase-env.sh配置文件和hbase-site.xml配置文件的正确配置；⑤ 注意regionservers配置文件的修改；⑥ 注意集群中的各个节点的时间必须同步，否则启动HBase集群将会报错。 19、直接将时间戳作为行健，在写入单个region 时候会发生热点问题，为什么呢？（☆☆☆☆☆）region中的rowkey是有序存储，若时间比较集中。就会存储到一个region中，这样一个region的数据变多，其它的region数据很少，加载数据就会很慢，直到region分裂，此问题才会得到缓解。 20、请描述如何解决HBase中region太小和region太大带来的冲突？（☆☆☆☆☆）Region过大会发生多次compaction，将数据读一遍并重写一遍到hdfs 上，占用io，region过小会造成多次split，region 会下线，影响访问服务，最佳的解决方法是调整hbase.hregion. max.filesize 为256m。]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据HBase系列——HBase API实践操作]]></title>
    <url>%2F2018%2F05%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHBase%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94HBase%20API%E5%AE%9E%E8%B7%B5%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[四、HBase API操作 1、环境准备新建项目后在pom.xml中添加依赖： &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-server&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hbase&lt;/groupId&gt; &lt;artifactId&gt;hbase-client&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;jdk.tools&lt;/groupId&gt; &lt;artifactId&gt;jdk.tools&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;scope&gt;system&lt;/scope&gt; &lt;systemPath&gt;${JAVA_HOME}/lib/tools.jar&lt;/systemPath&gt; &lt;/dependency&gt; 2、HBase API1）获取Configuration对象 public static Configuration conf; static{ //使用HBaseConfiguration的单例方法实例化 conf = HBaseConfiguration.create(); conf.set("hbase.zookeeper.quorum", "192.168.9.102"); conf.set("hbase.zookeeper.property.clientPort", "2181"); } 2）判断表是否存在 public static boolean isTableExist(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{ //在HBase中管理、访问表需要先创建HBaseAdmin对象 //Connection connection = ConnectionFactory.createConnection(conf); //HBaseAdmin admin = (HBaseAdmin) connection.getAdmin(); HBaseAdmin admin = new HBaseAdmin(conf); return admin.tableExists(tableName); } 3）创建表 public static void createTable(String tableName, String... columnFamily) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{ HBaseAdmin admin = new HBaseAdmin(conf); //判断表是否存在 if(isTableExist(tableName)){ System.out.println("表" + tableName + "已存在"); //System.exit(0); }else{ //创建表属性对象,表名需要转字节 HTableDescriptor descriptor = new HTableDescriptor(TableName.valueOf(tableName)); //创建多个列族 for(String cf : columnFamily){ descriptor.addFamily(new HColumnDescriptor(cf)); } //根据对表的配置，创建表 admin.createTable(descriptor); System.out.println("表" + tableName + "创建成功！"); } } 4）删除表 public static void dropTable(String tableName) throws MasterNotRunningException, ZooKeeperConnectionException, IOException{ HBaseAdmin admin = new HBaseAdmin(conf); if(isTableExist(tableName)){ admin.disableTable(tableName); admin.deleteTable(tableName); System.out.println("表" + tableName + "删除成功！"); }else{ System.out.println("表" + tableName + "不存在！"); } } 5）向表中插入数据向表中插入数据 public static void addRowData(String tableName, String rowKey, String columnFamily, String column, String value) throws IOException{ //创建HTable对象 HTable hTable = new HTable(conf, tableName); //向表中插入数据 Put put = new Put(Bytes.toBytes(rowKey)); //向Put对象中组装数据 put.add(Bytes.toBytes(columnFamily), Bytes.toBytes(column), Bytes.toBytes(value)); hTable.put(put); hTable.close(); System.out.println("插入数据成功"); } 6）删除多行数据 public static void deleteMultiRow(String tableName, String... rows) throws IOException{ HTable hTable = new HTable(conf, tableName); List&lt;Delete&gt; deleteList = new ArrayList&lt;Delete&gt;(); for(String row : rows){ Delete delete = new Delete(Bytes.toBytes(row)); deleteList.add(delete); } hTable.delete(deleteList); hTable.close(); } 7）获取所有数据 public static void getAllRows(String tableName) throws IOException{ HTable hTable = new HTable(conf, tableName); //得到用于扫描region的对象 Scan scan = new Scan(); //使用HTable得到resultcanner实现类的对象 ResultScanner resultScanner = hTable.getScanner(scan); for(Result result : resultScanner){ Cell[] cells = result.rawCells(); for(Cell cell : cells){ //得到rowkey System.out.println("行键:" + Bytes.toString(CellUtil.cloneRow(cell))); //得到列族 System.out.println("列族" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println("列:" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println("值:" + Bytes.toString(CellUtil.cloneValue(cell))); } } } 8）获取某一行数据 public static void getRow(String tableName, String rowKey) throws IOException{ HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); //get.setMaxVersions();显示所有版本 //get.setTimeStamp();显示指定时间戳的版本 Result result = table.get(get); for(Cell cell : result.rawCells()){ System.out.println("行键:" + Bytes.toString(result.getRow())); System.out.println("列族" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println("列:" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println("值:" + Bytes.toString(CellUtil.cloneValue(cell))); System.out.println("时间戳:" + cell.getTimestamp()); } } 9）获取某一行指定“列族:列”的数据 public static void getRowQualifier(String tableName, String rowKey, String family, String qualifier) throws IOException{ HTable table = new HTable(conf, tableName); Get get = new Get(Bytes.toBytes(rowKey)); get.addColumn(Bytes.toBytes(family), Bytes.toBytes(qualifier)); Result result = table.get(get); for(Cell cell : result.rawCells()){ System.out.println("行键:" + Bytes.toString(result.getRow())); System.out.println("列族" + Bytes.toString(CellUtil.cloneFamily(cell))); System.out.println("列:" + Bytes.toString(CellUtil.cloneQualifier(cell))); System.out.println("值:" + Bytes.toString(CellUtil.cloneValue(cell))); } }]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>实战</tag>
        <tag>大数据</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据HBase系列——HBase Shell操作]]></title>
    <url>%2F2018%2F05%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHBase%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94HBase%20Shell%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[三、HBase Shell操作 1、基本操作1）进入HBase客户端命令行 [drift@hadoop102 hbase]$ bin/hbase shell 2）查看帮助命令 hbase(main):001:0&gt; help 3）查看当前数据库中有哪些表 hbase(main):002:0&gt; list 2、表的操作1）创建表 hbase(main):002:0&gt; create &apos;student&apos;,&apos;info&apos; 2）插入数据到表 hbase(main):003:0&gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:sex&apos;,&apos;male&apos; hbase(main):004:0&gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:age&apos;,&apos;18&apos; hbase(main):005:0&gt; put &apos;student&apos;,&apos;1002&apos;,&apos;info:name&apos;,&apos;Janna&apos; hbase(main):006:0&gt; put &apos;student&apos;,&apos;1002&apos;,&apos;info:sex&apos;,&apos;female&apos; hbase(main):007:0&gt; put &apos;student&apos;,&apos;1002&apos;,&apos;info:age&apos;,&apos;20&apos; 3）扫描查看表数据 hbase(main):008:0&gt; scan &apos;student&apos; hbase(main):009:0&gt; scan &apos;student&apos;,{STARTROW =&gt; &apos;1001&apos;, STOPROW =&gt; &apos;1001&apos;} hbase(main):010:0&gt; scan &apos;student&apos;,{STARTROW =&gt; &apos;1001&apos;} 4）查看表结构 hbase(main):011:0&gt; describe &apos;student&apos; 5）更新指定字段的数据 hbase(main):012:0&gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:name&apos;,&apos;Nick&apos; hbase(main):013:0&gt; put &apos;student&apos;,&apos;1001&apos;,&apos;info:age&apos;,&apos;100&apos; 6）查看“指定行”或“指定列族:列”的数据 hbase(main):014:0&gt; get &apos;student&apos;,&apos;1001&apos; hbase(main):015:0&gt; get &apos;student&apos;,&apos;1001&apos;,&apos;info:name&apos; 7）统计表数据行数 hbase(main):021:0&gt; count &apos;student&apos; 8）删除数据 删除某rowkey的全部数据： hbase(main):016:0&gt; deleteall &apos;student&apos;,&apos;1001&apos; 删除某rowkey的某一列数据： hbase(main):017:0&gt; delete &apos;student&apos;,&apos;1002&apos;,&apos;info:sex&apos; 9）清空表数据 hbase(main):018:0&gt; truncate &apos;student&apos; 提示：清空表的操作顺序为先disable，然后再truncate。 10）删除表 首先需要先让该表为disable状态： hbase(main):019:0&gt; disable &apos;student&apos; 然后才能drop这个表： hbase(main):020:0&gt; drop &apos;student&apos; 提示：如果直接drop表，会报错：ERROR: Table student is enabled. Disable it first. 11）变更表信息 将info列族中的数据存放3个版本： hbase(main):022:0&gt; alter &apos;student&apos;,{NAME=&gt;&apos;info&apos;,VERSIONS=&gt;3} hbase(main):022:0&gt; get &apos;student&apos;,&apos;1001&apos;,{COLUMN=&gt;&apos;info:name&apos;,VERSIONS=&gt;3}]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>Shell</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据HBase系列——HBase数据结构]]></title>
    <url>%2F2018%2F05%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHBase%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94HBase%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[HBase数据结构1、RowKey与nosql数据库们一样,RowKey是用来检索记录的主键。访问HBASE table中的行，只有三种方式： 1）通过单个RowKey访问(get)2）通过RowKey的range（正则）(like)3）全表扫描(scan)RowKey行键 (RowKey)可以是任意字符串(最大长度是64KB，实际应用中长度一般为 10-100bytes)，在HBASE内部，RowKey保存为字节数组。存储时，数据按照RowKey的字典序(byte order)排序存储。设计RowKey时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性) 2、Column Family列族：HBASE表中的每个列，都归属于某个列族。列族是表的schema的一部 分(而列不是)，必须在使用表之前定义。列名都以列族作为前缀。例如 courses:history，courses:math都属于courses 这个列族。 3、Cell由{rowkey, column Family:columu, version} 唯一确定的单元。cell中的数据是没有类型的，全部是字节码形式存贮。关键字：无类型、字节码 4、Time StampHBASE 中通过rowkey和columns确定的为一个存贮单元称为cell。每个 cell都保存 着同一份数据的多个版本。版本通过时间戳来索引。时间戳的类型是 64位整型。时间戳可以由HBASE(在数据写入时自动 )赋值，此时时间戳是精确到毫秒 的当前系统时间。时间戳也可以由客户显式赋值。如果应用程序要避免数据版 本冲突，就必须自己生成具有唯一性的时间戳。每个 cell中，不同版本的数据按照时间倒序排序，即最新的数据排在最前面。为了避免数据存在过多版本造成的的管理 (包括存贮和索引)负担，HBASE提供 了两种数据版本回收方式。一是保存数据的最后n个版本，二是保存最近一段 时间内的版本（比如最近七天）。用户可以针对每个列族进行设置。 5、命名空间命名空间结构如下： 1) Table：表，所有的表都是命名空间的成员，即表必属于某个命名空间，如果没有指定，则在default默认的命名空间中。2) RegionServer group：一个命名空间包含了默认的RegionServer Group。3) Permission：权限，命名空间能够让我们来定义访问控制列表ACL（Access Control List）。例如，创建表，读取表，删除，更新等等操作。4) Quota：限额，可以强制一个命名空间可包含的region的数量。]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>HBase</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据HBase系列——HBase概述]]></title>
    <url>%2F2018%2F05%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHBase%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94HBase%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[一、HBase简介 HBase是一个高可靠性、高性能、面向列、可伸缩的分布式存储系统。 1、概述HBase的原型是Google的BigTable论文，受到了该论文思想的启发，目前作为Hadoop的子项目来开发维护，用于支持结构化的数据存储。Hbase历史：– 2006年Google发表BigTable白皮书– 2006年开始开发HBase– 2008年北京成功开奥运会，程序员默默地将HBase弄成了Hadoop的子项目– 2010年HBase成为Apache顶级项目可利用HBASE技术可在廉价PC Server上搭建起大规模结构化存储集群。HBase的目标是存储并处理大型的数据，更具体来说是仅需使用普通的硬件配置，就能够处理由成千上万的行和列所组成的大型数据。HBase是Google Bigtable的开源实现，但是也有很多不同之处。比如：Google Bigtable利用GFS作为其文件存储系统，HBase利用Hadoop HDFS作为其文件存储系统；Google运行MAPREDUCE来处理Bigtable中的海量数据，HBase同样利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用Zookeeper作为对应。 2、特点1）海量存储Hbase适合存储PB级别的海量数据，在PB级别的数据以及采用廉价PC存储的情况下，能在几十到百毫秒内返回数据。这与Hbase的极易扩展性息息相关。正式因为Hbase良好的扩展性，才为海量数据的存储提供了便利。2）列式存储这里的列式存储其实说的是列族（Column Family）存储，Hbase是根据列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定。3）极易扩展Hbase的扩展性主要体现在两个方面，一个是基于上层处理能力（RegionServer）的扩展，一个是基于存储的扩展（HDFS）。通过横向添加RegionSever的机器，进行水平扩展，提升Hbase上层的处理能力，提升Hbsae服务更多Region的能力。备注：RegionServer的作用是管理region、承接业务的访问，这个后面会详细的介绍通过横向添加Datanode的机器，进行存储层扩容，提升Hbase的数据存储能力和提升后端存储的读写能力。4）高并发（多核）由于目前大部分使用Hbase的架构，都是采用的廉价PC，因此单个IO的延迟其实并不小，一般在几十到上百ms之间。这里说的高并发，主要是在并发的情况下，Hbase的单个IO延迟下降并不多。能获得高并发、低延迟的服务。5）稀疏稀疏主要是针对Hbase列的灵活性，在列族中，你可以指定任意多的列，在列数据为空的情况下，是不会占用存储空间的。 3、HBase架构 从图中可以看出Hbase是由Client、Zookeeper、Master、HRegionServer、HDFS等几个组件组成，下面来介绍一下几个组件的相关功能：1）ClientClient包含了访问Hbase的接口，另外Client还维护了对应的cache来加速Hbase的访问，比如cache的.META.元数据的信息。2）ZookeeperHBase通过Zookeeper来做master的高可用、RegionServer的监控、元数据的入口以及集群配置的维护等工作。具体工作如下：通过Zoopkeeper来保证集群中只有1个master在运行，如果master异常，会通过竞争机制产生新的master提供服务、通过Zoopkeeper来监控RegionServer的状态，当RegionSevrer有异常的时候，通过回调的形式通知Master RegionServer上下线的信息、通过Zoopkeeper存储元数据的统一入口地址。3）Hmaster（NameNode）master节点的主要职责如下：为RegionServer分配Region、维护整个集群的负载均衡、维护集群的元数据信息、发现失效的Region，并将失效的Region分配到正常的RegionServer上、当RegionSever失效的时候，协调对应Hlog的拆分。4）HregionServer(DataNode)HregionServer直接对接用户的读写请求，是真正的“干活”的节点。它的功能概括如下：管理master为其分配的Region、处理来自客户端的读写请求、负责和底层HDFS的交互，存储数据到HDFS、负责Region变大以后的拆分、负责Storefile的合并工作。5）HDFSHDFS为Hbase提供最终的底层数据存储服务，同时为HBase提供高可用（Hlog存储在HDFS）的支持，具体功能概括如下：提供元数据和表数据的底层分布式存储服务、数据多副本，保证的高可靠和高可用性。 4、HBase中的角色及功能1）HMaster功能：1．监控RegionServer2．处理RegionServer故障转移3．处理元数据的变更4．处理region的分配或转移5．在空闲时间进行数据的负载均衡6．通过Zookeeper发布自己的位置给客户端2）RegionServer功能：1．负责存储HBase的实际数据2．处理分配给它的Region3．刷新缓存到HDFS4．维护Hlog5．执行压缩6．负责处理Region分片3）其他组件1．Write-Ahead logsHBase的修改记录，当对HBase读写数据的时候，数据不是直接写进磁盘，它会在内存中保留一段时间（时间以及数据量阈值可以设定）。但把数据保存在内存中可能有更高的概率引起数据丢失，为了解决这个问题，数据会先写在一个叫做Write-Ahead logfile的文件中，然后再写入内存中。所以在系统出现故障的时候，数据可以通过这个日志文件重建。2．RegionHbase表的分片，HBase表会根据RowKey值被切分成不同的region存储在RegionServer中，在一个RegionServer中可以有多个不同的region。3．StoreHFile存储在Store中，一个Store对应HBase表中的一个列族(列簇， Column Family)。4．MemStore顾名思义，就是内存存储，位于内存中，用来保存当前的数据操作，所以当数据保存在WAL中之后，RegsionServer会在内存中存储键值对。5．HFile这是在磁盘上保存原始数据的实际的物理文件，是实际的存储文件。StoreFile是以Hfile的形式存储在HDFS的。 二、HBase原理 1、HBase读流程 1）Client先访问zookeeper，从meta表读取region的位置，然后读取meta表中的数据。meta中又存储了用户表的region信息；2）根据namespace、表名和rowkey在meta表中找到对应的region信息；3）找到这个region对应的regionserver；4）查找对应的region；5）先从MemStore找数据，如果没有，再到BlockCache里面读；6）BlockCache还没有，再到StoreFile上读(为了读取的效率)；7）如果是从StoreFile里面读取的数据，不是直接返回给客户端，而是先写入BlockCache，再返回给客户端。 2、HBase写流程 1）Client向HregionServer发送写请求；2）HregionServer将数据写到HLog（write ahead log）。为了数据的持久化和恢复；3）HregionServer将数据写到内存（MemStore）；4）反馈Client写成功。 3、数据flush过程1）当MemStore数据达到阈值（默认是128M，老版本是64M），将数据刷到硬盘，将内存中的数据删除，同时删除HLog中的历史数据；2）并将数据存储到HDFS中；3）在HLog中做标记点。 4、数据合并过程1）当数据块达到3块，Hmaster触发合并操作，Region将数据块加载到本地，进行合并；2）当合并的数据超过256M，进行拆分，将拆分后的Region分配给不同的HregionServer管理；3）当HregionServer宕机后，将HregionServer上的hlog拆分，然后分配给不同的HregionServer加载，修改.META.；4）注意：HLog会同步到HDFS。]]></content>
      <categories>
        <category>HBase</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>HBase</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——面试题之Shuffle配置调优]]></title>
    <url>%2F2018%2F03%2F30%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8BShuffle%E9%85%8D%E7%BD%AE%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Spark面试题（八）——Spark的Shuffle配置调优1、Shuffle优化配置 -spark.shuffle.file.buffer默认值：32k参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 2、Shuffle优化配置 -spark.reducer.maxSizeInFlight默认值：48m参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 3、Shuffle优化配置 -spark.shuffle.io.maxRetries默认值：3参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。 4、Shuffle优化配置 -spark.shuffle.io.retryWait默认值：5s参数说明： shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的，该参数代表了每次重试拉取数据的等待间隔，默认是5s。调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。 5、Shuffle优化配置 -spark.shuffle.memoryFraction默认值：0.2参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 6、Shuffle优化配置 -spark.shuffle.manager默认值：sort参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。 7、Shuffle优化配置 -spark.shuffle.sort.bypassMergeThreshold默认值：200参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 8、Shuffle优化配置 -spark.shuffle.consolidateFiles默认值：false参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%。 总结：1、spark.shuffle.file.buffer：主要是设置的Shuffle过程中写文件的缓冲，默认32k，如果内存足够，可以适当调大，来减少写入磁盘的数量。2、spark.reducer.maxSizeInFight：主要是设置Shuffle过程中读文件的缓冲区，一次能够读取多少数据，如果内存足够，可以适当扩大，减少整个网络传输次数。3、spark.shuffle.io.maxRetries：主要是设置网络连接失败时，重试次数，适当调大能够增加稳定性。4、spark.shuffle.io.retryWait：主要设置每次重试之间的间隔时间，可以适当调大，增加程序稳定性。5、spark.shuffle.memoryFraction：Shuffle过程中的内存占用，如果程序中较多使用了Shuffle操作，那么可以适当调大该区域。6、spark.shuffle.manager：Hash和Sort方式，Sort是默认，Hash在reduce数量 比较少的时候，效率会很高。7、spark.shuffle.sort. bypassMergeThreshold：设置的是Sort方式中，启用Hash输出方式的临界值，如果你的程序数据不需要排序，而且reduce数量比较少，那推荐可以适当增大临界值。8、spark. shuffle.cosolidateFiles：如果你使用Hash shuffle方式，推荐打开该配置，实现更少的文件输出。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>优化</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——面试题之资源调优]]></title>
    <url>%2F2018%2F03%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8B%E8%B5%84%E6%BA%90%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Spark面试题（六）——Spark资源调优1、资源运行情况 2、资源运行中的集中情况（1）实践中跑的Spark job，有的特别慢，查看CPU利用率很低，可以尝试减少每个executor占用CPU core的数量，增加并行的executor数量，同时配合增加分片，整体上增加了CPU的利用率，加快数据处理速度。（2）发现某job很容易发生内存溢出，我们就增大分片数量，从而减少了每片数据的规模，同时还减少并行的executor数量，这样相同的内存资源分配给数量更少的executor，相当于增加了每个task的内存分配，这样运行速度可能慢了些，但是总比OOM强。（3）数据量特别少，有大量的小文件生成，就减少文件分片，没必要创建那么多task，这种情况，如果只是最原始的input比较小，一般都能被注意到；但是，如果是在运算过程中，比如应用某个reduceBy或者某个filter以后，数据大量减少，这种低效情况就很少被留意到。 3、运行资源优化配置一个CPU core同一时间只能执行一个线程。而每个Executor进程上分配到的多个task，都是以每个task一条线程的方式，多线程并发运行的。一个应用提交的时候设置多大的内存？设置多少Core？设置几个Executor？ ./bin/spark-submit \ --master yarn-cluster \ --num-executors 100 \ --executor-memory 6G \ --executor-cores 4 \ --driver-memory 1G \ --conf spark.default.parallelism=1000 \ --conf spark.storage.memoryFraction=0.5 \ --conf spark.shuffle.memoryFraction=0.3 \ 3.1 运行资源优化配置 -num-executors参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。参数调优建议：每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 3.2 运行资源优化配置 -executor-memory参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。参数调优建议：每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors * executor-memory，是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的内存量最好不要超过资源队列最大总内存的1/3~1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同事的作业无法运行。 3.3 运行资源优化配置 -executor-cores参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。参数调优建议：Executor的CPU core数量设置为2~4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3~1/2左右比较合适，也是避免影响其他同事的作业运行。 3.4 运行资源优化配置 -driver-memory参数说明：该参数用于设置Driver进程的内存。参数调优建议：Driver的内存通常来说不设置，或者设置1G左右应该就够了。唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理（或者是用map side join操作），那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。 3.5 运行资源优化配置 -spark.default.parallelism参数说明：该参数用于设置每个stage的默认task数量，也可以认为是分区数。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。参数调优建议：Spark作业的默认task数量为500~1000个较为合适。很多人常犯的一个错误就是不去设置这个参数，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。 3.6 运行资源优化配置 -spark.storage.memoryFraction参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 3.7 运行资源优化配置 -spark.shuffle.memoryFraction参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 总结：1、num-executors：应用运行时executor的数量，推荐50-100左右比较合适2、executor-memory：应用运行时executor的内存，推荐4-8G比较合适3、executor-cores：应用运行时executor的CPU核数，推荐2-4个比较合适4、driver-memory：应用运行时driver的内存量，主要考虑如果使用map side join或者一些类似于collect的操作，那么要相应调大内存量5、spark.default.parallelism：每个stage默认的task数量，推荐参数为num-executors * executor-cores的2~3倍较为合适6、spark.storage.memoryFraction：每一个executor中用于RDD缓存的内存比例，如果程序中有大量的数据缓存，可以考虑调大整个的比例，默认为60%7、spark.shuffle.memoryFraction：每一个executor中用于Shuffle操作的内存比例，默认是20%，如果程序中有大量的Shuffle类算子，那么可以考虑其它的比例]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>优化</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——面试题之程序开发调优]]></title>
    <url>%2F2018%2F03%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8B%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[Spark面试题（七）——Spark程序开发调优1、程序开发调优 ：避免创建重复的RDD需要对名为“hello.txt”的HDFS文件进行一次map操作，再进行一次reduce操作。也就是说，需要对一份数据执行两次算子操作。错误的做法： 对于同一份数据执行多次算子操作时，创建多个RDD。//这里执行了两次textFile方法，针对同一个HDFS文件，创建了两个RDD出来，然后分别对每个RDD都执行了一个算子操作。这种情况下，Spark需要从HDFS上两次加载hello.txt文件的内容，并创建两个单独的RDD；//第二次加载HDFS文件以及创建RDD的性能开销，很明显是白白浪费掉的。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758 val rdd1 = sc.textFile("hdfs://master:9000/hello.txt") rdd1.map(...) val rdd2 = sc.textFile("hdfs://master:9000/hello.txt") rdd2.reduce(...)``` **正确的用法**： 对于一份数据执行多次算子操作时，只使用一个RDD。 #### 2、程序开发调优 ：尽可能复用同一个RDD**错误的做法**： 有一个&lt;long , String&gt;格式的RDD，即rdd1。 接着由于业务需要，对rdd1执行了一个map操作，创建了一个rdd2，而rdd2中的数据仅仅是rdd1中的value值而已，也就是说，rdd2是rdd1的子集。 ```scala JavaPairRDD&lt;long , String&gt; rdd1 = ... JavaRDD&lt;string&gt; rdd2 = rdd1.map(...)``` 分别对rdd1和rdd2执行了不同的算子操作。 ```scala rdd1.reduceByKey(...) rdd2.map(...)``` **正确的做法**： rdd2的数据完全就是rdd1的子集而已，却创建了两个rdd，并对两个rdd都执行了一次算子操作。 此时会因为对rdd1执行map算子来创建rdd2，而多执行一次算子操作，进而增加性能开销。 其实在这种情况下完全可以复用同一个RDD。 我们可以使用rdd1，既做reduceByKey操作，也做map操作。 ```scala JavaPairRDD&lt;long , String&gt; rdd1 = ...rdd1.reduceByKey(...) rdd1.map(tuple._2...)``` #### 3、程序开发调优 ：对多次使用的RDD进行持久化**正确的做法**： cache()方法表示：使用非序列化的方式将RDD中的数据全部尝试持久化到内存中。 此时再对rdd1执行两次算子操作时，只有在第一次执行map算子时，才会将这个rdd1从源头处计算一次。 第二次执行reduce算子时，就会直接从内存中提取数据进行计算，不会重复计算一个rdd。 ```scala val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt").cache() rdd1.map(...) rdd1.reduce(...)``` 序列化的方式可以减少持久化的数据对内存/磁盘的占用量，进而避免内存被持久化数据占用过多，从而发生频繁GC。 ```scala val rdd1 = sc.textFile("hdfs://192.168.0.1:9000/hello.txt") .persist(StorageLevel.MEMORY_AND_DISK_SER) rdd1.map(...) rdd1.reduce(...)``` **注意**：通常不建议使用DISK_ONLY和后缀为_2的级别：因为完全基于磁盘文件进行数据的读写，会导致性能急剧降低，导致网络较大开销 #### 4、程序开发调优 ：尽量避免使用shuffle类算子如果有可能的话，要尽量避免使用shuffle类算子，最消耗性能的地方就是shuffle过程。 shuffle过程中，各个节点上的相同key都会先写入本地磁盘文件中，然后其他节点需要通过网络传输拉取各个节点上的磁盘文件中的相同key。而且相同key都拉取到同一个节点进行聚合操作时，还有可能会因为一个节点上处理的key过多，导致内存不够存放，进而溢写到磁盘文件中。因此在shuffle过程中，可能会发生大量的磁盘文件读写的IO操作，以及数据的网络传输操作。磁盘IO和网络数据传输也是shuffle性能较差的主要原因。 **尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子**。 传统的join操作会导致shuffle操作。 因为两个RDD中，相同的key都需要通过网络拉取到一个节点上，由一个task进行join操作。 ```scala val rdd3 = rdd1.join(rdd2) Broadcast+map的join操作，不会导致shuffle操作。使用Broadcast将一个数据量较小的RDD作为广播变量。 val rdd2Data = rdd2.collect() val rdd2DataBroadcast = sc.broadcast(rdd2Data) val rdd3 = rdd1.map(rdd2DataBroadcast...) 注意：以上操作，建议仅仅在rdd2的数据量比较少（比如几百M，或者一两G）的情况下使用。因为每个Executor的内存中，都会驻留一份rdd2的全量数据。 5、程序开发调优 ：使用map-side预聚合的shuffle操作如果因为业务需要，一定要使用shuffle操作，无法用map类的算子来替代，那么尽量使用可以map-side预聚合的算子，类似于MapReduce中的本地combiner。map-side预聚合之后，每个节点本地就只会有一条相同的key，因为多条相同的key都被聚合起来了。其他节点在拉取所有节点上的相同key时，就会大大减少需要拉取的数据数量，从而也就减少了磁盘IO以及网络传输开销。建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子 6、程序开发调优 ：使用高性能的算子使用reduceByKey/aggregateByKey替代groupByKey : map-side使用mapPartitions替代普通map : 函数执行频率使用foreachPartitions替代foreach : 函数执行频率使用filter之后进行coalesce操作 : filter后对分区进行压缩使用repartitionAndSortWithinPartitions替代repartition与sort类操作repartitionAndSortWithinPartitions是Spark官网推荐的一个算子，官方建议，如果需要在repartition重分区之后，还要进行排序，建议直接使用repartitionAndSortWithinPartitions算子 7、程序开发调优 ：广播大变量有时在开发过程中，会遇到需要在算子函数中使用外部变量的场景（尤其是大变量，比如100M以上的大集合），那么此时就应该使用Spark的广播（Broadcast）功能来提升性能。默认情况下，Spark会将该变量复制多个副本，通过网络传输到task中，此时每个task都有一个变量副本。如果变量本身比较大的话（比如100M，甚至1G），那么大量的变量副本在网络中传输的性能开销，以及在各个节点的Executor中占用过多内存导致的频繁GC，都会极大地影响性能。广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。 8、程序开发调优 ：使用Kryo优化序列化性能1）在算子函数中使用到外部变量时，该变量会被序列化后进行网络传输。2）将自定义的类型作为RDD的泛型类型时（比如JavaRDD，Student是自定义类型），所有自定义类型对象，都会进行序列化。因此这种情况下，也要求自定义的类必须实现Serializable接口。3）使用可序列化的持久化策略时（比如MEMORY_ONLY_SER），Spark会将RDD中的每个partition都序列化成一个大的字节数组。Spark默认使用的是Java的序列化机制，你可以使用Kryo作为序列化类库，效率要比Java的序列化机制要高 // 创建SparkConf对象。 val conf = new SparkConf().setMaster(...).setAppName(...) // 设置序列化器为KryoSerializer。 conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer") // 注册要序列化的自定义类型。 conf.registerKryoClasses(Array(classOf[MyClass1], classOf[MyClass2])) 9、程序开发调优 ：分区Shuffle优化当遇到userData和events进行join时，userData比较大，而且join操作比较频繁，这个时候，可以先将userData调用了 partitionBy()分区，可以极大提高效率。cogroup()、 groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、 combineByKey() 以及 lookup()等都能够受益 总结：如果遇到一个RDD频繁和其他RDD进行Shuffle类操作，比如 cogroup()、 groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、 combineByKey() 以及 lookup()等，那么最好将该RDD通过partitionBy()操作进行预分区，这些操作在Shuffle过程中会减少Shuffle的数据量 10、程序开发调优 ：优化数据结构Java中，有三种类型比较耗费内存：1）对象，每个Java对象都有对象头、引用等额外的信息，因此比较占用内存空间。2）字符串，每个字符串内部都有一个字符数组以及长度等额外信息。3）集合类型，比如HashMap、LinkedList等，因为集合类型内部通常会使用一些内部类来封装集合元素，比如Map.EntrySpark官方建议，在Spark编码实现中，特别是对于算子函数中的代码，尽量不要使用上述三种数据结构，尽量使用字符串替代对象，使用原始类型（比如Int、Long）替代字符串，使用数组替代集合类型，这样尽可能地减少内存占用，从而降低GC频率，提升性能。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>优化</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——面试题之数据倾斜]]></title>
    <url>%2F2018%2F03%2F26%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C%2F</url>
    <content type="text"><![CDATA[Spark面试题（五）——数据倾斜调优1、数据倾斜数据倾斜指的是，并行处理的数据集中，某一部分（如Spark或Kafka的一个Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈。 数据倾斜俩大直接致命后果。1、数据倾斜直接会导致一种情况：Out Of Memory。2、运行速度慢。主要是发生在Shuffle阶段。同样Key的数据条数太多了。导致了某个key(下图中的80亿条)所在的Task数据量太大了。远远超过其他Task所处理的数据量。 一个经验结论是：一般情况下，OOM的原因都是数据倾斜 2、如何定位数据倾斜数据倾斜一般会发生在shuffle过程中。很大程度上是你使用了可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。原因：查看任务-&gt;查看Stage-&gt;查看代码某个task执行特别慢的情况某个task莫名其妙内存溢出的情况查看导致数据倾斜的key的数据分布情况 也可从以下几种情况考虑：1、是不是有OOM情况出现，一般是少数内存溢出的问题2、是不是应用运行时间差异很大，总体时间很长3、需要了解你所处理的数据Key的分布情况，如果有些Key有大量的条数，那么就要小心数据倾斜的问题4、一般需要通过Spark Web UI和其他一些监控方式出现的异常来综合判断5、看看代码里面是否有一些导致Shuffle的算子出现 3、数据倾斜的几种典型情况3.1 数据源中的数据分布不均匀，Spark需要频繁交互3.2 数据集中的不同Key由于分区方式，导致数据倾斜3.3 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）3.4 聚合操作中，数据集中的数据分布不均匀（主要）3.5 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀3.6 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀3.7 数据集中少数几个key数据量很大，不重要，其他数据均匀 注意：1、需要处理的数据倾斜问题就是Shuffle后数据的分布是否均匀问题2、只要保证最后的结果是正确的，可以采用任何方式来处理数据倾斜，只要保证在处理过程中不发生数据倾斜就可以 4、数据倾斜的处理方法4.1 数据源中的数据分布不均匀，Spark需要频繁交互解决方案：避免数据源的数据倾斜实现原理：通过在Hive中对倾斜的数据进行预处理，以及在进行kafka数据分发时尽量进行平均分配。这种方案从根源上解决了数据倾斜，彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。方案缺点：治标不治本，Hive或者Kafka中还是会发生数据倾斜。适用情况：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。总结：前台的Java系统和Spark有很频繁的交互，这个时候如果Spark能够在最短的时间内处理数据，往往会给前端有非常好的体验。这个时候可以将数据倾斜的问题抛给数据源端，在数据源端进行数据倾斜的处理。但是这种方案没有真正的处理数据倾斜问题。 4.2 数据集中的不同Key由于分区方式，导致数据倾斜解决方案1：调整并行度实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，都无法处理。 总结：调整并行度：适合于有大量key由于分区算法或者分区数的问题，将key进行了不均匀分区，可以通过调大或者调小分区数来试试是否有效 解决方案2：缓解数据倾斜（自定义Partitioner）适用场景：大量不同的Key被分配到了相同的Task造成该Task数据量过大。解决方案： 使用自定义的Partitioner实现类代替默认的HashPartitioner，尽量将所有不同的Key均匀分配到不同的Task中。优势： 不影响原有的并行度设计。如果改变并行度，后续Stage的并行度也会默认改变，可能会影响后续Stage。劣势： 适用场景有限，只能将不同Key分散开，对于同一Key对应数据集非常大的场景不适用。效果与调整并行度类似，只能缓解数据倾斜而不能完全消除数据倾斜。而且需要根据数据特点自定义专用的Partitioner，不够灵活。 4.3 JOIN操作中，一个数据集中的数据分布不均匀，另一个数据集较小（主要）解决方案：Reduce side Join转变为Map side Join方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M），比较适用此方案。方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。 4.4 聚合操作中，数据集中的数据分布不均匀（主要）解决方案：两阶段聚合（局部聚合+全局聚合）适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案将相同key的数据分拆处理 4.5 JOIN操作中，两个数据集都比较大，其中只有几个Key的数据分布不均匀解决方案：为倾斜key增加随机前/后缀适用场景：两张表都比较大，无法使用Map侧Join。其中一个RDD有少数几个Key的数据量过大，另外一个RDD的Key分布较为均匀。解决方案：将有数据倾斜的RDD中倾斜Key对应的数据集单独抽取出来加上随机前缀，另外一个RDD每条数据分别与随机前缀结合形成新的RDD（笛卡尔积，相当于将其数据增到到原来的N倍，N即为随机前缀的总个数），然后将二者Join后去掉前缀。然后将不包含倾斜Key的剩余数据进行Join。最后将两次Join的结果集通过union合并，即可得到全部Join结果。优势：相对于Map侧Join，更能适应大数据集的Join。如果资源充足，倾斜部分数据集与非倾斜部分数据集可并行进行，效率提升明显。且只针对倾斜部分的数据做数据扩展，增加的资源消耗有限。劣势：如果倾斜Key非常多，则另一侧数据膨胀非常大，此方案不适用。而且此时对倾斜Key与非倾斜Key分开处理，需要扫描数据集两遍，增加了开销。注意：具有倾斜Key的RDD数据集中，key的数量比较少 4.6 JOIN操作中，两个数据集都比较大，有很多Key的数据分布不均匀解决方案：随机前缀和扩容RDD进行join适用场景：如果在进行join操作时，RDD中有大量的key导致数据倾斜，那么进行分拆key也没什么意义。实现思路：将该RDD的每条数据都打上一个n以内的随机前缀。同时对另外一个正常的RDD进行扩容，将每条数据都扩容成n条数据，扩容出来的每条数据都依次打上一个0~n的前缀。最后将两个处理后的RDD进行join即可。和上一种方案是尽量只对少数倾斜key对应的数据进行特殊处理，由于处理过程需要扩容RDD，因此上一种方案扩容RDD后对内存的占用并不大；而这一种方案是针对有大量倾斜key的情况，没法将部分key拆分出来进行单独处理，因此只能对整个RDD进行数据扩容，对内存资源要求很高。优点：对join类型的数据倾斜基本都可以处理，而且效果也相对比较显著，性能提升效果非常不错。缺点：该方案更多的是缓解数据倾斜，而不是彻底避免数据倾斜。而且需要对整个RDD进行扩容，对内存资源要求很高。实践经验：曾经开发一个数据需求的时候，发现一个join导致了数据倾斜。优化之前，作业的执行时间大约是60分钟左右；使用该方案优化之后，执行时间缩短到10分钟左右，性能提升了6倍。注意：将倾斜Key添加1-N的随机前缀，并将被Join的数据集相应的扩大N倍（需要将1-N数字添加到每一条数据上作为前缀） 4.7 数据集中少数几个key数据量很大，不重要，其他数据均匀解决方案：过滤少数倾斜Key适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Spark</tag>
        <tag>数据倾斜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——Spark面试题（四）]]></title>
    <url>%2F2018%2F03%2F22%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Spark%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E5%9B%9B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Spark面试题（四）1、Spark中的HashShufle的有哪些不足？1）shuffle产生海量的小文件在磁盘上，此时会产生大量耗时的、低效的IO操作；2）容易导致内存不够用，由于内存需要保存海量的文件操作句柄和临时缓存信息，如果数据处理规模比较大的话，容易出现OOM；3）容易出现数据倾斜，导致OOM。 2、 conslidate是如何优化Hash shuffle时在map端产生的小文件？1）conslidate为了解决Hash Shuffle同时打开过多文件导致Writer handler内存使用过大以及产生过多文件导致大量的随机读写带来的低效磁盘IO；2）conslidate根据CPU的个数来决定每个task shuffle map端产生多少个文件，假设原来有10个task，100个reduce，每个CPU有10个CPU，那么使用hash shuffle会产生10100=1000个文件，conslidate产生1010=100个文件注意：conslidate部分减少了文件和文件句柄，并行读很高的情况下（task很多时）还是会很多文件。 3、spark.default.parallelism这个参数有什么意义，实际生产中如何设置？1）参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能；2）很多人都不会设置这个参数，会使得集群非常低效，你的cpu，内存再多，如果task始终为1，那也是浪费，spark官网建议task个数为CPU的核数*executor的个数的2~3倍。 4、spark.shuffle.memoryFraction参数的含义，以及优化经验？1）spark.shuffle.memoryFraction是shuffle调优中 重要参数，shuffle从上一个task拉去数据过来，要在Executor进行聚合操作，聚合操作时使用Executor内存的比例由该参数决定，默认是20%如果聚合时数据超过了该大小，那么就会spill到磁盘，极大降低性能；2）如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。 5、Spark中standalone模式特点，有哪些优点和缺点？1）特点：（1）standalone是master/slave架构，集群由Master与Worker节点组成，程序通过与Master节点交互申请资源，Worker节点启动Executor运行；（2）standalone调度模式使用FIFO调度方式；（3）无依赖任何其他资源管理系统，Master负责管理集群资源。2）优点：（1）部署简单；（2）不依赖其他资源管理系统。3）缺点：（1）默认每个应用程序会独占所有可用节点的资源，当然可以通过spark.cores.max来决定一个应用可以申请的CPU cores个数；（2）可能有单点故障，需要自己配置master HA。 6、FIFO调度模式的基本原理、优点和缺点？基本原理：按照先后顺序决定资源的使用，资源优先满足最先来的job。第一个job优先获取所有可用的资源，接下来第二个job再获取剩余资源。以此类推，如果第一个job没有占用所有的资源，那么第二个job还可以继续获取剩余资源，这样多个job可以并行运行，如果第一个job很大，占用所有资源，则第二job就需要等待，等到第一个job释放所有资源。优点和缺点：1）适合长作业，不适合短作业；2）适合CPU繁忙型作业（计算时间长，相当于长作业），不利于IO繁忙型作业（计算时间短，相当于短作业）。 7、FAIR调度模式的优点和缺点？所有的任务拥有大致相当的优先级来共享集群资源，spark多以轮训的方式为任务分配资源，不管长任务还是端任务都可以获得资源，并且获得不错的响应时间，对于短任务，不会像FIFO那样等待较长时间了，通过参数spark.scheduler.mode 为FAIR指定。 8、CAPCACITY调度模式的优点和缺点？1）原理：计算能力调度器支持多个队列，每个队列可配置一定的资源量，每个队列采用 FIFO 调度策略，为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。调度时，首先按以下策略选择一个合适队列：计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值(即比较空闲的队列)，选择一个该比值最小的队列；然后按以下策略选择该队列中一个作业：按照作业优先级和提交时间顺序选择，同时考虑用户资源量限制和内存限制2）优点：（1）计算能力保证。支持多个队列，某个作业可被提交到某一个队列中。每个队列会配置一定比例的计算资源，且所有提交到队列中的作业共享该队列中的资源；（2）灵活性。空闲资源会被分配给那些未达到资源使用上限的队列，当某个未达到资源的队列需要资源时，一旦出现空闲资源资源，便会分配给他们；（3）支持优先级。队列支持作业优先级调度（默认是FIFO）；（4）多重租赁。综合考虑多种约束防止单个作业、用户或者队列独占队列或者集群中的资源；（5）基于资源的调度。 支持资源密集型作业，允许作业使用的资源量高于默认值，进而可容纳不同资源需求的作业。不过，当前仅支持内存资源的调度。 9、常见的数压缩方式，你们生产集群采用了什么压缩方式，提升了多少效率？1）数据压缩，大片连续区域进行数据存储并且存储区域中数据重复性高的状况下，可以使用适当的压缩算法。数组，对象序列化后都可以使用压缩，数更紧凑，减少空间开销。常见的压缩方式有snappy，LZO，gz等2）Hadoop生产环境常用的是snappy压缩方式（使用压缩，实际上是CPU换IO吞吐量和磁盘空间，所以如果CPU利用率不高，不忙的情况下，可以大大提升集群处理效率）。snappy压缩比一般20%~30%之间，并且压缩和解压缩效率也非常高（参考数据如下）：（1）GZIP的压缩率最高，但是其实CPU密集型的，对CPU的消耗比其他算法要多，压缩和解压速度也慢；（2）LZO的压缩率居中，比GZIP要低一些，但是压缩和解压速度明显要比GZIP快很多，其中解压速度快的更多；（3）Zippy/Snappy的压缩率最低，而压缩和解压速度要稍微比LZO要快一些。 提升了多少效率可以从2方面回答：1）数据存储节约多少存储，2）任务执行消耗时间节约了多少，可以举个实际例子展开描述。 10、使用scala代码实现WordCount？val conf = new SparkConf()val sc = new SparkContext(conf)val line = sc.textFile(“xxxx.txt”) line.flatMap(.split(“ “)).map((,1)).reduceByKey(_+_). collect().foreach(println) sc.stop() 11、Spark RDD 和 MapReduce2的区别？1）mr2只有2个阶段，数据需要大量访问磁盘，数据来源相对单一 ,spark RDD ,可以无数个阶段进行迭代计算，数据来源非常丰富，数据落地介质也非常丰富spark计算基于内存；2）MapReduce2需要频繁操作磁盘IO，需要大家明确的是如果是SparkRDD的话，你要知道每一种数据来源对应的是什么，RDD从数据源加载数据，将数据放到不同的partition针对这些partition中的数据进行迭代式计算计算完成之后，落地到不同的介质当中。 12、spark和Mapreduce快？ 为什么快呢？ 快在哪里呢？Spark更加快的主要原因有几点：1）基于内存计算，减少低效的磁盘交互；2）高效的调度算法，基于DAG；3）容错机制Lingage，主要是DAG和Lianage，即使spark不使用内存技术，也大大快于mapreduce。 13、Spark sql为什么比hive快呢？计算引擎不一样，一个是spark计算模型，一个是mapreudce计算模型。 14、RDD的数据结构是怎么样的？一个RDD对象，包含如下5个核心属性。1）一个分区列表，每个分区里是RDD的部分数据（或称数据块）。2）一个依赖列表，存储依赖的其他RDD。3）一个名为compute的计算函数，用于计算RDD各分区的值。4）分区器（可选），用于键/值类型的RDD，比如某个RDD是按散列来分区。5）计算各分区时优先的位置列表（可选），比如从HDFS上的文件生成RDD时，RDD分区的位置优先选择数据所在的节点，这样可以避免数据移动带来的开销。 15、RDD算子里操作一个外部map，比如往里面put数据，然后算子外再遍历map，会有什么问题吗？频繁创建额外对象，容易oom。 16、 说说你对Hadoop生态的认识。hadoop生态主要分为三大类型，1）分布式文件系统，2）分布式计算引擎，3）周边工具1）分布式系统：HDFS，hbase2）分布式计算引擎：Spark，MapReduce3）周边工具：如zookeeper，pig，hive，oozie，sqoop，ranger，kafka等 17、hbase region多大会分区，spark读取hbase数据是如何划分partition的？region超过了hbase.hregion.max.filesize这个参数配置的大小就会自动裂分，默认值是1G。默认情况下，hbase有多少个region，Spark读取时就会有多少个partition]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——Spark面试题（三）]]></title>
    <url>%2F2018%2F03%2F22%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Spark%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%89%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Spark面试题整理（三）1、为什么要进行序列化序列化？可以减少数据的体积，减少存储空间，高效存储和传输数据，不好的是使用的时候要反序列化，非常消耗CPU。 2、Yarn中的container是由谁负责销毁的，在Hadoop Mapreduce中container可以复用么？ApplicationMaster负责销毁，在Hadoop Mapreduce不可以复用，在spark on yarn程序container可以复用。 3、提交任务时，如何指定Spark Application的运行模式？1）cluster模式：./spark-submit –class xx.xx.xx –master yarn –deploy-mode cluster xx.jar2）client模式：./spark-submit –class xx.xx.xx –master yarn –deploy-mode client xx.jar 4、不启动Spark集群Master和work服务，可不可以运行Spark程序？可以，只要资源管理器第三方管理就可以，如由yarn管理，spark集群不启动也可以使用spark；spark集群启动的是work和master，这个其实就是资源管理框架，yarn中的resourceManager相当于master，NodeManager相当于worker，做计算是Executor，和spark集群的work和manager可以没关系，归根接底还是JVM的运行，只要所在的JVM上安装了spark就可以。 5、spark on yarn Cluster 模式下，ApplicationMaster和driver是在同一个进程么？是，driver 位于ApplicationMaster进程中。该进程负责申请资源，还负责监控程序、资源的动态情况。 6、运行在yarn中Application有几种类型的container？1）运行ApplicationMaster的Container：这是由ResourceManager（向内部的资源调度器）申请和启动的，用户提交应用程序时，可指定唯一的ApplicationMaster所需的资源；2）运行各类任务的Container：这是由ApplicationMaster向ResourceManager申请的，并由ApplicationMaster与NodeManager通信以启动之。 7、Executor启动时，资源通过哪几个参数指定？1）num-executors是executor的数量2）executor-memory 是每个executor使用的内存3）executor-cores 是每个executor分配的CPU 8、为什么会产生yarn，解决了什么问题，有什么优势？1）为什么产生yarn，针对MRV1的各种缺陷提出来的资源管理框架2）解决了什么问题，有什么优势，参考这篇博文：http://www.aboutyun.com/forum.php?mod=viewthread&amp;tid=6785 9、一个task的map数量由谁来决定？一般情况下，在输入源是文件的时候，一个task的map数量由splitSize来决定的那么splitSize是由以下几个来决定的goalSize = totalSize / mapred.map.tasksinSize = max {mapred.min.split.size, minSplitSize}splitSize = max (minSize, min(goalSize, dfs.block.size))一个task的reduce数量，由partition决定。 10、列出你所知道的调度器，说明其工作原理？1）FiFo schedular 默认的调度器 先进先出2）Capacity schedular 计算能力调度器 选择占用内存小 优先级高的3）Fair schedular 调度器 公平调度器 所有job 占用相同资源 11、导致Executor产生FULL gc 的原因，可能导致什么问题？可能导致Executor僵死问题，海量数据的shuffle和数据倾斜等都可能导致full gc。以shuffle为例，伴随着大量的Shuffle写操作，JVM的新生代不断GC，Eden Space写满了就往Survivor Space写，同时超过一定大小的数据会直接写到老生代，当新生代写满了之后，也会把老的数据搞到老生代，如果老生代空间不足了，就触发FULL GC，还是空间不够，那就OOM错误了，此时线程被Blocked，导致整个Executor处理数据的进程被卡住。 12、Spark累加器有哪些特点？1）累加器在全局唯一的，只增不减，记录全局集群的唯一状态；2）在exe中修改它，在driver读取；3）executor级别共享的，广播变量是task级别的共享两个application不可以共享累加器，但是同一个app不同的job可以共享。 13、spark hashParitioner的弊端是什么？HashPartitioner分区的原理很简单，对于给定的key，计算其hashCode，并除于分区的个数取余，如果余数小于0，则用余数+分区的个数，最后返回的值就是这个key所属的分区ID；弊端是数据不均匀，容易导致数据倾斜，极端情况下某几个分区会拥有rdd的所有数据。 14、RangePartitioner分区的原理？RangePartitioner分区则尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，也就是说一个分区中的元素肯定都是比另一个分区内的元素小或者大；但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。其原理是水塘抽样。 15、rangePartioner分区器特点？rangePartioner尽量保证每个分区中数据量的均匀，而且分区与分区之间是有序的，一个分区中的元素肯定都是比另一个分区内的元素小或者大；但是分区内的元素是不能保证顺序的。简单的说就是将一定范围内的数映射到某一个分区内。RangePartitioner作用：将一定范围内的数映射到某一个分区内，在实现中，分界的算法尤为重要。算法对应的函数是rangeBounds。 16、如何理解Standalone模式下，Spark资源分配是粗粒度的？spark默认情况下资源分配是粗粒度的，也就是说程序在提交时就分配好资源，后面执行的时候使用分配好的资源，除非资源出现了故障才会重新分配。比如Spark shell启动，已提交，一注册，哪怕没有任务，worker都会分配资源给executor。 17、union操作是产生宽依赖还是窄依赖？产生窄依赖。 18、窄依赖父RDD的partition和子RDD的parition是不是都是一对一的关系？不一定，除了一对一的窄依赖，还包含一对固定个数的窄依赖（就是对父RDD的依赖的Partition的数量不会随着RDD数量规模的改变而改变），比如join操作的每个partiion仅仅和已知的partition进行join，这个join操作是窄依赖，依赖固定数量的父rdd，因为是确定的partition关系。 19、Hadoop中，Mapreduce操作的mapper和reducer阶段相当于spark中的哪几个算子？相当于spark中的map算子和reduceByKey算子，当然还是有点区别的,MR会自动进行排序的，spark要看你用的是什么partitioner。 20、什么是shuffle，以及为什么需要shuffle？shuffle中文翻译为洗牌，需要shuffle的原因是：某种具有共同特征的数据汇聚到一个计算节点上进行计算。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——Spark面试题（二）]]></title>
    <url>%2F2018%2F03%2F22%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Spark%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Spark面试题（二）1、Spark有哪两种算子？Transformation（转化）算子和Action（执行）算子。 2、Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？在我们的开发过程中，能避免则尽可能避免使用reduceByKey、join、distinct、repartition等会进行shuffle的算子，尽量使用map类的非shuffle算子。这样的话，没有shuffle操作或者仅有较少shuffle操作的Spark作业，可以大大减少性能开销。 3、如何从Kafka中获取数据？1）基于Receiver的方式这种方式使用Receiver来获取数据。Receiver是使用Kafka的高层次Consumer API来实现的。receiver从Kafka中获取的数据都是存储在Spark Executor的内存中的，然后Spark Streaming启动的job会去处理那些数据。2）基于Direct的方式这种新的不基于Receiver的直接方式，是在Spark 1.3中引入的，从而能够确保更加健壮的机制。替代掉使用Receiver来接收数据后，这种方式会周期性地查询Kafka，来获得每个topic+partition的最新的offset，从而定义每个batch的offset的范围。当处理数据的job启动时，就会使用Kafka的简单consumer api来获取Kafka指定offset范围的数据。 4、RDD创建有哪几种方式？1）使用程序中的集合创建rdd2）使用本地文件系统创建rdd3）使用hdfs创建rdd4）基于数据库db创建rdd5）基于Nosql创建rdd，如hbase6）基于s3创建rdd7）基于数据流，如socket创建rdd 5、Spark并行度怎么设置比较合适？spark并行度，每个core承载2~4个partition,如，32个core，那么64~128之间的并行度，也就是设置64~128个partion，并行读和数据规模无关，只和内存使用量和cpu使用时间有关。 6、Spark如何处理不能被序列化的对象？将不能序列化的内容封装成object。 7、collect功能是什么，其底层是怎么实现的？driver通过collect把集群中各个节点的内容收集过来汇总成结果，collect返回结果是Array类型的，collect把各个节点上的数据抓过来，抓过来数据是Array型，collect对Array抓过来的结果进行合并，合并后Array中只有一个元素，是tuple类型（KV类型的）的。 8、为什么Spark Application在没有获得足够的资源，job就开始执行了，可能会导致什么什么问题发生？会导致执行该job时候集群资源不足，导致执行job结束也没有分配足够的资源，分配了部分Executor，该job就开始执行task，应该是task的调度线程和Executor资源申请是异步的；如果想等待申请完所有的资源再执行job的：需要将spark.scheduler.maxRegisteredResourcesWaitingTime设置的很大；spark.scheduler.minRegisteredResourcesRatio 设置为1，但是应该结合实际考虑否则很容易出现长时间分配不到资源，job一直不能运行的情况。 9、map与flatMap的区别？map：对RDD每个元素转换，文件中的每一行数据返回一个数组对象。flatMap：对RDD每个元素转换，然后再扁平化。将所有的对象合并为一个对象，文件中的所有行数据仅返回一个数组对象，会抛弃值为null的值。 10、Spark on Mesos中，什么是的粗粒度分配，什么是细粒度分配，各自的优点和缺点是什么？1）粗粒度：启动时就分配好资源， 程序启动，后续具体使用就使用分配好的资源，不需要再分配资源；优点：作业特别多时，资源复用率高，适合粗粒度；缺点：容易资源浪费，假如一个job有1000个task，完成了999个，还有一个没完成，那么使用粗粒度，999个资源就会闲置在那里，资源浪费。2）细粒度分配：用资源的时候分配，用完了就立即回收资源，启动会麻烦一点，启动一次分配一次，会比较麻烦。 11、driver的功能是什么？1）一个Spark作业运行时包括一个Driver进程，也是作业的主进程，具有main函数，并且有SparkContext的实例，是程序的入口点；2）功能：负责向集群申请资源，向master注册信息，负责了作业的调度，负责作业的解析、生成Stage并调度Task到Executor上。包括DAGScheduler，TaskScheduler。 12、Spark技术栈有哪些组件，每个组件都有什么功能，适合什么应用场景？可以画一个这样的技术栈图先，然后分别解释下每个组件的功能和场景1）Spark core：是其它组件的基础，spark的内核，主要包含：有向循环图、RDD、Lingage、Cache、broadcast等，并封装了底层通讯框架，是Spark的基础。2）SparkStreaming是一个对实时数据流进行高通量、容错处理的流式处理系统，可以对多种数据源（如Kafka、Flume、Twitter、Zero和TCP 套接字）进行类似Map、Reduce和Join等复杂操作，将流式计算分解成一系列短小的批处理作业。3）Spark sql：Shark是SparkSQL的前身，Spark SQL的一个重要特点是其能够统一处理关系表和RDD，使得开发人员可以轻松地使用SQL命令进行外部查询，同时进行更复杂的数据分析。4）BlinkDB ：是一个用于在海量数据上运行交互式 SQL 查询的大规模并行查询引擎，它允许用户通过权衡数据精度来提升查询响应时间，其数据的精度被控制在允许的误差范围内。5）MLBase是Spark生态圈的一部分专注于机器学习，让机器学习的门槛更低，让一些可能并不了解机器学习的用户也能方便地使用MLbase。MLBase分为四部分：MLlib、MLI、ML Optimizer和MLRuntime。6）GraphX是Spark中用于图和图并行计算。 13、Spark中Worker的主要工作是什么？主要功能：管理当前节点内存，CPU的使用状况，接收master分配过来的资源指令，通过ExecutorRunner启动程序分配任务，worker就类似于包工头，管理分配新进程，做计算的服务，相当于process服务。需要注意的是：1）worker会不会汇报当前信息给master，worker心跳给master主要只有workid，它不会发送资源信息以心跳的方式给mater，master分配的时候就知道work，只有出现故障的时候才会发送资源。2）worker不会运行代码，具体运行的是Executor是可以运行具体appliaction写的业务逻辑代码，操作代码的节点，它不会运行程序的代码的。 14、Mapreduce和Spark的都是并行计算，那么他们有什么相同和区别？两者都是用mr模型来进行并行计算:1）hadoop的一个作业称为job，job里面分为map task和reduce task，每个task都是在自己的进程中运行的，当task结束时，进程也会结束。2）spark用户提交的任务成为application，一个application对应一个SparkContext，app中存在多个job，每触发一次action操作就会产生一个job。这些job可以并行或串行执行，每个job中有多个stage，stage是shuffle过程中DAGSchaduler通过RDD之间的依赖关系划分job而来的，每个stage里面有多个task，组成taskset有TaskSchaduler分发到各个executor中执行，executor的生命周期是和app一样的，即使没有job运行也是存在的，所以task可以快速启动读取内存进行计算。3）hadoop的job只有map和reduce操作，表达能力比较欠缺而且在mr过程中会重复的读写hdfs，造成大量的io操作，多个job需要自己管理关系。4）spark的迭代计算都是在内存中进行的，API中提供了大量的RDD操作如join，groupby等，而且通过DAG图可以实现良好的容错。 15、RDD机制？rdd分布式弹性数据集，简单的理解成一种数据结构，是spark框架上的通用货币。 所有算子都是基于rdd来执行的，不同的场景会有不同的rdd实现类，但是都可以进行互相转换。rdd执行过程中会形成dag图，然后形成lineage保证容错性等。 从物理的角度来看rdd存储的是block和node之间的映射。 16、什么是RDD宽依赖和窄依赖？RDD和它依赖的parent RDD(s)的关系有两种不同的类型，即窄依赖（narrow dependency）和宽依赖（wide dependency）1）窄依赖指的是每一个parent RDD的Partition最多被子RDD的一个Partition使用2）宽依赖指的是多个子RDD的Partition会依赖同一个parent RDD的Partition 17、cache和pesist的区别？cache和persist都是用于将一个RDD进行缓存的，这样在之后使用的过程中就不需要重新计算了，可以大大节省程序运行时间1） cache只有一个默认的缓存级别MEMORY_ONLY ，cache调用了persist，而persist可以根据情况设置其它的缓存级别；2）executor执行的时候，默认60%做cache，40%做task操作，persist是最根本的函数，最底层的函数。 18、 cache后面能不能接其他算子,它是不是action操作？cache可以接其他算子，但是接了算子之后，起不到缓存应有的效果，因为会重新触发cache。cache不是action操作。 19、reduceByKey是不是action？不是，很多人都会以为是action，reduce rdd是action 20、 RDD通过Linage（记录数据更新）的方式为何很高效？1）lazy记录了数据的来源，RDD是不可变的，且是lazy级别的，且RDD之间构成了链条，lazy是弹性的基石。由于RDD不可变，所以每次操作就产生新的rdd，不存在全局修改的问题，控制难度下降，所有有计算链条将复杂计算链条存储下来，计算的时候从后往前回溯 900步是上一个stage的结束，要么就checkpoint。2）记录原数据，是每次修改都记录，代价很大如果修改一个集合，代价就很小，官方说rdd是粗粒度的操作，是为了效率，为了简化，每次都是操作数据集合，写或者修改操作，都是基于集合的rdd的写操作是粗粒度的，rdd的读操作既可以是粗粒度的也可以是细粒度，读可以读其中的一条条的记录。3）简化复杂度，是高效率的一方面，写的粗粒度限制了使用场景如网络爬虫，现实世界中，大多数写是粗粒度的场景。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——Spark面试题（一）]]></title>
    <url>%2F2018%2F03%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Spark%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Spark面试题（一）1、spark的有几种部署模式，每种模式特点？（☆☆☆☆☆）1）本地模式Spark不一定非要跑在hadoop集群，可以在本地，起多个线程的方式来指定。将Spark应用以多线程的方式直接运行在本地，一般都是为了方便调试，本地模式分三类 local：只启动一个executorlocal[k]:启动k个executorlocal[*]：启动跟cpu数目相同的 executor2）standalone模式分布式部署集群，自带完整的服务，资源管理和任务监控是Spark自己监控，这个模式也是其他模式的基础。3）Spark on yarn模式分布式部署集群，资源和任务监控交给yarn管理，但是目前仅支持粗粒度资源分配方式，包含cluster和client运行模式，cluster适合生产，driver运行在集群子节点，具有容错功能，client适合调试，dirver运行在客户端。4）Spark On Mesos模式。官方推荐这种模式（当然，原因之一是血缘关系）。正是由于Spark开发之初就考虑到支持Mesos，因此，目前而言，Spark运行在Mesos上会比运行在YARN上更加灵活，更加自然。用户可选择两种调度模式之一运行自己的应用程序：（1）粗粒度模式（Coarse-grained Mode）：每个应用程序的运行环境由一个Dirver和若干个Executor组成，其中，每个Executor占用若干资源，内部可运行多个Task（对应多少个“slot”）。应用程序的各个任务正式运行之前，需要将运行环境中的资源全部申请好，且运行过程中要一直占用这些资源，即使不用，最后程序运行结束后，回收这些资源。（2）细粒度模式（Fine-grained Mode）：鉴于粗粒度模式会造成大量资源浪费，Spark On Mesos还提供了另外一种调度模式：细粒度模式，这种模式类似于现在的云计算，思想是按需分配。 2、Spark为什么比mapreduce快？（☆☆☆☆☆）1）基于内存计算，减少低效的磁盘交互；2）高效的调度算法，基于DAG；3）容错机制Linage，精华部分就是DAG和Lingae 3、简单说一下hadoop和spark的shuffle相同和差异？（☆☆☆☆☆）1）从 high-level 的角度来看，两者并没有大的差别。 都是将 mapper（Spark 里是 ShuffleMapTask）的输出进行 partition，不同的 partition 送到不同的 reducer（Spark 里 reducer 可能是下一个 stage 里的 ShuffleMapTask，也可能是 ResultTask）。Reducer 以内存作缓冲区，边 shuffle 边 aggregate 数据，等到数据 aggregate 好以后进行 reduce() （Spark 里可能是后续的一系列操作）。2）从 low-level 的角度来看，两者差别不小。 Hadoop MapReduce 是 sort-based，进入 combine() 和 reduce() 的 records 必须先 sort。这样的好处在于 combine/reduce() 可以处理大规模的数据，因为其输入数据可以通过外排得到（mapper 对每段数据先做排序，reducer 的 shuffle 对排好序的每段数据做归并）。目前的 Spark 默认选择的是 hash-based，通常使用 HashMap 来对 shuffle 来的数据进行 aggregate，不会对数据进行提前排序。如果用户需要经过排序的数据，那么需要自己调用类似 sortByKey() 的操作；如果你是Spark 1.1的用户，可以将spark.shuffle.manager设置为sort，则会对数据进行排序。在Spark 1.2中，sort将作为默认的Shuffle实现。3）从实现角度来看，两者也有不少差别。 Hadoop MapReduce 将处理流程划分出明显的几个阶段：map(), spill, merge, shuffle, sort, reduce() 等。每个阶段各司其职，可以按照过程式的编程思想来逐一实现每个阶段的功能。在 Spark 中，没有这样功能明确的阶段，只有不同的 stage 和一系列的 transformation()，所以 spill, merge, aggregate 等操作需要蕴含在 transformation() 中。如果我们将 map 端划分数据、持久化数据的过程称为 shuffle write，而将 reducer 读入数据、aggregate 数据的过程称为 shuffle read。那么在 Spark 中，问题就变为怎么在 job 的逻辑或者物理执行图中加入 shuffle write 和 shuffle read的处理逻辑？以及两个处理逻辑应该怎么高效实现？Shuffle write由于不要求数据有序，shuffle write 的任务很简单：将数据 partition 好，并持久化。之所以要持久化，一方面是要减少内存存储空间压力，另一方面也是为了 fault-tolerance。 4、spark工作机制？（☆☆☆☆☆） ① 构建Application的运行环境，Driver创建一个SparkContext ② SparkContext向资源管理器（Standalone、Mesos、Yarn）申请Executor资源，资源管理器启动StandaloneExecutorbackend（Executor）③ Executor向SparkContext申请Task④ SparkContext将应用程序分发给Executor⑤ SparkContext就建成DAG图，DAGScheduler将DAG图解析成Stage，每个Stage有多个task，形成taskset发送给task Scheduler，由task Scheduler将Task发送给Executor运行⑥ Task在Executor上运行，运行完释放所有资源 5、spark的优化怎么做？ （☆☆☆☆☆）spark调优比较复杂，但是大体可以分为三个方面来进行1）平台层面的调优：防止不必要的jar包分发，提高数据的本地性，选择高效的存储格式如parquet2）应用程序层面的调优：过滤操作符的优化降低过多小任务，降低单条记录的资源开销，处理数据倾斜，复用RDD进行缓存，作业并行化执行等等3）JVM层面的调优：设置合适的资源量，设置合理的JVM，启用高效的序列化方法如kyro，增大off head内存等等 6、数据本地性是在哪个环节确定的？（☆☆☆☆☆）具体的task运行在那他机器上，dag划分stage的时候确定的 7、RDD的弹性表现在哪几点？（☆☆☆☆☆）1）自动的进行内存和磁盘的存储切换；2）基于Lineage的高效容错；3）task如果失败会自动进行特定次数的重试；4）stage如果失败会自动进行特定次数的重试，而且只会计算失败的分片；5）checkpoint和persist，数据计算之后持久化缓存；6）数据调度弹性，DAG TASK调度和资源无关；7）数据分片的高度弹性。 8、RDD有哪些缺陷？（☆☆☆☆☆）1）不支持细粒度的写和更新操作（如网络爬虫），spark写数据是粗粒度的。所谓粗粒度，就是批量写入数据，为了提高效率。但是读数据是细粒度的也就是说可以一条条的读。2）不支持增量迭代计算，Flink支持 9、Spark的shuffle过程？（☆☆☆☆☆）从下面三点去展开1）shuffle过程的划分2）shuffle的中间结果如何存储3）shuffle的数据如何拉取过来可以参考这篇博文：http://www.cnblogs.com/jxhd1/p/6528540.html 10、 Spark的数据本地性有哪几种？（☆☆☆☆☆）Spark中的数据本地性有三种：1）PROCESS_LOCAL是指读取缓存在本地节点的数据2）NODE_LOCAL是指读取本地节点硬盘数据3）ANY是指读取非本地节点数据通常读取数据PROCESS_LOCAL&gt;NODE_LOCAL&gt;ANY，尽量使数据以PROCESS_LOCAL或NODE_LOCAL方式读取。其中PROCESS_LOCAL还和cache有关，如果RDD经常用的话将该RDD cache到内存中，注意，由于cache是lazy的，所以必须通过一个action的触发，才能真正的将该RDD cache到内存中。 11、Spark为什么要持久化，一般什么场景下要进行persist操作？（☆☆☆）为什么要进行持久化？spark所有复杂一点的算法都会有persist身影，spark默认数据放在内存，spark很多内容都是放在内存的，非常适合高速迭代，1000个步骤只有第一个输入数据，中间不产生临时数据，但分布式系统风险很高，所以容易出错，就要容错，rdd出错或者分片可以根据血统算出来，如果没有对父rdd进行persist 或者cache的化，就需要重头做。以下场景会使用persist1）某个步骤计算非常耗时，需要进行persist持久化2）计算链条非常长，重新恢复要算很多步骤，很好使，persist3）checkpoint所在的rdd要持久化persist。checkpoint前，要持久化，写个rdd.cache或者rdd.persist，将结果保存起来，再写checkpoint操作，这样执行起来会非常快，不需要重新计算rdd链条了。checkpoint之前一定会进行persist。4）shuffle之后要persist，shuffle要进性网络传输，风险很大，数据丢失重来，恢复代价很大5）shuffle之前进行persist，框架默认将数据持久化到磁盘，这个是框架自动做的。 12、介绍一下join操作优化经验？（☆☆☆☆☆）join其实常见的就分为两类： map-side join 和 reduce-side join。当大表和小表join时，用map-side join能显著提高效率。将多份数据进行关联是数据处理过程中非常普遍的用法，不过在分布式计算系统中，这个问题往往会变的非常麻烦，因为框架提供的 join 操作一般会将所有数据根据 key 发送到所有的 reduce 分区中去，也就是 shuffle 的过程。造成大量的网络以及磁盘IO消耗，运行效率极其低下，这个过程一般被称为 reduce-side-join。如果其中有张表较小的话，我们则可以自己实现在 map 端实现数据关联，跳过大量数据进行 shuffle 的过程，运行时间得到大量缩短，根据不同数据可能会有几倍到数十倍的性能提升。备注：这个题目面试中非常非常大概率见到，务必搜索相关资料掌握，这里抛砖引玉。 13、描述Yarn执行一个任务的过程？（☆☆☆☆☆） 1）客户端client向ResouceManager提交Application，ResouceManager接受Application并根据集群资源状况选取一个node来启动Application的任务调度器driver（ApplicationMaster）。2）ResouceManager找到那个node，命令其该node上的nodeManager来启动一个新的 JVM进程运行程序的driver（ApplicationMaster）部分，driver（ApplicationMaster）启动时会首先向ResourceManager注册，说明由自己来负责当前程序的运行。3）driver（ApplicationMaster）开始下载相关jar包等各种资源，基于下载的jar等信息决定向ResourceManager申请具体的资源内容。4）ResouceManager接受到driver（ApplicationMaster）提出的申请后，会最大化的满足 资源分配请求，并发送资源的元数据信息给driver（ApplicationMaster）。5）driver（ApplicationMaster）收到发过来的资源元数据信息后会根据元数据信息发指令给具体机器上的NodeManager，让其启动具体的container。6）NodeManager收到driver发来的指令，启动container，container启动后必须向driver（ApplicationMaster）注册。7）driver（ApplicationMaster）收到container的注册，开始进行任务的调度和计算，直到 任务完成。注意：如果ResourceManager第一次没有能够满足driver（ApplicationMaster）的资源请求 ，后续发现有空闲的资源，会主动向driver（ApplicationMaster）发送可用资源的元数据信息以提供更多的资源用于当前程序的运行。 14、Spark on Yarn 模式有哪些优点？（☆☆☆☆☆）1）与其他计算框架共享集群资源（Spark框架与MapReduce框架同时运行，如果不用Yarn进行资源分配，MapReduce分到的内存资源会很少，效率低下）；资源按需分配，进而提高集群资源利用等。2）相较于Spark自带的Standalone模式，Yarn的资源分配更加细致。3）Application部署简化，例如Spark，Storm等多种框架的应用由客户端提交后，由Yarn负责资源的管理和调度，利用Container作为资源隔离的单位，以它为单位去使用内存,cpu等。4）Yarn通过队列的方式，管理同时运行在Yarn集群中的多个服务，可根据不同类型的应用程序负载情况，调整对应的资源使用量，实现资源弹性管理。 15、谈谈你对container的理解？（☆☆☆☆☆）1）Container作为资源分配和调度的基本单位，其中封装了的资源如内存，CPU，磁盘，网络带宽等。 目前yarn仅仅封装内存和CPU2）Container由ApplicationMaster向ResourceManager申请的，由ResouceManager中的资源调度器异步分配给ApplicationMaster3）Container的运行是由ApplicationMaster向资源所在的NodeManager发起的，Container运行时需提供内部执行的任务命令 16、Spark使用parquet文件存储格式能带来哪些好处？（☆☆☆☆☆）1）如果说HDFS是大数据时代分布式文件系统首选标准，那么parquet则是整个大数据时代文件存储格式实时首选标准。2）速度更快：从使用spark sql操作普通文件CSV和parquet文件速度对比上看，绝大多数情况会比使用csv等普通文件速度提升10倍左右，在一些普通文件系统无法在spark上成功运行的情况下，使用parquet很多时候可以成功运行。3）parquet的压缩技术非常稳定出色，在spark sql中对压缩技术的处理可能无法正常的完成工作（例如会导致lost task，lost executor）但是此时如果使用parquet就可以正常的完成。4）极大的减少磁盘I/o,通常情况下能够减少75%的存储空间，由此可以极大的减少spark sql处理数据的时候的数据输入内容，尤其是在spark1.6x中有个下推过滤器在一些情况下可以极大的减少磁盘的IO和内存的占用，（下推过滤器）。5）spark 1.6x parquet方式极大的提升了扫描的吞吐量，极大提高了数据的查找速度spark1.6和spark1.5x相比而言，提升了大约1倍的速度，在spark1.6X中，操作parquet时候cpu也进行了极大的优化，有效的降低了cpu消耗。6）采用parquet可以极大的优化spark的调度和执行。我们测试spark如果用parquet可以有效的减少stage的执行消耗，同时可以优化执行路径。 17、介绍parition和block有什么关联关系？（☆☆☆☆☆）1）hdfs中的block是分布式存储的最小单元，等分，可设置冗余，这样设计有一部分磁盘空间的浪费，但是整齐的block大小，便于快速找到、读取对应的内容；2）Spark中的partion是弹性分布式数据集RDD的最小单元，RDD是由分布在各个节点上的partion组成的。partion是指的spark在计算过程中，生成的数据在计算空间内最小单元，同一份数据（RDD）的partion大小不一，数量不定，是根据application里的算子和最初读入的数据分块数量决定；3）block位于存储空间、partion位于计算空间，block的大小是固定的、partion大小是不固定的，是从2个不同的角度去看数据。 18、Spark应用程序的执行过程是什么？（☆☆☆☆☆）1）构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源；2）资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上；3）SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor；4）Task在Executor上运行，运行完毕释放所有资源。 19、不需要排序的hash shuffle是否一定比需要排序的sort shuffle速度快？（☆☆☆☆☆）不一定，当数据规模小，Hash shuffle快于Sorted Shuffle数据规模大的时候；当数据量大，sorted Shuffle会比Hash shuffle快很多，因为数量大的有很多小文件，不均匀，甚至出现数据倾斜，消耗内存大，1.x之前spark使用hash，适合处理中小规模，1.x之后，增加了Sorted shuffle，Spark更能胜任大规模处理了。 20、Sort-based shuffle的缺陷? （☆☆☆☆☆）1）如果mapper中task的数量过大，依旧会产生很多小文件，此时在shuffle传递数据的过程中reducer段，reduce会需要同时大量的记录进行反序列化，导致大量的内存消耗和GC的巨大负担，造成系统缓慢甚至崩溃。2）如果需要在分片内也进行排序，此时需要进行mapper段和reducer段的两次排序。 21、spark.storage.memoryFraction参数的含义,实际生产中如何调优？（☆☆☆☆☆）1）用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6,，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘；2）如果持久化操作比较多，可以提高spark.storage.memoryFraction参数，使得更多的持久化数据保存在内存中，提高数据的读取性能，如果shuffle的操作比较多，有很多的数据读写操作到JVM中，那么应该调小一点，节约出更多的内存给JVM，避免过多的JVM gc发生。在web ui中观察如果发现gc时间很长，可以设置spark.storage.memoryFraction更小一点。 22、介绍一下你对Unified Memory Management内存管理模型的理解？（☆☆☆☆☆）Spark中的内存使用分为两部分：执行（execution）与存储（storage）。执行内存主要用于shuffles、joins、sorts和aggregations，存储内存则用于缓存或者跨节点的内部数据传输。1.6之前，对于一个Executor，内存都由以下部分构成：1）ExecutionMemory。这片内存区域是为了解决 shuffles,joins, sorts and aggregations 过程中为了避免频繁IO需要的buffer。 通过spark.shuffle.memoryFraction(默认 0.2) 配置。2）StorageMemory。这片内存区域是为了解决 block cache(就是你显示调用rdd.cache, rdd.persist等方法), 还有就是broadcasts,以及task results的存储。可以通过参数 spark.storage.memoryFraction(默认0.6)设置。3）OtherMemory。给系统预留的，因为程序本身运行也是需要内存的(默认为0.2)。传统内存管理的不足：1）Shuffle占用内存0.2*0.8，内存分配这么少，可能会将数据spill到磁盘，频繁的磁盘IO是很大的负担，Storage内存占用0.6，主要是为了迭代处理。传统的Spark内存分配对操作人的要求非常高。（Shuffle分配内存：ShuffleMemoryManager, TaskMemoryManager, ExecutorMemoryManager）一个Task获得全部的Execution的Memory，其他Task过来就没有内存了，只能等待；2）默认情况下，Task在线程中可能会占满整个内存，分片数据]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——Spark之RDD概述]]></title>
    <url>%2F2018%2F03%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Spark%E4%B9%8BRDD%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[RDD概述 1、RDD 的产生Hadoop的MapReduce是一种基于数据集的工作模式，面向数据，这种工作模式一般是从存储上加载数据集，然后操作数据集，最后写入物理存储设备。数据更多面临的是一次性处理。 MR的这种方式对数据领域两种常见的操作不是很高效。第一种是迭代式的算法。比如机器学习中ALS、凸优化梯度下降等。这些都需要基于数据集或者数据集的衍生数据反复查询反复操作。MR这种模式不太合适，即使多MR串行处理，性能和时间也是一个问题。数据的共享依赖于磁盘。另外一种是交互式数据挖掘，MR显然不擅长。MR和Spark中的迭代对比：MR中的迭代 Spark中的迭代 Spark是一个效率非常快，且能够支持迭代计算和有效数据共享的模型，适合数据领域常见的两种操作。而且RDD是基于工作集的工作模式，更多的是面向工作流。 2、什么是RDDRDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象。代码中是一个抽象类，它代表一个不可变、可分区、里面的元素可并行计算的集合。在Spark中，对数据的所有操作不外乎创建RDD、转化已有RDD 以及调用RDD操作进行求值。每个RDD都被分为多个分区，这些分区运行在集群中的不同节点上。RDD可以包含Python、Java、Scala中任意类型的对象，甚至可以包含用户自定义的对象。RDD具有数据流模型的特点：自动容错、位置感知性调度和可伸缩性。RDD允许用户在执行多个查询时显式地将工作集缓存在内存中，后续的查询能够重用工作集，这极大地提升了查询速度。RDD支持两种操作：转化操作和行动操作。RDD的转化操作是返回一个新的RDD的操作，比如map()和filter()，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作。比如count() 和first()。Spark采用惰性计算模式，RDD只有第一次在一个行动操作中用到时，才会真正计算。Spark可以优化整个计算过程。默认情况下，Spark的RDD会在你每次对它们进行行动操作时重新计算。如果想在多个行动操作中重用同一个RDD，可以使用RDD.persist()让Spark把这个RDD缓存下来。 3、RDD的属性1）一组分区（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。2）一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。3）RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。4）一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。5）一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。 4、Spark做了啥？RDD是一个应用层面的逻辑概念。一个RDD多个分片。RDD就是一个元数据记录集，记录了RDD内存所有的关系数据。 5、RDD弹性存储的弹性：内存与磁盘的自动切换容错的弹性：数据丢失可以自动恢复计算的弹性：计算出错重试机制分片的弹性：根据需要重新分片1）自动进行内存和磁盘数据存储的切换Spark优先把数据放到内存中，如果内存放不下，就会放到磁盘里面，程序进行自动的存储切换2）基于血统的高效容错机制在RDD进行转换和动作的时候，会形成RDD的Lineage依赖链，当某一个RDD失效的时候，可以通过重新计算上游的RDD来重新生成丢失的RDD数据。3）Task如果失败会自动进行特定次数的重试RDD的计算任务如果运行失败，会自动进行任务的重新计算，默认次数是4次。4）Stage如果失败会自动进行特定次数的重试如果Job的某个Stage阶段计算失败，框架也会自动进行任务的重新计算，默认次数也是4次。5）Checkpoint和Persist可主动或被动触发RDD可以通过Persist持久化将RDD缓存到内存或者磁盘，当再次用到该RDD时直接读取就行。也可以将RDD进行检查点，检查点会将数据存储在HDFS中，该RDD的所有父RDD依赖都会被移除。6）数据调度弹性Spark把这个JOB执行模型抽象为通用的有向无环图DAG，可以将多Stage的任务串联或并行执行，调度引擎自动处理Stage的失败以及Task的失败。7）数据分片的高度弹性可以根据业务的特征，动态调整数据分片的个数，提升整体的应用执行效率。 6、RDD特点RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。1）分区RDD逻辑上是分区的，每个分区的数据是抽象存在的，计算的时候会通过一个compute函数得到每个分区的数据。如果RDD是通过已有的文件系统构建，则compute函数是读取指定文件系统中的数据，如果RDD是通过其他RDD转换而来，则compute函数是执行转换逻辑将其他RDD的数据进行转换。 2）只读如下图所示，RDD是只读的，要想改变RDD中的数据，只能在现有的RDD基础上创建新的RDD。 由一个RDD转换到另一个RDD，可以通过丰富的操作算子实现，不再像MapReduce那样只能写map和reduce了，如下图所示。 RDD的操作算子包括两类，一类叫做transformations，它是用来将RDD进行转化，构建RDD的血缘关系；另一类叫做actions，它是用来触发RDD的计算，得到RDD的相关计算结果或者将RDD保存的文件系统中。下图是RDD所支持的操作算子列表。 3）依赖RDDs通过操作算子进行转换，转换得到的新RDD包含了从其他RDDs衍生所必需的信息，RDDs之间维护着这种血缘关系，也称之为依赖。如下图所示，依赖包括两种，一种是窄依赖，RDDs之间分区是一一对应的，另一种是宽依赖，下游RDD的每个分区与上游RDD(也称之为父RDD)的每个分区都有关，是多对多的关系。 通过RDDs之间的这种依赖关系，一个任务流可以描述为DAG(有向无环图)，如下图所示，在实际执行过程中宽依赖对应于Shuffle(图中的reduceByKey和join)，窄依赖中的所有转换操作可以通过类似于管道的方式一气呵成执行(图中map和union可以一起执行)。 4）缓存如果在应用程序中多次使用同一个RDD，可以将该RDD缓存起来，该RDD只有在第一次计算的时候会根据血缘关系得到分区的数据，在后续其他地方用到该RDD的时候，会直接从缓存处取而不用再根据血缘关系计算，这样就加速后期的重用。如下图所示，RDD-1经过一系列的转换后得到RDD-n并保存到hdfs，RDD-1在这一过程中会有个中间结果，如果将其缓存到内存，那么在随后的RDD-1转换到RDD-m这一过程中，就不会计算其之前的RDD-0了。 5）checkpoint虽然RDD的血缘关系天然地可以实现容错，当RDD的某个分区数据失败或丢失，可以通过血缘关系重建。但是对于长时间迭代型应用来说，随着迭代的进行，RDDs之间的血缘关系会越来越长，一旦在后续迭代过程中出错，则需要通过非常长的血缘关系去重建，势必影响性能。为此，RDD支持checkpoint将数据保存到持久化的存储中，这样就可以切断之前的血缘关系，因为checkpoint后的RDD不需要知道它的父RDDs了，它可以从checkpoint处拿到数据。 注意：给定一个RDD我们至少可以知道如下几点信息：1、分区数以及分区方式；2、由父RDDs衍生而来的相关依赖信息；3、计算每个分区的数据，计算步骤为：1）如果被缓存，则从缓存中取的分区的数据；2）如果被checkpoint，则从checkpoint处恢复数据；3）根据血缘关系计算分区的数据。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>RDD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——Spark案例实践]]></title>
    <url>%2F2018%2F03%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Spark%E6%A1%88%E4%BE%8B%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"><![CDATA[案例实践Spark Shell仅在测试和验证我们的程序时使用的较多，在生产环境中，通常会在IDE中编制程序，然后打成jar包，然后提交到集群，最常用的是创建一个Maven项目，利用Maven来管理jar包的依赖。 一、WordCount程序1、创建一个Maven项目WordCount并导入依赖如果是创建父子项目的话，如果只是一个项目则一起写在一个xml文件中即可 父module的xml文件编写 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.spark&lt;/groupId&gt; &lt;artifactId&gt;spark-core_2.11&lt;/artifactId&gt; &lt;version&gt;2.1.1&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; 子module的xml文件编写 &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;3.2.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;configuration&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.atguigu.WordCount&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;single&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; 2、编写代码def main(args: Array[String]): Unit = { //创建配置文件 val conf = new SparkConf().setAppName("wc") //创建SparkContext val sc = new SparkContext(conf); //读取数据 val lines = sc.textFile(args(0)) //flatmap压平（一行行读取数据） val words = lines.flatMap(_.split(" ")) //map(word,1) val k2v = words.map((_,1)) //reduceByKey(word,1) val result = k2v.reduceByKey(_+_) //保存数据 result.saveAsTextFile(args(1)) //关闭连接 sc.stop(); } 3、打包成jar包到集群中运行（一般公司运行调试）将jar包上传到集群的spark下，最好是把路径修改为HDFS的，本地路径可能有问题 bin/spark-submit \ --class com.WordCount \ --master spark://hadoop102:7077 \ --executor-memory 1G \ --total-executor-cores 2 \ ./wordcount.jar \ hdfs://hadoop102:9000/red.txt \ hdfs://hadoop102:9000/out1 4、本地测试（一般用于Debug）本地Spark程序调试需要使用local提交模式，即将本机当做运行环境，Master和Worker都为本机。运行时直接加断点调试即可。 执行结果： 5、远程调试将jar包放在本地，通过集群远程调试代码，输出结果保存到hdfs上 def main(args: Array[String]): Unit = { //创建配置文件 val conf = new SparkConf().setAppName("wc").setMaster("spark://hadoop102:7077") .setJars(List("E:\\IdeaProjects\\spark0922\\sparkCore\\target\\sparkCore-1.0-SNAPSHOT.jar")) //创建SparkContext val sc = new SparkContext(conf); //读取数据 val lines = sc.textFile("hdfs://hadoop102:9000/red.txt") //flatMap压平 val words = lines.flatMap(_.split(" ")) //map(word,1) val k2v = words.map((_,1)) //reduceByKey(word,1) val result = k2v.reduceByKey(_+_) //保存数据 result.saveAsTextFile("hdfs://hadoop102:9000/out3") //关闭连接 sc.stop()]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>实战</tag>
        <tag>大数据</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——Spark运行模式]]></title>
    <url>%2F2018%2F03%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Spark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Spark运行模式 一、集群角色 从物理部署层面上来看，Spark主要分为两种类型的节点：Master节点和Worker节点。Master节点主要运行集群管理器的中心化部分，所承载的作用是分配Application到Worker节点，维护Worker节点，Driver，Application的状态。Worker节点负责具体的业务运行。从Spark程序运行层面来看，Spark主要分为驱动器节点和执行器节点。 1、Driver（驱动器节点）Spark的驱动器是执行开发程序中的main方法的进程。它负责开发人员编写的用来创建SparkContext、创建RDD，以及进行RDD的转化操作和行动操作代码的执行。如果你是用spark shell，那么当你启动Spark shell的时候，系统后台自启了一个Spark驱动器程序，就是在Spark shell中预加载的一个叫作 sc的SparkContext对象。如果驱动器程序终止，那么Spark应用也就结束了。主要负责：1）把用户程序转为作业（JOB）2）跟踪Executor的运行状况3）为执行器节点调度任务4）UI展示应用运行状况2、Executor（执行器节点）Spark Executor是一个工作进程，负责在 Spark 作业中运行任务，任务间相互独立。Spark应用启动时，Executor节点被同时启动，并且始终伴随着整个Spark应用的生命周期而存在。如果有Executor节点发生了故障或崩溃，Spark应用也可以继续执行，会将出错节点上的任务调度到其他Executor节点上继续运行。主要负责：1）负责运行组成Spark应用的任务，并将结果返回给驱动器进程2）通过自身的块管理器（Block Manager）为用户程序中要求缓存的RDD提供内存式存储。RDD是直接缓存在Executor进程内的，因此任务可以在运行时充分利用缓存数据加速运算。 二、运行模式1、Local模式1）概述Local模式：Local模式就是运行在一台计算机上的模式，通常就是用于在本机上练手和测试。它可以通过以下集中方式设置master。local：所有计算都运行在一个线程当中，没有任何并行计算，通常我们在本机执行一些测试代码，或者练手，就用这种模式;local[K]：指定使用几个线程来运行计算，比如local[4]就是运行4个worker线程。通常我们的cpu有几个core，就指定几个线程，最大化利用cpu的计算能力;local[*]：这种模式直接帮你按照cpu最多cores来设置线程数了。 2）安装使用（1）上传并解压安装包 [drift@hadoop102 sorfware]$ tar -zxvf spark-2.1.1-bin-hadoop2.7.tgz -C /opt/module/ [drift@hadoop102 module]$ mv spark-2.1.1-bin-hadoop2.7 spark （2）测试官方用例 [drift@hadoop102 spark]$ bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --executor-memory 1G \ --total-executor-cores 2 \ ./examples/jars/spark-examples_2.11-2.1.1.jar \ 100 ==&gt; 该数字代表迭代次数 计算圆周率： 计算结果： （3）WordCount案例（直接在Spark-Shell模式运行）1.WordCount思路 2.代码实现新建input文件夹，创建相应的txt文档，并在文档中写入适量单词 scala&gt; sc.textFile("input").flatMap(_.split(" ")).map ((_,1)).reduceByKey(_+_).collect 3.数据流分析：textFile(“input”)：读取本地文件input文件夹数据；flatMap(.split(“ “))：压平操作，按照空格分割符将一行数据映射成一个个单词；map((,1))：对每一个元素操作，将单词映射为元组；reduceByKey(_+_)：按照key将值进行聚合，相加；collect：将数据收集到Driver端展示。 2、Standalone模式构建一个由Master+Slave构成的Spark集群，Spark运行在集群中。 1）安装使用进入spark的conf目录下，修改三个文件名将slaves.template复制为slaves将spark-env.sh.template复制为spark-env.sh将spark-defaults.conf.template复制为spark-defaults.conf.sh 修改slaves，添加work节点 hadoop102 hadoop103 hadoop104 修改spark-env.sh SPARK_MASTER_HOST=hadoop102 SPARK_MASTER_PORT=7077 修改sbin下面的spark-config.sh export JAVA_HOME=/opt/module/jdk1.8.0_144 分发spark到集群各个节点，之后启动Spark 2）官方案例：求π bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master spark://hadoop102:7077 \ --executor-memory 1G \ --total-executor-cores 2 \ ./examples/jars/spark-examples_2.11-2.1.1.jar \ 100 结果： 3）Spark-Shell模式测试 scala&gt; sc.textFile("input").flatMap(_.split(" ")).map((_,1)).reduceByKey(_+_).collect 注意：提交应用程序概述 3、Yarn模式Spark客户端直接连接Yarn，不需要额外构建Spark集群。有yarn-client和yarn-cluster两种模式，主要区别在于：Driver程序的运行节点。yarn-client：Driver程序运行在客户端，适用于交互、调试，希望立即看到app的输出。yarn-cluster：Driver程序运行在由RM（ResourceManager）启动的AP（APPMaster）适用于生产环境。 1）安装使用注意：在提交任务之前需启动HDFS以及YARN集群。（1）修改hadoop配置文件yarn-site.xml,添加如下内容： &lt;!--是否启动一个线程检查每个任务正使用的物理内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.pmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则直接将其杀掉，默认是true --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.vmem-check-enabled&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; （2）修改spark-env.sh，添加如下配置： HADOOP_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop YARN_CONF_DIR=/opt/module/hadoop-2.7.2/etc/hadoop 配置完之后分发yarn-site.xml、spark-env.sh（其实不分发也可以） 2)官方案例 bin/spark-submit \ --class org.apache.spark.examples.SparkPi \ --master yarn \ --deploy-mode client \ ./examples/jars/spark-examples_2.11-2.1.1.jar \ 100 执行结果： 4、Mesos模式（了解即可）Spark客户端直接连接Mesos，不需要额外构建Spark集群。国内应用比较少，更多的是运用yarn调度。 5、几种模式比较 6、总结 如果想Driver运行在客户端，则采用Yarn-Client模式（客户端模式）如果想Driver运行按照集群资源分配，则用Yarn-Cluster模式（集群模式）]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>运行模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——Spark概述]]></title>
    <url>%2F2018%2F03%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Spark%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Spark概述 1、什么是SparkSpark是一种基于内存的快速、通用、可扩展的大数据分析引擎Spark是基于内存计算的大数据并行计算框架，除了扩展了广泛使用的MapReduce计算模型，而且高效地支持更多计算模式，包括交互式查询和流处理。Spark适用于各种各样原先需要多种不同的分布式平台的场景，包括批处理、迭代算法、交互式查询、流处理。通过在一个统一的框架下支持这些不同的计算，Spark使用户可以简单而低耗地把各种处理流程整合在一起。而这样的组合，在实际的数据分析过程中是很有意义的。不仅如此，Spark 的这种特性还大大减轻了原先需要对各种平台分别管理的负担。 2、Spark内置模块 Spark CoreSpark Core实现了Spark的基本功能，包含任务调度、内存管理、错误恢复、与存储系统 交互等模块。Spark Core中还包含了对弹性分布式数据集(resilient distributed dataset，简称RDD)的API定义。Spark SQLSpark SQL是Spark用来操作结构化数据的程序包。通过Spark SQL我们可以使用 SQL或者Apache Hive版本的SQL方言(HQL)来查询数据。Spark SQL支持多种数据源，比如Hive表、Parquet以及JSON等。Spark StreamingSpark Streaming是Spark提供的对实时数据进行流式计算的组件。提供了用来操作数据流的API，并且与Spark Core中的RDD API高度对应。Spark MLlib提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。集群管理器Spark 设计为可以高效地在一个计算节点到数千个计算节点之间伸缩计算。为了实现这样的要求，同时获得最大灵活性，Spark支持在各种集群管理器(cluster manager)上运行，包括Hadoop YARN、Apache Mesos，以及Spark自带的一个简易调度器，叫作独立调度器（standalone）。 3、Spark特点快与Hadoop的MapReduce相比，Spark基于内存的运算要快100倍以上，基于硬盘的运算也要快10倍以上。Spark实现了高效的DAG执行引擎，可以通过基于内存来高效处理数据流。计算的中间结果是存在于内存中的。易用Spark支持Java、Python和Scala的API，还支持超过80种高级算法，使用户可以快速构建不同的应用。而且Spark支持交互式的Python和Scala的shell，可以非常方便地在这些shell中使用Spark集群来验证解决问题的方法。通用Spark提供了统一的解决方案。Spark可以用于批处理、交互式查询（Spark SQL）、实时流处理（Spark Streaming）、机器学习（Spark MLlib）和图计算（GraphX）。这些不同类型的处理都可以在同一个应用中无缝使用。Spark统一的解决方案非常具有吸引力，毕竟任何公司都想用统一的平台去处理遇到的问题，减少开发和维护的人力成本和部署平台的物力成本。兼容性Spark可以非常方便地与其他的开源产品进行融合。比如，Spark可以使用Hadoop的YARN和Apache Mesos作为它的资源管理和调度器，并且可以处理所有Hadoop支持的数据，包括HDFS、HBase和Cassandra等。这对于已经部署Hadoop集群的用户特别重要，因为不需要做任何数据迁移就可以使用Spark的强大处理能力。Spark也可以不依赖于第三方的资源管理和调度器，它实现了Standalone作为其内置的资源管理和调度框架，这样进一步降低了Spark的使用门槛，使得所有人都可以非常容易地部署和使用Spark。此外，Spark还提供了在EC2上部署Standalone的Spark集群的工具。]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>概述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Spark系列——Spark的诞生]]></title>
    <url>%2F2018%2F03%2F09%2F%E5%A4%A7%E6%95%B0%E6%8D%AESpark%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Spark%E7%9A%84%E8%AF%9E%E7%94%9F%2F</url>
    <content type="text"><![CDATA[Spark的诞生一、前提 MapReduce缺点：MR基于数据集计算，所以是面向数据1）基本运算规则从存储介质中获取（采集）数据，然后进行计算，最后将结果存储到介质中，所以主要应用于一次性计算，不适合于数据挖掘和机器学习这样的迭代计算和图形挖掘计算2）MR是基于文件存储介质进行操作的，所以性能非常的慢3）MR和Hadoop紧密耦合在一起，无法动态获取 二、Spark的诞生因为MR无法满足对数据的迭代计算，Spark就应运而生 Spark历史2013年6月发布Spark基于Hadoop1.X架构思想，采用自己的方式改善Hadoop1.X中的问题Spark基于内存计算，并且基于Scala语法开发，所以天生适合迭代式计算Spark只适用于计算，存储还是要使用Hadoop的HDFS，也就是Spark+HDFS，YARN是可以动态的插拔计算框架的（YARN是支持Spark的计算的），通过YARN连接Spark和Hadoop结合使用]]></content>
      <categories>
        <category>Spark</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Spark</tag>
        <tag>诞生</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hive系列——Hive面试题（二）]]></title>
    <url>%2F2018%2F03%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHive%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Hive%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Hive面试题整理（二）1、Fetch抓取Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT * FROM employees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。 2、本地模式大多数的Hadoop Job是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。 表的优化3、小表、大表Join将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。 4、大表Join大表1）空KEY过滤有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空。2）空key转换有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。 5、Group By默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。1）开启Map端聚合参数设置（1）是否在Map端进行聚合，默认为Truehive.map.aggr = true（2）在Map端进行聚合操作的条目数目hive.groupby.mapaggr.checkinterval = 100000（3）有数据倾斜的时候进行负载均衡（默认是false）hive.groupby.skewindata = true当选项设定为 true，生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key分布到Reduce中（这个过程可以保证相同的Group By Key被分布到同一个Reduce中），最后完成最终的聚合操作。 6、Count(Distinct) 去重统计数据量小的时候无所谓，数据量大的情况下，由于COUNT DISTINCT操作需要用一个Reduce Task来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNT DISTINCT使用先GROUP BY再COUNT的方式替换 7、笛卡尔积尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积 8、行列过滤列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT *。行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。 数据倾斜9、 Map数1）通常情况下，作业会通过input的目录产生一个或者多个map任务。主要的决定因素有：input的文件总个数，input的文件大小，集群设置的文件块大小。2）是不是map数越多越好？答案是否定的。如果一个任务有很多小文件（远远小于块大小128m），则每个小文件也会被当做一个块，用一个map任务来完成，而一个map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的map数是受限的。3）是不是保证每个map处理接近128m的文件块，就高枕无忧了？答案也是不一定。比如有一个127m的文件，正常会用一个map去完成，但这个文件只有一个或者两个小字段，却有几千万的记录，如果map处理的逻辑比较复杂，用一个map任务去做，肯定也比较耗时。针对上面的问题2和3，我们需要采取两种方式来解决：即减少map数和增加map数； 10、小文件进行合并在map执行前合并小文件，减少map数：CombineHiveInputFormat具有对小文件进行合并的功能（系统默认的格式）。HiveInputFormat没有对小文件合并功能。set hive.input.format= org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; 11、复杂文件增加Map数当input的文件都很大，任务逻辑复杂，map执行非常慢的时候，可以考虑增加Map数，来使得每个map处理的数据量减少，从而提高任务的执行效率。增加map的方法为：根据computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M公式，调整maxSize最大值。让maxSize最大值低于blocksize就可以增加map的个数。 12、Reduce数1）调整reduce个数方法一（1）每个Reduce处理的数据量默认是256MBhive.exec.reducers.bytes.per.reducer=256000000（2）每个任务最大的reduce数，默认为1009hive.exec.reducers.max=1009（3）计算reducer数的公式N=min(参数2，总输入数据量/参数1)2）调整reduce个数方法二在hadoop的mapred-default.xml文件中修改设置每个job的Reduce个数set mapreduce.job.reduces = 15;3）reduce个数并不是越多越好（1）过多的启动和初始化reduce也会消耗时间和资源；（2）另外，有多少个reduce，就会有多少个输出文件，如果生成了很多个小文件，那么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；在设置reduce个数的时候也需要考虑这两个原则：处理大数据量利用合适的reduce数；使单个reduce任务处理数据量大小要合适。 ========================== 13、并行执行Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hive系列——Hive面试题（一）]]></title>
    <url>%2F2018%2F03%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHive%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Hive%E9%9D%A2%E8%AF%95%E9%A2%98%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[Hive面试题整理（一）1、Hive表关联查询，如何解决数据倾斜的问题？（☆☆☆☆☆）1）倾斜原因：map输出数据按key Hash的分配到reduce中，由于key分布不均匀、业务数据本身的特、建表时考虑不周、等原因造成的reduce 上的数据量差异过大。 （1）key分布不均匀;（2）业务数据本身的特性;（3）建表时考虑不周;（4）某些SQL语句本身就有数据倾斜;如何避免：对于key为空产生的数据倾斜，可以对其赋予一个随机值。2）解决方案（1）参数调节：hive.map.aggr = truehive.groupby.skewindata=true有数据倾斜的时候进行负载均衡，当选项设定位true,生成的查询计划会有两个MR Job。第一个MR Job中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的Group By Key有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MR Job再根据预处理的数据结果按照Group By Key 分布到 Reduce 中（这个过程可以保证相同的 Group By Key 被分布到同一个Reduce中），最后完成最终的聚合操作。（2）SQL 语句调节：① 选用join key分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表做join 的时候，数据量相对变小的效果。② 大小表Join：使用map join让小的维度表（1000 条以下的记录条数）先进内存。在map端完成reduce。③ 大表Join大表：把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null 值关联不上，处理后并不影响最终结果。④ count distinct大量相同特殊值:count distinct 时，将值为空的情况单独处理，如果是计算count distinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行group by，可以先将值为空的记录单独处理，再和其他计算结果进行union。 2、Hive的HSQL转换为MapReduce的过程？（☆☆☆☆☆）HiveSQL -&gt;AST(抽象语法树) -&gt; QB(查询块) -&gt;OperatorTree（操作树）-&gt;优化后的操作树-&gt;mapreduce任务树-&gt;优化后的mapreduce任务树 过程描述如下：SQL Parser：Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree；Semantic Analyzer：遍历AST Tree，抽象出查询的基本组成单元QueryBlock；Logical plan：遍历QueryBlock，翻译为执行操作树OperatorTree；Logical plan optimizer: 逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量；Physical plan：遍历OperatorTree，翻译为MapReduce任务；Logical plan optimizer：物理层优化器进行MapReduce任务的变换，生成最终的执行计划。 3、Hive底层与数据库交互原理？（☆☆☆☆☆）由于Hive的元数据可能要面临不断地更新、修改和读取操作，所以它显然不适合使用Hadoop文件系统进行存储。目前Hive将元数据存储在RDBMS中，比如存储在MySQL、Derby中。元数据信息包括：存在的表、表的列、权限和更多的其他信息。 4、Hive的两张表关联，使用MapReduce怎么实现？（☆☆☆☆☆）如果其中有一张表为小表，直接使用map端join的方式（map端加载小表）进行聚合。如果两张都是大表，那么采用联合key，联合key的第一个组成部分是join on中的公共字段，第二部分是一个flag，0代表表A，1代表表B，由此让Reduce区分客户信息和订单信息；在Mapper中同时处理两张表的信息，将join on公共字段相同的数据划分到同一个分区中，进而传递到一个Reduce中，然后在Reduce中实现聚合。 5、请谈一下Hive的特点，Hive和RDBMS有什么异同？hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析，但是Hive不支持实时查询。Hive与关系型数据库的区别： 6、请说明hive中 Sort By，Order By，Cluster By，Distrbute By各代表什么意思？order by：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。sort by：不是全局排序，其在数据进入reducer前完成排序。distribute by：按照指定的字段对数据进行划分输出到不同的reduce中。cluster by：除了具有 distribute by 的功能外还兼具 sort by 的功能。 7、写出hive中split、coalesce及collect_list函数的用法（可举例）？split将字符串转化为数组，即：split(‘a,b,c,d’ , ‘,’) ==&gt; [“a”,”b”,”c”,”d”]。coalesce(T v1, T v2, …) 返回参数中的第一个非空值；如果所有值都为 NULL，那么返回NULL。collect_list列出该字段所有的值，不去重 =&gt; select collect_list(id) from table。 8、Hive有哪些方式保存元数据，各有哪些特点？Hive支持三种不同的元存储服务器，分别为：内嵌式元存储服务器、本地元存储服务器、远程元存储服务器，每种存储方式使用不同的配置参数。内嵌式元存储主要用于单元测试，在该模式下每次只有一个进程可以连接到元存储，Derby是内嵌式元存储的默认数据库。在本地模式下，每个Hive客户端都会打开到数据存储的连接并在该连接上请求SQL查询。在远程模式下，所有的Hive客户端都将打开一个到元数据服务器的连接，该服务器依次查询元数据，元数据服务器和客户端之间使用Thrift协议通信。 9、Hive内部表和外部表的区别？创建表时：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。删除表时：在删除表的时候，内部表的元数据和数据会被一起删除， 而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。 10、Hive 中的压缩格式TextFile、SequenceFile、RCfile 、ORCfile各有什么区别？1、TextFile默认格式，存储方式为行存储，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，压缩后的文件不支持split，Hive不会对数据进行切分，从而无法对数据进行并行操作。并且在反序列化过程中，必须逐个字符判断是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍。2、SequenceFileSequenceFile是Hadoop API提供的一种二进制文件支持，存储方式为行存储，其具有使用方便、可分割、可压缩的特点。SequenceFile支持三种压缩选择：NONE，RECORD，BLOCK。Record压缩率低，一般建议使用BLOCK压缩。优势是文件和hadoop api中的MapFile是相互兼容的3、RCFile存储方式：数据按行分块，每块按列存储。结合了行存储和列存储的优点：首先，RCFile 保证同一行的数据位于同一节点，因此元组重构的开销很低；其次，像列存储一样，RCFile 能够利用列维度的数据压缩，并且能跳过不必要的列读取；4、ORCFile存储方式：数据按行分块 每块按照列存储。压缩快、快速列存取。效率比rcfile高，是rcfile的改良版本。总结：相比TEXTFILE和SEQUENCEFILE，RCFILE由于列式存储方式，数据加载时性能消耗较大，但是具有较好的压缩比和查询响应。数据仓库的特点是一次写入、多次读取，因此，整体来看，RCFILE相比其余两种格式具有较明显的优势。 11、所有的Hive任务都会有MapReduce的执行吗？不是，从Hive0.10.0版本开始，对于简单的不需要聚合的类似SELECT from LIMIT n语句，不需要起MapReduce job，直接通过Fetch task获取数据。 12、Hive的函数：UDF、UDAF、UDTF的区别？UDF：单行进入，单行输出UDAF：多行进入，单行输出UDTF：单行输入，多行输出 13、说说对Hive桶表的理解？桶表是对数据进行哈希取值，然后放到不同文件中存储。数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。物理上，每个桶就是表(或分区）目录里的一个文件，一个作业产生的桶(输出文件)和reduce任务个数相同。桶表专门用于抽样查询，是很专业性的，不是日常用来存储数据的表，需要抽样查询时，才创建和使用桶表。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hive系列——Hive数据类型]]></title>
    <url>%2F2018%2F03%2F05%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHive%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Hive%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[二、Hive数据类型 1、基本数据结构 对于Hive的String类型相当于数据库的varchar类型，该类型是一个可变的字符串，不过它不能声明其中最多能存储多少个字符，理论上它可以存储2GB的字符数。 2、基本数据类型 Hive有三种复杂数据类型ARRAY、MAP和STRUCT。ARRAY和MAP与Java中的Array和Map类似，而STRUCT与C语言中的Struct类似，它封装了一个命名字段集合，复杂数据类型允许任意层次的嵌套。 案例实操1）假设某表有如下一行，我们用JSON格式来表示其数据结构。在Hive下访问的格式为 { "name": "songsong", "friends": ["bingbing" , "lili"] , //列表Array, "children": { //键值Map, "xiao song": 18 , "xiaoxiao song": 19 } "address": { //结构Struct, "street": "hui long guan" , "city": "beijing" } } 2）基于上述数据结构，创建测试文件test.txt，并导入到Hive表中 songsong,bingbing_lili,xiao song:18_xiaoxiao song:19,hui long guan_beijing yangyang,caicai_susu,xiao yang:18_xiaoxiao yang:19,chao yang_beijing 3）Hive上创建测试表test create table test( name string, friends array&lt;string&gt;, children map&lt;string, int&gt;, address struct&lt;street:string, city:string&gt; ) row format delimited fields terminated by &apos;,&apos; collection items terminated by &apos;_&apos; map keys terminated by &apos;:&apos; lines terminated by &apos;\n&apos;; 字段解释：row format delimited fields terminated by ‘,’—— 列分隔符collection items terminated by ‘_’—— MAP STRUCT 和 ARRAY 的分隔符(数据分割符号)map keys terminated by ‘:’ —— MAP中的key与value的分隔符lines terminated by ‘\n’;—— 行分隔符 4）导入文本数据到测试表 load data local inpath ‘/opt/module/datas/test.txt’into table test 5）访问三种集合列里的数据，以下分别是ARRAY，MAP，STRUCT的访问方式 select friends[1],children[&apos;xiao song&apos;],address.city from test where name=&quot;songsong&quot;;]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hive</tag>
        <tag>数据类型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hive系列——Hive概述]]></title>
    <url>%2F2018%2F03%2F02%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHive%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Hive%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[一、Hive概述 1、什么是HiveHive是由Facebook开源用于解决海量结构化日志的数据统计。Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。本质是：将HQL转化成MapReduce程序 1）Hive处理的数据存储在HDFS2）Hive分析数据底层的实现是MapReduce3）执行程序运行在Yarn上 2、Hive优缺点优点：1) 操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）。2) 避免了去写MapReduce，减少开发人员的学习成本。3) Hive的执行延迟比较高，因此Hive常用于数据分析，对实时性要求不高的场合。4) Hive优势在于处理大数据，对于处理小数据没有优势，因为Hive的执行延迟比较高。5) Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。缺点：1）Hive的HQL表达能力有限（1）迭代式算法无法表达（2）数据挖掘方面不擅长2）Hive的效率比较低（1）Hive自动生成的MapReduce作业，通常情况下不够智能化（2）Hive调优比较困难，粒度较粗 3、Hive架构原理 1）用户接口：ClientCLI（command-line interface）、JDBC/ODBC(jdbc访问hive)、WEBUI（浏览器访问hive）2）元数据：Metastore元数据包括：表名、表所属的数据库（默认是default）、表的拥有者、列/分区字段、表的类型（是否是外部表）、表的数据所在目录等；默认存储在自带的derby数据库中，推荐使用MySQL存储Metastore3）Hadoop使用HDFS进行存储，使用MapReduce进行计算。4）驱动器：Driver（1）解析器（SQL Parser）：将SQL字符串转换成抽象语法树AST，这一步一般都用第三方工具库完成，比如antlr；对AST进行语法分析，比如表是否存在、字段是否存在、SQL语义是否有误。（2）编译器（Physical Plan）：将AST编译生成逻辑执行计划。（3）优化器（Query Optimizer）：对逻辑执行计划进行优化。（4）执行器（Execution）：把逻辑执行计划转换成可以运行的物理计划。对于Hive来说，就是MR/Spark。 Hive通过给用户提供的一系列交互接口，接收到用户的指令(SQL)，使用自己的Driver，结合元数据(MetaStore)，将这些指令翻译成MapReduce，提交到Hadoop中执行，最后，将执行返回的结果输出到用户交互接口。]]></content>
      <categories>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>简介</tag>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——面试题之优化问题]]></title>
    <url>%2F2018%2F01%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94%E9%9D%A2%E8%AF%95%E9%A2%98%E4%B9%8B%E4%BC%98%E5%8C%96%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Hadoop面试题总结（五）——优化问题1、MapReduce跑得慢的原因？（☆☆☆☆☆）Mapreduce 程序效率的瓶颈在于两点： 1）计算机性能CPU、内存、磁盘健康、网络2）I/O 操作优化（1）数据倾斜（2）map和reduce数设置不合理（3）reduce等待过久（4）小文件过多（5）大量的不可分块的超大文件（6）spill次数过多（7）merge次数过多等 2、MapReduce优化方法（☆☆☆☆☆）1）数据输入（1）合并小文件：在执行mr任务前将小文件进行合并，大量的小文件会产生大量的map任务，增大map任务装载次数，而任务的装载比较耗时，从而导致mr运行较慢。（2）采用ConbinFileInputFormat来作为输入，解决输入端大量小文件场景。2）map阶段（1）减少spill次数：通过调整io.sort.mb及sort.spill.percent参数值，增大触发spill的内存上限，减少spill次数，从而减少磁盘 IO。（2）减少merge次数：通过调整io.sort.factor参数，增大merge的文件数目，减少merge的次数，从而缩短mr处理时间。（3）在 map 之后先进行combine处理，减少I/O。3）reduce阶段（1）合理设置map和reduce数：两个都不能设置太少，也不能设置太多。太少，会导致task等待，延长处理时间；太多，会导致 map、reduce任务间竞争资源，造成处理超时等错误。（2）设置map、reduce共存：调整slowstart.completedmaps参数，使map运行到一定程度后，reduce也开始运行，减少reduce的等待时间。（3）规避使用reduce，因为Reduce在用于连接数据集的时候将会产生大量的网络消耗。（4）合理设置reduce端的buffer，默认情况下，数据达到一个阈值的时候，buffer中的数据就会写入磁盘，然后reduce会从磁盘中获得所有的数据。也就是说，buffer和reduce是没有直接关联的，中间多个一个写磁盘-&gt;读磁盘的过程，既然有这个弊端，那么就可以通过参数来配置，使得buffer中的一部分数据可以直接输送到reduce，从而减少IO开销：mapred.job.reduce.input.buffer.percent，默认为0.0。当值大于0的时候，会保留指定比例的内存读buffer中的数据直接拿给reduce使用。这样一来，设置buffer需要内存，读取数据需要内存，reduce计算也要内存，所以要根据作业的运行情况进行调整。4）IO传输（1）采用数据压缩的方式，减少网络IO的的时间。安装Snappy和LZOP压缩编码器。（2）使用SequenceFile二进制文件5）数据倾斜问题（1）数据倾斜现象数据频率倾斜——某一个区域的数据量要远远大于其他区域。数据大小倾斜——部分记录的大小远远大于平均值。（2）如何收集倾斜数据在reduce方法中加入记录map输出键的详细情况的功能。123456789101112131415161718192021public static final String MAX_VALUES = "skew.maxvalues";private int maxValueThreshold;@Overridepublic void configure(JobConf job) &#123; maxValueThreshold = job.getInt(MAX_VALUES, 100);&#125;@Overridepublic void reduce(Text key, Iterator&lt;Text&gt; values, OutputCollector&lt;Text, Text&gt; output, Reporter reporter) throws IOException &#123; int i = 0; while (values.hasNext()) &#123; values.next(); i++; &#125; if (++i &gt; maxValueThreshold) &#123; log.info("Received " + i + " values for key " + key); &#125;&#125; （3）减少数据倾斜的方法方法1：抽样和范围分区可以通过对原始数据进行抽样得到的结果集来预设分区边界值。方法2：自定义分区另一个抽样和范围分区的替代方案是基于输出键的背景知识进行自定义分区。例如，如果map输出键的单词来源于一本书。其中大部分必然是省略词（stopword）。那么就可以将自定义分区将这部分省略词发送给固定的一部分reduce实例。而将其他的都发送给剩余的reduce实例。方法3：Combine使用Combine可以大量地减小数据频率倾斜和数据大小倾斜。在可能的情况下，combine的目的就是聚合并精简数据。 3、HDFS小文件优化方法（☆☆☆☆☆）1）HDFS小文件弊端：HDFS上每个文件都要在namenode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件，一方面会大量占用namenode的内存空间，另一方面就是索引文件过大是的索引速度变慢。2）解决的方式：（1）Hadoop本身提供了一些文件压缩的方案。（2）从系统层面改变现有HDFS存在的问题，其实主要还是小文件的合并，然后建立比较快速的索引。3）Hadoop自带小文件解决方案（1）Hadoop Archive：是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时。（2）Sequence file：sequence file由一系列的二进制key/value组成，如果为key小文件名，value为文件内容，则可以将大批小文件合并成一个大文件。（3）CombineFileInputFormat：CombineFileInputFormat是一种新的inputformat，用于将多个文件合并成一个单独的split，另外，它会考虑数据的存储位置。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Hadoop</tag>
        <tag>优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——YARN面试题]]></title>
    <url>%2F2018%2F01%2F19%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94YARN%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Hadoop面试题（四）——YARN1、简述hadoop1与hadoop2 的架构异同1）加入了yarn解决了资源调度的问题。2）加入了对zookeeper的支持实现比较可靠的高可用。 2、为什么会产生 yarn,它解决了什么问题，有什么优势？1）Yarn最主要的功能就是解决运行的用户程序与yarn框架完全解耦。2）Yarn上可以运行各种类型的分布式运算程序（mapreduce只是其中的一种），比如mapreduce、storm程序，spark程序…… 3、HDFS的数据压缩算法?（☆☆☆☆☆）Hadoop中常用的压缩算法有bzip2、gzip、lzo、snappy，其中lzo、snappy需要操作系统安装native库才可以支持。数据可以压缩的位置如下所示。 企业开发用的比较多的是snappy。 4、Hadoop的调度器总结（☆☆☆☆☆）（1）默认的调度器FIFOHadoop中默认的调度器，它先按照作业的优先级高低，再按照到达时间的先后选择被执行的作业。（2）计算能力调度器Capacity Scheduler支持多个队列，每个队列可配置一定的资源量，每个队列采用FIFO调度策略，为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。调度时，首先按以下策略选择一个合适队列：计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值，选择一个该比值最小的队列；然后按以下策略选择该队列中一个作业：按照作业优先级和提交时间顺序选择，同时考虑用户资源量限制和内存限制。（3）公平调度器Fair Scheduler同计算能力调度器类似，支持多队列多用户，每个队列中的资源量可以配置，同一队列中的作业公平共享队列中所有资源。实际上，Hadoop的调度器远不止以上三种，最近，出现了很多针对新型应用的Hadoop调度器。 5、MapReduce 2.0 容错性（☆☆☆☆☆）1）MRAppMaster容错性一旦运行失败，由YARN的ResourceManager负责重新启动，最多重启次数可由用户设置，默认是2次。一旦超过最高重启次数，则作业运行失败。2）Map Task/ReduceTask Task周期性向MRAppMaster汇报心跳；一旦Task挂掉，则MRAppMaster将为之重新申请资源，并运行之。最多重新运行次数可由用户设置，默认4次。 6、mapreduce推测执行算法及原理（☆☆☆☆☆）1）作业完成时间取决于最慢的任务完成时间一个作业由若干个Map 任务和Reduce 任务构成。因硬件老化、软件Bug 等，某些任务可能运行非常慢。典型案例：系统中有99%的Map任务都完成了，只有少数几个Map老是进度很慢，完不成，怎么办？2）推测执行机制发现拖后腿的任务，比如某个任务运行速度远慢于任务平均速度。为拖后腿任务启动一个备份任务，同时运行。谁先运行完，则采用谁的结果。3）不能启用推测执行机制情况（1）任务间存在严重的负载倾斜；（2）特殊任务，比如任务向数据库中写数据。4）算法原理假设某一时刻，任务T的执行进度为progress，则可通过一定的算法推测出该任务的最终完成时刻estimateEndTime。另一方面，如果此刻为该任务启动一个备份任务，则可推断出它可能的完成时刻estimateEndTime,于是可得出以下几个公式： estimateEndTime=estimatedRunTime+taskStartTime estimatedRunTime=(currentTimestamp-taskStartTime)/progress estimateEndTime= currentTimestamp+averageRunTime其中，currentTimestamp为当前时刻；taskStartTime为该任务的启动时刻；averageRunTime为已经成功运行完成的任务的平均运行时间。这样，MRv2总是选择（estimateEndTime- estimateEndTime·）差值最大的任务，并为之启动备份任务。为了防止大量任务同时启动备份任务造成的资源浪费，MRv2为每个作业设置了同时启动的备份任务数目上限。推测执行机制实际上采用了经典的算法优化方法：以空间换时间，它同时启动多个相同任务处理相同的数据，并让这些任务竞争以缩短数据处理时间。显然，这种方法需要占用更多的计算资源。在集群资源紧缺的情况下，应合理使用该机制，争取在多用少量资源的情况下，减少作业的计算时间。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Hadoop</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——MapReduce面试题]]></title>
    <url>%2F2018%2F01%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94MapReduce%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Hadoop面试题总结（三）——MapReduce1、谈谈Hadoop序列化和反序列化及自定义bean对象实现序列化?1）序列化和反序列化 （1）序列化就是把内存中的对象，转换成字节序列（或其他数据传输协议）以便于存储（持久化）和网络传输。（2）反序列化就是将收到字节序列（或其他数据传输协议）或者是硬盘的持久化数据，转换成内存中的对象。（3）Java的序列化是一个重量级序列化框架（Serializable），一个对象被序列化后，会附带很多额外的信息（各种校验信息，header，继承体系等），不便于在网络中高效传输。所以，hadoop自己开发了一套序列化机制（Writable），精简、高效。2）自定义bean对象要想序列化传输步骤及注意事项：（1）必须实现Writable接口（2）反序列化时，需要反射调用空参构造函数，所以必须有空参构造（3）重写序列化方法（4）重写反序列化方法（5）注意反序列化的顺序和序列化的顺序完全一致（6）要想把结果显示在文件中，需要重写toString()，且用”\t”分开，方便后续用（7）如果需要将自定义的bean放在key中传输，则还需要实现comparable接口，因为mapreduce框中的shuffle过程一定会对key进行排序 2、FileInputFormat切片机制（☆☆☆☆☆）job提交流程源码详解waitForCompletion()submit();// 1、建立连接connect();// 1）创建提交job的代理new Cluster(getConfiguration());// （1）判断是本地yarn还是远程initialize(jobTrackAddr, conf);// 2、提交jobsubmitter.submitJobInternal(Job.this, cluster)// 1）创建给集群提交数据的Stag路径Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf);// 2）获取jobid ，并创建job路径JobID jobId = submitClient.getNewJobID();// 3）拷贝jar包到集群copyAndConfigureFiles(job, submitJobDir);rUploader.uploadFiles(job, jobSubmitDir);// 4）计算切片，生成切片规划文件writeSplits(job, submitJobDir);maps = writeNewSplits(job, jobSubmitDir);input.getSplits(job);// 5）向Stag路径写xml配置文件writeConf(conf, submitJobFile);conf.writeXml(out);// 6）提交job,返回提交状态status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); 3、在一个运行的Hadoop 任务中，什么是InputSplit？（☆☆☆☆☆）FileInputFormat源码解析(input.getSplits(job))（1）找到你数据存储的目录。（2）开始遍历处理（规划切片）目录下的每一个文件。（3）遍历第一个文件ss.txt。a）获取文件大小fs.sizeOf(ss.txt);。b）计算切片大小computeSliteSize(Math.max(minSize,Math.min(maxSize,blocksize)))=blocksize=128M。c）默认情况下，切片大小=blocksize。d）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）。e）将切片信息写到一个切片规划文件中。f）整个切片的核心过程在getSplit()方法中完成。g）数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。h）注意：block是HDFS上物理上存储的存储的数据，切片是对数据逻辑上的划分。（4）提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。 4、如何判定一个job的map和reduce的数量?1）map数量splitSize=max{minSize,min{maxSize,blockSize}}map数量由处理的数据分成的block数量决定default_num = total_size / split_size;2）reduce数量reduce的数量job.setNumReduceTasks(x);x 为reduce的数量。不设置的话默认为 1。 5、 Maptask的个数由什么决定？一个job的map阶段MapTask并行度（个数），由客户端提交job时的切片个数决定。 6、MapTask和ReduceTask工作机制（☆☆☆☆☆）（也可回答MapReduce工作原理）MapTask工作机制 （1）Read阶段：Map Task通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。（2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。（4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。（5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。 ReduceTask工作机制 （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。（2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。（3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。 由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。（4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 7、描述mapReduce有几种排序及排序发生的阶段（☆☆☆☆☆）1）排序的分类：（1）部分排序：MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。（2）全排序：如何用Hadoop产生一个全局排序的文件？最简单的方法是使用一个分区。但该方法在处理大型文件时效率极低，因为一台机器必须处理所有输出文件，从而完全丧失了MapReduce所提供的并行架构。替代方案：首先创建一系列排好序的文件；其次，串联这些文件；最后，生成一个全局排序的文件。主要思路是使用一个分区来描述输出的全局排序。例如：可以为待分析文件创建3个分区，在第一分区中，记录的单词首字母a-g，第二分区记录单词首字母h-n, 第三分区记录单词首字母o-z。（3）辅助排序：（GroupingComparator分组）Mapreduce框架在记录到达reducer之前按键对记录排序，但键所对应的值并没有被排序。甚至在不同的执行轮次中，这些值的排序也不固定，因为它们来自不同的map任务且这些map任务在不同轮次中完成时间各不相同。一般来说，大多数MapReduce程序会避免让reduce函数依赖于值的排序。但是，有时也需要通过特定的方法对键进行排序和分组等以实现对值的排序。（4）二次排序：在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。2）自定义排序WritableComparablebean对象实现WritableComparable接口重写compareTo方法，就可以实现排序@Overridepublic int compareTo(FlowBean o) {// 倒序排列，从大到小return this.sumFlow &gt; o.getSumFlow() ? -1 : 1;}3）排序发生的阶段：（1）一个是在map side发生在spill后partition前。（2）一个是在reduce side发生在copy后 reduce前。 8、描述mapReduce中shuffle阶段的工作流程，如何优化shuffle阶段（☆☆☆☆☆） 分区，排序，溢写，拷贝到对应reduce机器上，增加combiner，压缩溢写的文件。 9、描述mapReduce中combiner的作用是什么，一般使用情景，哪些情况不需要，及和reduce的区别？1）Combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。2）Combiner能够应用的前提是不能影响最终的业务逻辑，而且，Combiner的输出kv应该跟reducer的输入kv类型要对应起来。3）Combiner和reducer的区别在于运行的位置。Combiner是在每一个maptask所在的节点运行；Reducer是接收全局所有Mapper的输出结果。 10、如果没有定义partitioner，那数据在被送达reducer前是如何被分区的？如果没有自定义的 partitioning，则默认的 partition 算法，即根据每一条数据的 key 的 hashcode 值摸运算（%）reduce 的数量，得到的数字就是“分区号“。 11、MapReduce 出现单点负载多大，怎么负载平衡？ （☆☆☆☆☆）通过Partitioner实现 12、MapReduce 怎么实现 TopN？ （☆☆☆☆☆）可以自定义groupingcomparator，对结果进行最大值排序，然后再reduce输出时，控制只输出前n个数。就达到了topn输出的目的。 13、Hadoop的缓存机制（Distributedcache）（☆☆☆☆☆）分布式缓存一个最重要的应用就是在进行join操作的时候，如果一个表很大，另一个表很小，我们就可以将这个小表进行广播处理，即每个计算节点上都存一份，然后进行map端的连接操作，经过我的实验验证，这种情况下处理效率大大高于一般的reduce端join，广播处理就运用到了分布式缓存的技术。DistributedCache将拷贝缓存的文件到Slave节点在任何Job在节点上执行之前，文件在每个Job中只会被拷贝一次，缓存的归档文件会被在Slave节点中解压缩。将本地文件复制到HDFS中去，接着Client会通过addCacheFile() 和addCacheArchive()方法告诉DistributedCache在HDFS中的位置。当文件存放到文地时，JobClient同样获得DistributedCache来创建符号链接，其形式为文件的URI加fragment标识。当用户需要获得缓存中所有有效文件的列表时，JobConf 的方法 getLocalCacheFiles() 和getLocalArchives()都返回一个指向本地文件路径对象数组。 14、如何使用mapReduce实现两个表的join?（☆☆☆☆☆）1）reduce side join : 在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签（tag）,比如：tag=0 表示来自文件File1，tag=2 表示来自文件File2。2）map side join : Map side join 是针对以下场景进行的优化：两个待连接表中，有一个表非常大，而另一个表非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多份，让每个map task 内存中存在一份（比如存放到hash table 中），然后只扫描大表：对于大表中的每一条记录key/value，在hash table 中查找是否有相同的key 的记录，如果有，则连接后输出即可。 15、什么样的计算不能用mr来提速？1）数据量很小。2）繁杂的小文件。3）索引是更好的存取机制的时候。4）事务处理。5）只有一台机器的时候。 16、ETL是哪三个单词的缩写Extraction-Transformation-Loading的缩写，中文名称为数据提取、转换和加载。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——HDFS面试题]]></title>
    <url>%2F2018%2F01%2F16%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94HDFS%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Hadoop面试题总结（二）——HDFS1、 HDFS 中的 block 默认保存几份？默认保存3份 2、HDFS 默认 BlockSize 是多大？默认64MB 3、负责HDFS数据存储的是哪一部分？DataNode负责数据存储 4、SecondaryNameNode的目的是什么？他的目的使帮助NameNode合并编辑日志，减少NameNode 启动时间 5、文件大小设置，增大有什么影响？HDFS中的文件在物理上是分块存储（block），块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M。思考：为什么块的大小不能设置的太小，也不能设置的太大？HDFS的块比磁盘的块大，其目的是为了最小化寻址开销。如果块设置得足够大，从磁盘传输数据的时间会明显大于定位这个块开始位置所需的时间。因而，传输一个由多个块组成的文件的时间取决于磁盘传输速率。如果寻址时间约为10ms，而传输速率为100MB/s，为了使寻址时间仅占传输时间的1%，我们要将块大小设置约为100MB。默认的块大小128MB。块的大小：10ms×100×100M/s = 100M，如图增加文件块大小，需要增加磁盘的传输速率。 6、hadoop的块大小，从哪个版本开始是128MHadoop1.x都是64M，hadoop2.x开始都是128M。 7、HDFS的存储机制（☆☆☆☆☆）HDFS存储机制，包括HDFS的写入数据过程和读取数据过程两部分HDFS写数据过程 1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。2）NameNode返回是否可以上传。3）客户端请求第一个 block上传到哪几个datanode服务器上。4）NameNode返回3个datanode节点，分别为dn1、dn2、dn3。5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。6）dn1、dn2、dn3逐级应答客户端。7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。8）当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步）。 HDFS读数据过程 1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以packet为单位来做校验）。4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。 8、secondary namenode工作机制（☆☆☆☆☆） 1）第一阶段：NameNode启动（1）第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。（2）客户端对元数据进行增删改的请求。（3）NameNode记录操作日志，更新滚动日志。（4）NameNode在内存中对数据进行增删改查。2）第二阶段：Secondary NameNode工作（1）Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。（2）Secondary NameNode请求执行checkpoint。（3）NameNode滚动正在写的edits日志。（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。（6）生成新的镜像文件fsimage.chkpoint。（7）拷贝fsimage.chkpoint到NameNode。（8）NameNode将fsimage.chkpoint重新命名成fsimage。 9、NameNode与SecondaryNameNode 的区别与联系？（☆☆☆☆☆）机制流程看第7题1）区别（1）NameNode负责管理整个文件系统的元数据，以及每一个路径（文件）所对应的数据块信息。（2）SecondaryNameNode主要用于定期合并命名空间镜像和命名空间镜像的编辑日志。2）联系：（1）SecondaryNameNode中保存了一份和namenode一致的镜像文件（fsimage）和编辑日志（edits）。（2）在主namenode发生故障时（假设没有及时备份数据），可以从SecondaryNameNode恢复数据。 10、HDFS组成架构（☆☆☆☆☆） 架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面我们分别介绍这四个组成部分。1）Client：就是客户端。（1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储；（2）与NameNode交互，获取文件的位置信息；（3）与DataNode交互，读取或者写入数据；（4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS；（5）Client可以通过一些命令来访问HDFS；2）NameNode：就是Master，它是一个主管、管理者。（1）管理HDFS的名称空间；（2）管理数据块（Block）映射信息；（3）配置副本策略；（4）处理客户端读写请求。3）DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。（1）存储实际的数据块；（2）执行数据块的读/写操作。4）Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。（1）辅助NameNode，分担其工作量；（2）定期合并Fsimage和Edits，并推送给NameNode；（3）在紧急情况下，可辅助恢复NameNode。 11、HAnamenode 是如何工作的? （☆☆☆☆☆） ZKFailoverController主要职责1）健康监测：周期性的向它监控的NN发送健康探测命令，从而来确定某个NameNode是否处于健康状态，如果机器宕机，心跳失败，那么zkfc就会标记它处于一个不健康的状态。2）会话管理：如果NN是健康的，zkfc就会在zookeeper中保持一个打开的会话，如果NameNode同时还是Active状态的，那么zkfc还会在Zookeeper中占有一个类型为短暂类型的znode，当这个NN挂掉时，这个znode将会被删除，然后备用的NN，将会得到这把锁，升级为主NN，同时标记状态为Active。3）当宕机的NN新启动时，它会再次注册zookeper，发现已经有znode锁了，便会自动变为Standby状态，如此往复循环，保证高可靠，需要注意，目前仅仅支持最多配置2个NN。4）master选举：如上所述，通过在zookeeper中维持一个短暂类型的znode，来实现抢占式的锁机制，从而判断那个NameNode为Active状态]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——Hadoop面试题]]></title>
    <url>%2F2018%2F01%2F15%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Hadoop%E9%9D%A2%E8%AF%95%E9%A2%98%2F</url>
    <content type="text"><![CDATA[Hadoop面试题（一）1、集群的最主要瓶颈磁盘IO 2、Hadoop运行模式单机版、伪分布式模式、完全分布式模式 3、Hadoop生态圈的组件并做简要描述1）Zookeeper：是一个开源的分布式应用程序协调服务,基于zookeeper可以实现同步服务，配置维护，命名服务。2）Flume：一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。3）Hbase：是一个分布式的、面向列的开源数据库, 利用Hadoop HDFS作为其存储系统。4）Hive：基于Hadoop的一个数据仓库工具，可以将结构化的数据档映射为一张数据库表，并提供简单的sql 查询功能，可以将sql语句转换为MapReduce任务进行运行。5）Sqoop：将一个关系型数据库中的数据导进到Hadoop的 HDFS中，也可以将HDFS的数据导进到关系型数据库中。 4、解释“hadoop”和“hadoop 生态系统”两个概念Hadoop是指Hadoop框架本身；hadoop生态系统，不仅包含hadoop，还包括保证hadoop框架正常高效运行其他框架，比如zookeeper、Flume、Hbase、Hive、Sqoop等辅助框架。 5、请列出正常工作的Hadoop集群中Hadoop都分别需要启动哪些进程，它们的作用分别是什么?1）NameNode：它是hadoop中的主服务器，管理文件系统名称空间和对集群中存储的文件的访问，保存有metadate。2）SecondaryNameNode：它不是namenode的冗余守护进程，而是提供周期检查点和清理任务。帮助NN合并editslog，减少NN启动时间。3）DataNode：它负责管理连接到节点的存储（一个集群中可以有多个节点）。每个存储数据的节点运行一个datanode守护进程。4）ResourceManager（JobTracker）：JobTracker负责调度DataNode上的工作。每个DataNode有一个TaskTracker，它们执行实际工作。5）NodeManager：（TaskTracker）执行任务。6）DFSZKFailoverController：高可用时它负责监控NN的状态，并及时的把状态信息写入ZK。它通过一个独立线程周期性的调用NN上的一个特定接口来获取NN的健康状态。FC也有选择谁作为Active NN的权利，因为最多只有两个节点，目前选择策略还比较简单（先到先得，轮换）。7）JournalNode：高可用情况下存放namenode的editlog文件。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>面试</tag>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——Hadoop数据压缩]]></title>
    <url>%2F2017%2F12%2F29%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94Hadoop%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"><![CDATA[Hadoop——数据压缩 1、压缩概述压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。在Hadoop下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下，I/O操作和网络数据传输要花大量的时间。还有，Shuffle与Merge过程同样也面临着巨大的I/O压力。 鉴于磁盘I/O和网络带宽是Hadoop的宝贵资源，数据压缩对于节省资源、最小化磁盘I/O和网络传输非常有帮助。不过，尽管压缩与解压操作的CPU开销不高，其性能的提升和资源的节省并非没有代价。如果磁盘I/O和网络带宽影响了MapReduce作业性能，在任意MapReduce阶段启用压缩都可以改善端到端处理时间并减少I/O和网络流量。压缩Mapreduce的一种优化策略：通过压缩编码对Mapper或者Reducer的输出进行压缩，以减少磁盘IO，提高MR程序运行速度（但相应增加了cpu运算负担）。注意：采用压缩技术减少了磁盘IO，但同时增加了CPU运算负担。所以，压缩特性运用得当能提高性能，但运用不当也可能降低性能。基本原则：（1）运算密集型的job，少用压缩（2）IO密集型的job，多用压缩 2、MR支持的压缩编码 压缩性能的比较： On a single core of a Core i7 processor in 64-bit mode, Snappy compresses at about 250 MB/sec or more and decompresses at about 500 MB/sec or more.### 3、压缩方式选择1）Gzip压缩优点：压缩率比较高，而且压缩/解压速度也比较快；Hadoop本身支持，在应用中处理gzip格式的文件就和直接处理文本一样；大部分linux系统都自带gzip命令，使用方便。缺点：不支持split。应用场景：当每个文件压缩之后在130M以内的（1个块大小内），都可以考虑用gzip压缩格式。例如说一天或者一个小时的日志压缩成一个gzip文件，运行mapreduce程序的时候通过多个gzip文件达到并发。hive程序，streaming程序，和java写的mapreduce程序完全和文本处理一样，压缩之后原来的程序不需要做任何修改。2）Bzip2压缩优点：支持split；具有很高的压缩率，比gzip压缩率都高；hadoop本身支持，但不支持native；在linux系统下自带bzip2命令，使用方便。缺点：压缩/解压速度慢。应用场景：适合对速度要求不高，但需要较高的压缩率的时候，可以作为mapreduce作业的输出格式；或者输出之后的数据比较大，处理之后的数据需要压缩存档减少磁盘空间并且以后数据用得比较少的情况；或者对单个很大的文本文件想压缩减少存储空间，同时又需要支持split，而且兼容之前的应用程序（即应用程序不需要修改）的情况。3）Lzo压缩优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；可以在linux系统下安装lzop命令，使用方便。缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越越明显。4）Snappy压缩优点：高速压缩速度和合理的压缩率。缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装；应用场景：当Mapreduce作业的Map输出的数据比较大的时候，作为Map到Reduce的中间数据的压缩格式；或者作为一个Mapreduce作业的输出和另外一个Mapreduce作业的输入。### 4、压缩位置选择]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>数据压缩</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——NN、2NN、DN工作机制]]></title>
    <url>%2F2017%2F12%2F25%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94NN%E3%80%812NN%E3%80%81DN%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[一、NameNode和SecondaryNameNode1、NN和2NN的工作机制 1）第一阶段：NameNode启动（1）第一次启动NameNode格式化后，创建fsimage和edits文件。如果不是第一次启动，直接加载编辑日志和镜像文件到内存。（2）客户端对元数据进行增删改的请求。（3）NameNode记录操作日志，更新滚动日志。（4）NameNode在内存中对数据进行增删改查。2）第二阶段：Secondary NameNode工作（1）Secondary NameNode询问NameNode是否需要checkpoint。直接带回NameNode是否检查结果。（2）Secondary NameNode请求执行checkpoint。（3）NameNode滚动正在写的edits日志。（4）将滚动前的编辑日志和镜像文件拷贝到Secondary NameNode。（5）Secondary NameNode加载编辑日志和镜像文件到内存，并合并。（6）生成新的镜像文件fsimage.chkpoint。（7）拷贝fsimage.chkpoint到NameNode。（8）NameNode将fsimage.chkpoint重新命名成fsimage。 NN和2NN工作机制详解：Fsimage：namenode内存中元数据序列化后形成的文件。Edits：记录客户端更新元数据信息的每一步操作（可通过Edits运算出元数据）。namenode启动时，先滚动edits并生成一个空的edits.inprogress，然后加载edits和fsimage到内存中，此时namenode内存就持有最新的元数据信息。client开始对namenode发送元数据的增删改查的请求，这些请求的操作首先会被记录的edits.inprogress中（查询元数据的操作不会被记录在edits中，因为查询操作不会更改元数据信息），如果此时namenode挂掉，重启后会从edits中读取元数据的信息。然后，namenode会在内存中执行元数据的增删改查的操作。由于edits中记录的操作会越来越多，edits文件会越来越大，导致namenode在启动加载edits时会很慢，所以需要对edits和fsimage进行合并（所谓合并，就是将edits和fsimage加载到内存中，照着edits中的操作一步步执行，最终形成新的fsimage）。secondarynamenode的作用就是帮助namenode进行edits和fsimage的合并工作。secondarynamenode首先会询问namenode是否需要checkpoint（触发checkpoint需要满足两个条件中的任意一个，定时时间到和edits中数据写满了）。直接带回namenode是否检查结果。secondarynamenode执行checkpoint操作，首先会让namenode滚动edits并生成一个空的edits.inprogress，滚动edits的目的是给edits打个标记，以后所有新的操作都写入edits.inprogress，其他未合并的edits和fsimage会拷贝到secondarynamenode的本地，然后将拷贝的edits和fsimage加载到内存中进行合并，生成fsimage.chkpoint，然后将fsimage.chkpoint拷贝给namenode，重命名为fsimage后替换掉原来的fsimage。namenode在启动时就只需要加载之前未合并的edits和fsimage即可，因为合并过的edits中的元数据信息已经被记录在fsimage中。 二、DataNode1、DataNode工作机制 1）一个数据块在DataNode上以文件形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，块数据的校验和，以及时间戳。2）DataNode启动后向NameNode注册，通过后，周期性（1小时）的向NameNode上报所有的块信息。3）心跳是每3秒一次，心跳返回结果带有NameNode给该DataNode的命令如复制块数据到另一台机器，或删除某个数据块。如果超过10分钟没有收到某个DataNode的心跳，则认为该节点不可用。4）集群运行中可以安全加入和退出一些机器。 2、数据完整性1）当DataNode读取block的时候，它会计算checksum。2）如果计算后的checksum，与block创建时值不一样，说明block已经损坏。3）client读取其他DataNode上的block。4）datanode在其文件创建后周期验证checksum，如下图 3、掉线时限参数设置DataNode进程死亡或者网络故障造成DataNode无法与NameNode通信，NameNode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：timeout = 2 dfs.namenode.heartbeat.recheck-interval + 10 dfs.heartbeat.interval。而默认的dfs.namenode.heartbeat.recheck-interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。12345678&lt;property&gt; &lt;name&gt;dfs.namenode.heartbeat.recheck-interval&lt;/name&gt; &lt;value&gt;300000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt; dfs.heartbeat.interval &lt;/name&gt; &lt;value&gt;3&lt;/value&gt; &lt;/property&gt;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>原理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——YARN简介]]></title>
    <url>%2F2017%2F12%2F24%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94YARN%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[YARN1、YARN概述Yarn是一个资源调度平台，负责为运算程序提供服务器运算资源，相当于一个分布式的操作系统平台，而MapReduce等运算程序则相当于运行于操作系统之上的应用程序。 2、YARN基本架构 YARN主要由ResourceManager、NodeManager、ApplicationMaster和Container等组件构成，如上图所示。 各组件主要作用：1）ResourceManager（RM）① 处理客户端请求② 监控NodeManager③ 启动或监控ApplicationMaster④ 资源的分配与调度 2）NodeManager（NM）① 管理每个节点上的资源② 处理来自的ResourceManager的命令③ 处理来自ApplicationMaster的命令 3）ApplicationMaster（AM）① 负责数据的切分② 为应用程序申请资源并分配给内部的任务③ 任务的监控与容错 4）ContainerContainer是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等。 3、YARN工作机制 工作机制详解：（1）MR程序提交到客户端所在的节点。（2）YarnRunner向ResourceManager申请一个Application。（3）RM将该应用程序的资源路径返回给YarnRunner。（4）该程序将运行所需资源提交到HDFS上。（5）程序资源提交完毕后，申请运行mrAppMaster。（6）RM将用户的请求初始化成一个Task。（7）其中一个NodeManager领取到Task任务。（8）该NodeManager创建容器Container，并产生MRAppmaster。（9）Container从HDFS上拷贝资源到本地。（10）MRAppmaster向RM 申请运行MapTask资源。（11）RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。（12）MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。（13）MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。（14）ReduceTask向MapTask获取相应分区的数据。（15）程序运行完毕后，MR会向RM申请注销自己。 YARN作业提交全过程（流程图和工作机制一致）：（1）作业提交第1步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。第2步：Client向RM申请一个作业id。第3步：RM给Client返回该job资源的提交路径和作业id。第4步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。第5步：Client提交完资源后，向RM申请运行MrAppMaster。 （2）作业初始化第6步：当RM收到Client的请求后，将该job添加到容量调度器中。第7步：某一个空闲的NM领取到该Job。第8步：该NM创建Container，并产生MRAppmaster。第9步：下载Client提交的资源到本地。 （3）任务分配第10步：MrAppMaster向RM申请运行多个MapTask任务资源。第11步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分别领取任务并创建容器。 （4）任务运行第12步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager分别启动MapTask，MapTask对数据分区排序。第13步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。第14步：ReduceTask向MapTask获取相应分区的数据。第15步：程序运行完毕后，MR会向RM申请注销自己。 （5）进度和状态更新YARN中的任务将其进度和状态(包括counter)返回给应用管理器, 客户端每秒(通过mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用户。 （6）作业完成除了向应用管理器请求作业进度外, 客户端每5秒都会通过调用waitForCompletion()来检查作业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备之后用户核查。 4、资源调度器目前，Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop2.7.2默认的资源调度器是Capacity Scheduler。具体设置详见：yarn-default.xml文件 &lt;property&gt; &lt;description&gt;The class to use as the resource scheduler.&lt;/description&gt; &lt;name&gt;yarn.resourcemanager.scheduler.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.**CapacityScheduler**&lt;/value&gt; &lt;/property&gt; 1）先进先出调度器（FIFO） 2）容量调度器（Capacity Scheduler） （1）支持多个队列，每个队列可配置一定的资源量，每个队列采用FIFO调度策略。（2）为了防止同一个用户的作业独占队列中的资源，该调度器会对同一用户提交的作业所占资源量进行限定。（3）首先，计算每个队列中正在运行的任务数与其应该分得的计算资源之间的比值，选择一个该比值最小的队列。（4）其次，按照作业优先级和提交时间顺序，同时考虑用户资源量限制和内存限制对队列内任务排序。（5）三个队列同时按照任务的先后顺序依次执行，比如，job11、job21和job31分 别排在队列最前面，是最先运行，也是同时运行。 3）公平调度器（Fair Scheduler） 支持多队列多用户，每个队列中的资源量可以配置，同一队列中的作业公平共享队列中所有资源。比如有三个队列: queueA、 queueB和queueC，每个队列中的ob按照优先级分配资源，优先级越高分配的资源越多，但是每个job都会分配到资源以确保公平。在资源有限的情况下，每个job理想情况下获得的计算资源与实际获得的计算资源存在一种差距，这个差距就叫做缺额。在同一一个队列中，job的资源缺额越大，越先获得资源优先执行。作业是按照缺额的高低来先后执行的，而且可以看到上图有多个作业同时运行。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>YARN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——MapReduce案例]]></title>
    <url>%2F2017%2F12%2F21%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94MapReduce%E6%A1%88%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[MapReduce案例实操1、MapReduce核心编程思想1）分布式的运算程序往往需要分成至少2个阶段2）第一个阶段的maptask并发实例，完全并行运行，互不相干3）第二个阶段的reduce task并发实例互不相干，但是他们的数据依赖于上一个阶段的所有maptask并发实例的输出4）MapReduce编程模型只能包含一个map阶段和一个reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个mapreduce程序，串行运行 2、MapReduce程序运行流程详解 MR程序具体运行步骤如下：1）在MapReduce程序读取文件的输入目录上存放相应的文件。2）客户端程序在submit()方法执行前，获取待处理的数据信息，然后根据集群中参数的配置形成一个任务分配规划。3）客户端提交job.split、jar包、job.xml等文件给yarn，yarn中的resourcemanager启动MRAppMaster。4）MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程。5）maptask利用客户指定的inputformat来读取数据，形成输入KV对。6）maptask将输入KV对传递给客户定义的map()方法，做逻辑运算。7）map()运算完毕后将KV对收集到maptask缓存。8）maptask缓存中的KV对按照K分区排序后不断写到磁盘文件。9）MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据分区。10）Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算。11）Reducetask运算完毕后，调用客户指定的outputformat将结果数据输出到外部存储。 3、案例实践新建Maven工程，在pom.xml文件中添加依赖123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt;``` 在项目的src/main/resources目录下，新建一个文件，命名为“log4j.properties”，在文件中填入。 ```xml log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n log4j.appender.logfile=org.apache.log4j.FileAppender log4j.appender.logfile.File=target/spring.log log4j.appender.logfile.layout=org.apache.log4j.PatternLayout log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n``` ### 3.1、WordCount案例需求：在给定的文本文件中统计输出每一个单词出现的总次数 输入数据： ```xmlhello worldhadoopsparkhello worldhadoopsparkhello worldhadoopspark``` **代码**： 1）**定义一个mapper类** ```javaimport org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;/** * （1）用户自定义的Mapper要继承自己的父类 * （2）Mapper的输入数据是K-V对的形式（K-V的类型可自定义） * （3）Mapper中的业务逻辑写在map()方法中 * （4）Mapper的输出数据是K-V对的形式（K-V的类型可自定义） * （5）map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次 */public class WordcountMapper extends Mapper&lt;LongWritable, Text, Text, IntWritable&gt; &#123; /** * map()方法（maptask进程）对每一个&lt;K,V&gt;调用一次 * * @param key : 数据的offset * @param value : 要处理的一行数据 * @param context : 上下文 * @throws IOException * @throws InterruptedException */ IntWritable v = new IntWritable(1); Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; //1、获取一行数据，将一行数据转化为String类型 String line = value.toString(); //2、切割 String[] words = line.split(" "); //3、循环写出 for (String word : words) &#123; k.set(word); context.write(k, v); &#125; &#125;&#125;``` 2）**定义一个reducer类** ```javaimport org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Reducer;import java.io.IOException;/** * （1）用户自定义的Reducer要继承自己的父类 * （2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV * （3）Reducer的业务逻辑写在reduce()方法中 * （4）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 */public class WordcountReduce extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; /** * Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法 * * @param key : 单词 * @param values : 单词个数（1）的集合 * @param context : 上下文 * @throws IOException * @throws InterruptedException */ IntWritable v = new IntWritable(); int sum; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; //1、累加 sum = 0; for (IntWritable value : values) &#123; sum += value.get(); &#125; //2、写出 v.set(sum); context.write(key, v); &#125;&#125;``` 3）**定义一个主类，用来描述job并提交job** ```javaimport org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;/** * 相当于yarn客户端，负责提交MapReduce程序 */public class WordcountDriver &#123; public static void main(String[] args) throws Exception &#123; // 1 获取配置信息以及封装任务 Configuration configuration = new Configuration(); Job job = Job.getInstance(configuration); // 2 设置jar加载路径 job.setJarByClass(WordcountDriver.class); // 3 设置map和reduce类 job.setMapperClass(WordcountMapper.class); job.setReducerClass(WordcountReduce.class); // 4 设置map输出 job.setMapOutputKeyClass(Text.class); job.setMapOutputValueClass(IntWritable.class); // 5 设置Reduce输出 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); // 6 设置job数据输入和输出路径 FileInputFormat.setInputPaths(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); // 7 提交 boolean result = job.waitForCompletion(true); System.exit(result ? 0 : 1); &#125;&#125;``` 第一种情况： 设置好job数据输入和输出路径，直接在IDEA上运行代码，得到输出结果即可。 第二种情况： 在IDEA上打包成jar，上传到集群上运行 运行命令： ```xmlhadoop jar mapreduce-1.0-SNAPSHOT.jar com.atguigu.wordcount.WordcountDriver /user/atguigu/wc/input/hello.txt /user/atguigu/wc/outputhadoop jar jar包名称 Driver类 数据输入路径 数据输出路径 得到输出结果即可。 3.2、数据清洗案例需求：去除日志中字段长度小于等于11的日志。输入数据：数据源期望输出数据：每行字段长度都大于11。（实际通过计数器计数：符合要求的为true，不符合的为false）需求分析：需要在Map阶段对输入的数据根据规则进行过滤清洗。 代码：1）编写LogParseMapper类123456789101112131415161718192021222324252627import org.apache.hadoop.io.LongWritable;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Mapper;import java.io.IOException;public class LogParseMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123; Text k = new Text(); @Override protected void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException &#123; String line = value.toString(); boolean result = LogParse(line, context); if (!result) &#123; return; &#125; k.set(line); context.write(k, NullWritable.get()); &#125; private boolean LogParse(String line, Context context) &#123; String[] fields = line.split(" "); if (fields.length &gt; 11) &#123; context.getCounter("map", "true").increment(1); return true; &#125; context.getCounter("map", "false").increment(1); return false; &#125;&#125; 2）编写LogParseDriver类1234567891011121314151617181920212223242526272829303132import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.NullWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;public class LogParseDriver &#123; public static void main(String[] args) throws Exception &#123; //输入输出路径需要根据自己##### web.log```code194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] "GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1" 304 0 "-" "Mozilla/4.0 (compatible;)"183.49.46.228 - - [18/Sep/2013:06:49:23 +0000] "-" 400 0 "-" "-"163.177.71.12 - - [18/Sep/2013:06:49:33 +0000] "HEAD / HTTP/1.1" 200 20 "-" "DNSPod-Monitor/1.0"163.177.71.12 - - [18/Sep/2013:06:49:36 +0000] "HEAD / HTTP/1.1" 200 20 "-" "DNSPod-Monitor/1.0"101.226.68.137 - - [18/Sep/2013:06:49:42 +0000] "HEAD / HTTP/1.1" 200 20 "-" "DNSPod-Monitor/1.0"101.226.68.137 - - [18/Sep/2013:06:49:45 +0000] "HEAD / HTTP/1.1" 200 20 "-" "DNSPod-Monitor/1.0"60.208.6.156 - - [18/Sep/2013:06:49:48 +0000] "GET /wp-content/uploads/2013/07/rcassandra.png HTTP/1.0" 200 185524 "http://cos.name/category/software/packages/" "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36"222.68.172.190 - - [18/Sep/2013:06:49:57 +0000] "GET /images/my.jpg HTTP/1.1" 200 19939 "http://www.angularjs.cn/A00n" "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36"222.68.172.190 - - [18/Sep/2013:06:50:08 +0000] "-" 400 0 "-" "-"183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] "HEAD / HTTP/1.1" 200 20 "-" "DNSPod-Monitor/1.0"183.195.232.138 - - [18/Sep/2013:06:50:16 +0000] "HEAD / HTTP/1.1" 200 20 "-" "DNSPod-Monitor/1.0"66.249.66.84 - - [18/Sep/2013:06:50:28 +0000] "GET /page/6/ HTTP/1.1" 200 27777 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"221.130.41.168 - - [18/Sep/2013:06:50:37 +0000] "GET /feed/ HTTP/1.1" 304 0 "-" "Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36"157.55.35.40 - - [18/Sep/2013:06:51:13 +0000] "GET /robots.txt HTTP/1.1" 200 150 "-" "Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)"50.116.27.194 - - [18/Sep/2013:06:51:35 +0000] "POST /wp-cron.php?doing_wp_cron=1379487095.2510800361633300781250 HTTP/1.0" 200 0 "-" "WordPress/3.6; http://blog.fens.me"58.215.204.118 - - [18/Sep/2013:06:51:35 +0000] "GET /nodejs-socketio-chat/ HTTP/1.1" 200 10818 "http://www.google.com/url?sa=t&amp;rct=j&amp;q=nodejs%20%E5%BC%82%E6%AD%A5%E5%B9%BF%E6%92%AD&amp;source=web&amp;cd=1&amp;cad=rja&amp;ved=0CCgQFjAA&amp;url=%68%74%74%70%3a%2f%2f%62%6c%6f%67%2e%66%65%6e%73%2e%6d%65%2f%6e%6f%64%65%6a%73%2d%73%6f%63%6b%65%74%69%6f%2d%63%68%61%74%2f&amp;ei=rko5UrylAefOiAe7_IGQBw&amp;usg=AFQjCNG6YWoZsJ_bSj8kTnMHcH51hYQkAA&amp;bvm=bv.52288139,d.aGc" "Mozilla/5.0 (Windows NT 5.1; rv:23.0) Gecko/20100101 Firefox/23.0"58.215.204.118 - - [18/Sep/2013:06:51:36 +0000] "GET /wp-includes/js/jquery/jquery-migrate.min.js?ver=1.2.1 HTTP/1.1" 304 0 "http://blog.fens.me/nodejs-socketio-chat/" "Mozilla/5.0 (Windows NT 5.1; rv:23.0) Gecko/20100101 Firefox/23.0"]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——MapReduce]]></title>
    <url>%2F2017%2F12%2F20%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94MapReduce%2F</url>
    <content type="text"><![CDATA[Hadoop——MapReduce一、MapReduce概述MapReduce是一个分布式运算程序的编程框架，是用户开发“基于hadoop的数据分析应用”的核心框架；Mapreduce核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序，并发运行在一个hadoop集群上。 1、为什么要MapReduce1）海量数据在单机上处理因为硬件资源限制，无法胜任2）而一旦将单机版程序扩展到集群来分布式运行，将极大增加程序的复杂度和开发难度3）引入mapreduce框架后，开发人员可以将绝大部分工作集中在业务逻辑的开发上，而将分布式计算中的复杂性交由框架来处理。4）mapreduce分布式方案考虑的问题（1）运算逻辑要不要先分后合？（2）程序如何分配运算任务（切片）？（3）两阶段的程序如何启动？如何协调？（4）整个程序运行过程中的监控？容错？重试？分布式方案需要考虑很多问题，但是我们可以将分布式程序中的公共功能封装成框架，让开发人员将精力集中于业务逻辑上。而mapreduce就是这样一个分布式程序的通用框架。 2、MapReduce优缺点优点：1）MapReduce 易于编程它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量廉价的PC机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模一样的。就是因为这个特点使得MapReduce编程变得非常流行。2）良好的扩展性当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。3）高容错性MapReduce设计的初衷就是使程序能够部署在廉价的PC机器上，这就要求它具有很高的容错性。比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行失败，而且这个过程不需要人工参与，而完全是由Hadoop内部完成的。4）适合PB级以上海量数据的离线处理可以实现上千台服务器集群并发工作，提供数据处理能力。 缺点：1）不擅长实时计算MapReduce无法像Mysql一样，在毫秒或者秒级内返回结果。2）不擅长流式计算流式计算的输入数据是动态的，而MapReduce的输入数据集是静态的，不能动态变化。这是因为MapReduce自身的设计特点决定了数据源必须是静态的。3）不擅长DAG（有向图）计算多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，MapReduce并不是不能做，而是使用后，每个MapReduce作业的输出结果都会写入到磁盘，会造成大量的磁盘IO，导致性能非常的低下。 3、MapReduce核心思想 1）分布式的运算程序往往需要分成至少2个阶段。2）第一个阶段的MapTask并发实例，完全并行运行，互不相干。3）第二个阶段的ReduceTask并发实例互不相干，但是他们的数据依赖于上一个阶段的所有MapTask并发实例的输出。4）MapReduce编程模型只能包含一个Map阶段和一个Reduce阶段，如果用户的业务逻辑非常复杂，那就只能多个MapReduce程序，串行运行。 4、MapReduce进程一个完整的mapreduce程序在分布式运行时有三类实例进程：1）MrAppMaster：负责整个程序的过程调度及状态协调。2）MapTask：负责map阶段的整个数据处理流程。3）ReduceTask：负责reduce阶段的整个数据处理流程。 5、常用数据序列化类型 6、MapReduce编程规范用户编写的程序分成三个部分：Mapper、Reducer和Driver。1）Mapper阶段（1）用户自定义的Mapper要继承自己的父类（2）Mapper的输入数据是KV对的形式（KV的类型可自定义）（3）Mapper中的业务逻辑写在map()方法中（4）Mapper的输出数据是KV对的形式（KV的类型可自定义）（5）map()方法（MapTask进程）对每一个&lt;K,V&gt;调用一次2）Reducer阶段（1）用户自定义的Reducer要继承自己的父类（2）Reducer的输入数据类型对应Mapper的输出数据类型，也是KV（3）Reducer的业务逻辑写在reduce()方法中（4）Reducetask进程对每一组相同k的&lt;k,v&gt;组调用一次reduce()方法3）Driver阶段相当于yarn集群的客户端，用于提交我们整个程序到yarn集群，提交的是封装了mapreduce程序相关运行参数的job对象 二、MapReduce框架原理介绍MapReduce框架原理之前，先看下MapReduce框架的流程图，了解MapReduce的具体流程MapReduce详细工作流程（一） MapReduce详细工作流程（二） 1-6步，是InputFormat数据输入阶段 -&gt; Map阶段7-16步，是Shuffle阶段 -&gt; Reduce阶段 -&gt; OutPutFormat阶段 MapReduce的数据流 1、InputFormat数据输入1）Job提交流程源码和切片源码详解（1）Job提交流程源码详解 waitForCompletion() submit(); // 1建立连接 connect(); // 1）创建提交Job的代理 new Cluster(getConfiguration()); //（1）判断是本地yarn还是远程 initialize(jobTrackAddr, conf); // 2 提交job submitter.submitJobInternal(Job.this, cluster) // 1）创建给集群提交数据的Stag路径 Path jobStagingArea = JobSubmissionFiles.getStagingDir(cluster, conf); // 2）获取jobid ，并创建Job路径 JobID jobId = submitClient.getNewJobID(); // 3）拷贝jar包到集群 copyAndConfigureFiles(job, submitJobDir); rUploader.uploadFiles(job, jobSubmitDir); // 4）计算切片，生成切片规划文件 writeSplits(job, submitJobDir); maps = writeNewSplits(job, jobSubmitDir); input.getSplits(job); // 5）向Stag路径写XML配置文件 writeConf(conf, submitJobFile); conf.writeXml(out); // 6）提交Job,返回提交状态 status = submitClient.submitJob(jobId, submitJobDir.toString(), job.getCredentials()); （2）FileInputFormat切片源码解析(input.getSplits(job))（1）找到你数据存储的目录。（2）开始遍历处理（规划切片）目录下的每一个文件（3）遍历第一个文件ss.txta）获取文件大小fs.sizeOf(ss.txt);b）计算切片大小computeSliteSize(Math.max(minSize,Math.max(maxSize,blocksize)))=blocksize=128Mc）默认情况下，切片大小=blocksized）开始切，形成第1个切片：ss.txt—0:128M 第2个切片ss.txt—128:256M 第3个切片ss.txt—256M:300M（每次切片时，都要判断切完剩下的部分是否大于块的1.1倍，不大于1.1倍就划分一块切片）e）将切片信息写到一个切片规划文件中f）整个切片的核心过程在getSplit()方法中完成。g）数据切片只是在逻辑上对输入数据进行分片，并不会再磁盘上将其切分成分片进行存储。InputSplit只记录了分片的元数据信息，比如起始位置、长度以及所在的节点列表等。h）注意：block是HDFS上物理上存储的存储的数据，切片是对数据逻辑上的划分。（4）提交切片规划文件到yarn上，yarn上的MrAppMaster就可以根据切片规划文件计算开启maptask个数。 （3）FileInputFormat切片机制1、默认切片机制（1）简单地按照文件的内容长度进行切片（2）切片大小，默认等于block大小（3）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片2、案例分析输入数据由两个文件：f1.txt300Mf2.txt100M经过FileInputFormat的切片机制运算后，形成的切片信息如下：f1.txt.split10~128Mf1.txt.split2128~256Mf1.txt.split3256~300Mf2.txt.split10~100M3、FileInputFormat切片大小的参数配置通过分析源码，在FileInputFormat中，计算切片大小的逻辑： Math.max(minSize, Math.min(maxSize, blockSize));切片主要由这几个值来运算决定mapreduce.input.fileinputformat.split.minsize=1默认值为1mapreduce.input.fileinputformat.split.maxsize= Long.MAXValue默认值Long.MAXValue因此，默认情况下，切片大小=blocksize。maxsize（切片最大值）：参数如果调得比blocksize小，则会让切片变小，而且就等于配置的这个参数的值。minsize（切片最小值）：参数调的比blockSize大，则可以让切片变得比blocksize还大。 2、MapTask工作机制1）数据切片与MapTask并行度决定机制（1）一个job的map阶段并行度由客户端在提交job时决定（2）每一个split切片分配一个mapTask并行实例处理（3）默认情况下，切片大小=blocksize（4）切片时不考虑数据集整体，而是逐个针对每一个文件单独切片 2）MapTask工作机制 （1）Read阶段：MapTask通过用户编写的RecordReader，从输入InputSplit中解析出一个个key/value。（2）Map阶段：该节点主要是将解析出的key/value交给用户编写map()函数处理，并产生一系列新的key/value。（3）Collect收集阶段：在用户编写map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。在该函数内部，它会将生成的key/value分区（调用Partitioner），并写入一个环形内存缓冲区中。（4）Spill阶段：即“溢写”，当环形缓冲区满后，MapReduce会将数据写到本地磁盘上，生成一个临时文件。需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进行合并、压缩等操作。溢写阶段详情：步骤1：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition进行排序，然后按照key进行排序。这样，经过排序后，数据以分区为单位聚集在一起，且同一分区内所有数据按照key有序。步骤2：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件output/spillN.out（N表示当前溢写次数）中。如果用户设置了Combiner，则写入文件之前，对每个分区中的数据进行一次聚集操作。步骤3：将分区数据的元信息写到内存索引数据结构SpillRecord中，其中每个分区的元信息包括在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过1MB，则将内存索引写到文件output/spillN.out.index中。（5）Combine阶段：当所有数据处理完成后，MapTask对所有临时文件进行一次合并，以确保最终只会生成一个数据文件。当所有数据处理完后，MapTask会将所有临时文件合并成一个大文件，并保存到文件output/file.out中，同时生成相应的索引文件output/file.out.index。在进行文件合并过程中，MapTask以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的方式。每轮合并io.sort.factor（默认100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，重复以上过程，直到最终得到一个大文件。让每个MapTask最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机读取带来的开销。 3、Shuffle机制Map方法之后，Reduce方法之前的数据处理过程称之为Shuffle。 具体Shuffle过程详解：1）MapTask收集我们的map()方法输出的kv对，放到内存缓冲区中2）从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件3）多个溢出文件会被合并成大的溢出文件4）在溢出过程及合并的过程中，都要调用Partitioner进行分区和针对key进行排序5）ReduceTask根据自己的分区号，去各个MapTask机器上取相应的结果分区数据6）ReduceTask会取到同一个分区的来自不同MapTask的结果文件，ReduceTask会将这些文件再进行合并（归并排序）7）合并成大文件后，Shuffle的过程也就结束了，后面进入ReduceTask的逻辑运算过程（从文件中取出一个一个的键值对Group，调用用户自定义的reduce()方法） 1）Partition分区要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）（1）默认Partition分区 public class HashPartitioner&lt;K, V&gt; extends Partitioner&lt;K, V&gt; { public int getPartition(K key, V value, int numReduceTasks) { return (key.hashCode() &amp; Integer.MAX_VALUE) % numReduceTasks; } } 默认分区是根据key的hashCode对reduceTasks个数取模得到的。用户没法控制哪个key存储到哪个分区。（2）自定义Partition步骤a、自定义类继承Partitioner，重写getPartition()方法 public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt; { @Override public int getPartition(Text key, FlowBean value, int numPartitions) { //控制分区逻辑代码 ...... } } b、在job驱动中，设置自定义partitionerjob.setPartitionerClass(CustomPartitioner.class);c、自定义partition后，要根据自定义partitioner的逻辑设置相应数量的reduce taskjob.setNumReduceTasks(5);（3）总结a、如果reduceTask的数量&gt; getPartition的结果数，则会多产生几个空的输出文件part-r-000xx；b、如果1&lt;reduceTask的数量&lt;getPartition的结果数，则有一部分分区数据无处安放，会Exception；c、如果reduceTask的数量=1，则不管mapTask端输出多少个分区文件，最终结果都交给这一个reduceTask，最终也就只会产生一个结果文件 part-r-00000；d、分区数必须从零开始，逐一累加。案例：假设自定义分区数为5，则（1）job.setNumReduceTasks(1);会正常运行，只不过会产生一个输出文件（2）job.setNumReduceTasks(2);会报错（3）job.setNumReduceTasks(6);大于5，程序会正常运行，会产生空文件 2）排序排序是MapReduce框架中最重要的操作之一。Map Task和Reduce Task均会对数据按照key进行排序。该操作属于Hadoop的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字典顺序排序，且实现该排序的方法是快速排序。对于Map Task，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后，再对缓冲区中的数据进行一次排序，并将这些有序数据写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行归并排序。对于Reduce Task，它从每个Map Task上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则放到磁盘上，否则放到内存中。如果磁盘上文件数目达到一定阈值，则进行一次合并以生成一个更大文件；如果内存中文件大小或者数目超过一定阈值，则进行一次合并后将数据写到磁盘上。当所有数据拷贝完毕后，Reduce Task统一对内存和磁盘上的所有数据进行一次合并。排序分类：a、部分排序（区内排序——环形缓冲区）MapReduce根据输入记录的键对数据集排序。保证输出的每个文件内部排序。b、全排序最终输出结果只有一个文件，且文件内部有序。实现方式是只设置一个ReduceTask，但该方法在处理大型文件时效率极低，因为只有一台机器处理所有文件，完全丧失了MapReduce所提供的并行架构。c、辅助排序（GroupingComparator分组）在Reduce端对Key进行分组。应用于：在接受的key为bean对象时，想让一个或几个字段相同（全部字段不相同）的key进入到同一个reduce方法时，可以采用分组排序。d、二次排序在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。 3）Combiner合并（1）combiner是MR程序中Mapper和Reducer之外的一种组件。（2）combiner组件的父类就是Reducer。（3）combiner和reducer的区别在于运行的位置：Combiner是在每一个maptask所在的节点运行;Reducer是接收全局所有Mapper的输出结果；（4）combiner的意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量。（5）combiner能够应用的前提是不能影响最终的业务逻辑，而且，combiner的输出kv应该跟reducer的输入kv类型要对应起来。 4、ReduceTask工作机制1）ReduceTask工作机制 （1）Copy阶段：ReduceTask从各个MapTask上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，则写到磁盘上，否则直接放到内存中。（2）Merge阶段：在远程拷贝数据的同时，ReduceTask启动了两个后台线程对内存和磁盘上的文件进行合并，以防止内存使用过多或磁盘上文件过多。（3）Sort阶段：按照MapReduce语义，用户编写reduce()函数输入数据是按key进行聚集的一组数据。为了将key相同的数据聚在一起，Hadoop采用了基于排序的策略。由于各个MapTask已经实现对自己的处理结果进行了局部排序，因此，ReduceTask只需对所有数据进行一次归并排序即可。（4）Reduce阶段：reduce()函数将计算结果写到HDFS上。 2）ReduceTask并行度（个数）ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定不同，ReduceTask数量的决定是可以直接手动设置。默认值是1，可自己手动设置为4job.setNumReduceTasks(4); 5、OutputFormat数据输出1）OutputFormat接口实现类OutputFormat是MapReduce输出的基类，所有实现MapReduce输出都实现了 OutputFormat接口。以下是几种常见的OutputFormat实现类。a、文本输出TextOutputFormat默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，因为TextOutputFormat调用toString()方法把它们转换为字符串。b、SequenceFileOutputFormatSequenceFileOutputFormat将它的输出写为一个顺序文件。如果输出需要作为后续 MapReduce任务的输入，这便是一种好的输出格式，因为它的格式紧凑，很容易被压缩。c、自定义OutputFormat根据用户需求，自定义实现输出。（1）自定义一个类继承FileOutputFormat。（2）改写recordwriter，具体改写输出数据的方法write()。 6、Join的应用Map Join1）适用场景一张表十分小、一张表很大的场景。2）优点思考：在Reduce端处理过多的表，非常容易产生数据倾斜。怎么办？在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。3）具体实现：（1）在Mapper的setup阶段，将文件读取到缓存集合中。（2）在驱动函数中加载缓存。缓存普通文件到Task运行节点。job.addCacheFile(new URI(“file:……”)); Reduce Join1）原理Map端的主要工作：为来自不同表(文件)的key/value对打标签以区别不同来源的记录。然后用连接字段作为key，其余部分和新加的标志作为value，最后进行输出。Reduce端的主要工作：在reduce端以连接字段作为key的分组已经完成，我们只需要在每一个分组当中将那些来源于不同文件的记录(在map阶段已经打标志)分开，最后进行合并就ok了。2）缺点这种方式的缺点很明显就是会造成map和reduce端也就是shuffle阶段出现大量的数据传输，效率很低。（①reduce端有一个数组，如果数据较大，很容易出现内存溢出；②reduce端进行join很容易出现数据倾斜；③ruduce端join需要从maptask端copy大量数据，会有大量网络IO问题） 7、数据清洗（ETL）在运行核心业务MapReduce程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。清理的过程往往只需要运行Mapper程序，不需要运行Reduce程序。注意：案例在后面]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>MapReduce</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——HDFS的Java API操作]]></title>
    <url>%2F2017%2F12%2F18%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94HDFS%E7%9A%84Java%20API%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[HDFS的Java API操作一、HDFS客户端环境准备1）根据自己电脑的操作系统拷贝对应的编译后的hadoop jar包到非中文路径 2）配置HADOOP_HOME环境变量和path路径 二、HDFS的API操作新建Maven工程并添加依赖 &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt; &lt;artifactId&gt;log4j-core&lt;/artifactId&gt; &lt;version&gt;2.8.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; API操作代码 1、HDFS文件上传@Test public void testPut() throws Exception { Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get( new URI("hdfs://hadoop102:9000"), configuration, "drift"); fileSystem.copyFromLocalFile( new Path("f:/hello.txt"), new Path("/0308_666/hello1.txt")); fileSystem.close(); } 2、HDFS文件下载@Test public void testDownload() throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get( new URI("hdfs://hadoop102:9000"), configuration, "drift"); // 2 执行下载操作 fileSystem.copyToLocalFile( false, new Path("/0308_666/hello.txt"), new Path("f:/hello1.txt"), true); // 3 关闭资源 fileSystem.close(); System.out.println("over"); } 3、HDFS文件夹删除@Test public void delete() throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get( new URI("hdfs://hadoop102:9000"), configuration, "drift"); // 2 执行删除操作 fileSystem.delete(new Path("/0308_777"), true); // 3 关闭资源 fileSystem.close(); System.out.println("over"); } 4、HDFS文件夹名更改@Test public void testRename() throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get( new URI("hdfs://hadoop102:9000"), configuration, "drift"); // 2 执行重命名操作 fileSystem.rename(new Path("/0308_666/hello.txt"), new Path("/0308_666/hello2.txt")); // 3 关闭资源 fileSystem.close(); System.out.println("over"); } 5、HDFS文件详情查看@Test public void testLS1() throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get( new URI("hdfs://hadoop102:9000"), configuration, "drift"); // 2 查询文件信息 RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fileSystem.listFiles(new Path("/"), true); while (listFiles.hasNext()) { LocatedFileStatus fileStatus = listFiles.next(); // 文件的长度 System.out.println(fileStatus.getLen()); // 文件的名字 System.out.println(fileStatus.getPath().getName()); // 文件的权限 System.out.println(fileStatus.getPermission()); BlockLocation[] locations = fileStatus.getBlockLocations(); for (BlockLocation location : locations) { String[] hosts = location.getHosts(); for (String host : hosts) { System.out.println(host); } } System.out.println("---------------分割线---------------"); } // 3 关闭资源 fileSystem.close(); } 6、HDFS文件和文件夹判断@Test public void testLS2() throws Exception { // 1 获取文件系统 Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get( new URI("hdfs://hadoop102:9000"), configuration, "drift"); // 2 文件和文件夹的判断 FileStatus[] fileStatuses = fileSystem.listStatus(new Path("/")); for (FileStatus fileStatus : fileStatuses) { if (fileStatus.isFile()) { System.out.println("F:" + fileStatus.getPath().getName()); } else { System.out.println("D:" + fileStatus.getPath().getName()); } } // 3 关闭资源 fileSystem.close(); } HDFS的I/O流操作 1、HDFS文件上传@Test public void testPut2() throws Exception { //1.获取hdfs的客户端 Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get( new URI("hdfs://hadoop102:9000"), configuration, "drift"); //2.创建输入流 FileInputStream fileInputStream = new FileInputStream(new File("f:/hello2.txt")); //3.创建输出流 FSDataOutputStream outputStream = fileSystem.create(new Path("/0308_666/hello3.txt")); //4.流的拷贝 IOUtils.copyBytes(fileInputStream, outputStream, configuration); //5.关闭资源 fileSystem.close(); } 2、HDFS文件下载@Test public void testDownload2() throws Exception { //1.获取hdfs的客户端 Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get( new URI("hdfs://hadoop102:9000"), configuration, "drift"); //2.创建输入流 FSDataInputStream inputStream = fileSystem.open(new Path("/0308_666/hello3.txt")); //3.创建输出流 FileOutputStream outputStream = new FileOutputStream(new File("f:/hello3.txt")); //4.流的拷贝 IOUtils.copyBytes(inputStream, outputStream, configuration); //5.关闭资源 fileSystem.close(); System.out.println("over"); } 3、定位文件读取需求：分块读取HDFS上的大文件 /** * 文件的下载： * 1.下载第一块 */ @Test public void testSeek1() throws Exception { //1.获取hdfs的客户端 Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get( new URI("hdfs://hadoop102:9000"), configuration, "drift"); //2.创建输入流 FSDataInputStream inputStream = fileSystem.open(new Path("/user/drift/hadoop-2.7.2.tar.gz")); //3.创建输出流 FileOutputStream outputStream = new FileOutputStream(new File("f:/hadoop-2.7.2.tar.gz.part1")); //4.流的拷贝 byte[] buf = new byte[1024]; for (int i = 0; i &lt; 1024 * 128; i++) { inputStream.read(buf); outputStream.write(buf); } //5.关闭资源 IOUtils.closeStream(inputStream); IOUtils.closeStream(outputStream); fileSystem.close(); System.out.println("over"); } /** * 文件的下载： * 2.下载第二块 */ @Test public void testSeek2() throws Exception { //1.获取hdfs的客户端 Configuration configuration = new Configuration(); FileSystem fileSystem = FileSystem.get(new URI("hdfs://hadoop102:9000"), configuration, "drift"); //2.创建输入流 FSDataInputStream inputStream = fileSystem.open(new Path("/user/drift/hadoop-2.7.2.tar.gz")); //3.创建输出流 FileOutputStream outputStream = new FileOutputStream(new File("f:/hadoop-2.7.2.tar.gz.part2")); //4.流的拷贝 inputStream.seek(1024 * 1024 * 128); IOUtils.copyBytes(inputStream, outputStream, configuration); //5.关闭资源 IOUtils.closeStream(inputStream); IOUtils.closeStream(outputStream); fileSystem.close(); System.out.println("over"); } }]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据Hadoop系列——HDFS的Shell操作]]></title>
    <url>%2F2017%2F12%2F17%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94HDFS%E7%9A%84Shell%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[HDFS的Shell操作1、基本语法bin/hadoop fs 具体命令 2、常用命令1、–ls：查看指定目录下内容hadoop fs –ls [文件目录] eg：hadoop fs –ls /user/wangkai.pt 2、–cat：显示文件内容hadoop dfs –cat [file_path] eg:hadoop fs -cat /user/wangkai.pt/data.txt 3、–put：将本地文件存储至hadoophadoop fs –put [本地地址] [hadoop目录] eg：hadoop fs –put /home/t/file.txt /user/t (file.txt是文件名) 4、–put：将本地文件夹存储至hadoophadoop fs –put [本地目录] [hadoop目录] eg：hadoop fs –put /home/t/dir_name /user/t (dir_name是文件夹名) 5、-get：将hadoop上某个文件down至本地已有目录下hadoop fs -get [文件目录] [本地目录] eg：hadoop fs –get /user/t/ok.txt /home/t 6、–rm：删除hadoop上指定文件或文件夹hadoop fs –rm [文件地址] eg：hadoop fs –rm /user/t/ok.txt 7、删除hadoop上指定文件夹（包含子目录等）hadoop fs –rm [目录地址] eg：hadoop fs –rm /user/t 8、–mkdir：在hadoop指定目录内创建新目录eg：hadoop fs –mkdir /user/t 9、-touchz：在hadoop指定目录下新建一个空文件使用touchz命令： eg：hadoop fs -touchz /user/new.txt 10、–mv：将hadoop上某个文件重命名使用mv命令： eg：hadoop fs –mv /user/test.txt /user/ok.txt （将test.txt重命名为ok.txt） 11、–getmerge：将hadoop指定目录下所有内容保存为一个文件，同时down至本地eg：hadoop fs –getmerge /user /home/t 12、将正在运行的hadoop作业kill掉eg：hadoop job –kill [job-id] 13、-help：输出这个命令参数eg：hadoop fs -help rm 14、-moveFromLocal：从本地剪切粘贴到HDFSeg：hadoop fs -moveFromLocal ./kongming.txt /sanguo/shuguo 15、-appendToFile：追加一个文件到已经存在的文件末尾eg：hadoop fs -appendToFile liubei.txt /sanguo/shuguo/kongming.txt 16、-chgrp 、-chmod、-chown：Linux文件系统中的用法一样，修改文件所属权限eg：hadoop fs -chmod 666 /sanguo/shuguo/kongming.txt eg：hadoop fs -chown atguigu:atguigu /sanguo/shuguo/kongming.txt 17、-copyFromLocal：从本地文件系统中拷贝文件到HDFS路径去eg：hadoop fs -copyFromLocal README.txt / 18、-copyToLocal：从HDFS拷贝到本地eg：hadoop fs -copyToLocal /sanguo/shuguo/kongming.txt ./ 19、-cp ：从HDFS的一个路径拷贝到HDFS的另一个路径eg：hadoop fs -cp /sanguo/shuguo/kongming.txt /zhuge.txt 20、-tail：显示一个文件的末尾eg：hadoop fs -tail /sanguo/shuguo/kongming.txt 21、-rmdir：删除空目录eg：hadoop fs -mkdir /test eg：hadoop fs -rmdir /test 22、-du：统计文件夹的大小信息eg：hadoop fs -du -s -h /user/atguigu/test 2.7 K /user/atguigu/test eg：hadoop fs -du -h /user/atguigu/test 1.3 K /user/atguigu/test/README.txt 15 /user/atguigu/test/jinlian.txt 1.4 K /user/atguigu/test/zaiyiqi.txt 23、-setrep：设置HDFS中文件的副本数量eg：hadoop fs -setrep 10 /sanguo/shuguo/kongming.txt]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据Hadoop系列——HDFS简介]]></title>
    <url>%2F2017%2F12%2F16%2F%E5%A4%A7%E6%95%B0%E6%8D%AEHadoop%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94HDFS%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[Hadoop——HDFS一、HDFS概述1、产生背景随着数据量越来越大，在一个操作系统管辖的范围内存不下了，那么就分配到更多的操作系统管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式文件管理系统。HDFS只是分布式文件管理系统中的一种。 2、概念HDFS（Hadoop Distributed File System），它是一个文件系统，用于存储文件，通过目录树来定位文件；其次，它是分布式的，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色。HDFS的设计适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并不适合用来做网盘应用。 3、优缺点优点：1）高容错性（1）数据自动保存多个副本。它通过增加副本的形式，提高容错性；（2）某一个副本丢失以后，它可以自动恢复。2）适合大数据处理（1）数据规模：能够处理数据规模达到GB、TB甚至PB级别的数据；（2）文件规模：能够处理百万规模以上的文件数量，数量相当之大。3）流式数据访问，它能保证数据的一致性。4）可构建在廉价机器上，通过多副本机制，提高可靠性。缺点：1）不适合低延时数据访问，比如毫秒级的存储数据，是做不到的。2）无法高效的对大量小文件进行存储。（1）存储大量小文件的话，它会占用 NameNode大量的内存来存储文件、目录和块信息。这样是不可取的，因为NameNode的内存总是有限的；（2）小文件存储的寻址时间会超过读取时间，它违反了HDFS的设计目标。3）不支持并发写入、文件随机修改。（1）一个文件只能有一个写，不允许多个线程同时写；（2）仅支持数据append（追加），不支持文件的随机修改。 4、HDFS组成架构（重点） 这种架构主要由四个部分组成，分别为HDFS Client、NameNode、DataNode和Secondary NameNode。下面我们分别介绍这四个组成部分。1）Client：就是客户端。（1）文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储；（2）与NameNode交互，获取文件的位置信息；（3）与DataNode交互，读取或者写入数据；（4）Client提供一些命令来管理HDFS，比如启动或者关闭HDFS；（5）Client可以通过一些命令来访问HDFS；2）NameNode：就是Master，它是一个主管、管理者。（1）管理HDFS的名称空间；（2）管理数据块（Block）映射信息；（3）配置副本策略；（4）处理客户端读写请求。3）DataNode：就是Slave。NameNode下达命令，DataNode执行实际的操作。（1）存储实际的数据块；（2）执行数据块的读/写操作。4）Secondary NameNode：并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务。（1）辅助NameNode，分担其工作量；（2）定期合并Fsimage和Edits，并推送给NameNode；（3）在紧急情况下，可辅助恢复NameNode。 NameNode、Secondary NameNode、DataNode工作机制详解 二、HDFS数据流1、HDFS的写数据流程 步骤：1）客户端通过Distributed FileSystem模块向NameNode请求上传文件，NameNode检查目标文件是否已存在，父目录是否存在。2）NameNode返回是否可以上传。3）客户端请求第一个 block上传到哪几个datanode服务器上。4）NameNode返回3个datanode节点，分别为dn1、dn2、dn3。5）客户端通过FSDataOutputStream模块请求dn1上传数据，dn1收到请求会继续调用dn2，然后dn2调用dn3，将这个通信管道建立完成。6）dn1、dn2、dn3逐级应答客户端。7）客户端开始往dn1上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，dn1收到一个packet就会传给dn2，dn2传给dn3；dn1每传一个packet会放入一个应答队列等待应答。8）当一个block传输完成之后，客户端再次请求NameNode上传第二个block的服务器。（重复执行3-7步）。 网络拓扑概念在本地网络中，两个节点被称为“彼此近邻”是什么意思？在海量数据处理中，其主要限制因素是节点之间数据的传输速率——带宽很稀缺。这里的想法是将两个节点间的带宽作为距离的衡量标准。节点距离：两个节点到达最近的共同祖先的距离总和。 例如，假设有数据中心d1机架r1中的节点n1。该节点可以表示为/d1/r1/n1。利用这种标记，这里给出四种距离描述，如上图 机架感知（1）官方ip地址（2）低版本Hadoop副本节点选择第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。第二个副本和第一个副本位于不相同机架的随机节点上。第三个副本和第二个副本位于相同机架，节点随机。 （3）Hadoop2.7.2副本节点选择第一个副本在Client所处的节点上。如果客户端在集群外，随机选一个。第二个副本和第一个副本位于相同机架，随机节点。第三个副本位于不同机架，随机节点。 2、HDFS的读数据流程 步骤：1）客户端通过Distributed FileSystem向NameNode请求下载文件，NameNode通过查询元数据，找到文件块所在的DataNode地址。2）挑选一台DataNode（就近原则，然后随机）服务器，请求读取数据。3）DataNode开始传输数据给客户端（从磁盘里面读取数据输入流，以packet为单位来做校验）。4）客户端以packet为单位接收，先在本地缓存，然后写入目标文件。]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>Hadoop</tag>
        <tag>HDFS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门系列——大数据学习路线]]></title>
    <url>%2F2017%2F12%2F13%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E8%B7%AF%E7%BA%BF%2F</url>
    <content type="text"><![CDATA[大数据学习路线 === 前言 要从事计算机行业的工作，不管是什么工作，开发、测试、还是算法等，都是要有一门自己比较熟练的编程语言，编程语言可以是C语言、Java、C++等，只要是和你后续工作所相关的就可以（后续用到其他语言的话，你有一门语言基础了，学起来就快了）。 一般初学者入门语言大多都会选择Java、C语言、C++或者Python，而且现在网上有很多好的视频，可以供初学者学习使用。关于学习视频或者资料的选择，知乎或者百度等都有很多讲解了，也可以跟师兄师姐咨询，这样可以少走很多弯路，当然，有人说，走一些弯路总是有好处的，但是我这里说的弯路不是说不犯错误，不调bug，而是指学习资料以及一些知识点的偏重点，这样可以尽量节约一部分时间，刚开始时，总会有点迷，而且当你真正投入进去学习时，会发现时间总是不够用。 下面就说一下我自己从Java开发到大数据开发的曲折学习之路（狗头保命.jpg）。因为我现在是做大数据相关的工作了，所以Java后端涉及到的一些SSM框架等知识点我就不介绍了，毕竟后续一段时间也没有做了。自己看过的大数据学习相关的视频+资料大概是200G-300G吧，从Linux-&gt;Hadoop-&gt;。。。-&gt;Spark-&gt;项目，还有就是一些面试文档，面经等。一些视频看了两遍或者更多，跟着学，跟着敲代码，做项目，准备面试。涉及到需要学习的东西包括：JavaSE，数据结构与算法（计算机行业必备），MySQL，Redis，ES（数据库这些可以看项目，也可以自己熟练一两个），Linux，Shell（这个可以后期补），Hadoop，Zookeeper，Hive，Flume，Kafka，HBase，Scala（Spark是Scala写的，会Scala做相关的项目会更容易入手），Spark，Flink（这个是找工作时有面试官问过几次liao不liao解，所以找完工作才开始接触学习），相关项目。 编程语言阶段学习如果是零基础的话，建议还是从视频开始入门比较好，毕竟一上来就看教材，这样有些代码的来龙去脉可能不是很了解。如果是有一些编程语言基础的话，从视频开始也会更简单，一些for、while循环你都知道了，学起来也会快很多。JavaSE我是选择的黑马刘意的为主，因为刚刚开始学Java看过一本从《Java从入门到精通》，没什么感觉，后续又在看了慕课网的Java初级视频，还是没感觉出来啥（当时就有点怀疑自己了。。。），可能有点没进入状态。还好后续找了黑马刘意老师的JavaSE视频（我是看的2015年版本，那时候19版还没出），觉得他讲的真的是很好很详细，每个知识点都会有例子，也都会带你敲代码，做测试，可能前面有C语言基础，然后也看过Java的一些语法，所以学起来还是比较顺利，后面的IO流、多线程等知识点时，也有看书看博客，或者看看其他老师的课程，讲解的可能自己比较容易接受就可以，反正都是多尝试（下面会给出视频链接），尽量懂一些，后续可以回头来复习。JavaSE相关的视频，先看一遍，后续有时间建议再看一遍，而且这些经典的视频，看两遍真的是享受。如果有一定基础了的，JavaSE前面七八天的视频可以加速看，但是不懂的一定要停下开仔细想想，零基础的还是尽量不要加速吧，慢慢来稳些。后面的视频建议还是跟着视频来，尽量不要加速，代码尽量都敲一敲，第一遍基本上一个月到一个半月可以结束。JavaSE可以说是很基础也很重要的东西，主要重点包括面向对象、集合（List、Map等），IO流，String/StringBuilder/StringBuffer、反射、多线程，这些最好是都要熟悉一些，面试也是重点。JavaSE之后，如果你是要走前端或后端开发路线的话，可以跟着一些网上的视频继续学习，这里我就不多做介绍了。=========================分割线，Scala可以后续Spark阶段再接触学习======================== Scala的学习，Scala是一门多范式 (multi-paradigm) 的编程语言，Scala支持面向对象和函数式编程，最主要的是后续Spark的内容需要用到Scala，所以前面学习了JavaSE，到Spark学习之前，再把Scala学习一波，美滋滋，而且Scala可以和Java进行无缝对接，混合使用，更是爽歪歪。后续Spark学习时基本都是用的Scala，也可能是和Java结合使用，所以Spark之前建议还是先学一波Scala，而且Scala用起来真是很舒服（wordcount一行代码搞定），适合迭代式计算，对数据处理有很大帮助，不过Scala看代码很容易看懂，但是学起来还是挺难的，比如样例类（case class）用起来真是nice，但是隐式转换学起来就相对比较难。学习Scala的建议：1. 学习scala 特有的语法，2. 搞清楚scala和java区别，3. 了解如何规范的使用scala。Scala对学习Spark是很重要的（后面Flink也是要用），虽然现在很多公司还是用Java开发比较多，而且Spark是Scala写的，如果要读源码，会Scala还是很重要的（至少要看得懂代码）。Scala主要重点包括：隐式转换和隐式参数、模式匹配、函数式编程。这里我看的是尚硅谷韩老师的Scala视频，韩老师讲的真的很不错，五星推荐，哈哈。也许有人会觉得Python也是需要的，但是学习阶段，可能用Java还是比较多，面试也基本都是问Java相关的内容，所以Python后续工作会用到的话，再看看Python的内容吧。 视频链接刘意JavaSE（2015版）刘意JavaSE（2019-IDEA版）毕向东JavaSE尚硅谷康师傅JavaSE（2019-IDEA版）尚硅谷韩老师Scala 大数据框架阶段学习大数据这方面的知识点自己可以说真的是从零开始的，刚刚开始学那会Linux基本都没用过，心里那个虚啊，而且时间也紧迫，想起来都是一把辛酸泪。刚刚开始学的时候，看了厦门大学林子雨的《 大数据技术原理与应用》课程，可能这个课程是面对上课的，所以看了一些，感觉对自己帮助不是很大（并不是说课程不好，可能不太适合自己，如果是要了解理论知识，很透彻，但是俺时间紧迫啊），所以就继续在网上找视频，然后发现尚硅谷的培训视频很多人去参加，而且知识点也很齐全，大数据相关组件都有讲课，还有一些项目比较好，所以就找了它相关的视频，看的是2018年的，所以视频不算旧。来一张推荐系统架构的图，先看看一般来说，Flume+Kafka对数据进行采集聚合传输，一方面Spark对实时数据进行处理，传输给相应的数据处理模块（比如实时数据处理的算法模块，Spark也有提供常见的机器学习算法的程序库），另一方面采集的数据也可以放入数据库（HBase、MongoDB等）中，后续MapReduce对离线数据进行离线处理，数据处理完毕用于后续的使用，数据采集处理的流程大概就是这样。如果是推荐系统，实时推荐会给用户产生实时的推荐结果，让用户进行查阅选择，比如你在界面浏览了或者看了新的物品，然后刷新下界面，可能给你展示的东西就有一些变成跟你刚刚浏览的相关了。离线推荐的话主要是对离线数据进行处理，为物品或种类做出相似的推荐，如果后续用户搜索相应的物品时，给用户展示相应的产品，比如你在淘宝搜索大数据书籍，淘宝会给你推荐相关的书籍，这就算是为大数据书籍产生的推荐结果。大数据学习路线：Linux -&gt; Hadoop -&gt; Zookeeper -&gt; Hive -&gt; Flume -&gt; Kafka -&gt; HBase -&gt; Scala -&gt; Spark -&gt; 项目 - &gt; Flink（前面有说Flink是后续学习的） 一、Linux（基本操作）一般我们使用的都是虚拟机来进行操作，所以要安装VM（ Virtual Machine），我使用的是CentOS，所以VM和CentOS都要跟着安装好，跟着视频操作，一定要动手实践，将一些Linux基本命令熟练掌握，一些VIM编辑器的命令也要会用，做相应的一些配置，使用SecureCRT来做远程登录操作（也可以使用其他的，自己顺手就行）。再强调一遍，基本操作命令尽量熟练一点，如果一下记不住，打印一些常用的，自己看看，多用多实践，慢慢就会用了。还有一些软件包的下载安装卸载等，跟着操作一遍，熟悉下，后续都会使用，Shell编程可以后续补。 视频：如果想了解下shell（后面乌班图的可以选择不看）没有shell讲解 二、Hadoop（重点中的重点）Hadoop是一个分布式系统基础框架，用于主要解决海量数据的存储和海量数据的分析计算问题，也可以说Hadoop是后续整个集群环境的基础，很多框架的使用都是会依赖于Hadoop。主要是由HDFS、MapReduce、YARN组成。这个部分安装Hadoop，Hadoop的三个主要组成部分是重点，对他们的概念要理解出来，知道他们是做什么的，搭建集群环境，伪分布式模式和完全分布式模式的搭建，重要的是完全分布式的搭建，这些部分一定要自己动手实践，自己搭建集群，仔细仔细再仔细，Hadoop的NameNode，DataNode，YARN的启动关闭命令一定要知道，以及他们的启动关闭顺序要记住，不要搞混。后续视频会有一些案例操作，跟着写代码，做测试，把基本环境都配置好，后续这个集群（完全分布式需要三台虚拟机）要一直使用。 视频：我开始看过的版本第二个看过的版本2019版本 三、ZookeeperZookeeper是一个开源的分布式的，为分布式应用提供协调服务的Apache项目。分布式安装ZK，对ZK有一定的了解就可以了，了解它的应用场景，以及内部原理，跟着做一些操作，基本上有一些了解即可。 视频：我看过的版本尚硅谷周洋版本（听说挺好）2019版本 四、Hive（重点）Hive是基于Hadoop的数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类SQL查询功能。Hive的安装，它的数据类型，以及它的数据定义、数据操作有较好的了解，怎么操作表（创建表、删除表，创建什么类型的表，他们有什么不同），怎么操作数据（加载数据，下载数据，对不同的表进行数据操作），对数据的查询一定要进行实践操作，以及对压缩方式和存储格式要有一些了解，用到时不懂也可以去查，最好是能理解清楚。这部分有什么面试可能会问，所以视频后续的面试讲解可以看看，理解清楚。 视频：我开始看过的版本第二个看过的版本2019版本 五、FlumeFlume是一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统。对于Flume，对它的组成架构，以及对Flume Agent的内部原理要理解清楚，Source、Channel、Sink一定要知道它们的各种类型以及作用，有哪些拓扑结构是常见常用的，例如一对一，单Source、多Channel、多Sink等，它们有什么作用，要理解清楚。还有一个重点，就是对Flume的配置文件一定要了解清楚，不懂的可以上官网查看案例，对于不同的情况，它的配置文件要做相应的修改，才能对数据进行采集处理，视频中的实践案例一定要跟着做。 视频：我开始看过的版本第二个看过的版本2019版本 六、Kafka（重点）Kafka是一个分布式消息队列，用来缓存数据的。比如说实时计算中可以通过Flume+Kafka对数据进行采集处理之后，Spark Streaming再使用Kafka相应的Topic中的数据，用于后续的计算使用。对于Kafka，要理解Kafka的架构，什么是Kafka，为什么需要Kafka，应用场景。基本的命令行操作要掌握，比如怎么创建删除Topic，怎么通过生产者生成数据，消费者怎么消费数据等基本操作，官网也是有一些案例可以查阅的。 视频：我看过的版本2019版本 七、HBase（重点）HBase是一个分布式的、基于列存储的开源数据库。HBase适合存储PB级别的海量数据，也可以说HBase是很适合大数据的存储的，它是基于列式存储数据的，列族下面可以有非常多的列，列族在创建表的时候就必须指定。所以对HBase的数据结构要有一定的理解，特别是RowKey的设计部分（因为面试被问到过，咳咳，所以点一下），对于它的原理要了解，一些基本操作也要都会，比如创建表，对表的操作，基本的API使用等。 视频：我看过的版本2019版本 八、SparkSpark是快速、易用、通用的大数据分析引擎。一说到Spark，就有一种哪哪都是重点感觉，哈哈。Spark的组成可以看下图Spark是基于内存计算的，对于数据的处理速度要比MapReduce快很多很多，而且数据挖掘这些都是要对数据做迭代式计算，MapReduce对数据的处理方式也不适合，而Spark是可以进行迭代式计算，很适合数据挖掘等场景。Spark的Spark SQL能够对结构化数据进行处理，Spark SQL的DataFrame或DataSet可以作为分布式SQL查询引擎的作用，可以直接使用Hive上的表，对数据进行处理。Spark Streaming主要用于对应用场景中的实时流数据进行处理，支持多种数据源，DStream是Spark Streaming的基础抽象，由一系列RDD组成，每个RDD中存放着一定时间段的数据，再对数据进行处理，而且是基于内存计算，速度快，所以很适合实时数据的处理。Spark MLlib提供常见的机器学习(ML)功能的程序库。包括分类、回归、聚类、协同过滤等，还提供了模型评估、数据 导入等额外的支持功能。对Spark的核心组件、部署模式（主要是Standalone模式和YARN模式）、通讯架构、任务调度要有一定了解（面试问到了可以说一波），Spark Shuffle要好好理解，还有内存管理要知道，对Spark的内核原理一定要好好理解，不仅面试可能要用，以后工作也是有帮助的。 视频：我开始看过的版本第二个看过的版本2019版本 九、Flink（重点中的重点）Flink是一个框架和分布式处理引擎，用于对无界（有开始无结束）和有界（有开始有结束）数据流进行有状态计算。现在主要是阿里这种大公司使用的比较多，中国很多公司使用的还是Spark居多，而且Flink基本上都是和Spark很多功能大体上一样的，但是以后Flink和Spark孰强孰弱还有待时间的考验，不过Flink近几年越来越火了这是事实，所以如果有时间有精力的话，可以学一学Flink相关的内容也是很不错的。Spark和Flink主要都是在数据处理方面应用，在数据处理方面的话，离线数据处理：Flink暂时比不上Spark，Spark SQL优点在于可以和Hive进行无缝连接，Spark SQL可以直接使用Hive中的表；Flink暂时做不到这一步，因为官方不支持这一操作，Flink只能将数据读取成自己的表，不能直接使用Hive中的表。对于实时数据的处理：Flink和Spark可以说是平分秋色吧，而且Flink是以事件为驱动对数据进行处理，而Spark是以事件为驱动对数据进行处理，在一些应用场景中，也许Flink的效果比Spark的效果还要好些，因为Flink对数据更加的敏感。比如一秒钟如果触发了成千上万个事件，那么时间驱动型就很难对数据做细致的计算，而事件驱动型可以以事件为单位，一个个事件进行处理，相比而言延迟更低，处理效果更好。还是那句话，虽然现在使用的公司较少，但是有时间接触学习下，也是没有坏处的。 视频：我看的版本（基础+项目） 项目阶段其实尚硅谷的视频里面有很多大数据相关的项目，而且都是文档配代码的，学习期间可以跟着视频做两到三个项目，自己理清思路，把项目理解透彻，还是可以学到很多东西的。根据自己情况，选择两到三个项目重点跟着做，理解透彻一点 大数据项目实战滴滴系统：Kafka+Storm友盟统计项目：HiveYouTube项目：Hive电商数据分析平台项目：Spark电信客服项目：Hadoop微博项目：HBase大数据离线平台：Hadoop+Flume+Hive+HBase电商数仓项目：Hadoop+Zookeeper+Hive+Flume+Kafka+Spark在线教育项目：Hadoop+Flume+Kafka+Hive+MySQL+Spark基于阿里云搭建数据仓库（离线）：ECS（日志生产服务器）+Flume+DataHub+MaxCompute/DataWorks+RDS（业务数据）+QuickBI 机器学习大数据岗位中也是有对算法要求比较高的，就是数据挖掘岗位，相对来说，对机器学习算法有一定要求。这里就不介绍了，有时间可以自己学习机器学习相关的内容，如果你有机器学习的基础，那自然是更好了。下面是我自己收集的一些资料，有需要的可以看看机器学习算法学习资料 书籍都是根据自己看过和准备的来的，所以你自己有更好的选择，根据自己的来就行Java《Java核心技术 卷Ⅰ》Scala《快学Scala》Linux《鸟叔的Linux私房菜-基础篇》《Linux命令行与Shell脚本编程大全》Hadoop《Hadoop权威指南》谷歌三大文章Google-Bigtable中文版Google-MapReduce中文版_1.0Google-File-System中文版_1.0Google三大论文英文版Hive《Hive编程指南》Kafka《Kafka权威指南》HBase《HBase权威指南》Spark《Spark快速大数据分析》《Apache Spark源码剖析》《Spark MLlib机器学习》大概就这些，看完就需要很久了，大部分我也是需要的时候看相应的部分，所以有时间可以好好看下，不然就需要哪一部分看哪一部分，有助于学习即可。 最后大数据开发也是需要编程基础的，并不是学会使用这些框架怎么样就可以了，所以对于编程语言，数据结构与算法，计算机网络这些基础也是要的，这些基础知识也有助于自己以后的发展，如果是应届生校招的话，面试基本上都是JavaSE和数据结构与算法等的知识点，还有大数据组件相关的知识点，以及对项目的理解，这些都是要自己面试前准备好的，多看面经，多找面试题看，面几次，心里有谱了，后续面试就好了。不管是从事什么样的计算机相关的岗位，编程都是很重要的，数据结构与算法特别重要，还有就是leetcode等编程网站刷题，提升自己的编程思维，后续笔试面试都要要的。要将一行行代码看做一叠叠rmb，但是一行行代码能不能转换成一叠叠rmb，自己就一定要：坚持，多敲代码；多敲代码，坚持；坚持。以上纯属个人总结，也许有理解不是很好的地方，每个人都有自己的学习方法，不喜勿喷，谢谢~]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门系列——大数据相关岗位介绍]]></title>
    <url>%2F2017%2F12%2F07%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%9B%B8%E5%85%B3%E5%B2%97%E4%BD%8D%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[大数据相关岗位介绍 主要是介绍现在大数据中的一些岗位的要求、岗位描述和岗位职责，相关岗位的信息是对各个招聘网站中的一些要求做了一个总结 一、大数据开发工程师：1.要求 编程基础扎实，熟悉Java，熟悉Scala/Shell/Python语言中其中一种更好（社招一般要求两种）； 熟悉MySQL等至少一种数据库，熟悉SQL语言，熟悉Linux系统； 熟悉Hadoop/Hive/Flume/Kafka/HBase/Spark/Storm等技术及其生态圈。 2.岗位描述 负责公司大数据平台的开发和维护，负责大数据平台持续集成相关工具平台的架构设计与产品开发等； 运用编程语言实现数据平台和数据管道开发，需要计算机编程能力。 3.岗位职责第1种 负责大数据采集，计算，分析处理等相关开发工作； 负责数据仓库的设计与开发等相关工作； 负责大数据治理相关的设计与开发工作。 第2种 负责数据采集、爬取、清洗、加工、分类和管理等工作； 管理、优化并维护Hadoop、Spark等集群，保证集群规模持续、稳定； 负责HDFS/hive/HBase的功能、性能和扩展，解决并实现业务需求； 协助建立数据模型，对数据进行挖掘、优化及统计； 负责大数据平台运维以及日常数据运营等管理，及技术攻关。 第3种 负责公司互联网数据ETL数据清洗工作； 负责公司各互联网产品的报表开发及版本迭代； 负责平台的整体数据架构设计，对数据有较高敏感性，完成从业务模型到数据模型的设计及开发工作； 负责Hadoop平台数据仓库、数据集成、数据管理的整体架构设计工作。 二、数据挖掘/算法工程师：1.要求 熟悉数据挖掘、机器学习、深度学习、推荐系统等相关技术； 有大数据相关组件开发经验，熟悉一种以上海量数据处理模型和框架，具备数据处理和加工能力； 熟悉常用数据处理工具， 扎实的算法基础和编程基础，熟悉Scala/Python/SQL等尤佳。 2.岗位描述 数据建模、机器学习和算法实现，需要业务理解、熟悉算法和计算机编程； 商业智能，用户体验分析，预测流失用户等；需要过硬的数学和统计学功底以外。 3.岗位职责第1种 熟悉数据挖掘、机器学习、推荐系统相关技术，对业界前沿技术持续关注； 熟悉一种以上海量数据处理模型和框架，具备数据处理和加工能力； 良好的数据科学思维，重数据驱动； 熟悉常用数据处理工具，熟悉Scala/Python/SQL/Spark/Hadoop等尤佳。 第2种 负责核心产品的用户数据体系建设工作，构建完善的数据体系和基础技术，支撑业务快速发展； 深入理解用户，运用数据挖掘、机器学习、深度学习等技术，构建多维度用户画像，从行为、兴趣、风险等等方面深刻理解用户； 推动用户数据体系和技术在推荐、搜索、评级、风控等业务场景落地，不断提升用户体验、降低风险。深入探索业务数据，创新性的思考和发现问题，并提出有效解决方案。 三、数据分析师：1.要求 至少掌握一种数据分析建模工具(R/Python/Scala等)，可进行模型开发以及实现算法优化； 熟练运用SQL/Hive,有丰富的数据分析、挖掘、清洗和建模经验； 熟悉大数据开发环境和工具，如Hadoop和Spark等； 具有丰富的数据分析、数据挖掘经验，熟悉常用数据挖掘算法：图论、预测、聚类、分类、关联、序列挖掘及无监督挖掘等； 至少熟悉一项以下领域：深度学习，强化学习，自然语言处理，时间序列，风控建模。2.岗位描述 进行数据搜集、整理、分析，针对数据分析结论给管理销售运营提供指导意义的分析意见； 3.岗位职责第1种 与各业务团队有效沟通，收集和整理数据分析需求，能快速理解相关业务。 配合产品经理进行相关产品及项目的定型，前端交互以及分析维度设计，参与相关产品及项目的解决方案梳理和制定； 对业务主题数据进行定期统计和分析，输出业务洞察报告； 根据数据分析和对业务形态的理解，对产品、服务、营销等活动提出合理化建议。 第2种 构建业务分析体系，对各类分析任务中发现的问题，进行跟踪、定位、分析、解决工作； 通过专题分析，对业务问题进行深入分析，为公司运营决策、产品方向、销售策略提供数据支持； 对现有业务数据建立日常跟踪监控体系，及时敏锐的发现业务数据变化趋势； 构建分析和预测模型，通过跟踪和监控重点数据，发现潜在的缺陷与问题，为业务决策提供数据支撑。 下面的的岗位可以看看招聘网站，就不具体介绍了。。。。。 四、数据管理： 数据库设计、数据迁移、数据库性能管理、数据安全管理，故障检修问题、数据备份、数据恢复等。 五、数据产品经理： 把数据和业务结合起来做成数据产品。 六、数据科学家： 清洗，管理和组织（大）数据，利用算法和模型提高数据处理效率、挖掘数据价值、实现从数据到知识的转换。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据入门系列——大数据简介]]></title>
    <url>%2F2017%2F12%2F03%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%85%A5%E9%97%A8%E7%B3%BB%E5%88%97%E2%80%94%E2%80%94%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[大数据简介一、大数据概论大数据（big data）：指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。 最小的基本单位是bit，按顺序给出所有单位：bit、Byte、KB、MB、GB、TB、PB、EB、ZB、YB、BB、NB、DB。 1Byte = 8bit1K = 1024bit1MB = 1024K1G = 1024M1T = 1024G1P = 1024T1E = 1024P1Z = 1024E1Y = 1024Z1B = 1024Y1N = 1024B1D = 1024N 二、大数据特点1.Volume（大量）截至目前，人类生产的所有印刷材料的数据量是200PB，而历史上全人类总共说过的话的数据量大约是5EB。当前，典型个人计算机硬盘的容量为TB量级，而一些大企业的数据量已经接近EB量级。 2.Velocity（高速）这是大数据区分于传统数据挖掘的最显著特征。根据IDC的“数字宇宙”的报告，预计到2020年，全球数据使用量将达到35.2ZB。在如此海量的数据面前，处理数据的效率就是企业的生命。天猫双十一：2017年3分01秒，天猫交易额超过100亿。 3.Variety（多样）这种类型的多样性也让数据被分为结构化数据和非结构化数据。相对于以往便于存储的以数据库/文本为主的结构化数据，非结构化数据越来越多，包括网络日志、音频、视频、图片、地理位置信息等，这些多类型的数据对数据的处理能力提出了更高要求。 4.Value（低价值密度）价值密度的高低与数据总量的大小成反比。比如，在一天监控视频中，我们只关心一个人晚上在健身房健身那一分钟，如何快速对有价值数据“提纯”成为目前大数据背景下待解决的难题。 三、大数据应用场景 物流仓库：大数据分析系统助理商家精细化运营、提升质量、节约成本 零售：分析用户习惯，为用户购买商品提供方便，从而提供商品销量。经典案例：纸尿裤+啤酒 旅游：深度结合大数据能力与旅游行业需求，共建旅游行业智慧管理、智慧服务和智慧营销的未来 商品广告推荐：给用户推荐可能喜欢的商品。案例：用户买了一本书，又推荐了若干本书 房产：大数据全面助力房地产行业，打造精准投策与营销，选出更合适的地，建造更合适的楼，卖给更合适的人 保险：海量数据挖掘及风险预测，助力保险行业精准营销，提升精细化定价能力 金融：多维度体现用户特征，帮助金融机构推荐优质客户，防范欺诈风险 人工智能 四、大数据发展前景 党的十八届五中全会提出“实施国家大数据战略”，国务院印发《促进大数据发展行动纲要》，大数据技术和应用处于创新突破期，国内市场需求处于爆发期，我国大数据产业面临重要的发展机遇。 国际数据公司IDC预测，到2020年，企业基于大数据计算分析平台的支出将突破5000亿美元。目前，我国大数据人才只有46万，未来3到5年人才缺口达150万之多。 2017年北京大学、中国人民大学、北京邮电大学等25所高校成功申请开设大数据课程。 具体薪资各个招聘网站上也都有详细说明。 五、企业数据部的业务流程分析 六、大数据部门组织结构 七、大数据技术生态体系图中涉及的技术名词解释如下：1）Sqoop：sqoop是一款开源的工具，主要用于在Hadoop(Hive)与传统的数据库(mysql)间进行数据的传递，可以将一个关系型数据库（例如 ： MySQL ,Oracle 等）中的数据导进到Hadoop的HDFS中，也可以将HDFS的数据导进到关系型数据库中。2）Flume：Flume是Cloudera提供的一个高可用的，高可靠的，分布式的海量日志采集、聚合和传输的系统，Flume支持在日志系统中定制各类数据发送方，用于收集数据；同时，Flume提供对数据进行简单处理，并写到各种数据接受方（可定制）的能力。3）Kafka：Kafka是一种高吞吐量的分布式发布订阅消息系统，有如下特性： 通过O(1)的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。 高吞吐量：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。 支持通过Kafka服务器和消费机集群来分区消息。 支持Hadoop并行数据加载。4）Storm：Storm为分布式实时计算提供了一组通用原语，可被用于“流处理”之中，实时处理消息并更新数据库。这是管理队列及工作者集群的另一种方式。 Storm也可被用于“连续计算”（continuous computation），对数据流做连续查询，在计算时就将结果以流的形式输出给用户。5）Spark：Spark是当前最流行的开源大数据内存计算框架。可以基于Hadoop上存储的大数据进行计算。6）Oozie：Oozie是一个管理Hadoop作业（job）的工作流程调度管理系统。Oozie协调作业就是通过时间（频率）和有效数据触发当前的Oozie工作流程。7）Hbase：HBase是一个分布式的、面向列的开源数据库。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存储的数据库。8）Hive：hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析。9）R语言：R是用于统计分析、绘图的语言和操作环境。R是属于GNU系统的一个自由、免费、源代码开放的软件，它是一个用于统计计算和统计制图的优秀工具。10）Mahout：Apache Mahout是个可扩展的机器学习和数据挖掘库，当前Mahout支持主要的4个用例： 推荐挖掘：搜集用户动作并以此给用户推荐可能喜欢的事物。 聚集：收集文件并进行相关文件分组。 分类：从现有的分类文档中学习，寻找文档中的相似特征，并为无标签的文档进行正确的归类。 频繁项集挖掘：将一组项分组，并识别哪些个别项会经常一起出现。11）ZooKeeper：Zookeeper是Google的Chubby一个开源的实现。它是一个针对大型分布式系统的可靠协调系统，提供的功能包括：配置维护、名字服务、 分布式同步、组服务等。ZooKeeper的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>入门</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[大数据系列之《大数据学习指南》]]></title>
    <url>%2F2017%2F12%2F01%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%B3%BB%E5%88%97%E4%B9%8B%E3%80%8A%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0%E6%8C%87%E5%8D%97%E3%80%8B%2F</url>
    <content type="text"><![CDATA[大数据学习指南 大数据学习指南，从零开始学习大数据开发，包含大数据学习各个阶段资汇总 大数据学习or转型的一些建议概述1.大数据简介2.大数据相关岗位介绍 大数据学习路线1.大数据学习路线（包含自己看过的视频链接） 完全分布式集群搭建可以参考我的博客，按照顺序进行操作即可1.集群搭建 大数据框架组件一、Hadoop1.Hadoop——分布式文件管理系统HDFS2.Hadoop——HDFS的Shell操作3.Hadoop——HDFS的Java API操作4.Hadoop——分布式计算框架MapReduce5.Hadoop——MapReduce案例6.Hadoop——资源调度器YARN7.Hadoop——Hadoop数据压缩 二、Zookeeper1.Zookeeper——Zookeeper概述 三、Hive1.Hive——Hive概述2.Hive——Hive数据类型 四、Flume1.Flume——Flume概述2.Flume——Flume实践操作3.Flume——Flume案例 五、Kafka1.Kafka——Kafka概述2.Kafka——Kafka深入解析3.Kafka——Kafka API操作实践3.Kafka——Kafka对接Flume实践 六、HBase1.HBase——HBase概述2.HBase——HBase数据结构3.HBase——HBase Shell操作4.HBase——HBase API实践操作 七、Spark+ Spark基础1.Spark基础——Spark的诞生2.Spark基础——Spark概述3.Spark基础——Spark运行模式4.Spark基础——案例实践 + Spark Core1.Spark Core——RDD概述 八、Flink面试题 一、Hadoop1.Hadoop面试题总结（一）2.Hadoop面试题总结（二）——HDFS3.Hadoop面试题总结（三）——MapReduce4.Hadoop面试题总结（四）——YARN5.Hadoop面试题总结（五）——优化问题 二、Zookeeper1.Zookeeper面试题总结（一） 三、Hive1.Hive面试题总结（一）2.Hive面试题总结（二） 四、HBase1.HBase面试题总结（一） 五、Flume1.Flume面试题总结（一） 六、Kafka1.Kafka面试题总结（一）2.Kafka面试题总结（二） 七、Spark1.Spark面试题总结（一）2.Spark面试题总结（二）3.Spark面试题总结（三）4.Spark面试题总结（四） Spark性能优化：5.Spark面试题总结（五）——几种常见的数据倾斜情况及调优方式6.Spark面试题总结（六）——Shuffle配置调优7.Spark面试题总结（七）——程序开发调优8.Spark面试题总结（八）——运行资源调优]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>大数据</tag>
        <tag>学习指南</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分区、分表、分库、分片入门]]></title>
    <url>%2F2017%2F11%2F23%2F%E5%88%86%E5%8C%BA%E3%80%81%E5%88%86%E8%A1%A8%E3%80%81%E5%88%86%E5%BA%93%E3%80%81%E5%88%86%E7%89%87%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[一、分区的概念数据分区是一种物理数据库的设计技术，它的目的是为了在特定的SQL操作中减少数据读写的总量以缩减响应时间。 分区并不是生成新的数据表，而是将表的数据均衡分摊到不同的硬盘，系统或是不同服务器存储介子中，实际上还是一张表。另外，分区可以做到将表的数据均衡到不同的地方，提高数据检索的效率，降低数据库的频繁IO压力值，分区的优点如下： 1、相对于单个文件系统或是硬盘，分区可以存储更多的数据； 2、数据管理比较方便，比如要清理或废弃某年的数据，就可以直接删除该日期的分区数据即可； 3、精准定位分区查询数据，不需要全表扫描查询，大大提高数据检索效率； 4、可跨多个分区磁盘查询，来提高查询的吞吐量； 5、在涉及聚合函数查询时，可以很容易进行数据的合并； 二、分类 （row 行 ，column 列） 1、水平分区 这种形式分区是对表的行进行分区，通过这样的方式不同分组里面的物理列分割的数据集得以组合，从而进行个体分割（单分区）或集体分割（1个或多个分区）。所有在表中定义的列在每个数据集中都能找到，所以表的特性依然得以保持。 举个简单例子：一个包含十年发票记录的表可以被分区为十个不同的分区，每个分区包含的是其中一年的记录。（朋奕注：这里具体使用的分区方式我们后面再说，可以先说一点，一定要通过某个属性列来分割，譬如这里使用的列就是年份） 2、垂直分区 这种分区方式一般来说是通过对表的垂直划分来减少目标表的宽度，使某些特定的列被划分到特定的分区，每个分区都包含了其中的列所对应的行。 举个简单例子：一个包含了大text和BLOB列的表，这些text和BLOB列又不经常被访问，这时候就要把这些不经常使用的text和BLOB了划分到另一个分区，在保证它们数据相关性的同时还能提高访问速度。 在数据库供应商开始在他们的数据库引擎中建立分区（主要是水平分区）时，DBA和建模者必须设计好表的物理分区结构，不要保存冗余的数据（不同表中同时都包含父表中的数据）或相互联结成一个逻辑父对象（通常是视图）。这种做法会使水平分区的大部分功能失效，有时候也会对垂直分区产生影响。 三、分片、分区、分表、分库的详细理解一、什么是分片、分区、分表、分库 分片 当数据库数据达到上亿级别时，数据库压力会很大，存不下，可以考虑使用数据库分片。 分区 就是把一张表的数据分成N个区块，在逻辑上看最终只是一张表，但底层是由N个物理区块组成的 分表 就是把一张表按一定的规则分解成N个具有独立存储空间的实体表。系统读写时需要根据定义好的规则得到对应的字表明，然后操作它。 分库 一旦分表，一个库中的表会越来越多 将整个数据库比作图书馆，一张表就是一本书。当要在一本书中查找某项内容时，如果不分章节，查找的效率将会下降。而同理，在数据库中就是分区。 常用的单机数据库的瓶颈 问题描述 单个表数据量越大，读写锁，插入操作重新建立索引效率越低。 单个库数据量太大（一个数据库数据量到1T-2T就是极限） 单个数据库服务器压力过大 读写速度遇到瓶颈（并发量几百） 二、分片不同的表放到不同的 数据库中—垂直切割。 数据量小，查询性能会提高。 不同数据库位于不同服务器上时，会减小服务器压力。 单张表数据量也很大，如用户量大产生操作量也会很大。单独查询时，压力也会很大。此时垂直分割也无济于事。可以考虑–水平分割。 一张表放到不同数据库中。用户表，放到不同数据库，每个数据库存储部分数据，单表数据量不大。 三、分区什么时候考虑使用分区？ 一张表的查询速度已经慢到影响使用的时候。 sql经过优化 数据量大 表中的数据是分段的 对数据的操作往往只涉及一部分数据，而不是所有的数据 分区解决的问题​ ​ 主要可以提升查询效率 分区的实现方式（简单）mysql5 开始支持分区功能 CREATE TABLE sales ( id INT AUTO_INCREMENT, amount DOUBLE NOT NULL, order_day DATETIME NOT NULL, PRIMARY KEY(id, order_day) ) ENGINE=Innodb PARTITION BY RANGE(YEAR(order_day)) ( PARTITION p_2010 VALUES LESS THAN (2010), PARTITION p_2011 VALUES LESS THAN (2011), PARTITION p_2012 VALUES LESS THAN (2012), PARTITION p_catchall VALUES LESS THAN MAXVALUE); MySQL5.1提供的分区(Partition)功能确实可以实现表的分区，但是这种分区是局限在单个数据库范围里的，它不能跨越服务器的限制。 如果能够保证数据量很难超过现有数据库服务器的物理承载量，那么只需利用MySQL5.1提供的分区(Partition)功能来改善数据库性能即可；否则，还是考虑应用Sharding理念吧，spider storage engine就是一个不错的选择。 Sharding与数据库分区(Partition)的区别 有的时候，Sharding 也被近似等同于水平分区(Horizontal Partitioning)，网上很多地方也用水平分区来指代 Sharding，但我个人认为二者之间实际上还是有区别的。的确，Sharding 的思想是从分区的思想而来，但数据库分区基本上是数据对象级别的处理，比如表和索引的分区，每个子数据集上能够有不同的物理存储属性，还是单个数据库范围内的操作，而 Sharding 是能够跨数据库，甚至跨越物理机器的。 四、分表什么时候考虑分表？ 一张表的查询速度已经慢到影响使用的时候。 sql经过优化 数据量大 当频繁插入或者联合查询时，速度变慢 分表解决的问题分表后，单表的并发能力提高了，磁盘I/O性能也提高了，写操作效率提高了 查询一次的时间短了 数据分布在不同的文件，磁盘I/O性能提高 读写锁影响的数据量变小 插入数据库需要重新建立索引的数据减少 分表的实现方式（复杂）​ 需要业务系统配合迁移升级，工作量较大 #####分区和分表的区别与联系 分区和分表的目的都是减少数据库的负担，提高表的增删改查效率。 分区只是一张表中的数据的存储位置发生改变，分表是将一张表分成多张表。 当访问量大，且表数据比较大时，两种方式可以互相配合使用。 当访问量不大，但表数据比较多时，可以只进行分区。 常见分区分表的规则策略（类似） Range（范围） Hash（哈希） List（链表） 按照时间拆分 Hash之后按照分表个数取模 在认证库中保存数据库配置，就是建立一个DB，这个DB单独保存user_id到DB的映射关系 五、分库什么时候考虑使用分库？ 单台DB的存储空间不够 随着查询量的增加单台数据库服务器已经没办法支撑 分库解决的问题​ 其主要目的是为突破单节点数据库服务器的 I/O 能力限制，解决数据库扩展性问题。 垂直拆分将系统中不存在关联关系或者需要join的表可以放在不同的数据库不同的服务器中。 按照业务垂直划分。比如：可以按照业务分为资金、会员、订单三个数据库。 需要解决的问题：跨数据库的事务、jion查询等问题。 水平拆分例如，大部分的站点。数据都是和用户有关，那么可以根据用户，将数据按照用户水平拆分。 按照规则划分，一般水平分库是在垂直分库之后的。比如每天处理的订单数量是海量的，可以按照一定的规则水平划分。需要解决的问题：数据路由、组装。 读写分离对于时效性不高的数据，可以通过读写分离缓解数据库压力。需要解决的问题：在业务上区分哪些业务上是允许一定时间延迟的，以及数据同步问题。 思路 垂直分库–&gt;水平分库–&gt;读写分离 六、拆分之后面临新的问题问题 事务的支持，分库分表，就变成了分布式事务 join时跨库，跨表的问题 分库分表，读写分离使用了分布式，分布式为了保证强一致性，必然带来延迟，导致性能降低，系统的复杂度变高。 常用的解决方案：对于不同的方式之间没有严格的界限，特点不同，侧重点不同。需要根据实际情况，结合每种方式的特点来进行处理。 选用第三方的数据库中间件（Atlas，Mycat，TDDL，DRDS），同时业务系统需要配合数据存储的升级。 七、数据存储的演进单库单表单库单表是最常见的数据库设计，例如，有一张用户(user)表放在数据库db中，所有的用户都可以在db库中的user表中查到。 单库多表随着用户数量的增加，user表的数据量会越来越大，当数据量达到一定程度的时候对user表的查询会渐渐的变慢，从而影响整个DB的性能。如果使用mysql, 还有一个更严重的问题是，当需要添加一列的时候，mysql会锁表，期间所有的读写操作只能等待。 可以通过某种方式将user进行水平的切分，产生两个表结构完全一样的user_0000,user_0001等表，user_0000 + user_0001 + …的数据刚好是一份完整的数据。 多库多表随着数据量增加也许单台DB的存储空间不够，随着查询量的增加单台数据库服务器已经没办法支撑。这个时候可以再对数据库进行水平拆分。 八、总结总的来说，优先考虑分区。当分区不能满足需求时，开始考虑分表，合理的分表对效率的提升会优于分区。 基础数据存储Mysql：只存储非文本的基础信息。包括：评论状态，用户，时间等基础数据。以及图片，标签，点赞等附加信息。数据组织形式（不同的数据又可选择不同的库表拆分方案）： 评论基础数据按用户ID进行拆库并拆表 图片及标签处于同一数据库下，根据商品编号分别进行拆表 其它的扩展信息数据，因数据量不大、访问量不高，处理于同一库下且不做分表即可 文本存储文本存储（评论的内容）使用了mongodb、hbase 选择nosql而非mysql 减轻了mysql存储压力，释放msyql，庞大的存储也有了可靠的保障 nosql的高性能读写大大提升了系统的吞吐量并降低了延迟]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>拓展</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器配置——内网CentOS7/PHP/Apache/Nginx/SVN/Git/GitLab服务器实战]]></title>
    <url>%2F2017%2F10%2F18%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E2%80%94%E2%80%94%E5%86%85%E7%BD%91CentOS7%3APHP%3AApache%3ANginx%3ASVN%3AGit%3AGitLab%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[完整实战，验证通过，可用….. 此文章看着好长，或者逼格好高的样子。其实很简单，只是因为这些是后端开发或者公司负责项目和内部服务器搭建人员必会的！ 适合人群： PHP后端开发 小团队或部门组负责人 服务器或相关运营负责人 想要学习PHP或者转型后端开发 业务学习，纯属搞着玩 这里就不对每一个模块和内容做解释了，只用实战记录，方便后期查阅也希望能得到更多前辈的点评和指点！ 首先分享一份教程，这是慕课网实战阿里云主机(ECS)与CentOS7教程！ 阿里云主机(ECS)与CentOS7实战 提取码：389j 注：本文部分外部操作基于Mac系统 1.U盘制作Centos系统盘准备 U盘或者硬盘一个，需要大于8G iso系统:(本文基础1708) https://pan.baidu.com/s/1sC8JSuCzPk6BGbfof7vouQ 提取码: 4df8 HFSExplorer：方面Windows系统访问Mac盘(U/硬盘) UltraISO：光盘映像文件制作 具体制作安装U盘的步骤由于过于简单，这里就过多说明，不知道的请出门左转找度娘。 2.Centos安装只要U盘正确制作，关于网上教程非常多，也是非常简单的！ 这里推荐被参考的比较多的教程： https://www.osyunwei.com/archives/7829.html 有些地方需要注意的： 可以的话，最好在安装信息摘要的时候连上网，方便一些相关的更新和下载。 出入学习推荐使用桌面版，当然如果比较喜欢专业或者非入门级的那就使用最小安装，最小安装是基本上很多东西都没有，连网络和相关需求都不支持。 关于分区可以使用自动分区，或者根据教程做相应的分配，建议不要自己随意配，不然后面难免出现问题。 期间遇到了一个问题 /dev/root does not exist, could not boot 网上都说直接修改 vmlinuz initrd=initrd.imginst.stage2=hd:LABEL=CentOS\x207\x20x86_64 rd.live.check quiet 改为： vmlinuz initrd=initrd.img inst.stage2=hd:/dev/sda4 quite 试了好几次都不行,最后发现是空格的问题，因为我格式化U盘之后直接命名为了Centos 7，导致产生了空格，而无法读取到U盘 解决方法： 到windows里面修改U盘名称（例如 ‘Centos7’） 进入U盘目录 进入 EFI/boot 修改grub.cfg文件，CENTOS\x207 全部修改为U盘名称（CENTOS7） 重启安装 正常安装并显示交互界面 总结：名称强烈不建议出现中文空格,容易出bug 安装完成之后，进入Centos命令行查看IP地址，然后在ssh连接工具上尝试连接，并且进行一些操作 CentOS7查看ip地址的方式为： ip addr 这里有一点需要知道的是，阿里云或腾讯云或者其他说明鬼云，都是提供的最新的相关Linux系统，所以一般系统安装是不需要我们来做的，除非有一些特殊的情况。 Ip地址也是直接对外提供，不需要我们去查的，所以就XXX云来说，我们做的事情就是从这里往后操作的流程。甚至有些会给你一些更完整或者更多的功能支持。 如果有需要需要额外配置其他相关功能和服务，推荐看这里： centos7 装机后的基本配置 3.Centos系统配置PHP服务器 注意：以下安装，我都是用的root权限。 一、关闭防火墙查了资料，说法是，CentOS7用的是Firewall-cmd，CentOS7之前用的是iptables防火墙；要想让外网能访问到apache主目录，就需要做以下的操作： 12345678910111213停止firewall systemctl stop firewalld.service禁止firewall开机启动 systemctl disable firewalld.service /** Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service. Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service. */查看默认防火墙的状态 firewall-cmd --state /** not running */ 或者可以这么做： 123firewall-cmd --permanent --zone=public --add-service=httpfirewall-cmd --permanent --zone=public --add-service=httpsfirewall-cmd --reload 二、安装Apache123456789101112131415安装Apache yum install httpd -y--- 一下httpd可以直接使用httpd.service --- 启动Apache systemctl start httpd停止Apache systemctl stop httpd重启Apache systemctl restart httpdApache开机自启 systemctl enable httpd /** Created symlink from /etc/systemd/system/multi-user.target.wants/httpd.service to /usr/lib/systemd/system/httpd.service. */ 在本机浏览器中前面ip addr查到的ip地址，如果看到apache默认的页面–有Testing 123…字样，便是成功安装了apache服务 三、安装php123456安装php yum install php php-devel或者yum -y install php安装Php扩展 yum install php-mysql php-gd php-imap php-ldap php-odbc php-pear php-xml php-xmlrpc重启Apache: systemctl restart httpd或者systemctl restart httpd.service 然后，你可以写一个php文件在浏览器中运行一下了; 12vi /var/www/html/info.php&lt;?php phpinfo(); ?&gt; 然后，在自己电脑浏览器输入 http://172.20.10.2/info.php运行，会出现php的一些信息,如果出现如下界面，说明已经成功了 四、安装mysql1234567891011121314安装MySQL yum install mariadb mariadb-server -y启动MariaDB systemctl start mariadb 停止MariaDB systemctl stop mariadb 重启MariaDB systemctl restart mariadb设置开机启动 systemctl enable mariadb/**Created symlink from /etc/systemd/system/multi-user.target.wants/mariadb.service to /usr/lib/systemd/system/mariadb.service.*/ 设置root帐户的密码 1mysql_secure_installation 然后会出现一串东西，可以仔细读一下，如果你懒得读，就在提示出来的时候，按Enter就好了，让你设置密码的时候，你就输入你想要的密码就行，然后继续在让你选择y/n是，Enter就好了；当一切结束的时候，你可以输入1mysql -u root 验证一下,输入密码，正常连接并可进行mysql操作 有时候由于安装命令的问题，会出现下面的报错，建议重现使用上面的操作一遍 12ERROR 2002 (HY000): Can't connect to local MySQL server through socket '/var/run/mysqld/mysqld.sock' (2) 五、配置Mysql，设置MySQL密码1.连接MySQL1mysql -u root 2.设置密码12mysql&gt; set password for 'root'@'localhost' = password('root');mysql&gt; exit 3.远程授权连接mysql12mysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY 'root' WITH GRANT OPTION;配置生效：FLUSH PRIVILEGES; 再次连接线使用：1mysql -u root -p 然后输入上面的root密码 六、将PHP和MySQL关联起来1yum search php 选择你需要的安装： 1yum -y install php-mysql 七、安装常用的PHP模块例如，GD库，curl，mbstring,… 1.安装：1yum -y install php-gd php-ldap php-odbc php-pear php-xml php-xmlrpc php-mbstring php-snmp php-soap curl curl-devel 2.重启apache服务1systemctl restart httpd.service 然后，再次在浏览器中运行info.php，你会看到安装的模块的信息； 至此，LAMP环境就搭建好了…… 4. nginx安装与配置 Nginx从安装到入门，基础篇 Nginx有三种安装方式 安装包编译安装(相对麻烦) yum源安装 使用docker安装 需确认80端口是否开放，如果是阿里云，得在控制台那边设置端口组开放 #确保防火墙是否开放80端口 如果没有 firewall-cmd --permanent --zone=public --add-port=80/tcp #重新加载 firewall-cmd --reload #查看列表 firewall-cmd --list-all 这里推荐直接使用yum源安装，因为比较快捷，方便 #yum安装nginx sudo yum install -y nginx #启动nginx sudo systemctl start nginx.service #设置开机自启动 sudo systemctl enable nginx.service #yum安装的nginx配置文件默认存放在 /etc/nginx/nginx.conf 查看 vi /etc/nginx/nginx.conf #卸载 yum remove nginx 如果可以你也可以直接使用安装包编译安装 # 下载源码 $ wget http://nginx.org/download/nginx-1.13.0.tar.gz # 解压源码 $ tar xvf nginx-1.13.0.tar.gz # 进入源码目录 $ cd nginx-1.13.0 # 配置、编译、安装 $ ./configure $ make $ make install 有时候回遇到，输入网址之后不是官方默认界面，可能是因为 有epel源的时候并且用yum install nginx 安装就会显示Welcome to nginx on Fedora!，因为epel源就是Fedora维护的 5.Centos配置Git服务器1、安装Git首先需要安装Git，可以使用yum源在线安装： 1yum install -y git 2、用户操作创建一个git用户，用来运行git服务 123adduser gitpasswd git/*输入git用户密码*/ 3、git初始化初始化git仓库：这里我们选择/git/icocos.git来作为我们的git仓库 123mkdir /git -pcd /gitgit init --bare icocos.git #初始化仓库 执行以上命令，会创建一个裸仓库，裸仓库没有工作区，因为服务器上的Git仓库纯粹是为了共享，所以不让用户直接登录到服务器上去改工作区，并且服务器上的Git仓库通常都以.git结尾。 4、然后，把owner改为git1chown git:git icocos.git -R 5、clone远程仓库在这里，Git服务器就已经搭得差不多了。下面我们在客户端clone一下远程仓库。 Mysql可以直接使用命令行，或者使用Tower 1234567git clone git@172.20.10.2:/git/icocos.gitCloning into 'icocos'...git@172.20.10.2's password: remote: Counting objects: 6, done.remote: Compressing objects: 100% (6/6), done.remote: Total 6 (delta 0), reused 0 (delta 0)Receiving objects: 100% (6/6), 2.63 MiB | 1.63 MiB/s, done. 输入git账户对应的密码 然后就可以根据公司或者具体项目做相应拓展和配置 Git实战记录总结配置与初始化实战流程1234567891011121314151617181920212223[root@centos /]# yum install -y git 已加载插件：fastestmirror, langpacks Loading mirror speeds from cached hostfile * base: mirror.jdcloud.com * extras: mirrors.163.com * updates: mirrors.163.com 软件包 git-1.8.3.1-20.el7.x86_64 已安装并且是最新版本 无须任何处理 [root@centos /]# adduser git [root@centos /]# passwd git 更改用户 git 的密码 。 新的 密码： 重新输入新的 密码： passwd：所有的身份验证令牌已经成功更新。 [root@centos /]# mkdir git [root@centos /]# cd git [root@centos git]# ls [root@centos git]# mkdir SwiftProject [root@centos git]# cd SwiftProject/ [root@centos SwiftProject]# ls [root@centos SwiftProject]# git init --bare SwiftProject.git #初始化仓库 初始化空的 Git 版本库于 /git/SwiftProject/SwiftProject.git/ [root@centos SwiftProject]# chown git:git SwiftProject.git -R 内外网访问和Clone1git@172.20.10.2:/git/SwiftProject/SwiftProject.git 然后就是根据需求创建项目提交或拉去代码 6.Centos配置SVN服务器1、subversion安装1[root@centos /]# yum install subversion 2、新建一个目录用于存储SVN目录1[root@centos /]mkdir /svn 3、SVN方式创建SVN项目目录1[root@centos svn]# svnadmin create /svn/SwiftProject/ 4、SVN项目目录查看内容12345678[root@centos svn]# ll SwiftProject总用量 8drwxr-xr-x. 2 root root 54 1月 12 01:09 confdrwxr-sr-x. 6 root root 233 1月 12 01:09 db-r--r--r--. 1 root root 2 1月 12 01:09 formatdrwxr-xr-x. 2 root root 231 1月 12 01:09 hooksdrwxr-xr-x. 2 root root 41 1月 12 01:09 locks-rw-r--r--. 1 root root 229 1月 12 01:09 README.txt 以下关于目录的说明： hooks目录：放置hook脚步文件的目录 locks目录：用来放置subversion的db锁文件和db_logs锁文件的目录，用来追踪存取文件库的客户端 format目录：是一个文本文件，里边只放了一个整数，表示当前文件库配置的版本号 conf目录：是这个仓库配置文件（仓库用户访问账户，权限） 5、SVN配置文件cd conf/进入conf目录（该svn版本库配置文件），一共有三个文件 authz文件是权限控制文件 passwd是帐号密码文件 svnserve.conf SVN服务配置文件 a、编辑用户文件passwd，新增两个用户：svn。 vim conf/passwd 12[users]svn = svn b、编辑权限文件authz，用户admin设置可读写权限，guest设置只读权限。 vim conf/authz 12[/]svn = rw c,进入所建立仓库的配置目录/svn/project/conf修改文件svnserve.conf vim conf/svnserve.conf 1234anon-access = none //匿名访问权限auth-access = write //认证用户权限password-db = passwd //密码配置文件路径，默认为同目录的passwd文件realm = My First Repository //认证标记而已 6、启动SVN服务器1svnserve -d -r /svn --listen-host 172.20.10.2 -d表示在后台运行，-r指定了服务器的根目录，这样在SVN客户端就可以用svn://172.20.10.2/SwiftProject来访问SVN服务器 1234svn co svn://172.20.10.2/SwiftProjectChecked out revision 0.lsSwiftProject 注意 使用以下命令可以查看正在运行的SVN进程ps -ef | grep svn使用命令 killall svnserve 来停止SVN服务器。 SVN实战记录总结配置与初始化实战流程12345678910111213141516171819202122[root@centos /]# yum install subversion 已加载插件：fastestmirror, langpacks Loading mirror speeds from cached hostfile * base: mirror.jdcloud.com * extras: mirrors.163.com * updates: mirrors.163.com 软件包 subversion-1.7.14-14.el7.x86_64 已安装并且是最新版本 无须任何处理 [root@centos /]# svnadmin create /svn/project[root@centos /]# cd /svn/SwiftProject [root@centos SwiftProject]# pwd /svn/SwiftProject [root@centos SwiftProject]# ls conf db format hooks locks README.txt [root@centos SwiftProject]# cd conf/ [root@centos conf]# ls authz passwd svnserve.conf [root@centos conf]# vi svnserve.conf [root@centos conf]# vim authz [root@centos conf]# vim passwd [root@centos conf]# cd / [root@centos /]# svnserve -d -r /svn --listen-host 172.20.10.2 内外网访问和Co1svn co svn://172.20.10.2/SwiftProject 然后就是根据需求创建项目提交或拉去代码 7.Centos配置GitLab服务器 安装依赖软件 1yum -y install policycoreutils openssh-server openssh-clients postfix 2.设置postfix开机自启，并启动，postfix支持gitlab发信功能 1systemctl enable postfix &amp;&amp; systemctl start postfix 3.下载gitlab安装包，然后安装 centos 6系统的下载地址 centos 7系统的下载地址 我的是centos7,所以我在https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7中找了个gitlab8.0.0版本,建议下载一个比较新的版本,我这里选了一个比较旧的版本仅仅是实验 123下载rpm包并安装:wget https://mirrors.tuna.tsinghua.edu.cn/gitlab-ce/yum/el7/gitlab-ce-8.0.0-ce.0.el7.x86_64.rpmrpm -i gitlab-ce-8.0.0-ce.0.el7.x86_64.rpm 4.修改gitlab配置文件指定服务器ip和自定义端口： vim /etc/gitlab/gitlab.rb 12345## Url on which GitLab will be reachable. ## For more details on configuring external_url see: ## https://gitlab.com/gitlab-org/omnibus-gitlab/blob/629def0a7a26e7c2326566f0758d4a27857b52a3/README.md#configuring-the-external-url-for-gitlab external_url 'http://172.20.10.2:8081' 最后，退出并保存 注: 这里设置的端口不能被占用，默认是8080端口，如果8080已经使用，请自定义其它端口，并在防火墙设置开放相对应得端口 5.重置并启动GitLab 12gitlab-ctl reconfiguregitlab-ctl restart 提示“ok: run:”表示启动成功 1234567ok: run: gitlab-git-http-server: (pid 3922) 1sok: run: logrotate: (pid 3929) 0sok: run: nginx: (pid 3936) 1sok: run: postgresql: (pid 3941) 0sok: run: redis: (pid 3950) 0sok: run: sidekiq: (pid 3955) 0sok: run: unicorn: (pid 3961) 1s 6.访问 GitLab页面 如果没有域名，直接输入服务器ip和指定端口进行访问,会出现如下页面并提示：You need to sign in before continuing. 初始账户: root 密码: 5iveL!fe注意，第一次登录最好修改密码 修改完成之后，就可以创建项目或者根据具体需求创建分组 7.设置gitlab发信功能，需要注意一点： 发信系统用的默认的postfix，smtp是默认开启的，两个都启用了，两个都不会工作。 我这里设置关闭smtp，开启postfix 12关闭smtp方法：vim /etc/gitlab/gitlab.rb找到#gitlab_rails['smtp_enable'] = true 改为 gitlab_rails['smtp_enable'] = false 修改后执行gitlab-ctl reconfigure另一种是关闭postfix，设置开启smtp + 相关教程请参考[官网](https://doc.gitlab.cc/omnibus/settings/smtp.html) 测试是否可以邮件通知：登录并添加一个用户，我这里使用qq邮箱添加一个用户 创建成功后，就可以去对应的邮箱查看邮件并设置密码 登录qq邮箱，可以收到邮件通知（如果收不到，请查看垃圾邮箱或者检查邮件是否被拦截并删除，如果有请添加到白名单并删除用户再重新添加用户就可以收到了，否则请检查邮件日志并做好相关设置） 到此，GitLab就基本配置完毕，如果有需要和可以结合GitLab，Jenkins，Fastlane实现CI和CD，当然这些一般的小公司不太会用到…… 错误一：报错502 gitlab报502 Whoops, GitLab is taking too much time to respond 配置启动后，访问gitlab，出现：502 ，Whoops, GitLab is taking too much time to respond. 出现这个问题，一般都是你本机的8080端口已经被其他的应用程序占用。 方法一： GitLab在使用的过程中，会开启8080端口，但是如果8080端口被其他的应用程序占用，则GitLab的该项服务不能使用，所以访问GitLab会失败。 方法二： 一般是权限问题，解决方法：chmod -R 755 /var/log/gitlab 如果还不行，请检查你的内存，安装使用GitLab需要至少4GB可用内存(RAM + Swap)! 由于操作系统和其他正在运行的应用也会使用内存, 所以安装GitLab前一定要注意当前服务器至少有4GB的可用内存. 少于4GB内存会出现各种诡异的问题, 而且在使用过程中也经常会出现500错误. 错误二：gitlab-ctl reconfigure 报错n itdb: could not obtain information about current user: Permission denied 1Error executing action `run` on resource 'execute[/opt/gitlab/embedded/bin/initdb -D /var/opt/gitlab/postgresql/data -E UTF8]' 根据报错信息大概锁定用户的权限问题,安装gitlab-ce会自动添加用户四个用户: 1234gitlab-www:x:497:498::/var/opt/gitlab/nginx:/bin/falsegit:x:496:497::/var/opt/gitlab:/bin/shgitlab-redis:x:495:496::/var/opt/gitlab/redis:/bin/nologingitlab-psql:x:494:495::/var/opt/gitlab/postgresql:/bin/sh google和百度都搜索不到解决方法,既然出错提示到权限问题，那么按照这个方向去查就不会有问题，后来查了文件/etc/passwd的权限是600,给予644权限后,成功解决报错问题 改成808112345# gitlab-ctl stop # vi /etc/gitlab/gitlab.rb (取消注释并修改端口) unicorn['port'] = 8801 # gitlab-ctl reconfigure (重新生成配置) # gitlab-ctl restart # lsof -i:8081(check whether unicorn has started properly) 8.Centos配置PHP各种拓展: Composer推荐composer官方更多教程与配置：https://www.phpcomposer.com/ 需要使用到curl，如果没有的话需要 1yum -y install curl ###安装 下载composer.phar1curl -sS https://getcomposer.org/installer | php 把composer.phar移动到环境下让其变成可执行1mv composer.phar /usr/local/bin/composer 测试composer12composer -V /** 输出：Composer version 1.0-dev (e64470c987fdd6bff03b85eed823eb4b865a4152) 2015-05-28 14:52:12 */ 实战操作1234567891011[root@centos /]# curl -sS https://getcomposer.org/installer | phpAll settings correct for using Composer Downloading...Composer (version 1.8.0) successfully installed to: //composer.pharUse it: php composer.phar[root@centos /]# mv composer.phar /usr/local/bin/composer[root@centos ~]# composer -V Do not run Composer as root/super user! See https://getcomposer.org/root for detailsComposer version 1.8.0 2018-12-03 10:31:16 推荐 实战Nginx与PHP（FastCGI）的安装、配置与优化 Nginx站点目录及文件URL访问控制]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[负载均衡和请求转发笔记总结]]></title>
    <url>%2F2017%2F10%2F18%2F%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%92%8C%E8%AF%B7%E6%B1%82%E8%BD%AC%E5%8F%91%E7%AC%94%E8%AE%B0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[负载均衡，请求转发七层负载均衡的实现 基于URL等应用层信息的负载均衡 用Nginx的proxy实现七层负载均衡，具有如下特点： 功能强大，运行稳定 配置简单灵活 能够自动剔除工作不正常的后端服务器 上传文件使用异步模式 支持多种分配策略(内置策略，扩展策略)，可以分配权重 内置策略：IP Hash 、 加权轮询 扩展策略： fair策略、通用hash、一致性hash 加权轮询策略： 首先将请求都分给权重高的机器，知道机器权重降低到了比其他机器低，在将请求分配给下一个权重高的机器。当所有机器都down掉时，Nginx会将所有机器标志位清成初始状态，以避免所有机器都处在timeout状态 IP Hash策略： 与轮询很类似，只是算法做了一些修改，相当于变向的轮询策略 fair策略： 根据后端的响应时间来判断负载的情况，从中选出负载最轻的机器，进行分流 通用hash、一致性hash策略： 通用hash使用Nginx内置的变量key进行hash，一致性hash采用了Nginx内置的一致性hash环，支持memcache 12345678910111213141516Nginx配置：http &#123; upstream cluster &#123; # ip hash; 指定策略为ip hash server svr1; # 配置权重 weight = 10； server svr2; server svr3; &#125; server &#123; listen 80; location / &#123; proxy_pass http://cluster; &#125; &#125;&#125; 四层负载均衡的实现 四层负载均衡是通过报文中目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。 软件(LVS)实现，LVS有三种方式：NAT、DR、TUN]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聚簇索引与非聚簇索引的区别]]></title>
    <url>%2F2017%2F10%2F17%2F%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E4%B8%8E%E9%9D%9E%E8%81%9A%E7%B0%87%E7%B4%A2%E5%BC%95%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[通常情况下，建立索引是加快查询速度的有效手段。但索引不是万能的，靠索 引并不能实现对所有数据的快速存取。事实上，如果索引策略和数据检索需求严重不符的话，建立索引反而会降低查询性能。因此在实际使用当中，应该充分考虑到 索引的开销，包括磁盘空间的开销及处理开销（如资源竞争和加锁）。例如，如果数据频繁的更新或删加，就不宜建立索引。 本文简要讨论一下聚簇索引的特点及其与非聚簇索引的区别。 聚簇索引数据库表的索引从数据存储方式上可以分为聚簇索引和非聚簇索引（又叫二级索引）两种。Innodb的聚簇索引在同一个B-Tree中保存了索引列和具体的数据，在聚簇索引中，实际的数据保存在叶子页中，中间的节点页保存指向下一层页面的指针。“聚簇”的意思是数据行被按照一定顺序一个个紧密地排列在一起存储。一个表只能有一个聚簇索引，因为在一个表中数据的存放方式只有一种。 一般来说，将通过主键作为聚簇索引的索引列，也就是通过主键聚集数据。 非聚簇索引非聚簇索引，又叫二级索引。二级索引的叶子节点中保存的不是指向行的物理指针，而是行的主键值。当通过二级索引查找行，存储引擎需要在二级索引中找到相应的叶子节点，获得行的主键值，然后使用主键去聚簇索引中查找数据行，这需要两次B-Tree查找。 建立索引：在SQL语言中，建立聚簇索引使用CREATE INDEX语句，格式为：CREATE CLUSTER INDEX index_name ON table_name(column_name1,column_name2,…); 存储特点： 聚集索引。表数据按照索引的顺序来存储的，也就是说索引项的顺序与表中记录的物理顺序一致。对于聚集索引，叶子结点即存储了真实的数据行，不再有另外单独的数据页。 在一张表上最多只能创建一个聚集索引，因为真实数据的物理顺序只能有一种。 非聚集索引。表数据存储顺序与索引顺序无关。对于非聚集索引，叶结点包含索引字段值及指向数据页数据行的逻辑指针，其行数量与数据表行数据量一致。 总结一下：聚集索引是一种稀疏索引，数据页上一级的索引页存储的是页指针，而不是行指针。而对于非聚集索引，则是密集索引，在数据页的上一级索引页它为每一个数据行存储一条索引记录。 更新表数据 1、向表中插入新数据行 ​ 如果一张表没有聚集索引，那么它被称为“堆集”（Heap）。这样的表中的数据行没有特定的顺序，所有的新行将被添加到表的末尾位置。而建立了聚簇索引的数据表则不同：最简单的情况下，插入操作根据索引找到对应的数据页，然后通过挪动已有的记录为新数据腾出空间，最后插入数据。如果数据页已满，则需要拆分数据页，调整索引指针（且如果表还有非聚集索引，还需要更新这些索引指向新的数据页）。而类似于自增列为聚集索引的，数据库系统可能并不拆分数据页，而只是简单的新添数据页。 2、从表中删除数据行 对删除数据行来说：删除行将导致其下方的数据行向上移动以填充删除记录造成的空白。如果删除的行是该数据页中的最后一行，那么该数据页将被回收，相应的索 引页中的记录将被删除。对于数据的删除操作，可能导致索引页中仅有一条记录，这时，该记录可能会被移至邻近的索引页中，原索引页将被回收，即所谓的“索引 合并”。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[架构篇——MySQL高可用集群(PXC)详解]]></title>
    <url>%2F2017%2F10%2F11%2F%E6%9E%B6%E6%9E%84%E7%AF%87%E2%80%94%E2%80%94MySQL%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4-PXC-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[在介绍PXC之前，先来看一个相关的技术：MyCatMyCat简介MyCat是阿里开源的分布式数据库分库分表中间件 MyCat是一个开源的分布式数据库系统，是一个实现了MySQL协议的服务器，前端用户可以把它看作是一个数据库代理，用MySQL客户端工具和命令行访问，而其后端可以用MySQL原生协议与多个MySQL服务器通信，也可以用JDBC协议与大多数主流数据库服务器通信 MyCat功能: 数据库读写分离(写操作在主,读操作在从数据库) 读的负载均衡(一主多从) 垂直拆分(将表分开为多个数据库) 水平拆分(对表取模拆分) MyCAT是mysql中间件，前身是阿里大名鼎鼎的Cobar，Cobar在开源了一段时间后，不了了之。于是MyCAT扛起了这面大旗，在大数据时代，其重要性愈发彰显。这篇文章主要是MyCAT的入门部署。 更多相关可以参考这里：https://www.jianshu.com/p/c6e29d724fca 下面是MyCat结合PXC的架构图 PXC简介PXC是percona公司的percona xtraDB cluster，简称PXC。它是基于Galera协议的高可用集群方案。可以实现多个节点间的数据同步复制以及读写，并且可保障数据库的服务高可用及数据强一致性。 PXC就属于一套近乎完美的MySQL高可用集群架构方案； 主要特点是： 读写强一致性(牺牲性能)PXC特性 1）同步复制，事务要么在所有节点提交或不提交。 2）多主复制，可以在任意节点进行写操作。 3）在从服务器上并行应用事件，真正意义上的并行复制。 4）节点自动配置，数据一致性，不再是异步复制。 PXC最大的优势：强一致性、无同步延迟 优点总结： 服务高可用 可以达到时时同步(并发复制)，无延迟现象发生 完全兼容MySQL 对于集群中新节点的加入(自动部署)，维护起来很简单 数据的强一致性 多个可同时读写节点，可实现写扩展，不过最好事先进行分库分表，让各个节点分别写不同的表或者库，避免让galera解决数据冲突； 不足之处总结： 只支持Innodb存储引擎 存在多节点update更新问题，也就是写放大问题 在线DDL语句，锁表问题 sst针对新节点加入的传输代价过高的问题 所有表都要有主键； 不支持LOCK TABLE等显式锁操作； 锁冲突、死锁问题相对更多； 不支持XA； 事实上，采用PXC的主要目的是解决数据的一致性问题，高可用是顺带实现的。因为PXC存在写扩大以及短板效应，并发效率会有较大损失，类似semi sync replication机制。 网络说明 基于Galere协议的高可用方案：pxc + Galera是Codership提供的多主数据同步复制机制，可以实现多个节点间的数据同步复制以及读写，并且+ 可保障数据库的服务高可用及数据一致性。 + 基于Galera的高可用方案主要有MariaDB Galera Cluster和Percona XtraDB Cluster（简称PXC），目前PXC用的会比较多一些。 + mariadb的集群原理跟PXC一样,maridb-cluster其实就是PXC，两者原理是一样的。 PXC原理Percona XtraDB Cluster（简称PXC集群）提供了MySQL高可用的一种实现方法。 1）集群是有节点组成的，推荐配置至少3个节点，但是也可以运行在2个节点上。 2）每个节点都是普通的mysql/percona服务器，可以将现有的数据库服务器组成集群，反之，也可以将集群拆分成单独的服务器。 3）每个节点都包含完整的数据副本。 PXC集群主要由两部分组成：Percona Server with XtraDB和Write Set Replication patches（使用了Galera library，一个通用的用于事务型应用的同步、多主复制插件）。 PXC会使用大概是4个端口号 3306 数据库对外服务的端口号 4444 请求SST SST: 指数据一个镜象传输 xtrabackup , rsync ,mysqldump 4567 : 组成员之间进行沟通的一个端口号 4568 : 传输IST用的。相对于SST来说的一个增量。 注：安装PXC过程中， iptables 禁掉 ，selinux 也禁掉 PXC的操作流程： 首先客户端先发起一个事务，该事务先在本地执行，执行完成之后就要发起对事务的提交操作了。 在提交之前需要将产生的复制写集广播出去，然后获取到一个全局的事务ID号，一并传送到另一个节点上面。 通过合并数据之后，发现没有冲突数据，执行apply_cd和commit_cb动作，否则就需要取消此次事务的操作。 当前server节点通过验证之后，执行提交操作，并返回OK，如果验证没通过，则执行回滚。 在生产中至少要有3个节点的集群环境，如果其中一个节点没有验证通过，出现了数据冲突，那么此时采取的方式就是讲出现不一致的节点踢出集群环境，而且它自己会执行shutdown命令，自动关机。 实战部署环境： CentOS7.X 1、执行 命令 vi /etc/selinux/configSELINUX=disabled #修改该项为disabled 2、执行命令 setenforce 03、查看防火墙是否开启 systemctl status firewalld如果防火墙是开启状态，则开放端口 3306 、4444、4567、4568 firewall-cmd --add-port=3306/tcp --permanent #开放了3306端口 开放完4个端口后，重新加载防火墙规则 firewall-cmd --reload 4、安装Persona仓库yum install http://www.percona.com/downloads/percona-release/redhat/0.1-4/percona-release-0.1-4.noarch.rpm 5、安装PXC（保证服务器没有装MySQL）卸载MySQL 参考链接：https://blog.csdn.net/tjcyjd/article/details/52189182yum install Percona-XtraDB-Cluster-57 6、开启PXC服务service mysql start 7、查看安装数据库的临时密码并记住grep &apos;temporary password&apos; /var/log/mysqld.log 8、登录MySQL数据库mysql -u root -p 输入临时密码, 登录成功后修改密码 ALTER USER &apos;root&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;你的密码&apos;; 9、停止MySQL服务service mysql stop （某些版本使用mysqld） 10、配置节点vi /etc/percona-xtradb-cluster.conf.d/wsrep.cnf 修改配置文件 # Cluster connection URL contains IPs of nodes #If no IP is found, this implies that a new cluster needs to be created, #in order to do that you need to bootstrap this node #集群中节点的IP地址（本机填最后） wsrep_cluster_address=gcomm://ip地址,IP地址,IP地址（用,号隔开） # In order for Galera to work correctly binlog format should be ROW binlog_format=ROW # MyISAM storage engine has only experimental support default_storage_engine=InnoDB # Slave thread to use wsrep_slave_threads= 8 wsrep_log_conflicts # This changes how InnoDB autoincrement locks are managed and is a requirement for Galera innodb_autoinc_lock_mode=2 # Node IP address #当前节点IP wsrep_node_address=IP地址 # Cluster name #集群名称 wsrep_cluster_name=pxc-cluster #If wsrep_node_name is not specified, then system hostname will be used #当前节点名称 wsrep_node_name=pxc-cluster-node-1 #pxc_strict_mode allowed values: DISABLED,PERMISSIVE,ENFORCING,MASTER #不使用实验功能 pxc_strict_mode=ENFORCING # SST method #状态快照传输（sst）方法，官方建议 wsrep_sst_method=xtrabackup-v2 #Authentication for SST method #用户凭证（mysql的用户名和密码） wsrep_sst_auth=&quot;用户名:密码&quot; 剩下的节点修改当前节点名、当前节点IP、集群中的节点IP，其他相同 注：1—10步骤 每个节点都要配置一次 11、初始化集群节点其中一个节点使用 systemctl start mysql@bootstrap.service 启动 登录mysqlmysql -u root -p 开启 wsrep_causal_reads set wsrep_causal_reads =1; 12、创建配置文件中对应的用户 所有节点的IP都要创建 创建用户： CREATE USER &apos;用户名&apos;@&apos;localhost&apos; IDENTIFIED BY &apos;密码&apos;; 刷新权限： GRANT all privileges ON *.* TO &apos;用户名&apos;@&apos;localhost&apos; ; FLUSH PRIVILEGES; 创建用户： CREATE USER &apos;用户名&apos;@&apos;当前需要访问数据库的IP地址&apos; IDENTIFIED BY &apos;密码&apos;; 刷新权限： GRANT all privileges ON *.* TO &apos;用户名&apos;@&apos;当前节点IP地址&apos; ; FLUSH PRIVILEGES; 13、其他节点使用 service mysql start 启动 ，登录mysql，配置wsrep_causal_reds，set wsrep_causal_reads =1;14、其他节点启动成功后在引导节点（使用 systemctl start mysql@bootstrap.service 命令启动的节点）验证集群： show status like &apos;wsrep%&apos;; 15、节点数据同步验证在当前节点创建一个数据库 CREATE DATABASE percona; 启动其他节点的数据库服务，进去后会发现新建的数据库，同理 其他节点创建的数据 当前节点也能看到 注意：服务的启动和停止要对应 service mysql stop ------&gt; 启动时用service mysql start 或者 systemctl stop mysql@bootstrap.service -----&gt; 启用是用 systemctl start mysql@bootstrap.service 更多相关实战配置可以参考这里：https://www.jianshu.com/p/0b7c050dfab6 推荐 带你玩转Mysql高可用方案–PXC https://blog.csdn.net/zisefeizhu/article/details/81873466 Docker搭建PXC集群 https://blog.csdn.net/weixin_41141219/article/details/82767832 MySQL高可用方案－PXC环境部署记录: 详细教程 http://www.cnblogs.com/kevingrace/p/5685371.html]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>MySQL</tag>
        <tag>PXC集群</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[架构篇——MySQL主从复制(Master-Slave)详解]]></title>
    <url>%2F2017%2F10%2F09%2F%E6%9E%B6%E6%9E%84%E7%AF%87%E2%80%94%E2%80%94MySQL%E4%B8%BB%E4%BB%8E%E5%A4%8D%E5%88%B6-Master-Slave-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[主从Mysql主从又叫Replication、AB复制(不同于PXC)。简单讲就是A与B两台机器做主从后，在A上写数据，另外一台B也会跟着写数据，实现数据实时同步 mysql主从是基于binlog，主上需开启binlog才能进行主从 主从过程大概有3个步骤 主将更改操作记录到binlog里 从将主的binlog事件（sql语句） 同步本机上并记录在relaylog里 从根据relaylog里面的sql语句按顺序执行 主从作用实时灾备，用于故障切换 读写分离，提供查询服务 备份，避免影响业务 主从形式 一主一从 主主复制 一主多从—扩展系统读取的性能，因为读是在从库读取的 多主一从—5.7版本开始支持 联级复制 MySQL数据库自身提供的主从复制功能可以方便的实现数据的多处自动备份，实现数据库的拓展。多个数据备份不仅可以加强数据的安全性，通过实现读写分离还能进一步提升数据库的负载性能。 原理： MySQL之间数据复制的基础是二进制日志文件（binary log file）。一台MySQL数据库一旦启用二进制日志后，其作为master，它的数据库中所有操作都会以“事件”的方式记录在二进制日志中，其他数据库作为slave通过一个I/O线程与主服务器保持通信，并监控master的二进制日志文件的变化，如果发现master二进制日志文件发生变化，则会把变化复制到自己的中继日志中，然后slave的一个SQL线程会把相关的“事件”执行到自己的数据库中，以此实现从数据库和主数据库的一致性，也就实现了主从复制。 主库将所有的写操作记录在binlog日志中，并生成log dump线程，将binlog日志传给从库的I/O线程 从库生成两个线程，一个是I/O线程，另一个是SQL线程 I/O线程去请求主库的binlog日志，并将binlog日志中的文件写入relay log（中继日志）中 SQL线程会读取relay loy中的内容，并解析成具体的操作，来实现主从的操作一致，达到最终数据一致的目的 配置实现MySQL主从复制需要进行的配置： 主服务器： 开启二进制日志 配置唯一的server-id 获得master二进制日志文件名及位置 创建一个用于slave和master通信的用户账号 从服务器： 配置唯一的server-id 使用master分配的用户账号读取master二进制日志 启用slave服务 具体实现过程如下：主从复制配置步骤： 确保从数据库与主数据库里的数据一致 在主数据库里创建一个同步账户授权给从数据库使用 配合主数据库（修改配置文件） 配置从数据库（修改配置文件） 一、准备工作： 主从数据库版本最好一致 主从数据库内数据保持一致 搭建两台MYSQL服务器，一台作为主服务器，一台作为从服务器，主服务器进行写操作，从服务器进行读操作 + 主数据库：192.168.0.1 /Linux-MySQL + 从数据库：192.168.0.2 /Linux-MySQL 二、主数据库master修改：1.修改mysql配置 找到主数据库的配置文件my.cnf(或者my.ini)，我的在/etc/mysql/my.cnf,在[mysqld]部分插入如下两行： [mysqld] log-bin=mysql-bin #开启二进制日志 server-id=1 #设置server-id 2.重启mysql，创建用于同步的用户账号 打开mysql会话shell mysql -hlocalhost -uname -ppassword 创建用户：用户：rel1密码：slavepass 3.授权 主服务器授权从服务器特定账号登录 mysql&gt; CREATE USER &apos;repl&apos;@&apos;192.168.0.2&apos; IDENTIFIED BY &apos;slavepass&apos;;#创建用户 mysql&gt; GRANT REPLICATION SLAVE ON *.* TO &apos;repl&apos;@&apos;192.168.0.2&apos;;#分配权限 mysql&gt;flush privileges; #刷新权限 4.查看master状态，记录二进制文件名(mysql-bin.000003)和位置(73)： mysql &gt; SHOW MASTER STATUS; +------------------+----------+--------------+------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +------------------+----------+--------------+------------------+ | mysql-bin.000003 | 73 | test | manual,mysql | +------------------+----------+--------------+------------------+ 三、从服务器slave修改：1.修改mysql配置 同样找到my.cnf配置文件，添加server-id [mysqld] server-id=2 #设置server-id，必须唯一 2.重启mysql，打开mysql会话，执行同步SQL语句(需要主服务器主机名，登陆凭据，二进制文件的名称和位置)：复制代码 mysql&gt; CHANGE MASTER TO -&gt; MASTER_HOST=&apos;192.168.0.1&apos;, -&gt; MASTER_USER=&apos;rep1&apos;, -&gt; MASTER_PASSWORD=&apos;slavepass&apos;, -&gt; MASTER_LOG_FILE=&apos;mysql-bin.000003&apos;, -&gt; MASTER_LOG_POS=73; 3.启动slave同步进程： mysql&gt;start slave; 4.查看slave状态： mysql&gt; show slave status\G; *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.0.1 Master_User: rep1 Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000013 Read_Master_Log_Pos: 11662 Relay_Log_File: mysqld-relay-bin.000022 Relay_Log_Pos: 11765 Relay_Master_Log_File: mysql-bin.000013 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: ... 当Slave_IO_Running和Slave_SQL_Running都为YES的时候就表示主从同步设置成功了。 接下来就可以进行一些验证了，比如在主master数据库的test数据库的一张表中插入一条数据，在slave的test库的相同数据表中查看是否有新增的数据即可验证主从复制功能是否有效，还可以关闭slave（mysql&gt;stop slave;）,然后再修改master，看slave是否也相应修改（停止slave后，master的修改不会同步到slave），就可以完成主从复制功能的验证了。 还可以用到的其他相关参数： master开启二进制日志后默认记录所有库所有表的操作，可以通过配置来指定只记录指定的数据库甚至指定的表的操作，具体在mysql配置文件的[mysqld]可添加修改如下选项： # 不同步哪些数据库 binlog-ignore-db = mysql binlog-ignore-db = test binlog-ignore-db = information_schema # 只同步哪些数据库，除此之外，其他不同步 binlog-do-db = game 如之前查看master状态时就可以看到只记录了test库，忽略了manual和mysql库。 操作流程汇总关闭防火墙以SELINUX[root@icocos ~]# systemctl stop firewalld [root@icocos ~]# systemctl disable firewalld [root@icocos ~]# sed -ri &apos;s/(SELINUX=).*/\1disabled/g&apos; /etc/selinux/config [root@icocos ~]# setenforce 0 安装mysql安装依赖包 [root@icocos ~]# yum -y install ncurses-devel openssl-devel openssl cmake mariadb-devel 创建用户和组[root@icocos ~]# groupadd -r -g 306 mysql [root@icocos ~]# useradd -M -s /sbin/nologin -g 306 -u 306 mysql 下载二进制格式的mysql软件包[root@icocos ~]# cd /usr/src/ [root@icocos src]#wget https://downloads.mysql.com/archives/get/file/mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz 解压软件至/usr/local/[root@icocos src]# ls debug kernels mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz [root@icocos src]# tar xf mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz -C /usr/local/ [root@icocos src]# ls /usr/local/ bin etc games include lib lib64 libexec mysql-5.7.22-linux-glibc2.12-x86_64 sbin share src [root@icocos src]# cd /usr/local/ [root@icocos local]# ln -sv mysql-5.7.22-linux-glibc2.12-x86_64/ mysql &quot;mysql&quot; -&gt; &quot;mysql-5.7.22-linux-glibc2.12-x86_64/&quot; [root@icocos local]# ll 总用量 0 drwxr-xr-x. 2 root root 6 11月 5 2016 bin drwxr-xr-x. 2 root root 6 11月 5 2016 etc drwxr-xr-x. 2 root root 6 11月 5 2016 games drwxr-xr-x. 2 root root 6 11月 5 2016 include drwxr-xr-x. 2 root root 6 11月 5 2016 lib drwxr-xr-x. 2 root root 6 11月 5 2016 lib64 drwxr-xr-x. 2 root root 6 11月 5 2016 libexec lrwxrwxrwx. 1 root root 36 9月 7 22:20 mysql -&gt; mysql-5.7.22-linux-glibc2.12-x86_64/ drwxr-xr-x. 9 root root 129 9月 7 22:19 mysql-5.7.22-linux-glibc2.12-x86_64 drwxr-xr-x. 2 root root 6 11月 5 2016 sbin drwxr-xr-x. 5 root root 49 9月 3 23:02 share drwxr-xr-x. 2 root root 6 11月 5 2016 src 修改目录/usr/locaal/mysql的属主属组[root@icocos local]# chown -R mysql.mysql /usr/local/mysql [root@icocos local]# ll /usr/local/mysql -d lrwxrwxrwx. 1 mysql mysql 36 9月 7 22:20 /usr/local/mysql -&gt; mysql-5.7.22-linux-glibc2.12-x86_64/ 添加环境变量[root@icocos local]# ls /usr/local/mysql bin COPYING docs include lib man README share support-files [root@icocos local]# cd [root@icocos ~]# echo &apos;export PATH=/usr/local/mysql/bin:$PATH&apos; &gt; /etc/profile.d/mysql.sh [root@icocos ~]# . /etc/profile.d/mysql.sh [root@icocos ~]# echo $PATH /usr/local/mysql/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin 建立数据存放目录[root@icocos ~]# cd /usr/local/mysql [root@icocos mysql]# mkdir /opt/data [root@icocos mysql]# chown -R mysql.mysql /opt/data/ [root@icocos mysql]# ll /opt/ 总用量 0 drwxr-xr-x. 2 mysql mysql 6 9月 7 22:25 data 初始化数据库[root@icocos mysql]# /usr/local/mysql/bin/mysqld --initialize --user=mysql --datadir=/opt/data/ //这个命令的最后会生成一个临时密码，此处密码是1EbNA-k*BtKo 配置mysql[root@icocos ~]# ln -sv /usr/local/mysql/include/ /usr/local/include/mysql &quot;/usr/local/include/mysql&quot; -&gt; &quot;/usr/local/mysql/include/&quot; [root@icocos ~]# echo &apos;/usr/local/mysql/lib&apos; &gt; /etc/ld.so.conf.d/mysql.conf [root@icocos ~]# ldconfig -v 生成配置文件[root@icocos ~]# cat &gt; /etc/my.cnf &lt;&lt;EOF &gt; [mysqld] &gt; basedir = /usr/local/mysql &gt; datadir = /opt/data &gt; socket = /tmp/mysql.sock &gt; port = 3306 &gt; pid-file = /opt/data/mysql.pid &gt; user = mysql &gt; skip-name-resolve &gt; EOF 配置服务启动脚本[root@icocos ~]# cp -a /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld [root@icocos ~]# sed -ri &apos;s#^(basedir=).*#\1/usr/local/mysql#g&apos; /etc/init.d/mysqld [root@icocos ~]# sed -ri &apos;s#^(datadir=).*#\1/opt/data#g&apos; /etc/init.d/mysqld 启动mysql[root@icocos ~]# service mysqld start Starting MySQL.Logging to &apos;/opt/data/icocos.err&apos;. .. SUCCESS! [root@icocos ~]# ps -ef|grep mysql root 4897 1 0 22:38 pts/2 00:00:00 /bin/sh /usr/local/mysql/bin/mysqld_safe --datadir=/opt/data --pid-file=/opt/data/mysql.pid mysql 5075 4897 6 22:38 pts/2 00:00:01 /usr/local/mysql/bin/mysqld --basedir=/usr/local/mysql --datadir=/opt/data --plugin-dir=/usr/local/mysql/lib/plugin --user=mysql --log-error=icocos.err --pid-file=/opt/data/mysql.pid --socket=/tmp/mysql.sock --port=3306 root 5109 4668 0 22:38 pts/2 00:00:00 grep --color=auto mysql [root@icocos ~]# ss -antl State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* LISTEN 0 100 127.0.0.1:25 *:* LISTEN 0 128 :::22 :::* LISTEN 0 100 ::1:25 :::* LISTEN 0 80 :::3306 :::* 修改密码使用临时密码修改 [root@icocos ~]# mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 2 Server version: 5.7.22 Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; set password = password(&apos;123456&apos;); Query OK, 0 rows affected, 1 warning (0.00 sec) mysql&gt; quit Bye mysql主从配置确保从数据库与主数据库的数据一样先在主数据库创建所需要同步的库和表 [root@icocos ~]# mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 4 Server version: 5.7.22 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. Al Oracle is a registered trademark of Oracle Corporation and affiliates. Other names may be trademarks of their respect owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the curr mysql&gt; create database yan; Query OK, 1 row affected (0.00 sec) mysql&gt; create database lisi; Query OK, 1 row affected (0.00 sec) mysql&gt; create database wangwu; Query OK, 1 row affected (0.00 sec) mysql&gt; use yan; Database changed mysql&gt; create table tom (id int not null,name varchar(100)not null ,age tinyint); Query OK, 0 rows affected (11.83 sec) mysql&gt; insert tom (id,name,age) values(1,&apos;zhangshan&apos;,20),(2,&apos;wangwu&apos;,7),(3,&apos;lisi&apos;,23); Query OK, 3 rows affected (0.07 sec) Records: 3 Duplicates: 0 Warnings: 0 mysql&gt; select * from tom; +----+-----------+------+ | id | name | age | +----+-----------+------+ | 1 | zhangshan | 20 | | 2 | wangwu | 7 | | 3 | lisi | 23 | +----+-----------+------+ 3 rows in set (0.00 sec) 备份主库备份主库时需要另开一个终端，给数据库上读锁，避免在备份期间有其他人在写入导致数据同步的不一致 [root@icocos ~]# mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 5 Server version: 5.7.22 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; flush tables with read lock; Query OK, 0 rows affected (0.76 sec) 此锁表的终端必须在备份完成以后才能退出（退出锁表失效） 备份主库并将备份文件传送到从库[root@icocos ~]# mysqldump -uroot -p123456 --all-databases &gt; /opt/all-20180907.sql mysqldump: [Warning] Using a password on the command line interface can be insecure. [root@icocos ~]# ls /opt/ all-20180907.sql data [root@icocos ~]# scp /opt/all-20180907.sql root@192.168.0.2:/opt/ The authenticity of host &apos;192.168.0.2 (192.168.0.2)&apos; can&apos;t be established. ECDSA key fingerprint is SHA256:7mLj77SFk7sPkhjpMPfdK3nZ98hOuyP4OKzjXeijSJ0. ECDSA key fingerprint is MD5:a0:1b:eb:7f:f0:b6:7b:73:97:91:4c:f3:b1:89:d8:ea. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added &apos;192.168.0.2&apos; (ECDSA) to the list of known hosts. root@192.168.0.2&apos;s password: all-20180907.sql 100% 784KB 783.3KB/s 00:01 解除主库的锁表状态，直接退出交互式界面即可mysql&gt; quit Bye 在从库上恢复主库的备份并查看是否与主库的数据保持一致[root@icocos ~]# mysql -uroot -p123456 &lt; /opt/all-20180907.sql mysql: [Warning] Using a password on the command line interface can be insecure. [root@icocos ~]# mysql -uroot -p123456 mysql: [Warning] Using a password on the command line interface can be insecure. Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 4 Server version: 5.7.22 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | lisi | | mysql | | performance_schema | | sys | | wangwu | | yan | +--------------------+ 7 rows in set (0.18 sec) mysql&gt; use yan; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql&gt; select * from tom; +----+-----------+------+ | id | name | age | +----+-----------+------+ | 1 | zhangshan | 20 | | 2 | wangwu | 7 | | 3 | lisi | 23 | +----+-----------+------+ 3 rows in set (0.06 sec) 在主数据库创建一个同步账户授权给从数据使用[root@icocos ~]# mysql -uroot -p Enter password: Welcome to the MySQL monitor. Commands end with ; or \g. Your MySQL connection id is 7 Server version: 5.7.22 MySQL Community Server (GPL) Copyright (c) 2000, 2018, Oracle and/or its affiliates. All rights reserved. Oracle is a registered trademark of Oracle Corporation and/or its affiliates. Other names may be trademarks of their respective owners. Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement. mysql&gt; create user &apos;repl&apos;@&apos;192.168.0.2&apos; identified by &apos;123456&apos;; Query OK, 0 rows affected (5.50 sec) mysql&gt; grant replication slave on *.* to &apos;repl&apos;@&apos;192.168.0.2&apos;; Query OK, 0 rows affected (0.04 sec) mysql&gt; flush privileges; Query OK, 0 rows affected (0.09 sec) 配置主数据库编辑配置文件[root@icocos ~]# vim /etc/my.cnf [root@icocos ~]# cat /etc/my.cnf [mysqld] basedir = /usr/local/mysql datadir = /opt/data socket = /tmp/mysql.sock port = 3306 pid-file = /opt/data/mysql.pid user = mysql skip-name-resolve //添加以下内容 log-bin=mysql-bin //启用binlog日志 server-id=1 //主数据库服务器唯一标识符 主的必须必从大 log-error=/opt/data/mysql.log 重启mysql服务[root@icocos ~]# service mysqld restart Shutting down MySQL..... SUCCESS! Starting MySQL.Logging to &apos;/opt/data/mysql.log&apos;. ............................... SUCCESS! [root@icocos ~]# ss -antl State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* LISTEN 0 100 127.0.0.1:25 *:* LISTEN 0 128 :::22 :::* LISTEN 0 100 ::1:25 :::* LISTEN 0 80 :::3306 :::* 查看主库的状态mysql&gt; show master status; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000001 | 154 | | | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 配置从数据库编辑配置文件 [root@icocos ~]# cat /etc/my.cnf [mysqld] basedir = /usr/local/mysql datadir = /opt/data socket = /tmp/mysql.sock port = 3306 pid-file = /opt/data/mysql.pid user = mysql skip-name-resolve //添加以下内容： server-id=2 //设置从库的唯一标识符 从的必须比主小 relay-log=mysql-relay-bin //启用中继日志relay log error-log=/opt/data/mysql.log 重启从库的mysql服务[root@icocos ~]# service mysqld restart Shutting down MySQL.. SUCCESS! Starting MySQL.. SUCCESS! [root@icocos ~]# ss -antl State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 128 *:22 *:* LISTEN 0 100 127.0.0.1:25 *:* LISTEN 0 128 :::22 :::* LISTEN 0 100 ::1:25 :::* LISTEN 0 80 :::3306 :::* 配置并启动主从复制mysql&gt; change master to -&gt; master_host=&apos;192.168.0.1&apos;, -&gt; master_user=&apos;repl&apos;, -&gt; master_password=&apos;123456&apos;, -&gt; master_log_file=&apos;mysql-bin.000001&apos;, -&gt; master_log_pos=154; Query OK, 0 rows affected, 2 warnings (0.28 sec) 查看从服务器状态mysql&gt; show slave status\G; *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 192.168.0.1 Master_User: repl Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 154 Relay_Log_File: mysql-relay-bin.000003 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes //此处必须是yes Slave_SQL_Running: Yes //此处必须是yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 154 Relay_Log_Space: 527 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: 5abf1791-b2af-11e8-b6ad-000c2980fbb4 Master_Info_File: /opt/data/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) ERROR: No query specified 测试验证在主服务器的yan库的tom表插入数据:mysql&gt; use yan; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql&gt; select * from tom; +----+-----------+------+ | id | name | age | +----+-----------+------+ | 1 | zhangshan | 20 | | 2 | wangwu | 7 | | 3 | lisi | 23 | +----+-----------+------+ 3 rows in set (0.09 sec) mysql&gt; insert tom(id,name,age) value (4,&quot;yyl&quot;,18); Query OK, 1 row affected (0.14 sec) mysql&gt; select * from tom; +----+-----------+------+ | id | name | age | +----+-----------+------+ | 1 | zhangshan | 20 | | 2 | wangwu | 7 | | 3 | lisi | 23 | | 4 | yyl | 18 | +----+-----------+------+ 4 rows in set (0.00 sec) 在从数据库查看是否数据同步mysql&gt; use yan; Reading table information for completion of table and column names You can turn off this feature to get a quicker startup with -A Database changed mysql&gt; select * from tom; +----+-----------+------+ | id | name | age | +----+-----------+------+ | 1 | zhangshan | 20 | | 2 | wangwu | 7 | | 3 | lisi | 23 | | 4 | yyl | 18 | +----+-----------+------+ 4 rows in set (0.00 sec)]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>MySQL</tag>
        <tag>主从复制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常用负载均衡调度算法实现]]></title>
    <url>%2F2017%2F10%2F07%2F%E5%B8%B8%E7%94%A8%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[介绍用 PHP 实现几种负载均衡调度算法，详细见 在分布式系统中，为了实现负载均衡，必然会涉及到负载调度算法，如 Nginx 和 RPC 服务发现等场景。常见的负载均衡算法有 轮询、源地址 Hash、最少连接数，而 轮询 是最简单且应用最广的算法。 调度算法3种常见的轮询调度算法，分别为 简单轮询、加权轮询、平滑加权轮询 普通轮询（general Round Robin)namespace Robin; class Robin implements RobinInterface { private $services = array(); private $total; private $currentPos = -1; public function init(array $services) { $this-&gt;services = $services; $this-&gt;total = count($services); } public function next() { // 已调度完一圈,重置currentPos值为第一个实例位置 $this-&gt;currentPos = ($this-&gt;currentPos + 1) % $this-&gt;total; return $this-&gt;services[$this-&gt;currentPos]; } } 加权轮询（Weighted Round Robin)namespace Robin; class WeightedRobin implements RobinInterface { private $services = array(); private $total; private $currentPos = -1; private $currentWeight; public function init(array $services) { foreach ($services as $ip =&gt; $weight) { $this-&gt;services[] = [ &apos;ip&apos; =&gt; $ip, &apos;weight&apos; =&gt; $weight, ]; } $this-&gt;total = count($this-&gt;services); } public function next() { $i = $this-&gt;currentPos; while (true) { $i = ($i + 1) % $this-&gt;total; // 已全部被遍历完一次 if (0 === $i) { // 减currentWeight $this-&gt;currentWeight -= $this-&gt;getGcd(); // 赋值currentWeight为0,回归到初始状态 if ($this-&gt;currentWeight &lt;= 0) { $this-&gt;currentWeight = $this-&gt;getMaxWeight(); } } // 直到当前遍历实例的weight大于或等于currentWeight if ($this-&gt;services[$i][&apos;weight&apos;] &gt;= $this-&gt;currentWeight) { $this-&gt;currentPos = $i; return $this-&gt;services[$this-&gt;currentPos][&apos;ip&apos;]; } } } /** * 求两数的最大公约数(基于欧几里德算法,可使用gmp_gcd()) * * @param integer $a * @param integer $b * * @return integer */ private function gcd($a, $b) { $rem = 0; while ($b) { $rem = $a % $b; $a = $b; $b = $rem; } return $a; } /** * 获取最大公约数 * * @return integer */ private function getGcd() { $gcd = $this-&gt;services[0][&apos;weight&apos;]; for ($i = 0; $i &lt; $this-&gt;total; $i++) { $gcd = $this-&gt;gcd($gcd, $this-&gt;services[$i][&apos;weight&apos;]); } return $gcd; } /** * 获取最大权重值 * * @return integer */ private function getMaxWeight() { $maxWeight = 0; foreach ($this-&gt;services as $node) { if ($node[&apos;weight&apos;] &gt;= $maxWeight) { $maxWeight = $node[&apos;weight&apos;]; } } return $maxWeight; } } 平滑加权轮询（Smooth Weighted Round Robin)namespace Robin; class SmoothWeightedRobin implements RobinInterface { /** * 服务群组 * @var array */ private $services = array(); /** * 同时累加所有peer的effective_weight，保存为total * @var */ private $total; /** * 后端目前的权重 * @var int */ private $currentPos = -1; /** * 初始化 * @param array $services */ public function init(array $services) { foreach ($services as $ip =&gt; $weight) { $this-&gt;services[] = [ &apos;ip&apos; =&gt; $ip, &apos;weight&apos; =&gt; $weight, &apos;current_weight&apos; =&gt; $weight, ]; } $this-&gt;total = count($this-&gt;services); } public function next() { // 获取最大当前有效权重的实例位置 $this-&gt;currentPos = $this-&gt;getMaxCurrentWeightPos(); // 当前权重减去权重和 $currentWeight = intval($this-&gt;getCurrentWeight($this-&gt;currentPos)) - intval($this-&gt;getSumWeight()); $this-&gt;setCurrentWeight($this-&gt;currentPos, $currentWeight); // 每个实例的当前有效权重加上配置权重 $this-&gt;recoverCurrentWeight(); return $this-&gt;services[$this-&gt;currentPos][&apos;ip&apos;]; } /** * 获取最大当前有效权重实例位置 * @return int */ public function getMaxCurrentWeightPos() { $currentWeight = $pos = 0; foreach ($this-&gt;services as $index =&gt; $service) { if ($service[&apos;current_weight&apos;] &gt; $currentWeight) { $currentWeight = $service[&apos;current_weight&apos;]; $pos = $index; } } return $pos; } /** * 配置权重和，累加所有后端的effective_weight * * @return integer */ public function getSumWeight() { $sum = 0; foreach ($this-&gt;services as $service) { $sum += intval($service[&apos;weight&apos;]); } return $sum; } /** * 设置当前有效权重 * @param integer $pos * @param integer $weight */ public function setCurrentWeight($pos, $weight) { $this-&gt;services[$pos][&apos;current_weight&apos;] = $weight; } /** * 获取当前有效权重 * * @param integer $pos * @return integer */ public function getCurrentWeight($pos) { return $this-&gt;services[$pos][&apos;current_weight&apos;]; } /** * 用配置权重调整当前有效权重 */ public function recoverCurrentWeight() { foreach ($this-&gt;services as $index =&gt; &amp;$service) { $service[&apos;current_weight&apos;] += intval($service[&apos;weight&apos;]); } } } 调度算法接口服务namespace Robin; interface RobinInterface { /** * 初始化服务权重 * * @param array $services */ public function init(array $services); /** * 获取一个服务 * * @return string */ public function next(); } 加权轮询 算法虽然通过配置实例权重，解决了 简单轮询 的资源利用问题，但是它还是存在一个比较明显的 缺陷。为了解决加权轮询调度不均匀的缺陷，一些人提出了 平滑加权轮询 调度算法，它会生成的更均匀的调度序列 {a, a, b, a, c, a, a}。对于神秘的平滑加权轮询算法，我将在后续文章中详细介绍它的原理和实现。 Installationlog1composer require tinywan/load-balancing Basic Usage12345678910111213141516171819202122232425262728293031323334353637383940414243// 服务器数$services = [ '192.168.10.1' =&gt; 6, '192.168.10.2' =&gt; 2, '192.168.10.3' =&gt; 1, '192.168.10.4' =&gt; 1,]; ------------- 1.简单轮询 ------------- $robin = new \Robin\Robin();$robin-&gt;init($services);$nodes = [];for ($i = 1; $i &lt;= count($services); $i++) &#123; $node = $robin-&gt;next(); $nodes[$i] = $node;&#125;var_export($nodes); ------------- 2.加权轮询 ------------- $robin = new \Robin\WeightedRobin();$robin-&gt;init($services);$nodes = [];for ($i = 1; $i &lt;= 10; $i++) &#123; $node = $robin-&gt;next(); $nodes[$i] = $node;&#125;var_export($nodes); ------------- 3.平滑加权轮询 ------------- $robin = new \Robin\SmoothWeightedRobin();$robin-&gt;init($services);$nodes = [];$sumWeight = $robin-&gt;getSumWeight();for ($i = 1; $i &lt;= $sumWeight; $i++) &#123; $node = $robin-&gt;next(); $nodes[$i] = $node;&#125;var_export($nodes); Composer管理安装提示错误： Could not find package tinywan/load-polling in a version matching 1.0 尝试改成Packagist的地址 https://packagist.org log123456&quot;repositories&quot;: &#123; &quot;packagist&quot;: &#123; &quot;type&quot;: &quot;composer&quot;, &quot;url&quot;: &quot;https://packagist.org&quot; &#125;&#125; 要使你发布的最新包可以使用，请使用以上的镜像源，为了学习 参考 负载均衡算法 负载均衡算法 — 轮询 负载均衡算法 — 平滑加权轮询 Nginx的负载均衡 - 加权轮询 (Weighted Round Robin) 上篇 Nginx的负载均衡 - 加权轮询 (Weighted Round Robin) 下篇 Composer/Packagist包]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于mysql最左前缀原则]]></title>
    <url>%2F2017%2F09%2F29%2F%E5%85%B3%E4%BA%8Emysql%E6%9C%80%E5%B7%A6%E5%89%8D%E7%BC%80%E5%8E%9F%E5%88%99%2F</url>
    <content type="text"><![CDATA[背景知识： mysql中可以使用explain关键字来查看sql语句的执行计划。 最左前缀原则主要使用在联合索引中 数据库版本Mysql5.5.53 最左前缀原则mysql建立多列索引（联合索引）有最左前缀的原则，即最左优先，如： 如果有一个2列的索引(col1,col2),则已经对(col1)、(col1,col2)上建立了索引； 如果有一个3列索引(col1,col2,col3)，则已经对(col1)、(col1,col2)、(col1,col2,col3)上建立了索引； 1、b+树的数据项是复合的数据结构，比如(name,age,sex)的时候，b+树是按照从左到右的顺序来建立搜索树的，比如当(张三,20,F)这样的数据来检索的时候，b+树会优先比较name来确定下一步的所搜方向，如果name相同再依次比较age和sex，最后得到检索的数据；但当(20,F)这样的没有name的数据来的时候，b+树就不知道第一步该查哪个节点，因为建立搜索树的时候name就是第一个比较因子，必须要先根据name来搜索才能知道下一步去哪里查询。 2、比如当(张三,F)这样的数据来检索时，b+树可以用name来指定搜索方向，但下一个字段age的缺失，所以只能把名字等于张三的数据都找到，然后再匹配性别是F的数据了， 这个是非常重要的性质，即索引的最左匹配特性。（这种情况无法用到联合索引） 关于最左前缀的使用，有下面两条说明： 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 联合索引有一个最左前缀原则，所以建立联合索引的时候，这个联合索引的字段顺序非常重要 下面写了例子说明这个： CREATE TABLE `test_myisam` ( `id` int(11) NOT NULL AUTO_INCREMENT, `conference_id` varchar(200) NOT NULL, `account` varchar(100) DEFAULT NULL, `status` int(2) DEFAULT NULL COMMENT &apos;0:invite, 1:cancel_invite, 2:decline, 3:connect&apos;, `duration` bigint(20) unsigned DEFAULT NULL, `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP, PRIMARY KEY (`id`) ) ENGINE=myisam AUTO_INCREMENT=1 DEFAULT CHARSET=utf8; 以上表结构，我想通过三列进行查询 account ,status,create_time进行查询统计。 如何建立索引？因为我们有可能按照acccount单独统计，或者按照account status，或者是account，status，create_time进行统计，如何建立索引？？？ 下面是建立索引前后的对比600万数据 如何生成：执行如下脚本，account和日期不同还有status不同，分别生成一百万。 CREATE PROCEDURE `add_data_myisam_cp_27`() begin declare v_rows int(10) default 1000000; declare v_count int(10) default 0; id_loop:LOOP insert into test_myisam values(null,round(rand()*1000000000),&apos;cloudp&apos;,round(rand()*3),round(rand()*100000),&apos;2016-07-27 00:00:22&apos;); set v_count= v_count + 1; if v_count&gt;v_rows then leave id_loop; end if; end loop id_loop; end; 测试结果利用建立的索引性能提高了三倍： MariaDB [prf]&gt; select count(1) from test_myisam where account=&apos;cloudp&apos; and status =3 and date(create_time)=&apos;2016-07-27&apos;; +----------+ | count(1) | +----------+ | 167400 | +----------+ 1 row in set (1.28 sec) MariaDB [prf]&gt; create index as_index on test_myisam(account,status,create_time); Query OK, 6000006 rows affected (31.60 sec) Records: 6000006 Duplicates: 0 Warnings: 0 MariaDB [prf]&gt; select count(1) from test_myisam where account=&apos;cloudp&apos; and status =3 and date(create_time)=&apos;2016-07-27&apos;; +----------+ | count(1) | +----------+ | 167400 | +----------+ 1 row in set (0.42 sec) MariaDB [prf]&gt; explain select count(1) from test_myisam where account=&apos;cloudp&apos; and status =3 and date(create_time)=&apos;2016-07-27&apos;; +------+-------------+-------------+------+---------------+----------+---------+-------------+--------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +------+-------------+-------------+------+---------------+----------+---------+-------------+--------+--------------------------+ | 1 | SIMPLE | test_myisam | ref | as_index | as_index | 308 | const,const | 520216 | Using where; Using index | +------+-------------+-------------+------+---------------+----------+---------+-------------+--------+--------------------------+ 1 row in set (0.00 sec) 从1.28秒下降到0.42秒但是这个date(create_time)会对每一列都会转换后对比，这里会比较消耗性能； 如何利用上索引？？修改为： MariaDB [prf]&gt; explain select count(1) from test_myisam where account=&apos;cloudp&apos; and status =3 and date(create_time)=&apos;2016-07-27&apos;; +------+-------------+-------------+------+---------------+----------+---------+-------------+--------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +------+-------------+-------------+------+---------------+----------+---------+-------------+--------+--------------------------+ | 1 | SIMPLE | test_myisam | ref | as_index | as_index | 308 | const,const | 520216 | Using where; Using index | +------+-------------+-------------+------+---------------+----------+---------+-------------+--------+--------------------------+ 1 row in set (0.00 sec) MariaDB [prf]&gt; select count(1) from test_myisam where account=&apos;cloudp&apos; and status =3 and create_time between &apos;2016-07-27&apos; and &apos;2016-07-28&apos;; +----------+ | count(1) | +----------+ | 167400 | +----------+ 1 row in set (0.15 sec) MariaDB [prf]&gt; explain select count(1) from test_myisam where account=&apos;cloudp&apos; and status =3 and create_time between &apos;2016-07-27&apos; and &apos;2016-07-28&apos;; +------+-------------+-------------+-------+---------------+----------+---------+------+--------+--------------------------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +------+-------------+-------------+-------+---------------+----------+---------+------+--------+--------------------------+ | 1 | SIMPLE | test_myisam | range | as_index | as_index | 312 | NULL | 174152 | Using where; Using index | +------+-------------+-------------+-------+---------------+----------+---------+------+--------+--------------------------+ 1 row in set (0.00 sec) 如上效率又提高了三倍，是因为扫描的数据行数减少了，最后一个create_time如果不用索引需要扫描52016行，如果使用了索引扫描174152行，命中的行数为：167400行，命中率非常高了。 这里有个疑问： 如果按照天进行统计，create_time作为联合索引的第一列，如何使用上这个索引呢？？？？ 至今没有想清楚，如果这一列是date类型可以直接用上索引，如果在oracle中可以date(create_time)建立函数式索引。但是mysql貌似不支持函数式索引。 一个解决方式是： create_time定义为 date类型，在每一列存入的时候，通过触发器自动把这一行修改为date类型。 如果有好的注意欢迎留言探讨，目前没有好的方式加上create_time，可以从业务上解决，就是每天的统计计算完成以后，直接把数据推到历史表中，统计结果单独存放。 最后说一下关于索引失效的问题： 如果条件中有or，即使其中有条件带索引也不会使用(这也是为什么尽量少用or的原因)。注意：要想使用or，又想让索引生效，只能将or条件中的每个列都加上索引 对于多列索引，不是使用的第一部分，则不会使用索引（即不符合最左前缀原则） like查询是以%开头 如果列类型是字符串，那一定要在条件中将数据使用引号引用起来,否则不使用索引 如果mysql估计使用全表扫描要比使用索引快,则不使用索引 此外，查看索引的使用情况 show status like ‘Handler_read%’; handler_read_key:这个值越高越好，越高表示使用索引查询到的次数 handler_read_rnd_next:这个值越高，说明查询低效]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>最佳左前缀</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何处理负载、高并发问题]]></title>
    <url>%2F2017%2F09%2F27%2F%E5%A6%82%E4%BD%95%E5%A4%84%E7%90%86%E8%B4%9F%E8%BD%BD%E3%80%81%E9%AB%98%E5%B9%B6%E5%8F%91%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[从低成本、高性能和高扩张性的角度来说有如下处理方案： 1、HTML静态化其实大家都知道，效率最高、消耗最小的就是纯静态化的html页面，所以我们尽可能使我们的 网站上的页面采用静态页面来实现，这个最简单的方法其实也是最有效的方法。 2、图片服务器分离​ 把图片单独存储，尽量减少图片等大流量的开销，可以放在一些相关的平台上，如骑牛等 3、数据库集群和库表散列及缓存数据库的并发连接为100，一台数据库远远不够，可以从读写分离、主从复制，数据库集群方面来着手。另外尽量减少数据库的访问，可以使用缓存数据库如memcache、redis。 4、镜像： 尽量减少下载，可以把不同的请求分发到多个镜像端。 5、负载均衡： Apache的最大并发连接为1500，只能增加服务器，可以从硬件上着手，如F5服务器。当然硬件的成本比较高，我们往往从软件方面着手。 负载均衡 （Load Balancing） 建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力，同时能够提高网络的灵活性和可用性。目前使用最为广泛的负载均衡软件是Nginx、LVS、HAProxy。我分别来说下三种的优缺点: Nginx的优点是： 工作在网络的7层之上，可以针对http应用做一些分流的策略，比如针对域名、目录结构，它的正则规则比HAProxy更为强大和灵活，这也是它目前广泛流行的主要原因之一，Nginx单凭这点可利用的场合就远多于LVS了。 Nginx对网络稳定性的依赖非常小，理论上能ping通就就能进行负载功能，这个也是它的优势之一；相反LVS对网络稳定性依赖比较大，这点本人深有体会； Nginx安装和配置比较简单，测试起来比较方便，它基本能把错误用日志打印出来。LVS的配置、测试就要花比较长的时间了，LVS对网络依赖比较大。 可以承担高负载压力且稳定，在硬件不差的情况下一般能支撑几万次的并发量，负载度比LVS相对小些。 Nginx可以通过端口检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点，不过其中缺点就是不支持url来检测。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了，如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而不满。 Nginx不仅仅是一款优秀的负载均衡器/反向代理软件，它同时也是功能强大的Web应用服务器。LNMP也是近几年非常流行的web架构，在高流量的环境中稳定性也很好。 Nginx现在作为Web反向加速缓存越来越成熟了，速度比传统的Squid服务器更快，可以考虑用其作为反向代理加速器。 Nginx可作为中层反向代理使用，这一层面Nginx基本上无对手，唯一可以对比Nginx的就只有 lighttpd了，不过 lighttpd目前还没有做到Nginx完全的功能，配置也不那么清晰易读，社区资料也远远没Nginx活跃。 Nginx也可作为静态网页和图片服务器，这方面的性能也无对手。还有Nginx社区非常活跃，第三方模块也很多。 Nginx的缺点是： Nginx仅能支持http、https和Email协议，这样就在适用范围上面小些，这个是它的缺点。 对后端服务器的健康检查，只支持通过端口来检测，不支持通过url来检测。不支持Session的直接保持，但能通过ip_hash来解决。 LVS：使用Linux内核集群实现一个高性能、高可用的负载均衡服务器，它具有很好的可伸缩性（Scalability)、可靠性（Reliability)和可管理性（Manageability)。LVS的优点是： 抗负载能力强、是工作在网络4层之上仅作分发之用，没有流量的产生，这个特点也决定了它在负载均衡软件里的性能最强的，对内存和cpu资源消耗比较低。 配置性比较低，这是一个缺点也是一个优点，因为没有可太多配置的东西，所以并不需要太多接触，大大减少了人为出错的几率。 工作稳定，因为其本身抗负载能力很强，自身有完整的双机热备方案，如LVS+Keepalived，不过我们在项目实施中用得最多的还是LVS/DR+Keepalived。 无流量，LVS只分发请求，而流量并不从它本身出去，这点保证了均衡器IO的性能不会受到大流量的影响。 应用范围比较广，因为LVS工作在4层，所以它几乎可以对所有应用做负载均衡，包括http、数据库、在线聊天室等等。 LVS的缺点是： 软件本身不支持正则表达式处理，不能做动静分离；而现在许多网站在这方面都有较强的需求，这个是Nginx/HAProxy+Keepalived的优势所在。 如果是网站应用比较庞大的话，LVS/DR+Keepalived实施起来就比较复杂了，特别后面有 Windows Server的机器的话，如果实施及配置还有维护过程就比较复杂了，相对而言，Nginx/HAProxy+Keepalived就简单多了。 HAProxy的特点是： HAProxy也是支持虚拟主机的。 HAProxy的优点能够补充Nginx的一些缺点，比如支持Session的保持，Cookie的引导；同时支持通过获取指定的url来检测后端服务器的状态。 HAProxy跟LVS类似，本身就只是一款负载均衡软件；单纯从效率上来讲HAProxy会比Nginx有更出色的负载均衡速度，在并发处理上也是优于Nginx的。 HAProxy支持TCP协议的负载均衡转发，可以对MySQL读进行负载均衡，对后端的MySQL节点进行检测和负载均衡，大家可以用LVS+Keepalived对MySQL主从做负载均衡。 HAProxy负载均衡策略非常多，HAProxy的负载均衡算法现在具体有如下8种： ① roundrobin，表示简单的轮询，这个不多说，这个是负载均衡基本都具备的；② static-rr，表示根据权重，建议关注；③ leastconn，表示最少连接者先处理，建议关注；④ source，表示根据请求源IP，这个跟Nginx的IP_hash机制类似，我们用其作为解决session问题的一种方法，建议关注；⑤ ri，表示根据请求的URI；⑥ rl_param，表示根据请求的URl参数’balance url_param’ requires an URL parametername；⑦ hdr(name)，表示根据HTTP请求头来锁定每一次HTTP请求；⑧ rdp-cookie(name)，表示根据据cookie(name)来锁定并哈希每一次TCP请求。 Nginx和LVS对比的总结： Nginx工作在网络的7层，所以它可以针对http应用本身来做分流策略，比如针对域名、目录结构等，相比之下LVS并不具备这样的功能，所以Nginx单凭这点可利用的场合就远多于LVS了；但Nginx有用的这些功能使其可调整度要高于LVS，所以经常要去触碰触碰，触碰多了，人为出问题的几率也就会大。 Nginx对网络稳定性的依赖较小，理论上只要ping得通，网页访问正常，Nginx就能连得通，这是Nginx的一大优势！Nginx同时还能区分内外网，如果是同时拥有内外网的节点，就相当于单机拥有了备份线路；LVS就比较依赖于网络环境，目前来看服务器在同一网段内并且LVS使用direct方式分流，效果较能得到保证。另外注意，LVS需要向托管商至少申请多一个ip来做Visual IP，貌似是不能用本身的IP来做VIP的。要做好LVS管理员，确实得跟进学习很多有关网络通信方面的知识，就不再是一个HTTP那么简单了。 Nginx安装和配置比较简单，测试起来也很方便，因为它基本能把错误用日志打印出来。LVS的安装和配置、测试就要花比较长的时间了；LVS对网络依赖比较大，很多时候不能配置成功都是因为网络问题而不是配置问题，出了问题要解决也相应的会麻烦得多。 Nginx也同样能承受很高负载且稳定，但负载度和稳定度差LVS还有几个等级：Nginx处理所有流量所以受限于机器IO和配置；本身的bug也还是难以避免的。 Nginx可以检测到服务器内部的故障，比如根据服务器处理网页返回的状态码、超时等等，并且会把返回错误的请求重新提交到另一个节点。目前LVS中 ldirectd也能支持针对服务器内部的情况来监控，但LVS的原理使其不能重发请求。比如用户正在上传一个文件，而处理该上传的节点刚好在上传过程中出现故障，Nginx会把上传切到另一台服务器重新处理，而LVS就直接断掉了，如果是上传一个很大的文件或者很重要的文件的话，用户可能会因此而恼火。 Nginx对请求的异步处理可以帮助节点服务器减轻负载，假如使用 apache直接对外服务，那么出现很多的窄带链接时apache服务器将会占用大量内存而不能释放，使用多一个Nginx做apache代理的话，这些窄带链接会被Nginx挡住，apache上就不会堆积过多的请求，这样就减少了相当多的资源占用。这点使用squid也有相同的作用，即使squid本身配置为不缓存，对apache还是有很大帮助的。 Nginx能支持http、https和email（email的功能比较少用），LVS所支持的应用在这点上会比Nginx更多。在使用上，一般最前端所采取的策略应是LVS，也就是DNS的指向应为LVS均衡器，LVS的优点令它非常适合做这个任务。重要的ip地址，最好交由LVS托管，比如数据库的 ip、webservice服务器的ip等等，这些ip地址随着时间推移，使用面会越来越大，如果更换ip则故障会接踵而至。所以将这些重要ip交给 LVS托管是最为稳妥的，这样做的唯一缺点是需要的VIP数量会比较多。Nginx可作为LVS节点机器使用，一是可以利用Nginx的功能，二是可以利用Nginx的性能。当然这一层面也可以直接使用squid，squid的功能方面就比Nginx弱不少了，性能上也有所逊色于Nginx。Nginx也可作为中层代理使用，这一层面Nginx基本上无对手，唯一可以撼动Nginx的就只有lighttpd了，不过lighttpd目前还没有能做到 Nginx完全的功能，配置也不那么清晰易读。另外，中层代理的IP也是重要的，所以中层代理也拥有一个VIP和LVS是最完美的方案了。具体的应用还得具体分析，如果是比较小的网站（日PV小于1000万），用Nginx就完全可以了，如果机器也不少，可以用DNS轮询，LVS所耗费的机器还是比较多的；大型网站或者重要的服务，机器不发愁的时候，要多多考虑利用LVS。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>高负载</tag>
        <tag>高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LVS负载均衡（LVS简介、三种工作模式、十种调度算法）]]></title>
    <url>%2F2017%2F09%2F25%2FLVS%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%EF%BC%88LVS%E7%AE%80%E4%BB%8B%E3%80%81%E4%B8%89%E7%A7%8D%E5%B7%A5%E4%BD%9C%E6%A8%A1%E5%BC%8F%E3%80%81%E5%8D%81%E7%A7%8D%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95%EF%BC%89%2F</url>
    <content type="text"><![CDATA[一、LVS简介LVS（Linux Virtual Server）即Linux虚拟服务器，是由章文嵩博士主导的开源负载均衡项目，目前LVS已经被集成到Linux内核模块中。 该项目在Linux内核中实现了基于IP的数据请求负载均衡调度方案，终端互联网用户从外部访问公司的外部负载均衡服务器，终端用户的Web请求会发送给LVS调度器，调度器根据自己预设的算法决定将该请求发送给后端的某台Web服务器，比如，轮询算法可以将外部的请求平均分发给后端的所有服务器，终端用户访问LVS调度器虽然会被转发到后端真实的服务器，但如果真实服务器连接的是相同的存储，提供的服务也是相同的服务，最终用户不管是访问哪台真实服务器，得到的服务内容都是一样的，整个集群对用户而言都是透明的。 最后根据LVS工作模式的不同，真实服务器会选择不同的方式将用户需要的数据发送到终端用户，LVS工作模式分为NAT模式、TUN模式、以及DR模式。 二、三种工作模式的解析。1、基于NAT的LVS模式负载均衡 NAT（Network Address Translation）即网络地址转换，其作用是通过数据报头的修改，使得位于企业内部的私有IP地址可以访问外网，以及外部用用户可以访问位于公司内部的私有IP主机。VS/NAT工作模式拓扑结构如图2所示，LVS负载调度器可以使用两块网卡配置不同的IP地址，eth0设置为私钥IP与内部网络通过交换设备相互连接，eth1设备为外网IP与外部网络联通。 第一步，用户通过互联网DNS服务器解析到公司负载均衡设备上面的外网地址，相对于真实服务器而言，LVS外网IP又称VIP（Virtual IP Address），用户通过访问VIP，即可连接后端的真实服务器（Real Server），而这一切对用户而言都是透明的，用户以为自己访问的就是真实服务器，但他并不知道自己访问的VIP仅仅是一个调度器，也不清楚后端的真实服务器到底在哪里、有多少真实服务器。 第二步，用户将请求发送至124.126.147.168，此时LVS将根据预设的算法选择后端的一台真实服务器（192.168.0.1~192.168.0.3），将数据请求包转发给真实服务器，并且在转发之前LVS会修改数据包中的目标地址以及目标端口，目标地址与目标端口将被修改为选出的真实服务器IP地址以及相应的端口。 第三步，真实的服务器将响应数据包返回给LVS调度器，调度器在得到响应的数据包后会将源地址和源端口修改为VIP及调度器相应的端口，修改完成后，由调度器将响应数据包发送回终端用户，另外，由于LVS调度器有一个连接Hash表，该表中会记录连接请求及转发信息，当同一个连接的下一个数据包发送给调度器时，从该Hash表中可以直接找到之前的连接记录，并根据记录信息选出相同的真实服务器及端口信息。 2、基于TUN的LVS负载均衡在LVS（NAT）模式的集群环境中，由于所有的数据请求及响应的数据包都需要经过LVS调度器转发，如果后端服务器的数量大于10台，则调度器就会成为整个集群环境的瓶颈。我们知道，数据请求包往往远小于响应数据包的大小。因为响应数据包中包含有客户需要的具体数据，所以LVS（TUN）的思路就是将请求与响应数据分离，让调度器仅处理数据请求，而让真实服务器响应数据包直接返回给客户端。 VS/TUN工作模式拓扑结构如图3所示。其中，IP隧道（IP tunning）是一种数据包封装技术，它可以将原始数据包封装并添加新的包头（内容包括新的源地址及端口、目标地址及端口），从而实现将一个目标为调度器的VIP地址的数据包封装，通过隧道转发给后端的真实服务器（Real Server），通过将客户端发往调度器的原始数据包封装，并在其基础上添加新的数据包头（修改目标地址为调度器选择出来的真实服务器的IP地址及对应端口），LVS（TUN）模式要求真实服务器可以直接与外部网络连接，真实服务器在收到请求数据包后直接给客户端主机响应数据。 3、基于DR的LVS负载均衡在LVS（TUN）模式下，由于需要在LVS调度器与真实服务器之间创建隧道连接，这同样会增加服务器的负担。与LVS（TUN）类似，DR模式也叫直接路由模式，其体系结构如图4所示，该模式中LVS依然仅承担数据的入站请求以及根据算法选出合理的真实服务器，最终由后端真实服务器负责将响应数据包发送返回给客户端。与隧道模式不同的是，直接路由模式（DR模式）要求调度器与后端服务器必须在同一个局域网内，VIP地址需要在调度器与后端所有的服务器间共享，因为最终的真实服务器给客户端回应数据包时需要设置源IP为VIP地址，目标IP为客户端IP，这样客户端访问的是调度器的VIP地址，回应的源地址也依然是该VIP地址（真实服务器上的VIP），客户端是感觉不到后端服务器存在的。由于多台计算机都设置了同样一个VIP地址，所以在直接路由模式中要求调度器的VIP地址是对外可见的，客户端需要将请求数据包发送到调度器主机，而所有的真实服务器的VIP地址必须配置在Non-ARP的网络设备上，也就是该网络设备并不会向外广播自己的MAC及对应的IP地址，真实服务器的VIP对外界是不可见的，但真实服务器却可以接受目标地址VIP的网络请求，并在回应数据包时将源地址设置为该VIP地址。调度器根据算法在选出真实服务器后，在不修改数据报文的情况下，将数据帧的MAC地址修改为选出的真实服务器的MAC地址，通过交换机将该数据帧发给真实服务器。整个过程中，真实服务器的VIP不需要对外界可见。 三、LVS负载均衡调度算法根据前面的介绍，我们了解了LVS的三种工作模式，但不管实际环境中采用的是哪种模式，调度算法进行调度的策略与算法都是LVS的核心技术，LVS在内核中主要实现了一下十种调度算法。 1.轮询调度轮询调度（Round Robin 简称’RR’）算法就是按依次循环的方式将请求调度到不同的服务器上，该算法最大的特点就是实现简单。轮询算法假设所有的服务器处理请求的能力都一样的，调度器会将所有的请求平均分配给每个真实服务器。 2.加权轮询调度加权轮询（Weight Round Robin 简称’WRR’）算法主要是对轮询算法的一种优化与补充，LVS会考虑每台服务器的性能，并给每台服务器添加一个权值，如果服务器A的权值为1，服务器B的权值为2，则调度器调度到服务器B的请求会是服务器A的两倍。权值越高的服务器，处理的请求越多。 3.最小连接调度最小连接调度（Least Connections 简称’LC’）算法是把新的连接请求分配到当前连接数最小的服务器。最小连接调度是一种动态的调度算法，它通过服务器当前活跃的连接数来估计服务器的情况。调度器需要记录各个服务器已建立连接的数目，当一个请求被调度到某台服务器，其连接数加1；当连接中断或者超时，其连接数减1。 （集群系统的真实服务器具有相近的系统性能，采用最小连接调度算法可以比较好地均衡负载。) 4.加权最小连接调度加权最少连接（Weight Least Connections 简称’WLC’）算法是最小连接调度的超集，各个服务器相应的权值表示其处理性能。服务器的缺省权值为1，系统管理员可以动态地设置服务器的权值。加权最小连接调度在调度新连接时尽可能使服务器的已建立连接数和其权值成比例。调度器可以自动问询真实服务器的负载情况，并动态地调整其权值。 5.基于局部的最少连接基于局部的最少连接调度（Locality-Based Least Connections 简称’LBLC’）算法是针对请求报文的目标IP地址的 负载均衡调度，目前主要用于Cache集群系统，因为在Cache集群客户请求报文的目标IP地址是变化的。这里假设任何后端服务器都可以处理任一请求，算法的设计目标是在服务器的负载基本平衡情况下，将相同目标IP地址的请求调度到同一台服务器，来提高各台服务器的访问局部性和Cache命中率，从而提升整个集群系统的处理能力。LBLC调度算法先根据请求的目标IP地址找出该目标IP地址最近使用的服务器，若该服务器是可用的且没有超载，将请求发送到该服务器；若服务器不存在，或者该服务器超载且有服务器处于一半的工作负载，则使用’最少连接’的原则选出一个可用的服务器，将请求发送到服务器。 6.带复制的基于局部性的最少连接带复制的基于局部性的最少连接（Locality-Based Least Connections with Replication 简称’LBLCR’）算法也是针对目标IP地址的负载均衡，目前主要用于Cache集群系统，它与LBLC算法不同之处是它要维护从一个目标IP地址到一组服务器的映射，而LBLC算法维护从一个目标IP地址到一台服务器的映射。按’最小连接’原则从该服务器组中选出一一台服务器，若服务器没有超载，将请求发送到该服务器；若服务器超载，则按’最小连接’原则从整个集群中选出一台服务器，将该服务器加入到这个服务器组中，将请求发送到该服务器。同时，当该服务器组有一段时间没有被修改，将最忙的服务器从服务器组中删除，以降低复制的程度。 7.目标地址散列调度目标地址散列调度（Destination Hashing 简称’DH’）算法先根据请求的目标IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。 8.源地址散列调度U源地址散列调度（Source Hashing 简称’SH’）算法先根据请求的源IP地址，作为散列键（Hash Key）从静态分配的散列表找出对应的服务器，若该服务器是可用的且并未超载，将请求发送到该服务器，否则返回空。它采用的散列函数与目标地址散列调度算法的相同，它的算法流程与目标地址散列调度算法的基本相似。 9.最短的期望的延迟最短的期望的延迟调度（Shortest Expected Delay 简称’SED’）算法基于WLC算法。举个例子吧，ABC三台服务器的权重分别为1、2、3 。那么如果使用WLC算法的话一个新请求进入时它可能会分给ABC中的任意一个。使用SED算法后会进行一个运算 A：（1+1）/1=2 B：（1+2）/2=3/2 C：（1+3）/3=4/3 就把请求交给得出运算结果最小的服务器。 10.最少队列调度最少队列调度（Never Queue 简称’NQ’）算法，无需队列。如果有realserver的连接数等于0就直接分配过去，不需要在进行SED运算。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx负载均衡入门]]></title>
    <url>%2F2017%2F09%2F24%2FNginx%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[负载均衡负载均衡（Load Balance）其意思就是分摊到多个操作单元上进行执行，从而共同完成工作任务。通过核心调度者，保证所有后端服务器都将性能充分发挥，从而保持服务器集群的整体性能最优。 算法 [加权]随机算法 通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。可以按后端机器的配置设置随机概率的权重。调用量越大分布越均匀，而且按概率使用权重后也比较均匀，有利于动态调整提供者权重。 [加权]轮询算法 将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。可以按后端机器的配置为轮询中的服务器附加一定权重的算法。比如服务器1权重1，服务器2权重2，服务器3权重3，则顺序为1-2-2-3-3-3-1-2-2-3-3-3- …… 当服务器群中各服务器的处理能力相同时，且每笔业务处理量差异不大时，最适合使用这种算法。 [加权]最小连接算法 在多个服务器中，与处理连接数(会话数)最少的服务器进行通信的算法。由于后端服务器的配置不尽相同，对于请求的处理有快有慢，最小连接数法根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。 可以事先为每台服务器分配处理连接的数量，并将客户端请求转至连接数最少的服务器上。 源地址哈希法 根据获取客户端的IP地址，通过哈希函数计算得到一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均衡，相同的IP客户端，如果服务器列表不变，将映射到同一个后台服务器进行访问。 当客户端有一系列业务需要处理而必须和一个服务器反复通信时，该算法能够以流(会话)为单位，保证来自相同客户端的通信能够一直在同一服务器中进行处理。 配置简单配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546# server1server &#123; listen 8080; server_name local.load.com; index index.html; root /home/www;&#125;# Load Balanceupstream load.com.conf &#123; server 192.168.1.101:80; server 192.168.1.102:80; server 127.0.0.1:8080;&#125; # web serverserver &#123; listen 80; server_name local.load.com; location / &#123; proxy_pass http://load.com.conf; #proxy_set_header Host $host; #proxy_set_header X-Real-IP $remotr_addr; #proxy_set_header X-Forwarde-For $proxy_add_x_forwarded_for; &#125;&#125;# server2 192.168.1.101server &#123; listen 80; server_name local.load.com; root /home/www; location / &#123; index index.html; &#125;&#125;# server3 192.168.1.102server &#123; listen 80; server_name local.load.com; root /home/www; location / &#123; index index.html; &#125;&#125; 详细配置轮询模式： 1234upstream load.com.conf &#123; server 192.168.0.1; server 192.168.0.2;&#125; 加权轮询模式： 1234upstream load.com.conf &#123; server 192.168.0.1 weight=3; server 192.168.0.2 weight=2;&#125; 源地址哈希法： 12345upstream load.com.conf &#123; ip_hash; server 192.168.0.1 weight=3; server 192.168.0.2 weight=2;&#125; fair（第三方）：按后端服务器的响应时间来分配请求，响应时间短的优先分配。 12345upstream load.com.conf &#123; server server1; server server2; fair; &#125; url_hash（第三方）：按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。 例：在upstream中加入hash语句，server语句中不能写入weight等其他的参数，hash_method是使用的hash算法。 123456upstream load.com.conf &#123; server squid1:3128; server squid2:3128; hash $request_uri; hash_method crc32; &#125; 提示： 1234567upstream bakend &#123; ip_hash; server 127.0.0.1:9090 down; server 127.0.0.1:8080 weight=2; server 127.0.0.1:6060; server 127.0.0.1:7070 backup; &#125; 在需要使用负载均衡的server中增加 1proxy_pass http://bakend/; 每个设备的状态设置为： down 表示单前的server暂时不参与负载 weight 默认为1.weight越大，负载的权重就越大。 max_fails ：允许请求失败的次数默认为1.当超过最大次数时，返回proxy_next_upstream 模块定义的错误 fail_timeout:max_fails次失败后，暂停的时间。 backup： 其它所有的非backup机器down或者忙的时候，请求backup]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>Nginx</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器配置——Nginx安装与配置]]></title>
    <url>%2F2017%2F09%2F23%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E2%80%94%E2%80%94Nginx%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[配置完阿里控制台之后，首先我们开始配置服务器，我的服务器各种配置都需要自己重新配，这里我现在Ngnix，不要问我为什么选择他，可能是之前用过Apache，对他印象不好吧…… 前面说了关于阿里云主机操作，这里打算开始一步一步搭建和配置服务器，并且开始Go语言和后台学习，并且徐后续更新，直到后面的后台实战开发 关于域名解析，实例，和安全组相关请自行参考阿里官方教程 如何连接服务器执行环境配置 我使用的是centos6.8 64位版本，如果您不是第一次接触linux那一定知道，linux不是一个可视化界面的系统，所以要摒弃windows的操作习惯，我使用的xshell这个软件链接的服务器，连接语句是 ssh root@ip地址 地址处就是写入你的ip地址，然后回车鞋面就会自动连接，弹框提示输入密码 输入服务器的密码，点击确定即链接上了 ok，下面就可以开始配置环境了，第一步我们先配置nginx nginx安装部分想在 CentOS 系统上安装 Nginx ，你得先去添加一个资源库，像这样： vim /etc/yum.repos.d/nginx.repo 使用 vim 命令去打开 /etc/yum.repos.d/nginx.repo ，如果 nginx.repo 不存在，就会去创建一个这样的文件，打开以后按一下小 i 键，进入编辑模式，然后复制粘贴下面这几行代码，完成以后按 esc 键退出，再输入 :wq （保存并退出） [nginx] name=nginx repo baseurl=http://nginx.org/packages/centos/$releasever/$basearch/ gpgcheck=0 enabled=1 完成以后，我们就可以使用 yum 命令去安装 nginx 了，像这样： yum install nginx 安装好以后测试一下 nginx 服务： service nginx status 应该会返回： nginx is stopped （nginx 已停止） 再测试一下 nginx 的配置文件： nginx -t 应该会返回： nginx: the configuration file /etc/nginx/nginx.conf syntax is ok nginx: configuration file /etc/nginx/nginx.conf test is successful … syntax is ok，… test is successful，说明配置文件没问题，同时这个结果里你可以找到 nginx 的配置文件 nginx.conf 所在的位置。 操纵 nginx 服务操纵服务，可以使用使用 service 命令，它可以启动（start），重启（restart），或停止服务（stop），比如要启动 nginx 服务： service nginx start 服务启动以后，你就可以在浏览器上使用服务器的 IP 地址，或者指向这个地址的域名访问服务器指定的目录了。你会看到类似下面的这些文字。 Welcome to nginx! If you see this page, the nginx web server is successfully installed and working. Further configuration is required. For online documentation and support please refer to nginx.org. Commercial support is available at nginx.com. Thank you for using nginx. nginx配置部分下面来看一下为 nginx 配置虚拟主机。先进入到 nginx 配置文件目录： cd /etc/nginx/conf.d 复制这个目录里的 default.conf ，复制以后的名字可以使用你的虚拟主机名字。比如创建一个 nginx.ninghao.net 的虚拟主机。复制文件可以使用 cp 命令，像这样： cp default.conf nginx.ninghao.net.conf 再去编辑一下这个复制以后的配置文件，可以使用 vim 命令： vim nginx.ninghao.net.conf 你会看到像这样的代码： server { listen 80; server_name localhost; #charset koi8-r; #access_log /var/log/nginx/log/host.access.log main; location / { root /usr/share/nginx/html; index index.html index.htm; } ... } server_name 就是主机名，也就是跟这个虚拟主机绑定在一块儿的域名，我事先把 nginx.ninghao.net 指向了服务器，这个虚拟主机就是为它准备的，所以，server_name 后面的东西就是 nginx.ninghao.net 。 紧接着 server_name 下面可以是一个root，就是这个虚拟主机的根目录，也就是网站所在的目录。比如我们要把 nginx.ninghao.net 这个网站的文件放在/home/www/nginx.ninghao.net 下面，那么这个 root 就是这个路径。 然后去掉 location / 里面的 root 这行代码。再在 index 后面加上一种索引文件名，也就是默认打开的文件，这里要加上一个 index.php ，这样访问 nginx.ninghao.net 就可以直接打开 root 目录下面的 index.php 了。稍后我们再去安装 php 。修改之后，看起来像这样： server { listen 80; server_name nginx.ninghao.net; root /home/www/nginx.ninghao.net; #charset koi8-r; #access_log /var/log/nginx/log/host.access.log main; location / { index index.php index.html index.htm; } ... } 这个配置文件先修改到这，稍后，我们再回来继续修改一下它。保存一下，按 esc ，输入 :wp（保存并退出）。现在虚拟主机应该就可以使用了。主机的域名是 nginx.ninghao.net，访问它的时候，打开的是 /home/www/nginx.ninghao.net 这个目录里面的东西，你可以在这个目录下放点东西。 重启 nginx 或者重新加载 nginx 可以让配置文件生效。 service nginx reload 现在，打开浏览器，输入你为虚拟主机设置的域名，看看是否能打开你指定的目录里的东西。]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Server</tag>
        <tag>服务器配置——Nginx安装与配置</tag>
        <tag>Nginx安装与配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器配置——Php安装与配置]]></title>
    <url>%2F2017%2F09%2F19%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E2%80%94%E2%80%94Php%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[由于之前学过一段时间的Php，偶尔看到一个不错的东西也会尝试一下，虽然可能最终结果和意义并不大，但是此次我还是觉决定让服务器支持Php，同时也会适当的更新一些Php的内容（能力有限，都是比较基础的内容）…… 说了服务器第一步Ngnix安装与配置之后，就要开始说说关于PHP和Go的安装与配置 配置php-fpm部分要让 nginx 能够执行 php 文件，需要去安装一下 php-fpm，它直接包含在了 CentOS 资源库里，所以直接使用 yum 命令可以安装它： yum install php-fpm 完成以后，可以检查一下 php-fpm 的运行状态，使用 service 命令： service php-fpm status 返回： php-fpm is stopped（php-fpm 已停止） 启动 php-fpm 同样可以使用 service 命令： service php-fpm start 让 nginx 可以执行 php现在我们应该就可以让 nginx 去执行 php 了。不过你需要修改一下 nginx 的配置文件，之前我们在配置虚拟主机的时候，创建了一个 nginx.ninghao.net.conf 的配置文件，需要去修改下 nginx 的这个配置文件，才能去执行 php 。使用 vim 命令去编辑它： vim /etc/nginx/conf.d/nginx.ninghao.net.conf 注意你的配置文件不一定叫 nginx.ninghao.net.conf，应该是你自己命名的配置文件。打开以后，找到下面这段字样的代码： # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \.php$ { # root html; # fastcgi_pass 127.0.0.1:9000; # fastcgi_index index.php; # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # include fastcgi_params; #} 这是 nginx 默认给我们的用来执行 php 的配置，从 location 开始取消注释，会让这个配置生效，然后我们还得简单去修改一下： # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \.php$ { # root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } 注意 root 那里仍然是被注释掉的，还有 SCRIPT_FILENAME 后面修改了一下，把 /scripts 换成了 $document_root 。保存并退出。然后重新启动 nginx： service nginx restart 测试是否可以执行 php现在，我们已经安装了 php-fpm，并修改了 nginx 的配置文件让它可以去执行 php，下面，我们得去测试一下，可以使用 php 的 phpinfo(); 函数，方法是在你的虚拟主机根目录下面，创建一个 php 文件，命名为 phpinfo.php，然后在这个文件里输入： &lt;?php phpinfo(); ?&gt; 保存文件并退出。在浏览器里打开刚才创建的这个 php 文件。我这里应该是 http://nginx.ninghao.net/phpinfo.php。打开以后，你应该能看到像下面这样的界面，如果能，说明 nginx 已经可以执行 php 了。 配置扩展 现在，我们有了可以提供 web 服务的 nginx ，并且安装了 php-fpm ，配置了 nginx 可以让它去执行 php ，也安装了数据库管理系统。 不过在运行真正的网站的时候，我们还需要为 php 安装一些额外的扩展，比如 处理 mysql 数据库的 mysql 扩展，缓存功能的 apc 扩展，处理图像的 gd 扩展等等。安装它们同样可以使用 yum 命令。 yum install php-pecl-apc php-mysql php-gd php-mcrypt php-pear php-mbstring php-xmlrpc php-dom 上面安装了一些 php 的扩展，如果你发现在安装网站的时候提示需要安装其它的扩展，同样可以使用 yum 命令去安装。安装完成以后，需要重启一下 php-fpm ： service php-fpm restart]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Server</tag>
        <tag>服务器配置——Php安装与配置</tag>
        <tag>Php安装与配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx配置文件(nginx.conf)详解]]></title>
    <url>%2F2017%2F09%2F16%2FNginx%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6-nginx-conf-%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Nginx 总的 配置文件 位置 /usr/local/nginx/conf/nginx.confnginx 正则匹配 一．正则表达式匹配，其中： ~ 为区分大小写匹配 ~* 为不区分大小写匹配 !~和!~*分别为区分大小写不匹配及不区分大小写不匹配 二．文件及目录匹配，其中： -f和!-f用来判断是否存在文件 -d和!-d用来判断是否存在目录 -e和!-e用来判断是否存在文件或目录 -x和!-x用来判断文件是否可执行 三．rewrite指令的最后一项参数为flag标记，flag标记有： last 相当于apache里面的[L]标记，表示rewrite。 break本条规则匹配完成后，终止匹配，不再匹配后面的规则。 redirect 返回302临时重定向，浏览器地址会显示跳转后的URL地址。 permanent 返回301永久重定向，浏览器地址会显示跳转后的URL地址。 12345使用last和break实现URI重写，浏览器地址栏不变。使用alias指令必须用last标记;使用proxy_pass指令时，需要使用break标记。Last标记在本条rewrite规则执行完毕后，会对其所在server&#123;......&#125;标签重新发起请求break标记则在本条规则匹配完成后，终止匹配。 四．NginxRewrite 规则相关指令1. break指令 使用环境：server,location,if; 该指令的作用是完成当前的规则集，不再处理rewrite指令。 2.if指令 使用环境：server,location 该指令用于检查一个条件是否符合，如果条件符合，则执行大括号内的语句。If指令不支持嵌套，不支持多个条件&amp;&amp;和||处理。 3.return指令 语法：return code ; 使用环境：server,location,if; 该指令用于结束规则的执行并返回状态码给客户端。示例：如果访问的URL以”.sh”或”.bash”结尾，则返回403状态码 location ~ .*\.(sh|bash)?$ { return 403; } 4.rewrite 指令 语法：rewriteregex replacement flag 使用环境：server,location,if 该指令根据表达式来重定向URI，或者修改字符串。指令根据配置文件中的顺序来执行。注意重写表达式只对相对路径有效。如果你想配对主机名，你应该使用if语句，示例如下： if ( $host ~* www\.(.*) ) { set $host_without_www $1; rewrite ^(.*)$ http://$host_without_www$1 permanent; } 5.Set指令 语法：setvariable value ; 默认值:none; 使用环境：server,location,if; 该指令用于定义一个变量，并给变量赋值。变量的值可以为文本、变量以及文本变量的联合。 示例：set $varname “hello world”; 6.Uninitialized_variable_warn指令 语法：uninitialized_variable_warnon|off 使用环境：http,server,location,if 该指令用于开启和关闭未初始化变量的警告信息，默认值为开启。 五．Nginx的Rewrite规则编写实例1.当访问的文件和目录不存在时，重定向到某个php文件if ( !-e $request_filename ) { Rewrite ^/(.*)$ index.php last; } 2.目录对换 /123456/xxxx ====&gt; /xxxx?id=123456rewrite ^/(\d+)/(.+)/ /$2?id=$1 last; 3.如果客户端使用的是IE浏览器，则重定向到/ie目录下if( $http_user_agent ~ MSIE) { Rewrite ^(.*)$ /ie/$1 break; } 4.禁止访问多个目录location ~ ^/(cron|templates)/ { deny all; break; } 5.禁止访问以/data开头的文件location ~ ^/data { deny all; } 6.禁止访问以.sh,.flv,.mp3为文件后缀名的文件location ~ .*\.(sh|flv|mp3)$ { return 403; } 7.设置某些类型文件的浏览器缓存时间location ~ .*\.(gif|jpg|jpeg|png|bmp|swf)$ { expires 30d; } location ~ .*\.(js|css)$ { expires 1h; } 8.给favicon.ico和robots.txt设置过期时间;这里为favicon.ico为99天,robots.txt为7天并不记录404错误日志 location ~(favicon.ico) { log_not_found off; expires 99d; break; } location ~(robots.txt) { log_not_found off; expires 7d; break; } 9.设定某个文件的过期时间;这里为600秒，并不记录访问日志location ^~ /html/scripts/loadhead_1.js { access_log off; root /opt/lampp/htdocs/web; expires 600; break; } 10.文件反盗链并设置过期时间 这里的return412 为自定义的http状态码，默认为403，方便找出正确的盗链的请求 “rewrite ^/ http://img.linuxidc.net/leech.gif;” 显示一张防盗链图片 “access_log off;” 不记录访问日志，减轻压力 “expires 3d” 所有文件3天的浏览器缓存 location ~*^.+\.(jpg|jpeg|gif|png|swf|rar|zip|css|js)$ { valid_referers none blocked *.linuxidc.com*.linuxidc.net localhost 208.97.167.194; if ($invalid_referer) { rewrite ^/ http://img.linuxidc.net/leech.gif; return 412; break; } access_log off; root /opt/lampp/htdocs/web; expires 3d; break; } 11.只允许固定ip访问网站，并加上密码root /opt/htdocs/www; allow 208.97.167.194; allow 222.33.1.2; allow 231.152.49.4; deny all; auth_basic “C1G_ADMIN”; auth_basic_user_file htpasswd; 12将多级目录下的文件转成一个文件，增强seo效果/job-123-456-789.html 指向/job/123/456/789.html rewrite ^/job-([0-9]+)-([0-9]+)-([0-9]+)\.html$ /job/$1/$2/jobshow_$3.html last; 13.文件和目录不存在的时候重定向：if (!-e $request_filename) { proxy_pass http://127.0.0.1; } 14.将根目录下某个文件夹指向2级目录如/shanghaijob/ 指向 /area/shanghai/如果你将last改成permanent，那么浏览器地址栏显是/location/shanghai/ rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2last; 面例子有个问题是访问/shanghai时将不会匹配 rewrite ^/([0-9a-z]+)job$ /area/$1/ last; Rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2last; 这样/shanghai 也可以访问了，但页面中的相对链接无法使用， 如./list_1.html真实地址是/area/shanghia/list_1.html会变成/list_1.html,导至无法访问。那我加上自动跳转也是不行咯 (-d $request_filename)它有个条件是必需为真实目录，而我的rewrite不是的，所以没有效果 if (-d $request_filename) { rewrite ^/(.*)([^/])$ http://$host/$1$2/permanent; } 知道原因后就好办了，让我手动跳转吧 rewrite ^/([0-9a-z]+)job$ /$1job/permanent; rewrite ^/([0-9a-z]+)job/(.*)$ /area/$1/$2last; 15.域名跳转server { listen 80; server_name jump.linuxidc.com; index index.html index.htm index.php; root /opt/lampp/htdocs/www; rewrite ^/ http://www.linuxidc.com/; access_log off; } 16.多域名转向server_name www.linuxidc.com www.linuxidc.net; index index.html index.htm index.php; root /opt/lampp/htdocs; if ($host ~ &quot;linuxidc\.net&quot;) { rewrite ^(.*) http://www.linuxidc.com$1permanent; } 六．nginx全局变量​arg_PARAMETER #这个变量包含GET请求中，如果有变量PARAMETER时的值。 args #这个变量等于请求行中(GET请求)的参数，如：foo=123&amp;bar=blahblah; binary_remote_addr #二进制的客户地址。 body_bytes_sent #响应时送出的body字节数数量。即使连接中断，这个数据也是精确的。 content_length #请求头中的Content-length字段。 content_type #请求头中的Content-Type字段。 cookie_COOKIE #cookie COOKIE变量的值 document_root #当前请求在root指令中指定的值。 document_uri #与uri相同。 host #请求主机头字段，否则为服务器名称。 hostname #Set to themachine’s hostname as returned by gethostname http_HEADER is_args #如果有args参数，这个变量等于”?”，否则等于”&quot;，空值。 http_user_agent #客户端agent信息 http_cookie #客户端cookie信息 limit_rate #这个变量可以限制连接速率。 query_string #与args相同。 request_body_file #客户端请求主体信息的临时文件名。 request_method #客户端请求的动作，通常为GET或POST。 remote_addr #客户端的IP地址。 remote_port #客户端的端口。 remote_user #已经经过Auth Basic Module验证的用户名。 request_completion #如果请求结束，设置为OK. 当请求未结束或如果该请求不是请求链串的最后一个时，为空(Empty)。 request_method #GET或POST request_filename #当前请求的文件路径，由root或alias指令与URI请求生成。 request_uri #包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。不能修改。 scheme #HTTP方法（如http，https）。 server_protocol #请求使用的协议，通常是HTTP/1.0或HTTP/1.1。 server_addr #服务器地址，在完成一次系统调用后可以确定这个值。 server_name #服务器名称。 server_port #请求到达服务器的端口号。 七．Apache和Nginx规则的对应关系Apache的RewriteCond对应Nginx的if Apache的RewriteRule对应Nginx的rewrite Apache的[R]对应Nginx的redirect Apache的[P]对应Nginx的last Apache的[R,L]对应Nginx的redirect Apache的[P,L]对应Nginx的last Apache的[PT,L]对应Nginx的last 例如：允许指定的域名访问本站，其他的域名一律转向www.linuxidc.net Apache:RewriteCond %{HTTP_HOST} !^(.*?)\.aaa\.com$[NC] RewriteCond %{HTTP_HOST} !^localhost$ RewriteCond %{HTTP_HOST}!^192\.168\.0\.(.*?)$ RewriteRule ^/(.*)$ http://www.linuxidc.net[R,L] Nginx:if( $host ~* ^(.*)\.aaa\.com$ ) { set $allowHost &apos;1&apos;; } if( $host ~* ^localhost ) { set $allowHost &apos;1&apos;; } if( $host ~* ^192\.168\.1\.(.*?)$ ) { set $allowHost &apos;1&apos;; } if( $allowHost !~ &apos;1&apos; ) { Rewrite ^/(.*)$ http://www.linuxidc.netredirect ; } nginx conf 配置文件nginx进程数，建议设置为等于CPU总核心数. worker_processes 8; 全局错误日志定义类型，[ debug | info | notice | warn | error | crit ] error_log /var/log/nginx/error.log info; 进程文件 pid /var/run/nginx.pid; 一个nginx进程打开的最多文件描述符数目，理论值应该是最多打开文件数（系统的值ulimit -n）与nginx进程数相除，但是nginx分配请求并不均匀，所以建议与ulimit -n的值保持一致。 worker_rlimit_nofile 65535; 工作模式与连接数上限 events { #参考事件模型，use [ kqueue | rtsig | epoll | /dev/poll | select | poll ]; epoll模型是Linux 2.6以上版本内核中的高性能网络I/O模型，如果跑在FreeBSD上面，就用kqueue模型。 use epoll; #单个进程最大连接数（最大连接数=连接数*进程数） worker_connections 65535; } 设定http服务器 http { ​ include mime.types; #文件扩展名与文件类型映射表 default_type application/octet-stream; #默认文件类型 #charset utf-8; #默认编码 server_names_hash_bucket_size 128; #服务器名字的hash表大小 client_header_buffer_size 32k; #上传文件大小限制 large_client_header_buffers 4 64k; #设定请求缓 client_max_body_size 8m; #设定请求缓 sendfile on; #开启高效文件传输模式，sendfile指令指定nginx是否调用sendfile函数来输出文件，对于普通应用设为 on，如果用来进行下载等应用磁盘IO重负载应用，可设置为off，以平衡磁盘与网络I/O处理速度，降低系统的负载。注意：如果图片显示不正常把这个改成off。 autoindex on; #开启目录列表访问，合适下载服务器，默认关闭。 tcp_nopush on; #防止网络阻塞 tcp_nodelay on; #防止网络阻塞 keepalive_timeout 120; #长连接超时时间，单位是秒 ​ #FastCGI相关参数是为了改善网站的性能：减少资源占用，提高访问速度。下面参数看字面意思都能理解。 fastcgi_connect_timeout 300; fastcgi_send_timeout 300; fastcgi_read_timeout 300; fastcgi_buffer_size 64k; fastcgi_buffers 4 64k; fastcgi_busy_buffers_size 128k; fastcgi_temp_file_write_size 128k; ​ #gzip模块设置 gzip on; #开启gzip压缩输出 gzip_min_length 1k; #最小压缩文件大小 gzip_buffers 4 16k; #压缩缓冲区 gzip_http_version 1.0; #压缩版本（默认1.1，前端如果是squid2.5请使用1.0） gzip_comp_level 2; #压缩等级 gzip_types text/plain application/x-javascript text/css application/xml; #压缩类型，默认就已经包含text/html，所以下面就不用再写了，写上去也不会有问题，但是会有一个warn。 gzip_vary on; #limit_zone crawler $binary_remote_addr 10m; #开启限制IP连接数的时候需要使用 ​ upstream blog.ha97.com { #upstream的负载均衡，weight是权重，可以根据机器配置定义权重。weigth参数表示权值，权值越高被分配到的几率越大。 server 192.168.80.121:80 weight=3; server 192.168.80.122:80 weight=2; server 192.168.80.123:80 weight=3; } 虚拟主机的配置 server { ​ listen 80; #监听端口 server_name aa.cn www.aa.cn ; #server_name end #域名可以有多个，用空格隔开 ​ index index.html index.htm index.php; # 设置访问主页 set $subdomain &apos;&apos;; # 绑定目录为二级域名 bbb.aa.com 根目录 /bbb 文件夹 if ( $host ~* &quot;(?:(\w+\.){0,})(\b(?!www\b)\w+)\.\b(?!(com|org|gov|net|cn)\b)\w+\.[a-zA-Z]+&quot; ) { set $subdomain &quot;/$2&quot;; } root /home/wwwroot/aa.cn/web$subdomain;# 访问域名跟目录 include rewrite/dedecms.conf; #rewrite end #载入其他配置文件 ​ location ~ .*.(php|php5)?$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; include fastcgi.conf; } #图片缓存时间设置 location ~ .*.(gif|jpg|jpeg|png|bmp|swf)$ { expires 10d; } #JS和CSS缓存时间设置 location ~ .*.(js|css)?$ { expires 1h; } } ​ 日志格式设定 ​ log_format access &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos; &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos; &apos;&quot;$http_user_agent&quot; $http_x_forwarded_for&apos;; #定义本虚拟主机的访问日志 access_log /var/log/nginx/ha97access.log access; ​ #对 &quot;/&quot; 启用反向代理 location / { ​ proxy_pass http://127.0.0.1:88; proxy_redirect off; proxy_set_header X-Real-IP $remote_addr; #后端的Web服务器可以通过X-Forwarded-For获取用户真实IP proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; #以下是一些反向代理的配置，可选。 proxy_set_header Host $host; client_max_body_size 10m; #允许客户端请求的最大单文件字节数 client_body_buffer_size 128k; #缓冲区代理缓冲用户端请求的最大字节数， proxy_connect_timeout 90; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_send_timeout 90; #后端服务器数据回传时间(代理发送超时) proxy_read_timeout 90; #连接成功后，后端服务器响应时间(代理接收超时) proxy_buffer_size 4k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的设置 proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 64k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传 ​ } ​ 设定查看Nginx状态的地址 ​ location /NginxStatus { ​ stub_status on; access_log on; auth_basic &quot;NginxStatus&quot;; auth_basic_user_file conf/htpasswd; #htpasswd文件的内容可以用apache提供的htpasswd工具来产生。 ​ } ​ #本地动静分离反向代理配置 #所有jsp的页面均交由tomcat或resin处理 location ~ .(jsp|jspx|do)?$ { ​ proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_pass http://127.0.0.1:8080; ​ } ​ #所有静态文件由nginx直接读取不经过tomcat或resin location ~ .*.(htm|html|gif|jpg|jpeg|png|bmp|swf|ioc|rar|zip|txt|flv|mid|doc|ppt|pdf|xls|mp3|wma)$ { expires 15d; } location ~ .*.(js|css)?$ { expires 1h; } } nginx 在thinkphp 的url 重写在/usr/local/nginx/conf/vhost/你的域名配置文件 中添加 location / { if (!-e $request_filename) { rewrite ^/(.*)/(.*)/(.*)/*$ /index.php?m=$1&amp;c=$2&amp;a=$3 last; # thinkphp 的配置文件中 &apos;URL_MODEL&apos; =&gt; 1 PATHINFO模式 ​ #或者 rewrite ^(.*)$ /index.php?s=$1 last; # thinkphp 的配置文件中 &apos;URL_MODEL&apos; =&gt;3 兼容模式 ​ #或者 rewrite /(.*)$ /index.php/$1 last; # thinkphp 的配置文件中 &apos;URL_MODEL&apos; =&gt; 2 REWRITE模式 break; } } 路径 pathinfo 模式[ thinkphp ] 添加location ~ \.php(.*)$ { fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_split_path_info ^((?U).+\.php)(/?.+)$; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PATH_INFO $fastcgi_path_info; fastcgi_param PATH_TRANSLATED $document_root$fastcgi_path_info; include fastcgi_params; } 重写 url +省略index.phplocation / { try_files $uri /index.php?$uri; } nginx -s reload 或者 /usr/local/nginx/sbin/nginx -s reload 重新加载Nginx配置文件]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>Nginx</tag>
        <tag>配置文件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器配置——阿里云主机配置与操作]]></title>
    <url>%2F2017%2F09%2F11%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E2%80%94%E2%80%94%E9%98%BF%E9%87%8C%E4%BA%91%E4%B8%BB%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[前几年在阿里买了一个属于自己的个性域名(笔者英文)：www.icocos.cn，虽然是CN，但是这都不是重点，重点是为了更全面的接触后台，更好的学习与实战后台开发与PHP，前天又在阿里买了个主机，打算开发玩弄一番，毕竟第一次，期间可能会遇到不少坑和不对的地方，望谅解与指出一起交流，谢谢...... 阿里云（先声明我不是来给阿里大广告的，😂😂😂😂😂😂） 阿里云主机 域名：www.icocos.cn 这里关于阿里云主机和域名的购买很简单，准备钱，一路点下去就可以了,具体操作强查看阿里文档…… 备案关于备案确实是不少人蛋疼的事情，说麻烦也没有多麻烦，说不麻烦但是搞起来又不是点几下就可以的，所以也有不少人选择了不需要备案的或者国外的主机，我当时也有这种想法，但是后来放弃了，投资嘛，自己都不舍得投资还怎么学东西，怎么提升呢！ 本案分个人备案和企业备案，具体详情阿里也有教程，或者网上也有相关教程 初始化配置在链接之前可能需要做一些配置，但是有些事必须的有些可以后面备案成功之后再做，我就是第一次弄所以一直不懂，以为需要备案以后才能操作的。 添加安全组织，允许22端口访问这里其实就是进入阿里控制台的云服务ECS，找到安全组，添加安全组，配置规则就可以. 这里只要注意 端口:22/22 授权对象：0.0.0.0/0 初步的这样就可以了，我是为了方便链接访问 然后就可以链接了 连接，并操作连接的话当然首选ssh，不要问我为什么，我也不知道，大家都用它，哈哈！ 关于SSHSSH是每一台Linux电脑的标准配置。 简单说，SSH是一种网络协议，用于计算机之间的加密登录。 如果一个用户从本地计算机，使用SSH协议登录另一台远程计算机，我们就可以认为，这种登录是安全的，即使被中途截获，密码也不会泄露。 最基本的用法SSH主要用于远程登录。假定你要以用户名user，登录远程主机host，只要一条简单命令就可以了。 $ ssh user@host 如果本地用户名与远程用户名一致，登录时可以省略用户名。 $ ssh host SSH的默认端口是22，也就是说，你的登录请求会送进远程主机的22端口。使用p参数，可以修改这个端口。 $ ssh -p 2222 user@host 上面这条命令表示，ssh直接连接远程主机的2222端口。 这里我使用的是SSH shell这个工具，Windows用什么不知道，网上应该一大堆命令行步骤 输入 ssh root@ip地址 输入yes 输入密码 就可以看到显示 [root@iZwz92qgus0ln1nx5dftjd2rZ ~]# SSH工具使用也很简单，进后点击Add Server输入ip，端口是默认22，输入root用户名，输入后台配置的密码，对了这里的密码是需要在控制台配置的 一切顺利的话可以看到一个和terminal一样的终端界面，并且已经连接成功的显示 [root@iZwz92qgus0ln1nx5dftjd2rZ ~]# 然后就可以使用命令操作主机文件了。 密码登录：Mac 客户端进入.ssh 文件夹，如果没有就创建一个.ssh文件夹mkdir ~/.ssh cd ~/.ssh/ 生成rsa秘钥：这个相信大部分人都弄过，github就需要ssh-keygen -t rsa 在该文件夹下就会产生三个文件夹：id_rsa,id_rsa.pub,know_hosts。id_rsa：存储私钥，记得只能自己看哦。别人那到这个文件就完蛋蛋咯。 id_rsa.pub:存储公钥，用来通信加密使用，有了这个人家才能确定这是你。 公钥拷贝到云主机scp id_rsa.pub root@78.129.23.45:/root/.ssh/id_rsa.pub Note:云主机上没有.ssh/文件时，你要自己建立一个。终端连接后输入ls只能看到default.pass文件，ls -a发现也是没有.ssh文件夹的，所以需要输入下面命令创建.ssh文件夹 mkdir .ssh #回车没有提示就成功再次ls -a就可以看到一个蓝色的隐藏文件夹 登录到云主机进入.ssh/文件夹cd /root/.ssh/ 将客户端公钥放入云主机识别keys文件夹中cat id_rsa.pub &gt;&gt; authorized_keys 销毁公钥rm id_rsa.pub 再次输入ssh root@ip地址就可以直接连接了，当然如果你使用工具的话就不用这么麻烦。 到这里就基本上初步，而且是很简单的实现的主机的查看与操作，具体更多操作和命令请查看相关资料，后期也会有一些配置，操作与实战，请期待]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Server</tag>
        <tag>服务器配置——阿里云主机配置与操作</tag>
        <tag>阿里云主机配置与操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器配置——实战验证与补充]]></title>
    <url>%2F2017%2F09%2F09%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E2%80%94%E2%80%94%E5%AE%9E%E6%88%98%E9%AA%8C%E8%AF%81%E4%B8%8E%E8%A1%A5%E5%85%85%2F</url>
    <content type="text"><![CDATA[下面关于之前一些流程（Ngnix，Php，Go，Mysql）安装配置与使用的一些总结，和遇到的一个坑，其中也有关于服务器配置与部署，代码上传与发布相关内容，主要是为了熟悉整个流程…… 这里补充一下前面没有提到，但是实际操作会用到或者能提升效率的东西 安装ftp大家看到了，所有的文件操作在ssh中可以通过vim方法来实现，但是，你知道在windows中用惯了，还是喜欢看图形界面，所以在这里我安装了ftp可以远程来上传修改文件 软件：winscp（百度下载就好了） 安装vsftpdyum install vsftpd 启动/重启/关闭vsftpd服务器[root@localhost ftp]# /sbin/service vsftpd restart Shutting down vsftpd: [ OK ] Starting vsftpd for vsftpd: [ OK ] OK表示重启成功了. 这里现在就可以直接使用root及你的密码来查看了，当然这样的是超级用户，留给自己用的，要配置指定要文件夹的用户，我就在不在这里写了，大家继续百度下吧 安装Gitsudo apt-get update sudo apt-get install git]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>Server</tag>
        <tag>服务器配置——实战验证与补充</tag>
        <tag>实战验证与补充</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器配置——MySql安装与配置]]></title>
    <url>%2F2017%2F09%2F05%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E2%80%94%E2%80%94MySql%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[服务器好了之后，首先我们应该从数据库入手，因为一切没有数据交互与存储的服务器和客户端都只是娱乐，这里我们选择大部分人都熟知而且与PHP成为天和之作的Mysql，当然他和Go也一样兼容…… 安装mysqlmysql 可以管理网站用到的数据库，WordPress 和 Drupal 也都支持 mysql 数据库。所以我们的 Web 运行环境里，需要安装一个 mysql 。之前我们已经添加了资源库，所以可以直接使用 yum 命令去安装 mysql ： yum install mysql-server 安装完成后，使用 service 命令启动 mysql 服务： service mysqld start 然后我们需要简单配置一下 mysql ，默认安装以后 mysql 的 root 用户是没有密码的，对于生产环境来说，这肯定是不行的. 另外还有一些安全相关的设置，可以使用下面这行命令去配置一下，它是一个向导，问你一些问题，你要给出答案，比如是否要设置 root 用户的密码， 密码是什么等等。 mysql_secure_installation 然后根据实际情况进行配置，也可以看看下面比较常用的配置方案 Enter current password for root (enter for none): 解释：输入当前 root 用户密码，默认为空，直接回车。 Set root password? [Y/n] y 解释：要设置 root 密码吗？输入 y 表示愿意。 Remove anonymous users? [Y/n] y 解释：要移除掉匿名用户吗？输入 y 表示愿意。 Disallow root login remotely? [Y/n] n 解释：不想让 root 远程登陆吗？输入 y 表示愿意。 Remove test database and access to it? [Y/n] y 解释：要去掉 test 数据库吗？输入 y 表示愿意。 Reload privilege tables now? [Y/n] y 解释：想要重新加载权限吗？输入 y 表示愿意。]]></content>
      <categories>
        <category>Server</category>
      </categories>
      <tags>
        <tag>服务器配置——MySql安装与配置</tag>
        <tag>Server</tag>
        <tag>MySql安装与配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx基本使用笔记]]></title>
    <url>%2F2017%2F09%2F04%2FNginx%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1.常见操作`nginx -s signal` signal的值可以是： stop 快速关机 quit 正常关机 reload 重新加载配置文件 reopen 重新打开日志文件 2.配置文件nginx是由模块组成的，这些模块在配置文件中又有指定的指令。 指令被分成简单指令和块指令。简单指令包括名称和用空格分割的参数以及用来结尾的分号(;)。 一个块指令和简单指令有相同的结构，但是它使用大括号({and})来包围一系列说明来替代使用分号作为结尾。 放在配置文件最外面的指令的称之为主文，event,http指令在主文中；server在http中， location在server中。 3.静态服务Web服务器一个重要的任务就是提供文件（如图像或者静态html页面）。 根据需求，你将实现一个例子，文件被本地不同的目录服务着，如/data/www 包含html文件，/data/images 包含图片。这需要编辑配置文件，在http块中设置server块。 首先，创建/data/www 目录并放置index.html文件（文件中可以是任意内容）。 然后创建/data/images目录并放置一些图片。 接下来，打开配置文件。默认的配置文件已经包含了几个server块的例子，大多数都被注释掉了。 现在注释掉所有的块，并开始一个新的server块。 1234http &#123; server &#123; &#125;&#125; 一般情况下，配置文件中包含多个server块，它们之间以监听的端口号和server name来区分。 一旦nginx决定了哪个server处理请求，它测试在请求的对server块内定义的位置指令的参数头中指定的URI。 添加location到server中123location / &#123; root /data/www;&#125; 与请求的URI相比，location块指定了“/”前缀。为了匹配请求，该URI会被添加到root指令指定的路径中， 即，到/data/www，在本地文件系统中组成请求文件的路径。如果有多个匹配的location块，nginx会选择前缀最长的。 上面的location块提供了最短的前缀，如果其他的location块匹配失败，这个location块就会被使用。 现在来添加第二个location:123location /images/ &#123; root /data;&#125; 它与带/images/的请求请求匹配。（location / ，当然也匹配，除非有更短的前缀。） #####现在server中是这样的： 12345678server &#123; location / &#123; root /data/www; &#125; location /images/ &#123; root /data; &#125;&#125; 重启配置文件，让配置生效 nginx -s reload 这已经是一个可以工作的服务器配置文件，它监听的是80端口，可在本地通过http://localhost/访问。 响应带/images/的URI路由请求时，服务器将会从/data/images目录发送文件。 例如，响应 http://localhost/images/example.png 路由请求，nginx将会发送/data/images/example.png 文件。如果这个文件不存在，nginx将会发出404错误的响应。不带/images/的URIs请求将会映射到/data/www目录。 例如，为了响应http://localhost/some/example.html请求，nginx将会发送/data/www/some/example.html文件。 3.代理服务器服务器A接受到请求后，将请求转发给其他的服务器B，从服务器B处获得响应，并将取得的相应返回给客户端，服务器B则是服务器A的代理服务器 首先，我们新增一个server 123456server &#123; listen 8080; root /data/up1; location / &#123; &#125;&#125; 这是一个简单的server块，监听8080端口（此前，listen指令没有被提起是由于已经使用了标准的80端口），并将所有的请求 映射到本地文件系统的/data/up1目录。创建这个目录，并将index.html文件放置其中。注意root指令已经被放置在server环境中。 当location块被选中服务请求时，root指令就会被使用，当然不包括自己的root指令。 修改第一个location块，放置proxy_pass指令与协议、名称和参数中指定的代理服务器端口 12345678server &#123; location / &#123; proxy_pass http://localhost:8080; &#125; location /images/ &#123; root /data; &#125;&#125; 修改第二个location块，它目前映射所有带/images/前缀的请求到/data/images 目录下的文件，是为了使其符合典型的文件扩展的图像请求 123location ~ \.(gif|jpg|png)$ &#123; root /data/images;&#125; 该参数是一个正则表达式，匹配所有.gif,.jpg,.png 结尾的路由。正则表达式应该优于～。相应的请求都会被映射到 /data/images目录。 当nginx选择一个location块服务一个请求时，它首先检查location指令的指定前缀，记住location最长的前缀， 然后检查正则表达式。如果有一个匹配的正则表达式，nginx会挑选location块，否则它会选择之前的。因此代理服务器的配置文件应该是这样的: 12345678server &#123; location / &#123; proxy_pass http://localhost:8080/; &#125; location ~ \.(gif|jpg|png)$ &#123; root /data/images; &#125;&#125; 此服务器会筛选出以.gif,.jpg,.png 结尾的请求，并将他们映射到/data/images目录下(通过添加URI到root指令的参数上)， 然后通过所有其它请求到代理服务器配置上 `nginx -s reload` 重启配置使更改生效 4. FastCGI代理nginx可用于路由请求FastCGI服务器，FastCGI服务器运行各种不同的框架和编程语言，如PHP，建立的应用。 最常用与 FastCGI server工作的nginx配置，用fastcgi_pass指令替代了proxy_pass指令，并设置fastcgi_param 参数传递给FastCGI server。 假设FastCGI server通过localhost:9000可以访问。 以上一节代理配置作为基础，用fastcgi_pass指令替换proxy_pass指令，并修改参数为localhost:9000。在PHP中， SCRIPT_FILENAME参数用来确定脚本名，QUERY_STRING参数用来传递请求参数。 12345678910server &#123; location / &#123; fastcgi_pass localhost:9000; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param QUERY_STRING $query_string; &#125; location ~ \.(gif|jpg|png)$ &#123; root /data/images; &#125;&#125; nginx与PHP通信首先我们先简单的看一份nginx server 12345678910111213server &#123; listen 80 default_server; index index.php; root /home/work/ location ~[^/]\.php(/l$) &#123; fastcgi_pass unix:/tmp/php-cgi.sock; fastcgi_index index.php; include fastcgi.conf; &#125; access_log /home/work/logs/default.log;&#125; CGI是通用网关协议，FastCGI则是一种常住进程的CGI模式程序。我们所熟知的PHP-FPM会通过用户配置来管理一批FastCGI进程，例如在PHP-FPM管理下的某个FastCGI进程挂了，PHP-FPM会根据用户配置来看是否要重启补全，PHP-FPM更像是管理器，而真正衔接Nginx与PHP的则是FastCGI进程。 我们可以看到server中包含了fastcgi.conf，里面是一些fastcgi_param的配置项，如下： 123456789101112131415161718192021222324fastcgi_param QUERY_STRING $query_string;fastcgi_param REQUEST_METHOD $request_method;fastcgi_param CONTENT_TYPE $content_type;fastcgi_param CONTENT_LENGTH $content_length;fastcgi_param SCRIPT_NAME $fastcgi_script_name;fastcgi_param REQUEST_URI $request_uri;fastcgi_param DOCUMENT_URI $document_uri;fastcgi_param DOCUMENT_ROOT $document_root;fastcgi_param SERVER_PROTOCOL $server_protocol;fastcgi_param HTTPS $https if_not_empty;fastcgi_param GATEWAY_INTERFACE CGI/1.1;fastcgi_param SERVER_SOFTWARE nginx/$nginx_version;fastcgi_param REMOTE_ADDR $remote_addr;fastcgi_param REMOTE_PORT $remote_port;fastcgi_param SERVER_ADDR $server_addr;fastcgi_param SERVER_PORT $server_port;fastcgi_param SERVER_NAME $server_name;# PHP only, required if PHP was built with --enable-force-cgi-redirectfastcgi_param REDIRECT_STATUS 200;fastcgi_param PHP_VALUE &quot;open_basedir=$document_root:/usr/share/pear:/usr/share/php:/etc/phpMyAdmin:/tmp:/proc&quot;; fastcig_param中所声明的内容会传到php-fpm（或者其他fast-cgi server）所管理的fast-cgi进程。我们可以看到，fastcgi_param中都是一些服务器的信息，如remote_addr(访问用户的ip)等，他就可以把这些信息传递给后端程序，如PHP的$_SERVER]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器配置——Redis安装，配置，简单使用]]></title>
    <url>%2F2017%2F09%2F02%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE%E2%80%94%E2%80%94Redis%E5%AE%89%E8%A3%85%EF%BC%8C%E9%85%8D%E7%BD%AE%EF%BC%8C%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[本文是因为在做API接口开发的时候，要数据每次都需要从数据读取而写，那样太耗性能了，其实类型iOS中的Cache或者NSUserDefault，用法都和NSUserDefault类似…. Redis安装下载安装包 redis-3.2.8.tar.gz 官网地址：http://redis.io/download 解压： tar -zvxf redis-3.2.8.tar.gz 将解压后的文件夹放到 /Users/local目录下 编译测试:接下来在终端中切换到/Users/local/redis目录下,输入：sudo make test 编译安装：在终端中输入命令：sudo make install 启动Redis,输入命令redis-server redis-server 78407:C 18 Apr 21:32:31.361 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 78407:M 18 Apr 21:32:31.362 * Increased maximum number of open files to 10032 (it was originally set to 256). _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 3.2.8 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6379 | `-._ `._ / _.-&apos; | PID: 78407 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos; 78407:M 18 Apr 21:32:31.363 # Server started, Redis version 3.2.8 78407:M 18 Apr 21:32:31.364 * DB loaded from disk: 0.001 seconds 78407:M 18 Apr 21:32:31.364 * The server is now ready to accept connections on port 6379 看到这个界面表示我们已经成功的安装Redis。 二配置1、在 /Users/local/redis目录下建立bin，etc，db三个目录2、把/Users/local/redis/src目录下的mkreleasehdr.sh，redis-benchmark， redis-cli， redis-server拷贝到bin目录3、在etc下，参考原/Users/local/redis目录下的redis.conf，新建一个redis.conf修改redis.conf，具体如下： #修改为守护模式 daemonize yes #设置进程锁文件 pidfile /Users/local/redis/redis.pid #端口 port 6379 #客户端超时时间 timeout 300 #日志级别 loglevel debug #日志文件位置 logfile /Users/local/redis/log-redis.log #设置数据库的数量，默认数据库为16，可以使用SELECT 命令在连接上指定数据库id databases 16 ##指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 #save #Redis默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 #指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间， #可以关闭该#选项，但会导致数据库文件变的巨大 rdbcompression yes #指定本地数据库文件名 dbfilename dump.rdb #指定本地数据库路径 dir /Users/local/redis/db/ #指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能 #会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有 #的数据会在一段时间内只存在于内存中 appendonly no #指定更新日志条件，共有3个可选值： #no：表示等操作系统进行数据缓存同步到磁盘（快） #always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） #everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec 其实只需要拷贝，然后修改对应的值就可以 4、保存后，启动redis：./bin/redis-server etc/redis.conf注意这里log-redis.log文件需要我自己创建 iCocosdeMacBook-Pro:redis icocos$ redis-server etc/redis.conf回车之后没有输出说明成功了 5、查看日志文件：tail -f log-redis.log| `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos; 78584:M 18 Apr 21:48:02.777 # Server started, Redis version 3.2.8 78584:M 18 Apr 21:48:02.777 * The server is now ready to accept connections on port 6379 78584:M 18 Apr 21:48:02.779 - 0 clients connected (0 slaves), 957072 bytes in use 到这里，我们已经完成redis安装和配置。 三使用1、检查是否启动：redis-cli ping2、redis-cli3、ping4、redis-cli 跟上命令会将参数发送到本地redis 6379 的端口，下面来看看基本操作：使用方式 iCocosdeMacBook-Pro:redis icocos$ redis-cli 127.0.0.1:6379&gt; ping PONG 127.0.0.1:6379&gt; set key1 name OK 127.0.0.1:6379&gt; get key1 &quot;name&quot; 5、关闭命令：redis-cli shutdown上面我们演示了redis安装与配置，接下来后面的工作在真实上线之前只需要熟练redis的使用就差不多了，注意细节，并应用到实战，没错就这么简单！ 四.服务器配置上面只是基本的安装，如果要用到生产环境，得配置一些文件 1.首先把redis-cli 移动到/Users/local/bin 目录，方便执行，和JAVA_HOME 类似默认install 的时候已经有了 2.创建你存放配置文件和数据文件的目录sudomkdir /etc/redissudo mkdir /var/redis 3.复制初始化脚本到/etc/init.d 目录,建议默认只有这个端口,作开机启动$ sudocp utils/redis_init_script /etc/init.d/redis_6379 4.进入脚本,确保你的REDISPORT 是你正在使用的sudovi /etc/init.d/redis_6379 5.复制redis.conf ,和脚本保持一致sudocp redis.conf /etc/redis/6379.conf 6.创建一个存放工作数据的目录sudomkdir /var/redis/6379 同时可以对redis.conf配置其他属性 # 来源：http://www.cnblogs.com/shanyou/archive/2012/01/28/2330451.htmldaemonize：是否以后台daemon方式运行pidfile：pid文件位置port：监听的端口号timeout：请求超时时间loglevel：log信息级别logfile：log文件位置databases：开启数据库的数量save **：保存快照的频率，第一个*表示多长时间，第三个*表示执行多少次写操作。在一定时间内执行一定数量的写操作时，自动保存快照。可设置多个条件。rdbcompression：是否使用压缩dbfilename：数据快照文件名（只是文件名，不包括目录）dir：数据快照的保存目录（这个是目录）appendonly：是否开启appendonlylog，开启的话每次写操作会记一条log，这会提高数据抗风险能力，但影响效率。appendfsync：appendonlylog如何同步到磁盘（三个选项，分别是每次写都强制调用fsync、每秒启用一次fsync、不调用fsync等待系统自己同步） 7.最后将脚本设置默认启动sudoupdate-rc.d redis_6379 defaults 8.现在可以执行脚本了/etc/init.d/redis_6379start 五.MAC 版本差异一个不幸的事实是：mac 上是没有init.d 目录的，mac启动脚本是 以.plist 结尾了， 好吧，我在 ~/Library/LaunchAgents 目录下自己建立一个 redis.plist 参考：http://www.js2node.com/redis-io/install-redis-io-2-4-17-on-mac-osx-as-service $ sudovi redis.plist 然后输入以下内容 Labelio.redis.redis-serverProgramArguments/Users/local/bin/redis-server/Users/local/etc/redis.confRunAtLoad 执行命令 $ sudo launchctlload/Library/LaunchDaemons/io.redis.redis-server.plist 打开关闭： $ sudo launchctlstartio.redis.redis-server$ sudo launchctl stop io.redis.redis-server 六.brew 进行安装下面我用brew来操作，关于brew 的安装我看的：http://my.oschina.net/liygheart/blog/284668 安装好了，然后 参考：https://gist.github.com/tonypujals/9631143 $brew info redis 会有提示，然后输入 $ln -nfs /Users/local/opt/redis/*.plist ~/Library/LaunchAgents 和 $load ~/Library/LaunchAgents/homebrew.mxcl.redis.plist 网上还找到一种方式，貌似可以成功1.获取github中的redis-php扩展代码： git clone https://github.com/phpredis/phpredis.git 2.cd phpredis/ 3.phpize mac os 如果这里有 Cannot find autoconf. Please check your autoconf installation and the $PHP_AUTOCONF environment variable. Then, rerun this script.的问题，那么： 执行：brew install autoconf 4.之后执行phpize 5../configure 6.make &amp;&amp; sudo make install 输入密码就会提示成功 Password: Installing shared extensions: /usr/lib/php/extensions/no-debug-non-zts-20131226/ 7.之后在php.ini中加一句 extension=redis.so 就ok了 路径：/etc/php.ini 然后重启 apache/nginx 和 php-fpm ，输入命令：php -m |grep redis 或者通过 phpinfo() 输出php信息查看redis是否安装成功。 期间我在执行install的时候出现这样的问题装不了PHP的扩展，make install失败 RudonMacBook:igbinary-master rudon$ make install Installing shared extensions: /usr/lib/php/extensions/no-debug-non-zts-20131226/ cp: /usr/lib/php/extensions/no-debug-non-zts-20131226/#INST@12567#: Operation not permitted make: *** [install-modules] Error 1 cp: /usr/lib/php/extensions/no-debug-non-zts-20121212/#INST@17000#: Operation not permitted 原因是 原来是OSX 10.11 El Capitan（或更高）新添加了一个新的安全机制叫系统完整性保护System Integrity Protection (SIP)，所以对于目录 /System /sbin /usr 不包含(/usr/local/) 仅仅供系统使用，其它用户或者程序无法直接使用，而我们的/usr/lib/php/extensions/刚好在受保护范围内 所以解决方法就是禁掉SIP保护机制，步骤是： 重启系统 按住Command + R （重新亮屏之后就开始按，象征地按几秒再松开，出现苹果标志，ok） 菜单“实用工具” ==&gt;&gt; “终端” ==&gt;&gt; 输入csrutil disable；执行后会输出：Successfully disabled System Integrity Protection. Please restart the machine for the changes to take effect. 再次重启系统 禁止掉SIP后，就可以顺利的安装了，当然装完了以后你可以重新打开SIP，方法同上，只是命令是csrutil enable 平时使用iCocosdeMacBook-Pro:redis icocos$ cd /Users/local/redis iCocosdeMacBook-Pro:redis icocos$ redis-server 78728:C 18 Apr 22:00:25.105 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf 78728:M 18 Apr 22:00:25.107 * Increased maximum number of open files to 10032 (it was originally set to 256). _._ _.-``__ &apos;&apos;-._ _.-`` `. `_. &apos;&apos;-._ Redis 3.2.8 (00000000/0) 64 bit .-`` .-```. ```\/ _.,_ &apos;&apos;-._ ( &apos; , .-` | `, ) Running in standalone mode |`-._`-...-` __...-.``-._|&apos;` _.-&apos;| Port: 6379 | `-._ `._ / _.-&apos; | PID: 78728 `-._ `-._ `-./ _.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | http://redis.io `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; |`-._`-._ `-.__.-&apos; _.-&apos;_.-&apos;| | `-._`-._ _.-&apos;_.-&apos; | `-._ `-._`-.__.-&apos;_.-&apos; _.-&apos; `-._ `-.__.-&apos; _.-&apos; `-._ _.-&apos; `-.__.-&apos; 78728:M 18 Apr 22:00:25.108 # Server started, Redis version 3.2.8 78728:M 18 Apr 22:00:25.108 * DB loaded from disk: 0.000 seconds 78728:M 18 Apr 22:00:25.108 * The server is now ready to accept connections on port 6379 ^C78728:signal-handler (1492524027) Received SIGINT scheduling shutdown... 78728:M 18 Apr 22:00:27.286 # User requested shutdown... 78728:M 18 Apr 22:00:27.286 * Saving the final RDB snapshot before exiting. 78728:M 18 Apr 22:00:27.288 * DB saved on disk 78728:M 18 Apr 22:00:27.288 # Redis is now ready to exit, bye bye... iCocosdeMacBook-Pro:redis icocos$ redis-cli 127.0.0.1:6379&gt; ping PONG 127.0.0.1:6379&gt; set key1 name OK 127.0.0.1:6379&gt; get key1 &quot;name&quot; 127.0.0.1:6379&gt; PHP中使用的话&lt;?php $redis = new Redis(); $redis-&gt;connect(&apos;127.0.0.1&apos;, 6379); $redis-&gt;set(&apos;iCocos&apos;,1234); $redis-&gt;get(&apos;iCocos&apos;); 执行PHP文件，可以直接网页执行，也可以直接命令执行，方法基本上类似. Redis基本命令Redis数据类型及操作: String: 指令:{ set setnx :如果key已经存在, 返回0 setex :指定键值对的有效期, setex [key] [expire] [value] /类似于:set [key] [value] [EX expire] setrange :设置key-value值的子字符串,下标从0开始到第i位置,不包含i mset msetnx get getset getrange mget incr incrby decr decrby append strlen } Hash:是一个string类型的field-value的映射表,它的添加删除平均都是O(1)的效率 指令:{ hset hsetnx hmset hget hmget hincrby hexists hlen hdel hkeys hvals hgetall } List:是一个链表结构, 主要功能是pop,push,获取一个范围的所有值等,key理解为链表的名字list类型其实就是每一个子元素都是string类型的双向链表. 指令:{ lpush rpush linsert : linsert key [BEFORE|AFTER] [pivot] [value] 在list的特定的值的位置之前或之后添加字符串元素 lset : lset key [index] [value] 设置list中指定下标的元素值 lrem : lrem key [count] [value] 从list中删除count个和value相同的元素 count&gt;0,从头删除; count=0,全部删除; count&lt;0,从尾删除; ltrim : ltrim key [start] [stop] 保留key的值中指定范围内的数据 lpop : 从list的头部删除元素 rpop : 从list的尾部删除元素 rpoplpush : rpoplpush [source] [destination], 整个操作是原子的 从source的尾部移除元素,并添加到destination的头部,返回被移除的元素值 lindex : lindex key [index] 取list中index位置的元素 llen : 返回key对应list的长度 } Set: 指令:{} 键值指令: [ keys :返回给定pattern的所有key exists: 确认一个key是否存在 del : 删除一个key expire : 设置一个key的过期时间(秒为单位) ttl : 用于获取key的有效时长 persist : 移除给定key的过期时间 move : 把当前数据库中的key移动到其它库中 rename : 重命名key ] 服务指令: [ select :选择数据库存取, redis数据库编号从 0~15 默认16个库 dbsize :获取当前数据库的key info : 获取服务器的信息和统计,用于说明服务器的基础信息,包括版本启动时间等; config get: 获取服务器配置信息 flushdb : 删除当前选择数据库中的所有key flushall : 删除所有库中的所有key ] 主从复制基本配置: Redis主从复制中一个主服务可以有多个服务, 一个从服务可以有多个从服务; 对应配置,只需要修改redis.conf中的slaveof参数 #slaveof &lt;masterip&gt; &lt;masterport&gt; slaveof 127.0.0.1 6379 启动master #redis-server /usr/local/redis/conf/redis_6379.conf #redis-cli 启动slave(假设端口3689) #redis-server /usr/local/redis/conf/slave.conf #redis-cli -p 6389 如果master服务设置有密码,则需要配置masterauth参数 masterauth &lt;master-password&gt; 查看redis连接数 redis-cli info | grep connected 如果客户端的idle空闲时间太长,连接池维持了太多的连接,则需要把不用的连接及时释放掉; redis 127.0.0.1:6379&gt; client list redis 127.0.0.1:6379&gt; CONFIG SET timeout 30 Cluster集群配置 cluster-enabled yes 开启 每一个集群节点有一个集群配置文件;它不是手工创建的,是redis节点创建和更新的; 确保同一系统上运行的redis集群节点的配置文件名不重叠 redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件, 并且在此基础上实现了master-slave(主从)同步 pipeline 压缩请求数量 pipeline机制将多个命令汇聚到一个请求中,可以有效减少请求数量,减少网络延时。 script 压缩复杂请求 script核心思想是在redis命令里嵌入Lua脚本,来实现一些复杂操作。 cluster Redis 集群不像单机版本的 Redis 那样支持多个数据库，集群只有数据库 0，而且也不支持 SELECT 命令。 学习Redis命令请参考 http://www.redis.cn/http://www.cnblogs.com/woshimrf/p/5198361.html]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于事务入门]]></title>
    <url>%2F2017%2F08%2F19%2F%E5%85%B3%E4%BA%8E%E4%BA%8B%E5%8A%A1%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[事务一、事务的属性ACID 原子性(atomicity):事务是一个原子操作单元，要么全部执行，要么全部都不执行 一致性(consistent): 在事务开始和完成时，数据都必须保持一致性。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。 隔离性(lsolation): 数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的独立环境执行。这意味着事务处理过程中的中间状态对外部是不可见的。 持久性(Durable): 事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 ###二、并发事务带来的问题 1.更新丢失当两个或多个事务选择同一行，然后基于最初选定的值更新该行时，由于每个事务都不知道其他事务的存在，就会发生丢失更新问题－－最后的更新覆盖了由其他事务所做的更新。例如，两个编辑人员制作了同一文档的电子副本。每个编辑人员独立地更改其副本，然后保存更改后的副本，这样就覆盖了原始文档。最后保存其更改副本的编辑人员覆盖另一个编辑人员所做的更改。如果在一个编辑人员完成并提交事务之前，另一个编辑人员不能访问同一文件，则可避免此问题。 2.脏读一个事务正在对一条记录做修改，在这个事务完成并提交前，这条记录的数据就处于不一致状态；这时，另一个事务也来读取同一条记录，如果不加控制，第二个事务读取了这些“脏”数据，并据此做进一步的处理，就会产生未提交的数据依赖关系。这种现象被形象地叫做”脏读”。 3.不可重复读不可重复读（Non-Repeatable Reads）：一个事务在读取某些数据后的某个时间，再次读取以前读过的数据，却发现其读出的数据已经发生了改变、或某些记录已经被删除了！这种现象就叫做“不可重复读”。 4.幻读一个事务按相同的查询条件重新读取以前检索过的数据，却发现其他事务插入了满足其查询条件的新数据，这种现象就称为“幻读”。 ###三、事务的隔离级别 1.读取未提交的内容(Read Uncommitted) (1)所有事务都可以看到其他未提交事务的执行结果 (2)该隔离级别引发的问题就是脏读：读取到了未提交的数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#首先，修改隔离级别set tx_isolation=&apos;READ-UNCOMMITTED&apos;;select @@tx_isolation;+------------------+| @@tx_isolation |+------------------+| READ-UNCOMMITTED |+------------------+#事务A：启动一个事务start transaction;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务B：也启动一个事务(那么两个事务交叉了) 在事务B中执行更新语句，且不提交start transaction;update tx set num=10 where id=1;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+#事务A：那么这时候事务A能看到这个更新了的数据吗?select * from tx;+------+------+| id | num |+------+------+| 1 | 10 | ---&gt;可以看到！说明我们读到了事务B还没有提交的数据| 2 | 2 || 3 | 3 |+------+------+#事务B：事务B回滚,仍然未提交rollback;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务A：在事务A里面看到的也是B没有提交的数据select * from tx;+------+------+| id | num |+------+------+| 1 | 1 | ---&gt;脏读意味着我在这个事务中(A中)，事务B虽然没有提交，但它任何一条数据变化我 都可以看到！| 2 | 2 || 3 | 3 |+------+------+ 2.读取提交内容(Read Committed) 它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变。 但是它出现的问题是：不可重复读，不可重复读意味着我们在同一个事务中执行完全相同的select语句看到的可能是完全不一样的结果。 导致这种情况的原因有： (1)有一个交叉的事务有新的commit，导致了数据的改变 （2）一个数据库被多个实例操作时，同一个事务的其他实例在该实例处理其间有可能会有新的commit 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#首先修改隔离级别set tx_isolation=&apos;read-committed&apos;;select @@tx_isolation;+----------------+| @@tx_isolation |+----------------+| READ-COMMITTED |+----------------+#事务A：启动一个事务start transaction;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务B：也启动一个事务(那么两个事务交叉了) 在这事务中更新数据，且未提交start transaction;update tx set num=10 where id=1;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+#事务A：这个时候我们在事务A中能看到数据的变化吗?select * from tx; ---------------&gt;+------+------+ || id | num | |+------+------+ || 1 | 1 |---&gt;并不能看到！ || 2 | 2 | || 3 | 3 | |+------+------+ |——&gt;相同的select语句，结果却不一样 |#事务B：如果提交了事务B呢? |commit; | |#事务A: |select * from tx; ---------------&gt;+------+------+| id | num |+------+------+| 1 | 10 |---&gt;因为事务B已经提交了，所以在A中我们看到了数据变化| 2 | 2 || 3 | 3 |+------+------+ 3.可重读（Repeatable Read） (1)这是MySQL的默认事务隔离级别 (2)它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行 (3)此级别可能出现的问题——幻读(Phantom Read)：当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行 (4)InnoDB和Falcon存储引擎通过多版本并发控制(MVCC，Multiversion Concurrency Control)机制解决了该问题 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#首先，更改隔离级别set tx_isolation=&apos;repeatable-read&apos;;select @@tx_isolation;+-----------------+| @@tx_isolation |+-----------------+| REPEATABLE-READ |+-----------------+#事务A：启动一个事务start transaction;select * from tx;+------+------+| id | num |+------+------+| 1 | 1 || 2 | 2 || 3 | 3 |+------+------+#事务B：开启一个新事务(那么这两个事务交叉了) 在事务B中更新数据，并提交start transaction;update tx set num=10 where id=1;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+commit;#事务A：这时候即使事务B已经提交了,但A能不能看到数据变化？select * from tx;+------+------+| id | num |+------+------+| 1 | 1 | ---&gt;还是看不到的！(这个级别2不一样，也说明级别3解决了不可重复读问题)| 2 | 2 || 3 | 3 |+------+------+#事务A：只有当事务A也提交了，它才能够看到数据变化commit;select * from tx;+------+------+| id | num |+------+------+| 1 | 10 || 2 | 2 || 3 | 3 |+------+------+ 4.可串行化(Serializable) (1)这是最高的隔离级别 (2)它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之,它是在每个读的数据行上加上共享锁。 (3)在这个级别，可能导致大量的超时现象和锁竞争 123456789101112131415161718#首先修改隔离界别set tx_isolation=&apos;serializable&apos;;select @@tx_isolation;+----------------+| @@tx_isolation |+----------------+| SERIALIZABLE |+----------------+#事务A：开启一个新事务start transaction;#事务B：在A没有commit之前，这个交叉事务是不能更改数据的start transaction;insert tx values(&apos;4&apos;,&apos;4&apos;);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionupdate tx set num=10 where id=1;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
        <tag>事务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高并发大流解决量方案]]></title>
    <url>%2F2017%2F08%2F17%2F%E9%AB%98%E5%B9%B6%E5%8F%91%E5%A4%A7%E6%B5%81%E8%A7%A3%E5%86%B3%E9%87%8F%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[一、概念 QPS:每秒钟请求或者查询的数量，通常是指每秒相应请求数（http） QPS不等于并发连接数，QPS是每秒的HTTP请求数量，并发连接数是指系统同时处理的请求数量 峰值的每秒请求数（QPS）= (总PV数 80%)/(6小时秒数 20%) 峰值QPS的计算规律主要是80%的访问量集中在20%的访问时间 吞吐量：单位时间内处理请求的数量 响应时间：从请求发出到收到响应花费的时间 PV(page view)综合浏览量，即页面点击数。通常日PV在千万级就是高并发的网站 UV(unique visitor)独立访客，一定时间内相同访客访问网站，只计算为1个独立访客 带宽：计算带宽大小，我们需要关注两个指标，峰值流量和平均大小 日网站的带宽 = PV/统计时间（秒） 平均页面大小（KB） 8 压力测试 1.测试能承受的最大并发 2.测试最大承受QPS值 ab(apache benchmark)工具使用:ab会创建多个并发访问线程，模拟多个访问者同时对一个URL地址进行访问。 12345ab的基本使用ab -c 100 -n 5000 url并发请求100次，总共5000次测试时注意被测试机器的CPU、内存、网络都不超过最高限度的75% 此处介绍一些QPS的数值： 50：小型网站，一般服务器即可应付 100：假设数据库每次请求都在0.01秒内完成，单个页面只有一个sql，100QPS意味着1秒钟要完成100次请求，但是我们数据库不一定能完成100次查询。此时优化方案为：数据库缓存、数据库负载均衡 800：假设网站有百兆带宽，意味着实际出口的带宽为8M左右，如果每个页面只有10K，在这个QPS下，带宽已经吃完。此时的方案：CDN加速，负载均衡 1000： 假设使用nosql来缓存数据库查询（memcache或redis），每个页面对nosql的请求远大于直接对DB的请求 2000： 业务分离，做分布式存储 二、优化方案 流量优化： 防盗链处理 减少前端http请求（合并css、js等静态资源） 添加异步请求，减少http请求的并发量 启用浏览器的缓存和使用文件压缩 CDN加速，减轻服务器压力和带宽压力 服务端优化 页面静态化 并发处理 数据库优化 数据库缓存 分库分表，分区操作 读写分离 负载均衡 web服务器优化 负载均衡]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Web安全的一次探讨]]></title>
    <url>%2F2017%2F08%2F13%2FWeb%E5%AE%89%E5%85%A8%E7%9A%84%E4%B8%80%E6%AC%A1%E6%8E%A2%E8%AE%A8%2F</url>
    <content type="text"><![CDATA[一、SQL注入攻击(SQL Injection)攻击者把sql命令插入到web表单的输入域或页面请求的字符串，欺骗服务器执行恶意的sql命令。常见的sql注入攻击类似： 登录页面中输入内容直接用来构造动态的sql语句，例如：1$query = &apos;select * from users where login = &apos;. $username. &apos;and password = &apos;. $password; 攻击者如果在用户名或者密码框输入or &#39;1&#39; =1，这样我们执行的sql语句就变成了： 1select * from users where login = &apos;&apos; or &apos;1&apos; = 1 and ... 这样就绕过了我们的登录验证。类似的还有很多，用户通过输入恶意的sql命令来绕过我们的验证，欺骗我们的系统。 防范的方法： 检查变量数据类型和格式 过滤特殊的符号 绑定变量，使用预处理语句（当我们绑定变量的时候，就算有特殊字符sql也会认为是个变量而不是sql命令） 二、跨站脚本攻击(Cross Site Scripting, XSS；因为CSS被用了所以叫XSS)攻击者将恶意代码注入到网页上，其他用户在加载网页时就会执行代码，攻击者可能会得到各种私密的信息，如cookie等。例如： 12&lt;?php echo &apos;你好！&apos;.$_GET[&apos;name&apos;]; 如果用户传入一段脚本&lt;script&gt;[code]&lt;/script&gt;，那么脚本也会执行，如果code的内容是获取到cookie并发送到某个指定的位置，获取了敏感的信息。亦或是利用用户的身份去执行一些不正当的操作。 防范的方法： 输出的时候过滤特殊的字符，转换成html编码，过滤输出的变量（PHP可以使用htmlspecialchars） 三、跨站请求伪造攻击(Cross Site Request Forgeries, CSRF)攻击者伪造目标用户的HTTP请求，然后此请求发送到有CSRF漏洞的网站，网站执行此请求后，引发跨站请求伪造攻击。攻击者利用隐蔽的HTTP连接，让目标用户在不注意的情况下单击这个链接，由于是用户自己点击的，而他又是合法用户拥有合法权限，所以目标用户能够在网站内执行特定的HTTP链接，从而达到攻击者的目的。 用户刚刚登陆了银行A网站，建立了会话，A网站可以进行转账操作http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000在没有退出的情况下去访问危险网站B网站，B网站有一个图片是这样的&lt;img src=http://www.mybank.com/Transfer.php?toBankId=11&amp;money=1000&gt;，不小心点了B网站，用户发现账上少了1000块。 可能有人会说，修改操作并不会用get请求。那么假设银行A网站的表单如下 12345&lt;form action=&quot;Transfer.php&quot; method=&quot;POST&quot;&gt; &lt;p&gt;ToBankId: &lt;input type=&quot;text&quot; name=&quot;toBankId&quot; /&gt;&lt;/p&gt; &lt;p&gt;Money: &lt;input type=&quot;text&quot; name=&quot;money&quot; /&gt;&lt;/p&gt; &lt;p&gt;&lt;input type=&quot;submit&quot; value=&quot;Transfer&quot; /&gt;&lt;/p&gt;&lt;/form&gt; 后台处理页面如下： 1234567&lt;?phpsession_start();if (isset($_REQUEST[&apos;toBankId&apos;] &amp;&amp; isset($_POST[&apos;money&apos;]))&#123; buy_stocks($_REQUEST[&apos;toBankId&apos;],$_REQUEST[&apos;money&apos;]);&#125;?&gt; B网站这时候也相应的改了代码: 1234567891011121314151617181920&lt;html&gt; &lt;head&gt; &lt;script type=&quot;text/javascript&quot;&gt; function steal() &#123; iframe = document.frames[&quot;steal&quot;]; iframe.document.Submit(&quot;transfer&quot;); &#125; &lt;/script&gt; &lt;/head&gt; &lt;body onload=&quot;steal()&quot;&gt; &lt;iframe name=&quot;steal&quot; display=&quot;none&quot;&gt; &lt;form method=&quot;POST&quot; name=&quot;transfer&quot; action=&quot;http://www.myBank.com/Transfer.php&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;toBankId&quot; value=&quot;11&quot;&gt; &lt;input type=&quot;hidden&quot; name=&quot;money&quot; value=&quot;1000&quot;&gt; &lt;/form&gt; &lt;/iframe&gt; &lt;/body&gt;&lt;/html&gt; 用户一点到B网站，发现又少了1000块……. 防范方法： 对表单进行cookie hash校验，将一个随机值的hash写入cookie，每次提交表单，都在服务端对这个hash进行校验（建立在用户的cookie没有被盗取） 四、Session固定攻击(Session Fixation)攻击者预先设定session id，让合法用户使用这个session id来访问被攻击的应用程序，一旦用户的会话ID被成功固定，攻击者就可以通过此session id来冒充用户访问应用程序。例如： 攻击者先访问目标网站，获得了自己的session_id，如SID=123 攻击者给目标用户发送链接，并带上了自己的session_id，如http:///www.bank.com/?SID=123， 目标用户点击了http:///www.bank.com/?SID=123，输入用户名密码登录，由于session_id不会变更，那么攻击者就可以通过访问http:///www.bank.com/?SID=123来获取目标用户的身份。 防范方法： 定期更改session_id 更改session_id的名字 五、Session劫持(Session Hijacking)攻击者利用各种手段来获取目标用户的session id。一旦获取到session id，那么攻击者可以利用目标用户的身份来登录网站，获取目标用户的操作权限。 攻击者获取目标用户session id的方法: 暴力破解:尝试各种session id，直到破解为止; 计算:如果session id使用非随机的方式产生，那么就有可能计算出来; 窃取:使用网络截获，xss攻击等方法获得防范方法： 定期更改session id 更改session的名称 关闭透明化session id 设置HttpOnly。通过设置Cookie的HttpOnly为true，可以防止客户端脚本访问这个Cookie，从而有效的防止XSS攻击。 六、文件上传漏洞(File Upload Attack)攻击者利用程序缺陷绕过系统对文件的验证与处理策略将恶意代码上传到服务器并获得执行服务器端命令的能力。 常用的攻击手段有： 上传Web脚本代码，Web容器解释执行上传的恶意脚本； 上传Flash跨域策略文件crossdomain.xml，修改访问权限(其他策略文件利用方式类似)； 上传病毒、木马文件，诱骗用户和管理员下载执行； 上传包含脚本的图片，某些浏览器的低级版本会执行该脚本，用于钓鱼和欺诈。总的来说，利用的上传文件要么具备可执行能力(恶意代码)，要么具备影响服务器行为的能力(配置文件)。防范方法： 文件上传的目录设置为不可执行； 判断文件类型，设置白名单。对于图片的处理，可以使用压缩函数或者resize函数，在处理图片的同时破坏图片中可能包含的HTML代码； 使用随机数改写文件名和文件路径：一个是上传后无法访问；再来就是像shell、.php 、.rar和crossdomain.xml这种文件，都将因为重命名而无法攻击； 单独设置文件服务器的域名：由于浏览器同源策略的关系，一系列客户端攻击将失效，比如上传crossdomain.xml、上传包含Javascript的XSS利用等问题将得到解决。 MYSQL安全 使用预处理语句防止sql注入 写入数据库的数据要进行特殊字符转义 查询的错误信息不要返回给用户，将错误记录到日志 定期做数据库备份 不给查询用户root权限，合理分配权限 关闭远程访问数据库的权限]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于流量优化方案]]></title>
    <url>%2F2017%2F08%2F11%2F%E5%85%B3%E4%BA%8E%E6%B5%81%E9%87%8F%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[一、防盗链盗链 在自己的页面上展示一些并不在自己服务器上的内容。获取他人服务器上的资源地址，绕过别人的资源展示页面，直接在自己的页面上向用户输出防盗链则是防止别人通过技术手段盗取本站的资源，让不是本站展示的资源链接失效 防盗链原理：通过referer或者签名（在资源地址后面带上一串签名，每次收到请求验证签名），网站可以检测目标访问的来源网页，如果是资源文件，则可以跟踪到他显示的网页地址。一旦检测到来源不是本站则进行组织或者返回指定页面。 实现 Referer 123456789101112131415161718Nginx模块 ngx_http_referer_module 用来阻挡来源非法的域名请求Nginx指令 valid_referers 全局变量$invalid_referervalid_referers none|blocked|server_names|string....;none: referer 来源头部为空blocked: referer不为空，但是里面的值被代理或者防火墙删除了，这些值并不以http：//或者https://开头server_names: referer来源头部包含当前的server_names例如：location ~ .*\.(gif|jpeg|png|flv|swf|rar|zip)$&#123; valid_referer none blocked haobin.com *.haobin.com; if($invalid_referer) &#123; #return 403; rewrite ^/http://www.haobin.com/403.html; &#125;&#125; 如果有人伪造referer，可以通过签名的方法解决 加密签名 通过第三方模块HttpAccessKeyModule实现Nginx防盗链 1234567891011121314首先安装这个模块accesskey on|off 模块开关accesskey_hashmethod md5 | sha-1 指定签名加密方式accesskey_arg GET参数的名称accesskey_signature 加密规则例如：location ~ .*\.(gif|jpeg|png|flv|swf|zip|rar)$&#123; accesskey on; accesskey_hashmethod md5; accesskey_arg &quot;key&quot;; accesskey_signature &quot;sign$remote_addr&quot;;&#125; 二、减少HTTP请求性能黄金法则：只有10%-20%的最终用户响应时间花在接受请求的HTML文档上，剩下的80%-90%时间花在HTML文档所引用的组件（图片、css、script等）进行的http请求上 1.图片地图：允许在一个图片上关联多个url，目标的url选择取决于用户单击了图片上的哪个位置。例如有五张图片，每张图片对应一个超链接。此时就产生了五个http请求，我们将五张图片合成为一张图片，然后以图片的位置定位超链接。 1234567实现&lt;map&gt; &lt;area&gt;&lt;/area&gt; &lt;area&gt;&lt;/area&gt; &lt;area&gt;&lt;/area&gt; ........&lt;/map&gt; 2.CSS Sprites（CSS 精灵）通过使用合并图片，指定css的background-image和background-position来显示元素 12background-position属性background-position:x,y; x和y可以写正值也可以写负值，我们可以想象图片左上方(0,0)，以(0,0)坐标向右的是负数的x轴，以(0,0)坐标向下的是负数的y轴 3.合并脚本和样式表4.图片使用base64编码减少页面请求数 采用Base64编码直接将图片嵌入网页当中 三、浏览器缓存和压缩技术1.HTTP缓存分类 http缓存类型中，请求成功会有三种情况： 200 from cache：直接从本地缓存中获取相应，最快速，最省流量（network的size字段） 304 not modify： 协商缓存，浏览器在没有命中的情况下请求头中发送一定的校验数据到服务端，如果服务端没有改变，浏览器从本地缓存相应，返回304。 该方式，只返回一些基本的头信息，不发送实际的相应体 200 ok： 以上两种缓存失败，服务器返回完整的相应。 该方式没有用到缓存，是最慢的。 2.本地缓存1234567891011Pragma： HTTP1.0的属性，该字段设置为no-cache，会告知浏览器禁用本地缓存Expires: HTTP1.0的属性，用来启用本地缓存。expires的值对应为一个类似Thu, 31 Dec 2017 20:11:20 GMT的格林威治时间，告诉浏览器如果还没有到该时间，则缓存有效，无须发送请求。这个时间是服务器返回的，是以服务器的时间为基准，如果服务器和客户端的时间不一致就可能产生差错。Cache-Control: 告知浏览器缓存过期的时间间隔。no-store: 禁止浏览器缓存no-cache: 不允许直接使用本地缓存，先发起请求和服务器协商max-age=delta-seconds: 告知浏览器响应本地缓存的最长期限，以秒为单位优先级：Pragrma &gt; Cache-Control &gt; Expires 3.协商缓存 浏览器没有命中本地缓存，如果本地缓存过期或者响应不允许直接使用本地缓存，那么浏览器会发起服务端请求，服务端会验证数据是否被修改，如果没有被修改就通知浏览器使用本地缓存 相关的Header： 12345Last-Modified: 通知浏览器资源的最后修改时间（一个格林威治时间）If-Modified-Since: 得到资源最后修改时间后，会将这个If-Modified-Since（Last-Modified的值）提交到服务器做检查，如果没有修改，就返回304ETag: HTTP1.1属性，指纹标识符，如果文件发生更改，指纹会改变If-None_Match: 本地缓存失效，会携带此值（ETage 的值）去请求服务端，服务端判断资源是否改变，如果没有改变，直接使用本地缓存，返回304 4.缓存对象的选择 不变的内容适合本地缓存：图像，js，css，可下载的媒体文件等 适合协商缓存的文件：HTML文件，经常替换的图片，经常修改的js、css等文件 5.Nginx配置缓存策略1234567891011*****通过PHP模拟Last-Modified-&gt;If-Modified-Since模式******// 读取上一次修改时间$since = $_SERVER[&apos;HTTP_IF_MODIFIED-SINCE&apos;];$lifetime = 3600; // 模拟缓存一分钟// 如果没过期就返回304if(strtotime($since) + $lifetiem &gt; time())&#123; header(&apos;HTTP/1.1 304 NOT MODIFIED&apos;); exit;&#125;// 返回Last-Modified相应头header(&apos;Last-Modified: &apos;.gmdate(&apos;D, d M Y H:i:s&apos;, time())). &apos;GMT&apos;); Nginx缓存配置： 本地缓存配置指令： add_header： 添加状态码为2XX和3XX的响应头 add_header name value \[always]; 语法格式 可以通过该指令来设置Pragma/Expires/Cache-Control expires指令： 通知浏览器过期时长 expires time; 语法格式 为负值表示Cache-Control: no-cache 为正直表示Cache-Control:max-age=指定时间 123456789101112Nginx缓存配置：# 遇到图片等资源就缓存30天location ~ .*\.(gif|jpeg|png|flv|swf|zip|rar)$&#123; expires 30d;&#125;$ 遇到js/css等资源就缓存12小时location ~ .*\.(js|css)?$&#123; expires 12h;&#125;# expires max; 代表设置十年的缓存 Nginx协商缓存配置： ETage指令： 指定签名 etage: on|off; 开关，默认是on 12345location ~ .*\.(gif|jpeg|png|flv|swf|zip|rar)$&#123; # 默认是开启的 etag off;&#125; 6.前端代码和资源的压缩 JavaScript压缩：去掉多余的空格和回车，替换长变量名，简写代码等 CSS压缩： 同样是去掉空白符、注释并且优化CSS语义规则 图片压缩： 借助压缩工具压缩（tinypng、jpegMini、imageoption等） Gzip压缩 12345678Nginx配置:gizp on|off; #是否开启gzipgzip_buffers 32 4K|16 8k #缓冲（在内存中有几块 每块多大）gzip_comp_level [1-9] #压缩级别（推荐使用6） 级别越高，压缩越小，越占用CPU资源gzip_disable #正则表达式匹配UA 什么样的uri不进行gzip gzip_min_length 200 #开始压缩的最小长度gzip_http_version 1.0|1.1 #开始压缩的http版本协议gzip_types text/plain application/xml #对那些类型进行压缩，如text、css、html等 现在前端有很多工具可以对资源进行压缩，打包等。如grunt、webpack等已经很流行了。比较流行的前端框架也有相应的脚手架来帮助打包：vue-cli、angular-cli等]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>服务器</tag>
        <tag>流量</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL锁机制和PHP锁机制]]></title>
    <url>%2F2017%2F08%2F06%2FMySQL%E9%94%81%E6%9C%BA%E5%88%B6%E5%92%8CPHP%E9%94%81%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[PHP中的文件锁 （锁的是文件，不是表）文件锁的文件与表有什么关系？：一点关系也没有，与令牌相似，谁拿到谁操作。所以表根本没锁。测试时，有个文件就行，叫什么名无所谓 bool flock ( int handle, int operation [, int &amp;wouldblock] );flock() 操作的 handle 必须是一个已经打开的文件指针。operation 可以是以下值之一： 要取得共享锁定（读取程序），将 operation 设为 LOCK_SH（PHP 4.0.1 以前的版本设置为 1） 要取得独占锁定（写入程序），将 operation 设为 LOCK_EX（PHP 4.0.1 以前的版本中设置为 2） 要释放锁定（无论共享或独占），将 operation 设为 LOCK_UN（PHP 4.0.1 以前的版本中设置为 3） 如果你不希望 flock() 在锁定时堵塞，则给 operation 加上 LOCK_NB（PHP 4.0.1 以前的版本中设置为4） 建两个文件(1) a.php$file = &quot;temp.txt&quot;; $fp = fopen($file , &apos;w&apos;); if(flock($fp , LOCK_EX)){ fwrite($fp , &quot;abc\n&quot;); sleep(10); fwrite($fp , &quot;123\n&quot;); flock($fp , LOCK_UN); } fclose($fp); (2) b.php$file = &quot;temp.txt&quot;; $fp = fopen($file , &apos;r&apos;); echo fread($fp , 100); fclose($fp); 运行 a.php 后，马上运行 b.php ，可以看到输出： abc 等 a.php 运行完后运行 b.php ，可以看到输出： abc 123 显然，当 a.php 写文件时数据太大，导致时间比较长时，这时 b.php 读取数据不完整 修改 b.php 为： $file = &quot;temp.txt&quot;; $fp = fopen($file , &apos;r&apos;); if(flock($fp , LOCK_EX)){ echo fread($fp , 100); flock($fp , LOCK_UN); } else{ echo &quot;Lock file failed...\n&quot;; } fclose($fp); 运行 a.php 后，马上运行 b.php ，可以发现 b.php 会等到 a.php 运行完成后(即 10 秒后)才显示： abc 123 读取数据完整，但时间过长，他要等待写锁释放。 修改 b.php 为： $file = &quot;temp.txt&quot;; $fp = fopen($file , &apos;r&apos;); if(flock($fp , LOCK_SH | LOCK_NB)){ echo fread($fp , 100); flock($fp , LOCK_UN); } else{ echo &quot;Lock file failed...\n&quot;; } fclose($fp); 运行 a.php 后，马上运行 b.php ，可以看到输出： Lock file failed… 证明可以返回锁文件失败状态，而不是向上面一样要等很久。 结论： 建议作文件缓存时，选好相关的锁，不然可能导致读取数据不完整，或重复写入数据。file_get_contents 好像选择不了锁，不知道他默认用的什么锁，反正和不锁得到的输出一样，是不完整的数据。 我是要做文件缓存，所以只需要知道是否有写锁存在即可，有的话就查数据库就可以了。测试环境：Linux(Ubuntu 6) , PHP 5.1.2 , Apache 2 再转：文件锁有两种：共享锁和排他锁，也就是读锁(LOCK_SH)和写锁(LOCK_EX)文件的锁一般这么使用： $fp = fopen(&quot;filename&quot;, &quot;a&quot;); flock($fp, LOCK_SH) or die(&quot;lock error&quot;) $str = fread($fp, 1024); flock($fp, LOCK_UN); fclose($fp); 注意fwrite之后，文件立即就被更新了，而不是等fwrite然后fclose之后文件才会更新，这个可以通过在fwrite之后fclose之前读取这个文件进行检查 但是什么时候使用lock_ex什么时候使用lock_sh呢？ 读的时候：如果不想出现dirty数据，那么最好使用lock_sh共享锁。可以考虑以下三种情况： 如果读的时候没有加共享锁，那么其他程序要写的话（不管这个写是加锁还是不加锁）都会立即写成功。如果正好读了一半，然后被其他程序给写了，那么读的后一半就有可能跟前一半对不上（前一半是修改前的，后一半是修改后的） 如果读的时候加上了共享锁（因为只是读，没有必要使用排他锁），这个时候，其他程序开始写，这个写程序没有使用锁，那么写程序会直接修改这个文件，也会导致前面一样的问题 最理想的情况是，读的时候加锁(lock_sh),写的时候也进行加锁(lock_ex),这样写程序会等着读程序完成之后才进行操作，而不会出现贸然操作的情况 写的时候：如果多个写程序不加锁同时对文件进行操作，那么最后的数据有可能一部分是a程序写的，一部分是b程序写的如果写的时候加锁了，这个时候有其他的程序来读，那么他会读到什么东西呢？ 如果读程序没有申请共享锁，那么他会读到dirty的数据。比如写程序要写a,b,c三部分，写完a,这时候读读到的是a，继续写b，这时候读读到的是ab，然后写c，这时候读到的是abc. 如果读程序在之前申请了共享锁，那么读程序会等写程序将abc写完并释放锁之后才进行读。 总结：项目中应该只使用PHP中的文件锁，尽量避免锁表，因为如果表被锁定了，那么整个网站中所有和这个表相关的功能都被拖慢了（例如：前台很多用户一直下订单，商品表mysql锁表，其他与商品表相关的操作一直处于阻塞状态【读不出来商品表】，因为一个功能把整个网站速度拖慢）。 比如在一个O2O外卖项目中，中午12-2点，晚上6点都是订单高并发时，这种情况下，MySQL锁显然是不考虑的，用户体验太差。其实根据实际的需求，外卖可以不用设计库存量的，当然除了秒杀活动模块还是需要php文件锁的。 应用场景： 高并发下单时，减库存量时要加锁 高并发抢单、抢票时要使用 MySQL锁示例代码： &lt;?php /** 模拟秒杀活动-- 商品100件 CREATE TABLE a ( id int comment &apos;模拟100件活动商品的数量&apos; ); INSERT INTO a VALUES(100); 模仿：以10的并发量访问这个脚本！ 使用apache自带的ab.exe软件 */ error_reporting(0); mysql_connect(&apos;localhost&apos;,&apos;root&apos;,&apos;admin123&apos;); mysql_select_db(&apos;test&apos;); # mysql 锁 mysql_query(&apos;LOCK TABLE a WRITE&apos;);// 只有一个客户端可以锁定表，其他客户端阻塞在这 $rs = mysql_query(&apos;SELECT id FROM a&apos;); $id = mysql_result($rs, 0, 0); if($id &gt; 0) { --$id; mysql_query(&apos;UPDATE a SET id=&apos;.$id); } # mysql 解锁 mysql_query(&apos;UNLOCK TABLES&apos;); PHP文件锁示例代码： &lt;?php /** 模拟秒杀活动-- 商品100件 CREATE TABLE a ( id int comment &apos;模拟100件活动商品的数量&apos; ); INSERT INTO a VALUES(100); 模仿：以10的并发量访问这个脚本！ 使用apache自带的ab.exe软件 */ error_reporting(0); mysql_connect(&apos;localhost&apos;,&apos;root&apos;,&apos;admin123&apos;); mysql_select_db(&apos;test&apos;); # php中的文件锁 $fp = fopen(&apos;./a.lock&apos;, &apos;r&apos;); // php的文件锁和表没关系，随便一个文件即可 flock($fp, LOCK_EX);// 排他锁 $rs = mysql_query(&apos;SELECT id FROM a&apos;); $id = mysql_result($rs, 0, 0); if($id &gt; 0) { --$id; mysql_query(&apos;UPDATE a SET id=&apos;.$id); } # php的文件锁，释放锁 flock($fp, LOCK_UN); fclose($fp); MYSQL中的锁：语法 ：LOCK TABLE 表名1 READ|WRITE, 表名2 READ|WRITE ……………… 【锁表】UNLOCK TABLES 【释放表】 Read:读锁|共享锁 ： 所有的客户端只能读这个表不能写这个表 Write:写锁|排它锁： 所有当前锁定客户端可以操作这个表，其他客户端只能阻塞 注意：在锁表的过程中只能操作被锁定的表，如果要操作其他表，必须把所有要操作的表都锁定起来！ 1.表级锁定（table-level）表级别的锁定是MySQL各存储引擎中最大颗粒度的锁定机制。该锁定机制最大的特点是实现逻辑非常简单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。由于表级锁一次会将整个表锁定，所以可以很好的避免困扰我们的死锁问题。当然，锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并大度大打折扣。使用表级锁定的主要是MyISAM等一些非事务性存储引擎。 2.行级锁定（row-level）行级锁定最大的特点就是锁定对象的颗粒度很小，也是目前各大数据库管理软件所实现的锁定颗粒度最小的。由于锁定颗粒度很小，所以发生锁定资源争用的概率也最小，能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能。虽然能够在并发处理能力上面有较大的优势，但是行级锁定也因此带来了不少弊端。由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也更多，带来的消耗自然也就更大了。此外，行级锁定也最容易发生死锁。使用行级锁定的主要是InnoDB存储引擎。 3.页级锁定（page-level）页级锁定是MySQL中比较独特的一种锁定级别，在其他数据库管理软件中也并不是太常见。页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力也同样是介于上面二者之间。另外，页级锁定和行级锁定一样，会发生死锁。在数据库实现资源锁定的过程中，随着锁定资源颗粒度的减小，锁定相同数据量的数据所需要消耗的内存数量是越来越多的，实现算法也会越来越复杂。不过，随着锁定资源颗粒度的减小，应用程序的访问请求遇到锁等待的可能性也会随之降低，系统整体并发度也随之提升。 总结： 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低； 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高； 页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般。 适用：从锁的角度来说，表级锁更适合于以查询为主，只有少量按索引条件更新数据的应用，如Web应用；而行级锁则更适合于有大量按索引条件并发更新少量不同数据，同时又有并发查询的应用，如一些在线事务处理（OLTP）系统。 二、表级锁定在mysql中，MyISAM引擎使用的锁定机制完全是mysql的表级锁定，下面将以MYISAM引擎作为示例 1.MySQL表级锁的模式 MySQL的表级锁有两种模式：表共享读锁（Table Read Lock）和表独占写锁（Table Write Lock）。 兼容性： 对MyISAM表的读操作，不会阻塞其他用户对同一表的读请求，但会阻塞对同一表的写请求； 对MyISAM表的写操作，则会阻塞其他用户对同一表的读和写操作； MyISAM表的读操作与写操作之间，以及写操作之间是串行的。当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作。其他线程的读、写操作都会等待，直到锁被释放为止。 2.加锁 MyISAM在执行查询语句（SELECT）前，会自动给涉及的所有表加读锁，在执行更新操作（UPDATE、DELETE、INSERT等）前，会自动给涉及的表加写锁，这个过程并不需要用户干预，因此，用户一般不需要直接用LOCK TABLE命令给MyISAM表显式加锁。 3.MyISAM锁的优化 对于MyISAM存储引擎，虽然使用表级锁定在锁定实现的过程中比实现行级锁定或者页级锁所带来的附加成本都要小，锁定本身所消耗的资源也是最少。但是由于锁定的颗粒度比较到，所以造成锁定资源的争用情况也会比其他的锁定级别都要多，从而在较大程度上会降低并发处理能力。所以，在优化MyISAM存储引擎锁定问题的时候，最关键的就是如何让其提高并发度。由于锁定级别是不可能改变的了，所以我们首先需要尽可能让锁定的时间变短，然后就是让可能并发进行的操作尽可能的并发。 （1）查询表锁争用情况 1234567mysql&gt; show status like &apos;table%&apos;;+----------------------------+---------+| Variable_name | Value |+----------------------------+---------+| Table_locks_immediate | 100 || Table_locks_waited | 11 |+----------------------------+---------+ 这里有两个状态变量记录MySQL内部表级锁定的情况，两个变量说明如下： Table_locks_immediate：产生表级锁定的次数; Table_locks_waited：出现表级锁定争用而发生等待的次数； 两个状态值都是从系统启动后开始记录，出现一次对应的事件则数量加1。如果这里的Table_locks_waited状态值比较高，那么说明系统中表级锁定争用现象比较严重，就需要进一步分析为什么会有较多的锁定资源争用了 （2）缩短锁定时间 如何让锁定时间尽可能的短呢？唯一的办法就是让我们的Query执行时间尽可能的短。 + a)尽两减少大的复杂Query，将复杂Query分拆成几个小的Query分布进行； + b)尽可能的建立足够高效的索引，让数据检索更迅速； + c)尽量让MyISAM存储引擎的表只存放必要的信息，控制字段类型； + d)利用合适的机会优化MyISAM表数据文件 (3)分离并行的操作]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL两种存储引擎- MyISAM和InnoDB 简单总结]]></title>
    <url>%2F2017%2F08%2F03%2FMySQL%E4%B8%A4%E7%A7%8D%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E-%20MyISAM%E5%92%8CInnoDB%20%E7%AE%80%E5%8D%95%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[MyISAM是MySQL的默认数据库引擎（5.5版之前），由早期的ISAM（Indexed Sequential Access Method：有索引的顺序访问方法）所改良。虽然性能极佳，但却有一个缺点：不支持事务处理（transaction）。不过，在这几年的发展下，MySQL也导入了InnoDB（另一种数据库引擎），以强化参考完整性与并发违规处理机制，后来就逐渐取代MyISAM。 InnoDB，是MySQL的数据库引擎之一，为MySQL AB发布binary的标准之一。InnoDB由Innobase Oy公司所开发，2006年五月时由甲骨文公司并购。与传统的ISAM与MyISAM相比，InnoDB的最大特色就是支持了ACID兼容的事务（Transaction）功能，类似于PostgreSQL。目前InnoDB采用双轨制授权，一是GPL授权，另一是专有软件授权。 MyISAM和InnoDB两者之间有着明显区别，简单梳理如下: 1) 事务支持MyISAM不支持事务，而InnoDB支持。InnoDB的AUTOCOMMIT默认是打开的，即每条SQL语句会默认被封装成一个事务，自动提交，这样会影响速度，所以最好是把多条SQL语句显示放在begin和commit之间，组成一个事务去提交。 MyISAM是非事务安全型的，而InnoDB是事务安全型的，默认开启自动提交，宜合并事务，一同提交，减小数据库多次提交导致的开销，大大提高性能。 2) 存储结构MyISAM：每个MyISAM在磁盘上存储成三个文件。第一个文件的名字以表的名字开始，扩展名指出文件类型。.frm文件存储表定义。数据文件的扩展名为.MYD (MYData)。索引文件的扩展名是.MYI (MYIndex)。 InnoDB：所有的表都保存在同一个数据文件中（也可能是多个文件，或者是独立的表空间文件），InnoDB表的大小只受限于操作系统文件的大小，一般为2GB。 3) 存储空间MyISAM：可被压缩，存储空间较小。支持三种不同的存储格式：静态表(默认，但是注意数据末尾不能有空格，会被去掉)、动态表、压缩表。 InnoDB：需要更多的内存和存储，它会在主内存中建立其专用的缓冲池用于高速缓冲数据和索引。 4) 可移植性、备份及恢复MyISAM：数据是以文件的形式存储，所以在跨平台的数据转移中会很方便。在备份和恢复时可单独针对某个表进行操作。 InnoDB：免费的方案可以是拷贝数据文件、备份 binlog，或者用 mysqldump，在数据量达到几十G的时候就相对痛苦了。 5) 事务支持MyISAM：强调的是性能，每次查询具有原子性,其执行数度比InnoDB类型更快，但是不提供事务支持。 InnoDB：提供事务支持事务，外部键等高级数据库功能。 具有事务(commit)、回滚(rollback)和崩溃修复能力(crash recovery capabilities)的事务安全(transaction-safe (ACID compliant))型表。 6) AUTO_INCREMENTMyISAM：可以和其他字段一起建立联合索引。引擎的自动增长列必须是索引，如果是组合索引，自动增长可以不是第一列，他可以根据前面几列进行排序后递增。 InnoDB：InnoDB中必须包含只有该字段的索引。引擎的自动增长列必须是索引，如果是组合索引也必须是组合索引的第一列。 7) 表锁差异MyISAM：只支持表级锁，用户在操作myisam表时，select，update，delete，insert语句都会给表自动加锁，如果加锁以后的表满足insert并发的情况下，可以在表的尾部插入新的数据。 InnoDB：支持事务和行级锁，是innodb的最大特色。行锁大幅度提高了多用户并发操作的新能。但是InnoDB的行锁，只是在WHERE的主键是有效的，非主键的WHERE都会锁全表的。 MyISAM锁的粒度是表级，而InnoDB支持行级锁定。简单来说就是, InnoDB支持数据行锁定，而MyISAM不支持行锁定，只支持锁定整个表。即MyISAM同一个表上的读锁和写锁是互斥的，MyISAM并发读写时如果等待队列中既有读请求又有写请求，默认写请求的优先级高，即使读请求先到，所以MyISAM不适合于有大量查询和修改并存的情况，那样查询进程会长时间阻塞。因为MyISAM是锁表，所以某项读操作比较耗时会使其他写进程饿死。 8) 全文索引MyISAM：支持(FULLTEXT类型的)全文索引 InnoDB：不支持(FULLTEXT类型的)全文索引，但是innodb可以使用sphinx插件支持全文索引，并且效果更好。 全文索引是指对char、varchar和text中的每个词（停用词除外）建立倒排序索引。MyISAM的全文索引其实没啥用，因为它不支持中文分词，必须由使用者分词后加入空格再写到数据表里，而且少于4个汉字的词会和停用词一样被忽略掉。 另外，MyIsam索引和数据分离，InnoDB在一起，MyIsam天生非聚簇索引，最多有一个unique的性质，InnoDB的数据文件本身就是主键索引文件，这样的索引被称为“聚簇索引” 9) 表主键MyISAM：允许没有任何索引和主键的表存在，索引都是保存行的地址。 InnoDB：如果没有设定主键或者非空唯一索引，就会自动生成一个6字节的主键(用户不可见)，数据是主索引的一部分，附加索引保存的是主索引的值。InnoDB的主键范围更大，最大是MyISAM的2倍。 10) 表的具体行数MyISAM：保存有表的总行数，如果select count(*) from table;会直接取出出该值。 InnoDB：没有保存表的总行数(只能遍历)，如果使用select count(*) from table；就会遍历整个表，消耗相当大，但是在加了wehre条件后，myisam和innodb处理的方式都一样。 11) CURD操作MyISAM：如果执行大量的SELECT，MyISAM是更好的选择。 InnoDB：如果你的数据执行大量的INSERT或UPDATE，出于性能方面的考虑，应该使用InnoDB表。DELETE 从性能上InnoDB更优，但DELETE FROM table时，InnoDB不会重新建立表，而是一行一行的删除，在innodb上如果要清空保存有大量数据的表，最好使用truncate table这个命令。 12) 外键MyISAM：不支持 InnoDB：支持 13) 查询效率没有where的count()使用MyISAM要比InnoDB快得多。因为MyISAM内置了一个计数器，count()时它直接从计数器中读，而InnoDB必须扫描全表。所以在InnoDB上执行count()时一般要伴随where，且where中要包含主键以外的索引列。为什么这里特别强调“主键以外”？因为InnoDB中primary index是和raw data存放在一起的，而secondary index则是单独存放，然后有个指针指向primary key。所以只是count()的话使用secondary index扫描更快，而primary key则主要在扫描索引同时要返回raw data时的作用较大。MyISAM相对简单，所以在效率上要优于InnoDB，小型应用可以考虑使用MyISAM。 通过上述的分析，基本上可以考虑使用InnoDB来替代MyISAM引擎了，原因是InnoDB自身很多良好的特点，比如事务支持、存储 过程、视图、行级锁定等等，在并发很多的情况下，相信InnoDB的表现肯定要比MyISAM强很多。另外，任何一种表都不是万能的，只用恰当的针对业务类型来选择合适的表类型，才能最大的发挥MySQL的性能优势。如果不是很复杂的Web应用，非关键应用，还是可以继续考虑MyISAM的，这个具体情况可以自己斟酌。 MyISAM和InnoDB两者的应用场景： 1) MyISAM管理非事务表。它提供高速存储和检索，以及全文搜索能力。如果应用中需要执行大量的SELECT查询，那么MyISAM是更好的选择。 2) InnoDB用于事务处理应用程序，具有众多特性，包括ACID事务支持。如果应用中需要执行大量的INSERT或UPDATE操作，则应该使用InnoDB，这样可以提高多用户并发操作的性能。 但是实际场景中，针对具体问题需要具体分析，一般而言可以遵循以下几个问题： 数据库是否有外键？ 是否需要事务支持？ 是否需要全文索引？ 数据库经常使用什么样的查询模式？在写多读少的应用中还是Innodb插入性能更稳定，在并发情况下也能基本，如果是对读取速度要求比较快的应用还是选MyISAM。 数据库的数据有多大？ 大尺寸倾向于innodb，因为事务日志，故障恢复。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SQL查询优化简介]]></title>
    <url>%2F2017%2F07%2F29%2FSQL%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[在了解MySQL查询优化之前，先来看看关于MySQL的执行流程，因为关于数据层优化来说，归根结底还是关于IO的优化，只有知道了关于MySQL的执行流程，我们才能把优化做的更好 MySQL的流程分析1.连接1. 客户端发起一条Query请求，监听客户端的‘连接管理模块’接收请求 2. 将请求转发到‘连接进/线程模块’ 3. 调用‘用户模块’来进行授权检查 4. 通过检查后，‘连接进/线程模块’从‘线程连接池’中取出空闲的被缓存的连接线程和客户端请求对接，如果失败则创建一个新的连接请求 2.处理1. 先查询缓存，检查Query语句是否完全匹配，接着再检查是否具有权限，都成功则直接取数据返回 2. 上一步有失败则转交给‘命令解析器’，经过词法分析，语法分析后生成解析树 3. 接下来是预处理阶段，处理解析器无法解决的语义，检查权限等，生成新的解析树 4. 再转交给对应的模块处理 5. 如果是SELECT查询还会经由‘查询优化器’做大量的优化，生成执行计划 6. 模块收到请求后，通过‘访问控制模块’检查所连接的用户是否有访问目标表和目标字段的权限 7. 有则调用‘表管理模块’，先是查看table cache中是否存在，有则直接对应的表和获取锁，否则重新打开表文件 8. 根据表的meta数据，获取表的存储引擎类型等信息，通过接口调用对应的存储引擎处理 9. 上述过程中产生数据变化的时候，若打开日志功能，则会记录到相应二进制日志文件中 3.结果1. Query请求完成后，将结果集返回给‘连接进/线程模块’ 2. 返回的也可以是相应的状态标识，如成功或失败等 3. ‘连接进/线程模块’进行后续的清理工作，并继续等待请求或断开与客户端的连接 4:小结 用户模块校验用户,然后去线程连接池拿线程(连接足够的话),找命令分发器,到查询缓存模块查SQL语句,如果没有,走命令解析器,然后访问控制模块,设定用户的权限,设定好后走表管理模块,获取锁和缓存,然后获取各种信息,存储的方式:存储引擎,从存储引擎获取数据,然后返回 一、优化的入手点 查找分析查询慢的原因 1.记录慢查询日志（慢查询日志的使用以及分析见本章slow_query.md) 2.show profile: set profiling = 1;开启，服务器上执行的所有语句会检测消耗的时间，存到临时表 show profiles 查看语句执行消耗的时间 show profile for query 临时表ID 查看某个查询的详细消耗 3.分析单条语句使用explain（explain.md查看explain用法） 优化查询中的数据访问 1.访问数据太多导致查询性能下降，尽量不要使用select * 2.确定应用程序逻辑需要的数据量，使用limit返回一部分即可 3.确定MySQL是否检索了索引，避免全表扫描 4.重复查询相同的数据，可以缓存数据，下次直接读缓存 5.是否存在扫描额外的记录（使用explain分析发现需要扫描大量的数据，却只返回少数行）： 使用索引覆盖扫描，把所用到的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果 优化长难语句 一个复杂的查询和多个简单的查询相比较，mysql每秒可以查询上百万的数据，响应给客户端的速度就要慢得多，所以使用尽可能少的查询是好的，但是有时候把一个大查询分解为多个小的查询也是必要的。因为长时间的sql会产生临时表、锁表、占用数据连接等情况，影响其他的查询。 解决方案： 1.切分查询，将一条大的查询切分成多个小的查询，分批次执行 2.分解关联查询， 将一个关联语句分解成多个sql来执行，减少锁的竞争，并且在应用层进行关联，以后更容易拆分数据库 优化特定类型的查询语句 1.优化关联查询，确定on的子句有没有索引，避免全表扫描 2.确保group by和order by中只有一个表中的列，这样才会使用到索引 3.优化子查询，尽量使用关联查询来替代子查询(因为mysql对关联查询会有一些优化器，但是高性能mysql上说了，在mysql5.6以上版本或者MariaDB中，子查询和关联查询的效率是差不多的) 4.优化limit分页，当limit偏移量大的时候，查询效率就会很低。此时我们可以记录上次查询的最大id，下次查询时直接根据该id来查询 比如每页需要显示10条数据，到limit 10000,10的时候，其实是查出了10010条结果集，返回了10条。如果我们记住上一次查询的最大id，10000.我们可以使用 where id &gt; 10000 limit 10，这样还是只在10条数据中返回，极大的提升了运行效率]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解PHP之——LANMP原理详解]]></title>
    <url>%2F2017%2F07%2F19%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3PHP%E4%B9%8B%E2%80%94%E2%80%94LANMP%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[LNMP的工作原理浏览器发送http request请求到服务器（Nginx）,服务器响应并处理web请求。如果是静态文本直接返回，否则将脚本（PHP）通过接口传输协议（网关协议）PHP-FCGI（fast-cgi）传输给PHP-FPM（进程管理程序）,然后PHP-FPM调用PHP解析器的其中一个进程PHP-CGI来解析php脚本信息。【PHP-FPM在启动时启动了多个PHP-CGI子进程，并发执行。】然后将解析后的脚本返回到PHP-FPM，PHP-FPM再通过fast-cgi的形式将脚本信息传送给Nginx。服务器再通过Http response的形式传送给浏览器。浏览器再进行解析与渲染然后进行呈现。 LAMP的工作原理浏览器向服务器发送http请求，服务器 (Apache) 接受请求,由于php作为Apache的组件模块也会一起启动，它们具有相同的生命周期。Apache会将一些静态资源保存，然后调用php去处理模块进行php脚本的处理。脚本处理完后，Apache将处理完的信息通过http response的方式发送给浏览器，浏览器解析，渲染等一系列操作后呈现整个网页。 LAMP LNMP差别 1、在LNMP中，Nginx本身对脚本不做任何的处理，而是把请求发fast-cgi管理进程处理，fast-cgi管理进程选择cgi子进程处理结果并返回，二者是相互独立的，通过管道进程通信。 2、在LAMP中，PHP是Apache的一个模块，具有相同的生命周期。两者通过共享内存的方式通信。 两者的PHP环境不相互适用相比 Apache，Nginx 使用更少的资源，支持更多的并发连接，体现更高的效率 nginx的的优势 作为web服务器处理静态文件，索引文件，自动索引的效率非常高 作为代理服务器，可以实现无缓存的反向代理，提高网站运行速度 作为负载均衡服务器，可以在内部直接支持rails和php等 性能方面，采用epoll模型，可以支持多并发并且占用低内存 稳定方面，采取分段资源分配技术，CPU和内存占用率非常低,少量的dos攻击对nginx基本无作用， 高可用方面,支持热部署，启动迅速，可以在不间断服务的情况下，直接升级7x24小时不间容灾 nginx的模块和工作原理 nginx由内核和模块组成，模块结构分为核心模块，基础模块，第三方模块 核心模块: http模块，event模块，mail模块 基础模块: http fastCGI模块，http proxy模块，http rewrite模块 第三方模块: http upstream request hash 模块，notice模块，htttp access key 模块 Fast-CGI 介绍cgi是通用网关接口，是外部应用程序与Web服务器之间的接口标准，cgi是为了保证web server传递过来的数据是标准格式的，方便cgi程序的编写者。 Fast-CGI是语言无关的、可伸缩架构的CGI开放扩展，其主要行为是将CGI解释器进程保持在内存中并因此获得较高的性能。众所周知，CGI解释器的反复加载是CGI性能低下的主要原因，如果CGI解释器保持在内存中并接受FastCGI进程管理器调度，则可以提供良好的性能、伸缩性、Fail- Over特性等等。 Fast-cgi像是一个常驻(long-live)型的cgi，是用来提高cgi程序性能的。 fast-CGI是nginx和php之间的一个通信接口，该接口实际处理过程通过启动php-fpm进程来解析php脚本，即php-fpm相当于一个动态应用服务器，从而实现nginx动态解析php。因此，如果nginx服务器需要支持php解析，需要在nginx.conf中增加php的配置；将php脚本转发到fastCGI进程监听的IP地址和端口（php-fpm.conf中指定）。同时，php安装的时候，需要开启支持fastCGI选项，并且编译安装php-fpm补丁/扩展，同时，需要启动php-fpm进程，才可以解析nginx通过fastCGI转发过来的php脚本。 Fast-CGI的工作原理 Web Server启动时载入Fast-CGI进程管理器（IIS ISAPI或Apache Module) FastCGI进程管理器自身初始化，启动多个CGI解释器进程(可见多个php-cgi)并等待来自Web Server的连接。 当客户端请求到达Web Server时，Fast-CGI进程管理器选择并连接到一个CGI解释器。Web server将CGI环境变量和标准输入发送到Fast-CGI子进程php-cgi。 Fast-CGI子进程完成处理后将标准输出和错误信息从同一连接返回Web Server。当Fast-CGI子进程关闭连接时，请求便告处理完成。Fast-CGI子进程接着等待并处理来自Fast-CGI进程管理器(运行在Web Server中)的下一个连接。 在CGI模式中，php-cgi在此便退出了。 在上述情况中，你可以想象CGI通常有多慢。每一个Web请求PHP都必须重新解析php.ini、重新载入全部扩展并重初始化全部数据结构。使用Fast-CGI，所有这些都只在进程启动时发生一次。一个额外的好处是，持续数据库连接(Persistent database connection)可以工作。 Fast-CGI的不足因为是多进程，所以比CGI多线程消耗更多的服务器内存，PHP-CGI解释器每进程消耗7至25兆内存，将这个数字乘以50或100就是很大的内存数。 Nginx 0.8.46+PHP 5.2.14(FastCGI)服务器在3万并发连接下，开启的10个Nginx进程消耗150M内存（15M10=150M），开启的64个php-cgi进程消耗1280M内存（20M64=1280M），加上系统自身消耗的内存，总共消耗不到2GB内存。如果服务器内存较小，完全可以只开启25个php-cgi进程，这样php-cgi消耗的总内存数才500M。 上面的数据摘自Nginx 0.8.x + PHP 5.2.13(FastCGI)搭建胜过Apache十倍的Web服务器(第6版) PHP-FPMPHP-FPM是一个实现了Fastcgi的程序，PHP-FPM的管理对象是php-cgi。被PHP官方收了。 后来PHP内核集成了PHP-FPM之后就方便多了，使用–enalbe-fpm这个编译参数即可。 123456789101、php-fpm是一个完全独立的程序,不依赖php-cgi,也不依赖php。因为php-fpm是一个内置了php解释器的FastCGI服务,启动时能够自行读取php.ini配置和php-fpm.conf配置.2、一个master进程,支持多个pool,每个pool由master进程监听不同的端口,pool中有多个worker进程.3、每个worker进程都内置PHP解释器,并且进程常驻后台,支持prefork动态增加.4、每个worker进程支持在运行时编译脚本并在内存中缓存生成的opcode来提升性能.5、每个worker进程支持配置响应指定请求数后自动重启,master进程会重启挂掉的worker进程.6、每个worker进程能保持一个到MySQL/Memcached/Redis的持久连接,实现&quot;连接池&quot;,避免重复建立连接,对程序透明.7、master进程采用epoll模型异步接收和分发请求,listen监听端口,epoll_wait等待连接,8、然后分发给对应pool里的worker进程,worker进程accpet请求后poll处理连接,9、如果worker进程不够用,master进程会prefork更多进程,A、如果prefork达到了pm.max_children上限,worker进程又全都繁忙,这时master进程会把请求挂起到连接队列backlog里(默认值是511).12345678910 PHP-CGIphp-cgi 是解释PHP脚本的程序，只是个CGI程序，他自己本身只能解析请求，返回结果，不会进程管理 Nginx+FastCGI运行原理 nginx不支持对外部程序的直接调用或者解析，所有的外部程序（包括PHP）必须通过FastCGI接口来调用。FastCGI接口在Linux下是socket（这个socket可以是文件socket，也可以是ip socket）。 wrapper： 为了调用CGI程序，还需要一个FastCGI的wrapper（wrapper可以理解为用于启动另一个程序的程序），这个wrapper绑定在某个固定socket上，如端口或者文件socket。当Nginx将CGI请求发送给这个socket的时候，通过FastCGI接口，wrapper接收到请求，然后Fork(派生）出一个新的线程，这个线程调用解释器或者外部程序处理脚本并读取返回数据；接着，wrapper再将返回的数据通过FastCGI接口，沿着固定的socket传递给Nginx；最后，Nginx将返回的数据（html页面或者图片）发送给客户端。这就是Nginx+FastCGI的整个运作过程， 所以，我们首先需要一个wrapper，这个wrapper需要完成的工作： 1、通过调用fastcgi（库）的函数通过socket和ningx通信（读写socket是fastcgi内部实现的功能，对wrapper是非透明的） 2、调度thread，进行fork和kill 3、和application（php）进行通信]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>LNMP</tag>
        <tag>LAMP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引与优化(explain)]]></title>
    <url>%2F2017%2F07%2F17%2FMySQL%E7%B4%A2%E5%BC%95%E4%B8%8E%E4%BC%98%E5%8C%96(explain)%2F</url>
    <content type="text"><![CDATA[一、 创建索引1.普通索引(最基本的索引，无任何限制)(1)创建索引 1CREATE INDEX index_name ON table(column(length)) (2)删除索引1DROP INDEX index_name ON table 2.唯一索引（索引列的值必须唯一，但允许有空值）创建索引：1CREATE UNIQUE INDEX indexName ON table(column(length)) 3.主键索引（一个表的主键）12345CREATE TABLE `table` ( `id` int(11) NOT NULL AUTO_INCREMENT , `title` char(255) NOT NULL , PRIMARY KEY (`id`)); 4.组合索引（将多个列组合在一起创建索引）mysql执行查询中，只会使用到一个索引 最左前缀：意思是使用组合索引的时候，从左到右依次匹配，否则不会使用组合索引。 例如ALTER TABLE people ADD INDEX lname_fname_age (lame,fname,age);其实我们是建立了三个索引，分别是：单列索引lame，组合索引（lame，fname），组合索引（lame，fname，age），mysql索引的时候只会使用其中一个索引。 所以创建组合索引的时候，尽量把使用频繁的放在左边 二、索引创建的原则和注意事项 1.最适合创建索引的是出现在where子句中的列或是出现在连接子句中的列 2.对字符串类型进行索引的时候，应该指定一个前缀长度，比如索引前多少个字符 3.根据业务情况创建组合索引（比如某个业务需要查询两个列） 4.组合索引遵循前缀原则（最左前缀原则）TODO 5.like查询，%不能在前，可以使用全文检索引擎 例如： where name like ‘%wang%’，查询姓名中有wang的，此时索引不会生效，还是会全表扫描，因为前面有个%，如果是like ‘wang%’这样会使用到索引，但是没有前缀匹配了，如果想达到索引的效果，可以使用全文检索引擎，例如es（Elasticsearch） 6.如果mysql觉得全表扫描比索引扫描快，他会自动放弃使用索引 7.mysql查询只使用一个索引，如果where子句中使用了索引，那么order by中的列是不会使用索引的 8.列中包含null值是不会使用索引的，如果column_name is null还是会使用索引，但是建表的时候尽量设置一个非null的默认值。 explain分析sql语句现有如下的sql语句EXPLAIN SELECT * FROM inventory WHERE item_id = 16102176;打印结果如下： 12345678910id: 1 select_type: SIMPLE table: inventory type: ref possible_keys: item_id key: item_id key_len: 4 ref: const rows: 1 Extra: Using where 1.key: 指出优化器使用的索引。 2.rows: mysql认为他查询必须要检查的行数，优化器估计值。 3.possible_keys: 支出优化器为查询选定的索引 4.key_len: sql语句的连接条件的键的长度 5.select_type: select使用的类型。 + simple（简单的select不含union或子查询）、 + primary（最外面的select）、 + union（union中第二个或后面的select）、 + dependent union（union中第二个或后面的select，取决于外面的查询）、 + union result（union的结果）、 + subquery（子查询中第一个select） 6.type： 连接类型。system（表仅有一行）、const（表最多有一个匹配行）、eq_ref(对于每个前面的表的行组合，从该表中读取一行)、ref（对于每个来自于前面表的行组合，所有匹配索引值将从这张表中读取）、index_merge(使用了索引合并优化方法)、all（完整的表扫描） 7.ref： 显示使用哪个列或常数与key一起从表中选择行]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解PHP——内存管理垃圾回收]]></title>
    <url>%2F2017%2F07%2F16%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3PHP%E2%80%94%E2%80%94%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[一、内存管理机制概述 1） 操作系统直接管理着内存，所以操作系统也需要进行内存管理，计算机中通常都有内存管理单元(MMU) 用于处理CPU对内存的访问。 2） 应用程序无法直接调用物理内存， 只能向系统申请内存。 向操作系统申请内存空间会引发系统调用。 系统调用会将CPU从用户态切换到内核。 为了减少系统调用开销。通常在用户态进行内存管理。 申请大块内存备用。使用完的内存不马上释放，将内存复用，避免多次内存申请和释放所带来性能消耗。 3） PHP不需要显示内存管理，由Zend引擎进行管理。 PHP内存限制 1）php.ini中的默认32MB memory_limit = 32M 2）动态修改内存 ini_set (“memory_limit”, “128M”) 3）获取目前内存占用 memory_get_usage() : 获取PHP脚本所用的内存大小 memory_get_peak_usage() ：返回当前脚本到目前位置所占用的内存峰值。 先看一段代码： &lt;?php //内存管理机制 var_dump(memory_get_usage());//获取内存方法，加上true返回实际内存，不加则返回表现内存 $a = &quot;laruence&quot;; var_dump(memory_get_usage()); unset($a); var_dump(memory_get_usage()); //输出(在我的个人电脑上, 可能会因为系统,PHP版本,载入的扩展不同而不同): //int 240552 //int 240720 //int 240552 定义变量之后，内存增加，清除变量之后，内存恢复（有些可能不会恢复和以前一样），好像定义变量时申请了一次内存，其实不是这样的，php会预先申请一块内存，不会每次定义变量就申请内存。 首先我们要打破一个思维: PHP不像C语言那样, 只有你显示的调用内存分配相关API才会有内存的分配. 也就是说, 在PHP中, 有很多我们看不到的内存分配过程. 比如对于: $a = &quot;laruence&quot;; 隐式的内存分配点就有: 1.1. 为变量名分配内存, 存入符号表 2.2. 为变量值分配内存 所以, PHP的unset确实会释放内存, 但这个释放不是C编程意义上的释放, 不是交回给OS.对于PHP来说, 它自身提供了一套和C语言对内存分配相似的内存管理API: emalloc(size_t size); efree(void *ptr); ecalloc(size_t nmemb, size_t size); erealloc(void *ptr, size_t size); estrdup(const char *s); estrndup(const char *s, unsigned int length); 这些API和C的API意义对应, 在PHP内部都是通过这些API来管理内存的. 当我们调用emalloc申请内存的时候, PHP并不是简单的向OS要内存, 而是会像OS要一个大块的内存, 然后把其中的一块分配给申请者, 这样当再有逻辑来申请内存的时候, 就不再需要向OS申请内存了, 避免了频繁的系统调用. 比如如下的例子: var_dump(memory_get_usage(true));//注意获取的是real_size $a = &quot;laruence&quot;; var_dump(memory_get_usage(true)); unset($a); var_dump(memory_get_usage(true)); //输出 //int 262144 //int 262144 //int 262144 也就是我们在定义变量$a的时候, PHP并没有向系统申请新内存.同样的, 在我们调用efree释放内存的时候, PHP也不会把内存还给OS, 而会把这块内存, 归入自己维护的空闲内存列表. 对于小块内存来说, 更可能的是, 把它放到内存缓存列表中去(后记, 某些版本的PHP, 比如我验证过的PHP5.2.4, 5.2.6, 5.2.8, 在调用get_memory_usage()的时候, 不会减去内存缓存列表中的可用内存块大小, 导致看起来, unset以后内存不变). $a = “hello”; 定义变量时，存储两个方面： 变量名，存储在符号表 变量值存储在内存空间 在删除变量的时候，会将变量值存储的空间释放，而变量名所在的符号表不会减小（只增不减） 只增不减的数组Hashtable是PHP的核心结构, 数组也是用她来表示的, 而符号表也是一种关联数组, 对于如下代码: var_dump(memory_get_usage()); for($i=0;$i&lt;100;$i++) { $a = &quot;test&quot;.$i; $$a = &quot;hello&quot;; } var_dump(memory_get_usage()); for($i=0;$i&lt;100;$i++) { $a = &quot;test&quot;.$i; unset($$a); } var_dump(memory_get_usage()); 我们定义了100个变量, 然后又按个Unset了他们, 来看看输出: //int 242104 //int 259768 //int 242920 Wow, 怎么少了这么多内存?这是因为对于Hashtable来说, 定义它的时候, 不可能一次性分配足够多的内存块, 来保存未知个数的元素, 所以PHP会在初始化的时候, 只是分配一小部分内存块给HashTable, 当不够用的时候再RESIZE扩容。 Hashtable, 只能扩容, 不会减少, 对于上面的例子, 当我们存入100个变量的时候, 符号表不够用了, 做了一次扩容, 而当我们依次unset掉这100个变量以后, 变量占用的内存是释放了(118848 – 104448), 但是符号表并没有缩小, 所以这些少的内存是被符号表本身占去了… 二、垃圾回收机制PHP变量存储在一个zval容器里面的 变量类型 变量值 is_ref 代表是否有地址引用 refcount 指向该值的变量数量 变量赋值的时候：is_ref为false， refcount为1 $a = 1; xdebug_debug_zval(&apos;a&apos;); echo PHP_EOL;//换行符，提高代码的源代码级可移植性 输出： a: (refcount=1, is_ref=0), int 1 将变量a的值赋给变量b，变量b不会立刻去在内存中存储值，而是先指向变量a的值，一直到变量a有任何操作的时候 $b = $a; xdebug_debug_zval(&apos;a&apos;); echo PHP_EOL; 输出： a: (refcount=2, is_ref=0), int 1 再来看一个$c = &amp;$a; xdebug_debug_zval(&apos;a&apos;); echo PHP_EOL; xdebug_debug_zval(&apos;b&apos;); echo PHP_EOL; 输出： a: (refcount=2, is_ref=1), int 1 b: (refcount=1, is_ref=0), int 1 因为程序又操作了变量a，所以变量b会自己申请一块内存将值放进去。所以变量a的zval容器中refcount会减1变为1，变量c指向a，所以refcount会加1变为2，is_ref变为true 垃圾回收 在5.2版本或之前版本，PHP会根据refcount值来判断是不是垃圾 如果refcount值为0，PHP会当做垃圾释放掉 这种回收机制有缺陷，对于环状引用的变量无法回收 环状引用：$attr = array(&quot;hello&quot;); $attr[]= &amp;$attr; xdebug_debug_zval(&apos;attr&apos;); echo PHP_EOL; 输出： attr: (refcount=2, is_ref=1), array (size=2) 0 =&gt; (refcount=1, is_ref=0), string &apos;hello&apos; (length=5) 1 =&gt; (refcount=2, is_ref=1), &amp;array 2.在5.3之后版本改进了垃圾回收机制 如果发现一个zval容器中的refcount在增加，说明不是垃圾 如果发现一个zval容器中的refcount在减少，如果减到了0，直接当做垃圾回收 如果发现一个zval容器中的refcount在减少，并没有减到0，PHP会把该值放到缓冲区，当做有可能是垃圾的怀疑对象 当缓冲区达到临界值，PHP会自动调用一个方法取遍历每一个值，如果发现是垃圾就清理]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>内存管理</tag>
        <tag>垃圾回收</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解PHP之：Nginx 与 FPM 的工作机制]]></title>
    <url>%2F2017%2F07%2F15%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3PHP%E4%B9%8B%EF%BC%9ANginx-%E4%B8%8E-FPM-%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[概述网络上有很多关于如何配置 Nginx + FPM 的文章，但它们更多从操作的角度出发，告诉我们怎么做，但却没有告诉我们为什么要这么做，本文从 Nginx 与 FPM 的工作机制出发，探讨配置背后的原理，让我们真正理解 Nginx 与 PHP 是如何协同工作的。 内容：要说 Nginx 与 PHP 是如何协同工作的，首先得说 CGI (Common Gateway Interface) 和 FastCGI 这两个协议。 CGI 是 Web Server 与后台语言交互的协议，有了这个协议，开发者可以使用任何语言处理 Web Server 发来的请求，动态的生成内容。但 CGI 有一个致命的缺点，那就是每处理一个请求都需要 fork 一个全新的进程，随着 Web 的兴起，高并发越来越成为常态，这样低效的方式明显不能满足需求。就这样，FastCGI 诞生了，CGI 很快就退出了历史的舞台。FastCGI，顾名思义为更快的 CGI，它允许在一个进程内处理多个请求，而不是一个请求处理完毕就直接结束进程，性能上有了很大的提高。 FPM (FastCGI Process Manager)，它是 FastCGI 的实现，任何实现了 FastCGI 协议的 Web Server 都能够与之通信。FPM 之于标准的 FastCGI，也提供了一些增强功能，具体可以参考官方文档：PHP: FPM Installation。 FPM 是一个 PHP 进程管理器，包含 master 进程和 worker 进程两种进程：master 进程只有一个，负责监听端口，接收来自 Web Server 的请求，而 worker 进程则一般有多个 (具体数量根据实际需要配置)，每个进程内部都嵌入了一个 PHP 解释器，是 PHP 代码真正执行的地方，下图是我本机上 fpm 的进程情况，1一个 master 进程，3个 worker 进程： 从 FPM 接收到请求，到处理完毕，其具体的流程如下： 1. FPM 的 master 进程接收到请求 2. master 进程根据配置指派特定的 worker进程进行请求处理，如果没有可用进程，返回错误，这也是我们配合 Nginx 遇到502错误比较多的原因。 3. worker 进程处理请求，如果超时，返回504错误 4. 请求处理结束，返回结果 FPM 从接收到处理请求的流程就是这样了，那么 Nginx 又是如何发送请求给 fpm 的呢？这就需要从 Nginx 层面来说明了。 Nginx 不仅仅是一个 Web 服务器，也是一个功能强大的 Proxy 服务器，除了进行 http 请求的代理，也可以进行许多其他协议请求的代理，包括本文与 fpm 相关的 fastcgi 协议。为了能够使 Nginx 理解 fastcgi 协议，Nginx 提供了 fastcgi 模块来将 http 请求映射为对应的 fastcgi 请求。 Nginx 的 fastcgi 模块提供了 fastcgi_param 指令来主要处理这些映射关系，下面 Ubuntu 下 Nginx 的一个配置文件，其主要完成的工作是将 Nginx 中的变量翻译成 PHP 中能够理解的变量。 除此之外，非常重要的就是 fastcgi_pass 指令了，这个指令用于指定 fpm 进程监听的地址，Nginx 会把所有的 php 请求翻译成 fastcgi 请求之后再发送到这个地址。下面一个简单的可以工作的 Nginx 配置文件： 在这个配置文件中，我们新建了一个虚拟主机，监听在 80 端口，Web 根目录为 /home/rf/projects/wordpress。然后我们通过 location 指令，将所有的以 .php 结尾的请求都交给 fastcgi 模块处理，从而把所有的 php 请求都交给了 fpm 处理，从而完成 Nginx 到 fpm 的闭环。 如此以来，Nginx 与 FPM 通信的整个流程应该比较清晰了吧。 nginx是web服务器，提供http服务。 php代码需要php解析器解析。所以这里要有个nginx和php解析器通信的问题。就出现了一个fastcgi的协议来解决通信问题。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>Nginx</tag>
        <tag>FPM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解PHP之——HTTP请求全过程]]></title>
    <url>%2F2017%2F07%2F13%2F%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3PHP%E4%B9%8B%E2%80%94%E2%80%94HTTP%E8%AF%B7%E6%B1%82%E5%85%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一个完整的HTTP请求流程 域名解析 —&gt; 与服务器建立连接 —&gt; 发起HTTP请求 —&gt; 服务器响应HTTP请求，浏览器得到html代码 —&gt; 浏览器解析html代码，并请求html代码中的资源（如js、css、图片） —&gt; 浏览器对页面进行渲染呈现给用户 1. 域名解析以Chrome浏览器为例： ① Chrome浏览器 会首先搜索浏览器自身的DNS缓存（缓存时间比较短，大概只有1分钟，且只能容纳1000条缓存），看自身的缓存中是否有https://www.cnblogs.com 对应的条目，而且没有过期，如果有且没有过期则解析到此结束。 注：我们怎么查看Chrome自身的缓存？可以使用 chrome://net-internals/#dns 来进行查看 ② 如果浏览器自身的缓存里面没有找到对应的条目，那么Chrome会搜索操作系统自身的DNS缓存,如果找到且没有过期则停止搜索解析到此结束. 注：怎么查看操作系统自身的DNS缓存，以Windows系统为例，可以在命令行下使用 ipconfig /displaydns 来进行查看 ③ 如果在Windows系统的DNS缓存也没有找到，那么尝试读取hosts文件（位于C:\Windows\System32\drivers\etc），看看这里面有没有该域名对应的IP地址，如果有则解析成功。 ④ 如果在hosts文件中也没有找到对应的条目，浏览器就会发起一个DNS的系统调用，就会向本地配置的首选DNS服务器（一般是电信运营商提供的，也可以使用像Google提供的DNS服务器）发起域名解析请求（通过的是UDP协议向DNS的53端口发起请求，这个请求是递归的请求，也就是运营商的DNS服务器必须得提供给我们该域名的IP地址），运营商的DNS服务器首先查找自身的缓存，找到对应的条目，且没有过期，则解析成功。如果没有找到对应的条目，则有运营商的DNS代我们的浏览器发起迭代DNS解析请求，它首先是会找根域的DNS的IP地址（这个DNS服务器都内置13台根域的DNS的IP地址），找打根域的DNS地址，就会向其发起请求（请问www.cnblogs.com这个域名的IP地址是多少啊？），根域发现这是一个顶级域com域的一个域名，于是就告诉运营商的DNS我不知道这个域名的IP地址，但是我知道com域的IP地址，你去找它去，于是运营商的DNS就得到了com域的IP地址，又向com域的IP地址发起了请求（请问www.cnblogs.com这个域名的IP地址是多少?）,com域这台服务器告诉运营商的DNS我不知道www.cnblogs.com这个域名的IP地址，但是我知道cnblogs.com这个域的DNS地址，你去找它去，于是运营商的DNS又向cnblogs.com这个域名的DNS地址（这个一般就是由域名注册商提供的，像万网，新网等）发起请求（请问www.cnblogs.com这个域名的IP地址是多少？），这个时候cnblogs.com域的DNS服务器一查，诶，果真在我这里，于是就把找到的结果发送给运营商的DNS服务器，这个时候运营商的DNS服务器就拿到了www.cnblogs.com这个域名对应的IP地址，并返回给Windows系统内核，内核又把结果返回给浏览器，终于浏览器拿到了www.cnblogs.com 对应的IP地址，该进行一步的动作了。 注：一般情况下是不会进行以下步骤的 如果经过以上的4个步骤，还没有解析成功，那么会进行如下步骤（以下是针对Windows操作系统）： ⑤ 操作系统就会查找NetBIOS name Cache（NetBIOS名称缓存，就存在客户端电脑中的），那这个缓存有什么东西呢？凡是最近一段时间内和我成功通讯的计算机的计算机名和Ip地址，就都会存在这个缓存里面。什么情况下该步能解析成功呢？就是该名称正好是几分钟前和我成功通信过，那么这一步就可以成功解析。 ⑥ 如果第⑤步也没有成功，那会查询WINS 服务器（是NETBIOS名称和IP地址对应的服务器） ⑦ 如果第⑥步也没有查询成功，那么客户端就要进行广播查找 ⑧ 如果第⑦步也没有成功，那么客户端就读取LMHOSTS文件（和HOSTS文件同一个目录下，写法也一样） 如果第八步还没有解析成功，那么就宣告这次解析失败，那就无法跟目标计算机进行通信。只要这八步中有一步可以解析成功，那就可以成功和目标计算机进行通信。 2. 与服务器建立连接2.1 TCP连接的建立客户端的请求到达服务器，首先就是建立TCP连接 Client首先发送一个连接试探，ACK=0 表示确认号无效，SYN = 1 表示这是一个连接请求或连接接受报文，同时表示这个数据报不能携带数据，seq = x 表示Client自己的初始序号（seq = 0 就代表这是第0号包），这时候Client进入syn_sent状态，表示客户端等待服务器的回复 Server监听到连接请求报文后，如同意建立连接，则向Client发送确认。TCP报文首部中的SYN 和 ACK都置1 ，ack = x + 1表示期望收到对方下一个报文段的第一个数据字节序号是x+1，同时表明x为止的所有数据都已正确收到（ack=1其实是ack=0+1,也就是期望客户端的第1个包），seq = y 表示Server 自己的初始序号（seq=0就代表这是服务器这边发出的第0号包）。这时服务器进入syn_rcvd，表示服务器已经收到Client的连接请求，等待client的确认。 Client收到确认后还需再次发送确认，同时携带要发送给Server的数据。ACK 置1 表示确认号ack= y + 1 有效（代表期望收到服务器的第1个包），Client自己的序号seq= x + 1（表示这就是我的第1个包，相对于第0个包来说的），一旦收到Client的确认之后，这个TCP连接就进入Established状态，就可以发起http请求了。 问题1：TCP 为什么需要3次握手？2个计算机通信是靠协议（目前流行的TCP/IP协议）来实现,如果2个计算机使用的协议不一样，那是不能进行通信的，所以这个3次握手就相当于试探一下对方是否遵循TCP/IP协议，协商完成后就可以进行通信了，当然这样理解不是那么准确。 问题2：为什么HTTP协议要基于TCP来实现？目前在Internet中所有的传输都是通过TCP/IP进行的，HTTP协议作为TCP/IP模型中应用层的协议也不例外，TCP是一个端到端的可靠的面向连接的协议，所以HTTP基于传输层TCP协议不用担心数据的传输的各种问题。 2.2 常见TCP连接限制 2.2.1 修改用户进程可打开文件数限制 在Linux平台上，无论编写客户端程序还是服务端程序，在进行高并发TCP连接处理时，最高的并发数量都要受到系统对用户单一进程同时可打开文件数量的限制(这是因为系统为每个TCP连接都要创建一个socket句柄，每个socket句柄同时也是一个文件句柄)。可使用ulimit命令查看系统允许当前用户进程打开的文件数限制，windows上是256，linux是1024，这个博客的服务器是65535 2.2.2 修改网络内核对TCP连接的有关限制 在Linux上编写支持高并发TCP连接的客户端通讯处理程序时，有时会发现尽管已经解除了系统对用户同时打开文件数的限制，但仍会出现并发TCP连接数增加到一定数量时，再也无法成功建立新的TCP连接的现象。出现这种现在的原因有多种。第一种原因可能是因为Linux网络内核对本地端口号范围有限制。此时，进一步分析为什么无法建立TCP连接，会发现问题出在connect()调用返回失败，查看系统错误提示消息是“Can’t assign requestedaddress”。同时，如果在此时用tcpdump工具监视网络，会发现根本没有TCP连接时客户端发SYN包的网络流量。这些情况说明问题在于本地Linux系统内核中有限制。 其实，问题的根本原因在于Linux内核的TCP/IP协议实现模块对系统中所有的客户端TCP连接对应的本地端口号的范围进行了限制(例如，内核限制本地端口号的范围为1024~32768之间)。当系统中某一时刻同时存在太多的TCP客户端连接时，由于每个TCP客户端连接都要占用一个唯一的本地端口号(此端口号在系统的本地端口号范围限制中)，如果现有的TCP客户端连接已将所有的本地端口号占满，则此时就无法为新的TCP客户端连接分配一个本地端口号了，因此系统会在这种情况下在connect()调用中返回失败，并将错误提示消息设为“Can’t assignrequested address”。 2.3 TCP四次挥手当客户端和服务器通过三次握手建立了TCP连接以后，当数据传送完毕，肯定是要断开TCP连接的啊。那对于TCP的断开连接，这里就有了神秘的“四次分手”。 第一次分手：主机1（可以使客户端，也可以是服务器端），设置Sequence Number，向主机2发送一个FIN报文段；此时，主机1进入FIN_WAIT_1状态；这表示主机1没有数据要发送给主机2了； 第二次分手：主机2收到了主机1发送的FIN报文段，向主机1回一个ACK报文段，Acknowledgment Number为Sequence Number加1；主机1进入FIN_WAIT_2状态；主机2告诉主机1，我“同意”你的关闭请求； 第三次分手：主机2向主机1发送FIN报文段，请求关闭连接，同时主机2进入LAST_ACK状态； 第四次分手：主机1收到主机2发送的FIN报文段，向主机2发送ACK报文段，然后主机1进入TIME_WAIT状态；主机2收到主机1的ACK报文段以后，就关闭连接；此时，主机1等待2MSL后依然没有收到回复，则证明Server端已正常关闭，那好，主机1也可以关闭连接了。 问题1：为什么要四次分手？ TCP协议是一种面向连接的、可靠的、基于字节流的运输层通信协议。TCP是全双工模式，这就意味着，当主机1发出FIN报文段时，只是表示主机1已经没有数据要发送了，主机1告诉主机2，它的数据已经全部发送完毕了；但是，这个时候主机1还是可以接受来自主机2的数据；当主机2返回ACK报文段时，表示它已经知道主机1没有数据发送了，但是主机2还是可以发送数据到主机1的；当主机2也发送了FIN报文段时，这个时候就表示主机2也没有数据要发送了，就会告诉主机1，我也没有数据要发送了，之后彼此就会愉快的中断这次TCP连接。 3. 发起HTTP请求3.1 HTTP协议HTTP是一个客户端和服务器端请求和应答的标准（TCP）。客户端是终端用户，服务器端是网站。通过使用Web浏览器、网络爬虫或者其它的工具，客户端发起一个到服务器上指定端口（默认端口为80）的HTTP请求。 通俗来讲，他就是计算机通过网络进行通信的规则，是一个基于请求与响应，无状态的，应用层的协议，常基于TCP/IP协议传输数据。目前任何终端（手机，笔记本电脑。。）之间进行任何一种通信都必须按照Http协议进行，否则无法连接。 3.1.1 四个基于 1234请求与响应：客户端发送请求，服务器端响应数据无状态的：协议对于事务处理没有记忆能力，客户端第一次与服务器建立连接发送请求时需要进行一系列的安全认证匹配等，因此增加页面等待时间，当客户端向服务器端发送请求，服务器端响应完毕后，两者断开连接，也不保存连接状态，一刀两断！恩断义绝！从此路人！下一次客户端向同样的服务器发送请求时，由于他们之前已经遗忘了彼此，所以需要重新建立连接。应用层： Http是属于应用层的协议，配合TCP/IP使用。TCP/IP： Http使用TCP作为它的支撑运输协议。HTTP客户机发起一个与服务器的TCP连接，一旦连接建立，浏览器（客户机）和服务器进程就可以通过套接字接口访问TCP。 3.2 HTTP请求报文一个HTTP请求报文由请求行（request line）、请求头部（header）、空行和请求数据4个部分组成. 3.2.1 请求行 请求行分为三个部分：请求方法、请求地址和协议版本 请求方法HTTP/1.1 定义的请求方法有8种：GET、POST、PUT、DELETE、PATCH、HEAD、OPTIONS、TRACE。 最常的两种GET和POST，如果是RESTful接口的话一般会用到GET、POST、DELETE、PUT。 请求地址 URL:统一资源定位符，是一种自愿位置的抽象唯一识别方法。 组成：&lt;协议&gt;：//&lt;主机&gt;：&lt;端口&gt;/&lt;路径&gt; 注：端口和路径有时可以省略（HTTP默认端口号是80） https://localhost:8080/index.html?key1=value1&amp;keys2=value2 协议版本协议版本的格式为：HTTP/主版本号.次版本号，常用的有HTTP/1.0和HTTP/1.1 3.2.2 请求头部 请求头部为请求报文添加了一些附加信息，由“名/值”对组成，每行一对，名和值之间使用冒号分隔。 请求头部的最后会有一个空行，表示请求头部结束，接下来为请求数据，这一行非常重要，必不可少。 3.2.3 请求数据 可选部分，比如GET请求就没有请求数据。 下面是一个POST方法的请求报文： POST /index.php HTTP/1.1 请求行 Host: localhost User-Agent: Mozilla/5.0 (Windows NT 5.1; rv:10.0.2) Gecko/20100101 Firefox/10.0.2 请求头 Accept: text/html,application/xhtml+xml,application/xml;q=0.9,/;q=0.8 Accept-Language: zh-cn,zh;q=0.5 Accept-Encoding: gzip, deflate Connection: keep-alive Referer: http://localhost/ Content-Length：25 Content-Type：application/x-www-form-urlencoded 空行 username=aa&amp;password=1234 请求数据 4. 服务器响应HTTP请求，浏览器得到html代码4.1 负载均衡接收到HTTP请求之后，就轮到负载均衡登场了，它位于网站的最前端，把短时间内较高的访问量分摊到不同机器上处理。负载均衡方案有软件、硬件两种 4.1.1 负载均衡硬件方案 F5 BIG-IP是著名的硬件方案，但这里不作讨论 4.1.2 负载均衡软件方案 有LVS HAProxy Nginx等，留作以后补充 在典型的Rails应用部署方案中，Nginx的作用有两个 处理静态文件请求 转发请求给后端的Rails应用 这是一个简单的Nginx配置文件 后端的Rails服务器通过unix socket与Nginx通信，Nginx伺服public文件夹里的静态文件给用户 4.2 Rails(应用服务器)4.3 数据库（数据库服务器）4.4 Redis、Memercache（缓存服务器）4.5 消息队列4.6 搜索4.7 HTTP响应报文HTTP响应报文主要由状态行、响应头部、空行以及响应数据组成。 4.7.1 状态行 由3部分组成，分别为：协议版本，状态码，状态码描述。 其中协议版本与请求报文一致，状态码描述是对状态码的简单描述，所以这里就只介绍状态码。 4.7.2 状态码 状态代码为3位数字。 1xx：指示信息–表示请求已接收，继续处理。 2xx：成功–表示请求已被成功接收、理解、接受。 3xx：重定向–要完成请求必须进行更进一步的操作。 4xx：客户端错误–请求有语法错误或请求无法实现。 5xx：服务器端错误–服务器未能实现合法的请求。 4.7.3 响应头部 与请求头部类似，为响应报文添加了一些附加信息 4.7.4 响应数据 用于存放需要返回给客户端的数据信息。 下面是一个响应报文的实例： HTTP/1.1 200 OK 状态行 Date: Sun, 17 Mar 2013 08:12:54 GMT 响应头部 Server: Apache/2.2.8 (Win32) PHP/5.2.5 X-Powered-By: PHP/5.2.5 Set-Cookie: PHPSESSID=c0huq7pdkmm5gg6osoe3mgjmm3; path=/ Expires: Thu, 19 Nov 1981 08:52:00 GMT Cache-Control: no-store, no-cache, must-revalidate, post-check=0, pre-check=0 Pragma: no-cache Content-Length: 4393 Keep-Alive: timeout=5, max=100 Connection: Keep-Alive Content-Type: text/html; charset=utf-8 空行 响应数据]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>HTTP</tag>
        <tag>请求流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux软件管理]]></title>
    <url>%2F2017%2F07%2F07%2FLinux%E8%BD%AF%E4%BB%B6%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[包管理工具上，影响最大的两个系列，就是debian的dpkg包和Red Hat的rpm包两大系列。 dpkg系的前端工具是apt，用于Debian系统及其衍生系统如Ubuntu系统，以及Ubuntu的衍生系统如LinuxMint。 rpm系的影响更广，主要有两大分支： zypper：用于OpenSUSE系统。OpenSUSE上还有强大的yast2工具 yum：用于CentOS系统。 dnf：由于yum很久不维护，所以Fedora使用了yum的一个新的分支dnf 【1】软件包管理在系统管理中，软件包的管理是最重要的，是系统管理的基础的基础，只有我们学会软件包的管理才能谈得上其它的应用。 RPMRPM是软件管理程序，提供软件的安装、升级、查询、反安装的功能。 优点： a、安装方便，软件中所有数据都经过编译和打包 b、查询、升级、反安装方便 缺点： a、缺乏灵活性 b、存在相依属性 用法： rpm 参数 软件包 指令选项 -i：安装。 -U：升级安装，如果不存在也安装。 -F：更新安装，如果不存在不安装。 -v：查看信息。 -h：有进度条。 --replacepkgs：强制覆盖安装。 --nodeps：不考虑相依属性。 -q &lt;软件名&gt;：查询。 -qa：查询所有。 -ql &lt;软件名&gt;：列出软件的文件清单。 -qi &lt;软件名&gt;：列出软件信息。 -qf &lt;文件名&gt;：查询文件所属软件。 -e &lt;软件名&gt;：删除软件。 例子： rpm -ivh bind-9.3.6-4.P1.el5_4.2.i386.rpm#安装 rpm -Uvh/Fvh bind-9.3.6-4.P1.el5_4.2.i386.rpm #升级安装 rpm -e bind-9.3.6-4.P1.el5_4.2.i386 #删除软件包 rpm -qpi *.rpm #查看软件包里的内容 rpm -qpl *.rpm #查看软件包将会在系统里安装哪些部分 rpm –qa #列出所有被安装的rpm 软件包 【2】YUMyum = Yellow dog Updater, Modified主要功能是更方便的添加/删除/更新RPM包.它能自动解决包的倚赖性问题. 它能便于管理大量系统的更新问题 yum特点 可以同时配置多个资源库(Repository) 简洁的配置文件(/etc/yum.conf) 自动解决增加或删除rpm包时遇到的倚赖性问题 使用方便 保持与RPM数据库的一致性 yum配置挂载光盘文件到硬盘 mkdir –p /mnt/cdrom chmod 777 /mnt mount -t iso9660 /dev/cdrom /mnt/cdrom 配置/etc/yum.repos.d/server.repo [cdrom] name=cdrom baseurl=file:///mnt/cdrom enabled=1 清除原有的yum信息 [root@localhost ~]# yum clean all 查看yum 信息 [root@localhost ~]# yum list 创建本地YUM仓库 ① 新建一个目录——该目录作为软件仓库，将光盘中的rpm包（位于Packages文件夹）保存到该目录下。 ② 对该目录内的rpm软件包建立索引：createrepo -v /rpm-directory，创建该目录下的rpm包索引，索引创建完成后，会在该文件夹下出现一个repodata目录，里面保存了仓库索引信息。 注：第②步中的createrepo工具在默认情况下是没有安装的，需要自己手动安装（Packages中已包含createrepo安装包），安装方法如下：rpm -ivh createrepo-…….rpm。安装createrepo需要手动解决依赖关系。（createrepo工具主要用于收集目录中RPM包文件的头信息，以创建repodata软件仓库数据（经gzip压缩的xml文件）） ③在/etc/yum.repos.d目录下 新建一个.repo文件，其中baseurl = file:///rpm-directory 绝对路径 ④清理一下yum缓存yum clean all，列出所有的软件包yum list all，查看是否成功。 yum管理软件yum install –y software #安装软件 yum update -y software #更新软件 yum remove -y software #删除软件 yum list #列出资源库中特定的可以安装或更新以及已经安装的rpm包 yum grouplist # 累出所有软件包群组 yum groupinstall ‘包群组名’ # 安装软件包群组 yum groupremove ‘包群组名 yum info 包名 #查看软件包信息 yum search 包名 # 搜索 yum clean all #清除缓存 yum添加163源地址： http://mirrors.163.com/.help/centos.html 下载方式： wget http://mirrors.163.com/.help/CentOS6-Base-163.repo 【3】apt-get用Linux apt-get命令的第一步就是引入必需的软件库，Debian的软件库也就是所有Debian软件包的集合，它们存在互联网上的一些公共站点上。把它们的地址加入，apt-get就能搜索到我们想要的软件。/etc/apt/sources.list是存放这些地址列表的配置文件，其格式如下： deb [web或ftp地址] [发行版名字][main/contrib/non-free] 我们常用的Ubuntu就是一个基于Debian的发行 命令： apt-get update：在修改/etc/apt/sources.list或者/etc/apt/preferences之后运行该命令。此外您需要定期运行这一命令以确保您的软件包列表是最新的。 apt-get install packagename：安装一个新软件包 apt-get remove packagename：卸载一个已安装的软件包（保留配置文件） apt-get –purge remove packagename：卸载一个已安装的软件包（删除配置文件） apt-get autoclean apt：会把已装或已卸的软件都备份在硬盘上，所以如果需要空间的话，可以让这个命令来删除你已经删掉的软件 apt-get clean：这个命令会把安装的软件的备份也删除，不过这样不会影响软件的使用的。 apt-get upgrade：更新所有已安装的软件包 apt-get dist-upgrade：将系统升级到新版本 apt-get autoclean：定期运行这个命令来清除那些已经卸载的软件包的.deb文件。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一步一步教你如何写开发文档]]></title>
    <url>%2F2017%2F07%2F02%2F%E4%B8%80%E6%AD%A5%E4%B8%80%E6%AD%A5%E6%95%99%E4%BD%A0%E5%A6%82%E4%BD%95%E5%86%99%E5%BC%80%E5%8F%91%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[你有没有遇到过，突然老板或者老大跟你说，你根据项目或者根据功能写一份开发文档，当时很开心的答应了，后来想想，既然懵了。 开发文档是什么鬼？写了这么多年代码都没写过什么开发文档，最多也就只是产品的需求文档，说明文档，代码的注释与规范文档，或者说过苹果的开发文档，因为之前刚好写过一次很简单的，最近公司又开始要写上面开发文档了，而且发现确实很多同学都不会，我在想，我是不是该做点什么……。 前言App开发过程中的文档分为很多种，比如最常见的就是官方的开发文档，这种比较倾向代码和接口，但是你可能还见过或者听过其他文档。 比如，这里根据个人理解整理了几个。 开发文档 需求(原型)文档 需求(说明)文档 技术方案文档 Bug修复文档 注释文档 代码与UI规范文档 性能优化文档 是不是有点晕了，哪有这么多鬼，其实按照之前的习惯，我都是一份开发文档就够了，基本上包含上面的东西，只是看你怎么细分。 开发文档概述实际开发中如果真的遇到要写上面开发文档可以从下面几个角度写。 一. 开发环境及工具 二. 项目或功能（需求）描述（最好有流程图或者UML）- 需求文档 三. 编写目的（用户特征和水平） 四. 项目或功能背景 五. 模块与关系 六. 类或术语说明 七. 参考资料（网络或公司内部资料，UI，原型，说明文档） 八. 项目进度预估 九. 难点预估（条件与限制） 十. 功能与所计划采用的技术 - 技术方案文档 十一. 用户界面与交互 十二. 软件（代码）接口 - 注释文档 十三. 通信（网络）接口 - 接口文档 十四. 问题与修复说明 - Bug修复文档 十五. 性能分析与优化 当然也不是说这些全部要写，可以根据项目或者功能适当编写。 下面大概一个个的说明一些每一个步骤是什么意思，需要怎么写，这里主要以iOS开发中App开发文档为规范，并使用苹果最新的语言Swift作为唯一语言。 一. 开发环境及工具 Mac OX 10 iPhone或者iPad 5+ 2+ 必须真机 iOS 8+ Xcode 8+ 其他工具：Tower，cornerstone 主要指明开发在工具，开发平台，开发版本的支持。描述软件的运行环境，包括硬件平台、硬件要求、操作系统和版本，以及其他的软 件或与其共存的应用程序等。 二. 项目或功能（需求）描述（最好有流程图或者UML）- 需求文档 顶层数据流图； 用例UseCase图； 系统流程图； 层次方框图。 主要根据产品给出的需求结合原型进行描述，并适当给出相应的图。 三. 编写目的（用户特征和水平） 描述最终用户应具有的受教育水平、工作经验及技术专长。 次软件或者功能编写的目的，对项目，对用户，对公司有什么好处。 四. 项目或功能背景 标识待开发软件产品的名称、代码； 列出本项目的任务提出者、项目负责人、系统分析员、系统设计员、程序设计员、程序员、资料员以及与本项目开展工作直接有关的人员和用户； 说明该软件产品与其他有关软件产品的相互关系。 此项目或功能编写之前市面上的情况，公司和用户的情况 五. 模块与关系 项目或功能对应模块在位置，入口，和其他模块的关系 六. 类或术语说明 项目或功能对应类的说明，和开发中使用到的一些相关的术语的说明 七. 参考资料（网络或公司内部资料，UI，原型，说明文档） 列举编写软件需求规格说明时所参考的资料，包括项目经核准的计划任务书、合同、引用的标准和规范、项目开发计划、需求规格说明、使用实例文档，以及相关产品的软件需求规格说明。 在这里应该给出详细的信息，包括标题、作者、版本号、发表日期、出版单位或资料来源。 网络资料，尤其是苹果的，也可以群里或者博客，文章等。公司内部的UI，原型，说明，网络接口资料 八. 项目进度预估 预计从上面开始到指定的时间节点完成任务或者完成对应的部分 九. 难点预估（条件与限制） 其中考虑到或者可能会遇到什么技术或者实现难点 十. 功能与所计划采用的技术 - 技术方案文档 将要采用的图形用户界面标准或产品系列的风格； 屏幕布局； 菜单布局； 输入输出格式； 错误信息显示格式； 建议采用RAD开发工具， 比如Visio，构造用户界面。 根据项目或者功能需求，在代码层面所使用的技术或者实现方案，或者比如说ios中布局方式的使用。 十一. 用户界面与交互 根据用户界面和入口说明交互与使用步骤并 十二. 软件（代码）接口 - 注释文档 每一个方法和属性对应的注释，一般是私有的话使用private但是也要注释，公开的都会使用标准的注释说明，苹果有自带的快捷键（command+option+/），之前有个插件叫VVDocument 十三. 通信（网络）接口 - 接口文档 网络请求对应的说明包括对应的参数，字段和返回值，也可以是数据模型层对应的模型属性和方法的说明 十四. 问题与修复说明 - Bug修复文档 开发或者测试的过程中出现了什么比较重要的bug，不要什么bug都写上，然后说明解决的方案 十五. 性能分析与优化 时间特性 响应时间； 更新处理时间； 数据转换与传输时间； 运行时间等。 适应性 在操作方式、运行环境、与其他软件的接口以及开发计划等发生变化时，软件的适应能力。 到此完成之后，根据实际需求和个人能力，个人理解分析项目或者功能那些地方需要进行优化一下，打算怎么去优化他。 后期会继续完善(根据项目或功能整理一套完整的开发文档)……. 注：这里是按照功能，并不是按照整个项目分，如果要写整个项目的开发文档也可以再根据功能细分。]]></content>
      <categories>
        <category>Swift</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>文档</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL慢查询机制]]></title>
    <url>%2F2017%2F07%2F02%2FMySQL%E6%85%A2%E6%9F%A5%E8%AF%A2%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[一、简介开启慢查询日志，可以让MySQL记录下查询超过指定时间的语句，通过定位分析性能的瓶颈，才能更好的优化数据库系统的性能。 二、参数说明 slow_query_log 慢查询开启状态 slow_query_log_file 慢查询日志存放的位置（这个目录需要MySQL的运行帐号的可写权限，一般设置为MySQL的数据存放目录） long_query_time 查询超过多少秒才记录 ###三、设置步骤 1. 查询慢查询相关的参数1234567891011121314mysql&gt; show variables like &apos;slow_query%&apos;;+---------------------------+----------------------------------+| Variable_name | Value |+---------------------------+----------------------------------+| slow_query_log | OFF || slow_query_log_file | /mysql/data/localhost-slow.log |+---------------------------+----------------------------------+mysql&gt; show variables like &apos;long_query_time&apos;;+-----------------+-----------+| Variable_name | Value |+-----------------+-----------+| long_query_time | 10.000000 |+-----------------+-----------+ 2. 设置方法(1)全局变量设置： 123456设置slow_query_log为onmysql&gt; set global slow_query_log=&apos;ON&apos;; 设置慢查询日志存放位置mysql&gt; set global slow_query_log_file=&apos;/var/logs/mysql/data/slow.log&apos;;设置查询时间超过一秒就记录mysql&gt; set global long_query_time=1; (2)修改配置文件： 1234[mysqld]slow_query_log = ONslow_query_log_file = /usr/local/mysql/data/slow.loglong_query_time = 1 四、pt-query-digest分析慢查询日志1. 简介pt-query-digest是用于分析mysql慢查询的一个工具，它可以分析binlog、General log、slowlog，也可以通过SHOWPROCESSLIST或者通过tcpdump抓取的MySQL协议数据来进行分析。可以把分析结果输出到文件中，分析过程是先对查询语句的条件进行参数化，然后对参数化以后的查询进行分组统计，统计出各查询的执行时间、次数、占比等，可以借助分析结果找出问题进行优化。 2. 安装 官网(1)安装perl的模块 yum install -y perl-CPAN perl-Time-HiRes (2)安装步骤 123456789101112rpm安装：cd /usr/local/srcwget percona.com/get/percona-toolkit.rpmyum install -y percona-toolkit.rpm源码安装：cd /usr/local/srcwget percona.com/get/percona-toolkit.tar.gztar zxf percona-toolkit.tar.gzcd percona-toolkit-2.2.19perl Makefile.PL PREFIX=/usr/local/percona-toolkitmake &amp;&amp; make install (3) 用法简介 123456781.慢查询日志分析pt-query-digest /var/logs/mysql/data/slow.log2.服务器摘要pt-summary3.服务器磁盘监测pt-diskstats4.mysql服务状态摘要pt-mysql-summary -- --user=root --password=root123 3. pt-query-digest语法及重要选项12345678910111213pt-query-digest [OPTIONS] [FILES] [DSN]--create-review-table 当使用--review参数把分析结果输出到表中时，如果没有表就自动创建。--create-history-table 当使用--history参数把分析结果输出到表中时，如果没有表就自动创建。--filter 对输入的慢查询按指定的字符串进行匹配过滤后再进行分析--limit 限制输出结果百分比或数量，默认值是20,即将最慢的20条语句输出，如果是50%则按总响应时间占比从大到小排序，输出到总和达到50%位置截止。--host mysql服务器地址--user mysql用户名--password mysql用户密码--history 将分析结果保存到表中，分析结果比较详细，下次再使用--history时，如果存在相同的语句，且查询所在的时间区间和历史表中的不同，则会记录到数据表中，可以通过查询同一CHECKSUM来比较某类型查询的历史变化。--review 将分析结果保存到表中，这个分析只是对查询条件进行参数化，一个类型的查询一条记录，比较简单。当下次使用--review时，如果存在相同的语句分析，就不会记录到数据表中。--output 分析结果输出类型，值可以是report(标准分析报告)、slowlog(Mysql slow log)、json、json-anon，一般使用report，以便于阅读。--since 从什么时间开始分析，值为字符串，可以是指定的某个”yyyy-mm-dd [hh:mm:ss]”格式的时间点，也可以是简单的一个时间值：s(秒)、h(小时)、m(分钟)、d(天)，如12h就表示从12小时前开始统计。--until 截止时间，配合—since可以分析一段时间内的慢查询。 4. 分析pt-query-digest输出结果 第一部分：总体统计结果 Overall：总共有多少条查询 Time range：查询执行的时间范围 unique：唯一查询数量，即对查询条件进行参数化以后，总共有多少个不同的查询 total：总计 min：最小 max：最大 avg：平均 95%：把所有值从小到大排列，位置位于95%的那个数，这个数一般最具有参考价值 median：中位数，把所有值从小到大排列，位置位于中间那个数 12345678910111213141516171819202122232425# 该工具执行日志分析的用户时间，系统时间，物理内存占用大小，虚拟内存占用大小# 340ms user time, 140ms system time, 23.99M rss, 203.11M vsz# 工具执行时间# Current date: Fri Nov 25 02:37:18 2016# 运行分析工具的主机名# Hostname: localhost.localdomain# 被分析的文件名# Files: slow.log# 语句总数量，唯一的语句数量，QPS，并发数# Overall: 2 total, 2 unique, 0.01 QPS, 0.01x concurrency ________________# 日志记录的时间范围# Time range: 2016-11-22 06:06:18 to 06:11:40# 属性 总计 最小 最大 平均 95% 标准 中等# Attribute total min max avg 95% stddev median# ============ ======= ======= ======= ======= ======= ======= =======# 语句执行时间# Exec time 3s 640ms 2s 1s 2s 999ms 1s# 锁占用时间# Lock time 1ms 0 1ms 723us 1ms 1ms 723us# 发送到客户端的行数# Rows sent 5 1 4 2.50 4 2.12 2.50# select语句扫描行数# Rows examine 186.17k 0 186.17k 93.09k 186.17k 131.64k 93.09k# 查询的字符数# Query size 455 15 440 227.50 440 300.52 227.50 第二部分：查询分组统计结果 Rank：所有语句的排名，默认按查询时间降序排列，通过–order-by指定 Query ID：语句的ID，（去掉多余空格和文本字符，计算hash值） Response：总的响应时间 time：该查询在本次分析中总的时间占比 calls：执行次数，即本次分析总共有多少条这种类型的查询语句 R/Call：平均每次执行的响应时间 V/M：响应时间Variance-to-mean的比率 Item：查询对象 12345# Profile# Rank Query ID Response time Calls R/Call V/M Item# ==== ================== ============= ===== ====== ===== ===============# 1 0xF9A57DD5A41825CA 2.0529 76.2% 1 2.0529 0.00 SELECT# 2 0x4194D8F83F4F9365 0.6401 23.8% 1 0.6401 0.00 SELECT wx_member_base 第三部分：每一种查询的详细统计结果 由下面查询的详细统计结果，最上面的表格列出了执行次数、最大、最小、平均、95%等各项目的统计。 ID：查询的ID号，和上图的Query ID对应 Databases：数据库名 Users：各个用户执行的次数（占比） Query_time distribution ：查询时间分布, 长短体现区间占比，本例中1s-10s之间查询数量是10s以上的两倍。 Tables：查询中涉及到的表 Explain：SQL语句 123456789101112131415161718192021222324252627# Query 1: 0 QPS, 0x concurrency, ID 0xF9A57DD5A41825CA at byte 802 ______# This item is included in the report because it matches --limit.# Scores: V/M = 0.00# Time range: all events occurred at 2016-11-22 06:11:40# Attribute pct total min max avg 95% stddev median# ============ === ======= ======= ======= ======= ======= ======= =======# Count 50 1# Exec time 76 2s 2s 2s 2s 2s 0 2s# Lock time 0 0 0 0 0 0 0 0# Rows sent 20 1 1 1 1 1 0 1# Rows examine 0 0 0 0 0 0 0 0# Query size 3 15 15 15 15 15 0 15# String:# Databases test# Hosts 192.168.8.1# Users mysql# Query_time distribution# 1us# 10us# 100us# 1ms# 10ms# 100ms# 1s ################################################################# 10s+# EXPLAIN /*!50100 PARTITIONS*/select sleep(2)\G 五、用法1.直接分析慢查询文件1pt-query-digest slow.log &gt; slow_report.log 2.分析最近十二小时内的查询1pt-query-digest --since=12h slow.log &gt; slow_report2.log 3.分析指定时间范围内的查询**1pt-query-digest slow.log --since &apos;2017-01-07 09:30:00&apos; --until &apos;2017-01-07 10:00:00&apos;&gt; &gt; slow_report3.log 4.分析含有select语句的慢查询1pt-query-digest --filter &apos;$event-&gt;&#123;fingerprint&#125; =~ m/^select/i&apos; slow.log&gt; slow_report4.log 5.针对某个用户的查询1pt-query-digest --filter &apos;($event-&gt;&#123;user&#125; || &quot;&quot;) =~ m/^root/i&apos; slow.log&gt; slow_report5.log 6.查询所有 所有的全表扫描或full join的慢查询1pt-query-digest --filter &apos;(($event-&gt;&#123;Full_scan&#125; || &quot;&quot;) eq &quot;yes&quot;) ||(($event-&gt;&#123;Full_join&#125; || &quot;&quot;) eq &quot;yes&quot;)&apos; slow.log&gt; slow_report6.log 7.把查询保存到query_review表1pt-query-digest --user=root –password=abc123 --review h=localhost,D=test,t=query_review--create-review-table slow.log 8.把查询结果保存到query_history表12pt-query-digest --user=root –password=abc123 --review h=localhost,D=test,t=query_history--create-review-table slow.log_0001pt-query-digest --user=root –password=abc123 --review h=localhost,D=test,t=query_history--create-review-table slow.log_0002 9.通过tcpdump抓取mysql的tcp协议数据，然后分析12tcpdump -s 65535 -x -nn -q -tttt -i any -c 1000 port 3306 &gt; mysql.tcp.txtpt-query-digest --type tcpdump mysql.tcp.txt&gt; slow_report9.log]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2F2017%2F06%2F22%2FLinux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[目录 系统安全 进程管理 用户管理 文件系统 网络测试 文件处理 压缩 定时任务（crontab） 系统安全 sudo su chmod setfacl 对某个用户单独设置某个文件的权限 进程管理 top ps kill -9强制杀死 -15正常杀死 pkill killall 用户管理 usermod 修改系统账户文件 useradd groupadd userdel 文件系统 mount umount fsck df du 网络测试 netstat 文件查看 head tail less/more 文件处理 touch unlink rename ln cp 压缩 tar 定时任务（crontab） crontab的使用 at 命令]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux命令--查找与统计（grep、awk、sort、uniq、wc）]]></title>
    <url>%2F2017%2F06%2F16%2Flinux%E5%91%BD%E4%BB%A4--%E6%9F%A5%E6%89%BE%E4%B8%8E%E7%BB%9F%E8%AE%A1%EF%BC%88grep%E3%80%81awk%E3%80%81sort%E3%80%81uniq%E3%80%81wc%EF%BC%89%2F</url>
    <content type="text"><![CDATA[在做日志分析时或者配置分析时，通常会遇到查找出符合某一条件的行，并统计，主要应用的就是grep、awk、sort、uniq、wc五个命令 1. grep命令grep（global search regular expression(RE) and print out the line，全面搜索正则表达式并把行打印出来）是一种强大的文本搜索工具，它能使用正则表达式搜索文本，并把匹配的行打印出来。 语法格式为： grep [option] pattern file //pattern 通常可以为正则表达式 常见的参数为： 选项 -a ：将 binary 文件以 text 文件的方式搜寻数据 -c ：计算找到 &apos;搜寻字符串&apos; 的次数 -i ：忽略大小写的不同，所以大小写视为相同 -n ：顺便输出行号 -v ：反向选择，亦即显示出没有 &apos;搜寻字符串&apos; 内容的那一行！ --color=auto ：可以将找到的关键词部分加上颜色的显示喔！ 2.awk命令 awk命令通常是将所列出的行，根据条件打印出某一列或几列 常用形式为：awk -F ‘:’ ‘BEGIN {print “name,shell”} {if($1=”root”) print $1”,”$7} END {print “blue,/bin/nosh”}’ 3.sort命令sort排序原则：sort将文件的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。 sort [optional] filename 常用参数： -u：去除重复行 -r：默认是升序排序，-r是改为降序 -o：将排序结果写入到源文件 -n：默认是通过ASCII码值排序，但是这时会出现10比2小的情况，-n就是说要以数值进行排序 -k和-t：-k是指定以哪一列进行排序，-t是指定分隔符 举例： （1）sort -n -t “ “ -k 2 -k 3 facebook.txt //人数相同的按照员工平均工资升序排序 （2） sort -n -t ‘ ‘ -k 3r -k 2 facebook.txt //按照员工工资降序排序，如果员工人数相同的，则按照公司人数升序排序 （3）sort -t ‘ ‘ -k 1.2 facebook.txt //从公司英文名称的第二个字母开始进行排序 （4）sort -t ‘ ‘ -k 1.2,1.2 -k 3,3nr facebook.txt //只针对公司英文名称的第二个字母进行排序，如果相同的按照员工工资进行降序排序 4.uniq命令uniq命令可以去除排序过的文件中的重复行，因此uniq经常和sort合用。也就是说，为了使uniq起作用，所有的重复行必须是相邻的。 -i ：忽略大小写字符的不同； -c ：进行计数,即统计该行的重复次数 -u ：显示不存在的行 -d：显示存在重复的行 5.wc 命令统计指定文件中的字节数、字数、行数，并将统计结果显示输出。该命令统计指定文件中的字节数、字数、行数。如果没有给出文件名，则从标准输入读取。wc同时也给出所指定文件的总统计数。 -c 统计字节数。 -l 统计行数。 -m 统计字符数。这个标志不能与 -c 标志一起使用。 -w 统计字数。一个字被定义为由空白、跳格或换行字符分隔的字符串。 -L 打印最长行的长度。 -help 显示帮助信息 –version 显示版本信息 12[root@localhost test]# wc test.txt7 8 70 test.txt 7 表示行数 8 表示单词数 70 字节数]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JWT（JSON Web Token]）入门简介]]></title>
    <url>%2F2017%2F06%2F16%2FJWT%EF%BC%88JSON-Web-Token-%EF%BC%89%E5%85%A5%E9%97%A8%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[JwtJWT是（JSON Web Token]）的缩写，主要用来做用户身份验证的。 随着当前分布式应用、前后端分离的技术广泛使用，早年通过session管理用户状态的成本越来越高，session共享问题以及之后出现的token认证基本都是通过如Redis之类的中间件实现的。 JWT通过将数据保存在客户端，每次请求时将token发送至服务端校验，服务端无需存储token，实现完全无状态化。 流程 客户端登录请求认证 服务端认证通过后，生成包含数据的JSON对象，并将此对象进行签名生成token 服务端将token返回客户端，客户端存储在本地，如cookie或localStorage 客户端下次请求时携带token到服务端，常用的是放在 HTTP 请求的头的Authorization字段中，Authorization: Bearer &lt;token&gt; 服务端验证token有效性 结构Token是一个使用.分割的三部分组成的长字符串，Header.Payload.Signature HeaderHeader是一个Base64URL之后的json对象，{&quot;typ&quot;:&quot;JWT&quot;,&quot;alg&quot;:&quot;HS256&quot;}，alg表示签名的算法（algorithm），默认是 HMAC SHA256（HS256），typ表示这个令牌（token）的类型（type），JWT 令牌统一写为JWT。 PayloadPayload 也是一个Base64URL之后的JSON对象，用来存放传递的数据。JWT 规定了7个官方字段可用： iss (issuer)：签发人 iat (Issued At)：签发时间 exp (expiration time)：过期时间 nbf (Not Before)：生效时间 jti (JWT ID)：编号 sub (subject)：主题 aud (audience)：受众 除了官方字段，还可以在这个部分定义私有字段，比如 12345&#123; "sub": "101", "name": "ruesin", "LoginToken":"abcd123"&#125; 因为默认是Base64URL编码不加密的，所以客户端是可以解码读取这些数据，不要把秘密信息放在这个部分。 Signature Signature 是对前两部分的签名，校验tonken的有效性，防止数据篡改。 签名是通过服务端指定的密钥（secret）及Header中指定的签名算法产生的。 1234HMACSHA256( base64UrlEncode(header) + &quot;.&quot; + base64UrlEncode(payload), secret)]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>JWT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Composer实战指南]]></title>
    <url>%2F2017%2F06%2F15%2FComposer%E5%AE%9E%E6%88%98%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[Composer 是 PHP 用来管理依赖（dependency）关系的工具。你可以在自己的项目中声明所依赖的外部工具库（libraries），Composer 会帮你安装这些依赖的库文件。 PHP 开发过程中，难免会使用第三方包或自己的功能吧。那么，很容易出来第三方包之间的包名一样的情况。以及，同一包的不版本与其他包之间的依赖混乱的情况。 其它的语言也有类似的包依赖管理工具：Java有Maven，Python有pip，Ruby有gem，Nodejs有npm。 PHP的则是PEAR，不过PEAR坑不少： 依赖处理容易出问题 配置非常复杂 难用的命令行接口 既然有问题，那一定会有人跳出来解决。于是，Composer就出来了。 运行 Composer 需要 PHP 5.3.2+ 以上版本。一些敏感的 PHP 设置和编译标志也是必须的，但对于任何不兼容项安装程序都会抛出警告。 一、Composer在各平台的安装事实上只要你使用的系统中已经开启或支持Curl命令或将php可执行文件添加到了系统PATH中。那么，安装composer是相当的简单的。 在你的客户端运行如下命令安装 composer。默认安装最新版。 1curl -sS https://getcomposer.org/installer | php 如果你的系统没有curl,则： 1php -r "readfile('https://getcomposer.org/installer');" | php 官方英文安装文档：https://getcomposer.org/download/ 二、一些常用的 Composer 命令假定你已经有一个新项目对应的文件夹名称是 test 。那么，在 test 项目下你要创建一个composer.json。这样，在之后使用composer.json命令才能成功。 1）安装扩展包以安装 monolog 扩展为例。这个包可以在 packagist.org 搜索得到。可以看到每个包的引入条件。 在composer.json里面增加引入配置，增加之后composer.json文件内容如下： 12345&#123; &quot;require&quot;: &#123; &quot;monolog/monolog&quot;: &quot;1.2.*&quot; &#125;&#125; 这个时候，我们可以在命令行的 test 项目目录下执行如下命令安装它： 1composer install 这个时候在项目根目录下会生成一个 composer.lock 文件，以及一个 vendor 文件夹。在vendor文件夹下有一个 monolog 文件夹，说明我们已经安装成功。 2）更新扩展包更新扩展包，可以一起更新，也可以单独更新某一个包。 1composer update 以上命令会将当前扩展包整体更新。 1composer update monolog/monolog 以上命令只会更新monolog/monolog扩展包。 当修改了 composer.json 文件且已经存在composer.lock文件的时候，使用如下命令并不会更新： 1composer install 必须使用： 1composer update 3）移除扩展包扩展包移动命令会将扩展包在vendor文件夹中彻底删除。命令如下： 1composer remove monolog/monolog 4）包版本号介绍在前面的例子中，我们引入的 monolog 版本指定为 1.0.*。这表示任何从 1.0 开始的开发分支，它将会匹配 1.0.0、1.0.2 或者 1.0.20。 版本约束可以用几个不同的方法来指定。 名称 实例 描述 确切的版本号 1.0.2 你可以指定包的确切版本。 范围 &gt;=1.0 &gt;=1.0,&lt;2.0 &gt;=1.0 通过使用比较操作符可以指定有效的版本范围。 有效的运算符： &gt;、&gt;=、&lt;、&lt;=、!=。 你可以定义多个范围，用逗号隔开，这将被视为一个逻辑AND处理。 AND 的优先级高于 OR。 通配符 1.0.* 你可以使用通配符来指定一种模式。1.0.与&gt;=1.0,&lt;1.1是等效的。 赋值运算符 ~1.2 这对于遵循语义化版本号的项目非常有用。~1.2相当于&gt;=1.2,&lt;2.0。想要了解更多，请阅读下一小节。 下一个重要版本（波浪号运算符）最好用例子来解释： ~1.2 相当于 &gt;=1.2,&lt;2.0，而 ~1.2.3 相当于 &gt;=1.2.3,&lt;1.3。正如你所看到的这对于遵循 语义化版本号 的项目最有用。一个常见的用法是标记你所依赖的最低版本，像 ~1.2 （允许1.2以上的任何版本，但不包括2.0）。由于理论上直到2.0应该都没有向后兼容性问题，所以效果很好。你还会看到它的另一种用法，使用 ~ 指定最低版本，但允许版本号的最后一位数字上升。 注意： 虽然 2.0-beta.1 严格地说是早于 2.0，但是，根据版本约束条件， 例如 ~1.2 却不会安装这个版本。就像前面所讲的 ~1.2 只意味着 .2 部分可以改变，但是 1. 部分是固定的。 5）稳定性默认情况下只有稳定的发行版才会被考虑在内。如果你也想获得 RC、beta、alpha 或 dev 版本，你可以使用 稳定标志。你可以对所有的包做 最小稳定性 设置，而不是每个依赖逐一设置。 稳定性对应 composer.json文件中的 “minimum-stability”: “dev”。 6）composer.lock - 锁文件在安装依赖后，Composer 将把安装时确切的版本号列表写入 composer.lock 文件。这将锁定改项目的特定版本。 请提交你应用程序的 composer.lock （包括 composer.json）到你的版本库中。 这是非常重要的，因为 install 命令将会检查锁文件是否存在，如果存在，它将下载指定的版本（忽略 composer.json 文件中的定义）。 这意味着，任何人建立项目都将下载与指定版本完全相同的依赖。你的持续集成服务器、生产环境、你团队中的其他开发人员、每件事、每个人都使用相同的依赖，从而减轻潜在的错误对部署的影响。即使你独自开发项目，在六个月内重新安装项目时，你也可以放心的继续工作，即使从那时起你的依赖已经发布了许多新的版本。 如果不存在 composer.lock 文件，Composer 将读取 composer.json 并创建锁文件。 这意味着如果你的依赖更新了新的版本，你将不会获得任何更新。此时要更新你的依赖版本请使用 update 命令。这将获取最新匹配的版本（根据你的 composer.json 文件）并将新版本更新进锁文件。 1php composer.phar update 如果只想安装或更新一个依赖，你可以白名单它们： 1php composer.phar update monolog/monolog [...] 注意： 对于库，并不一定建议提交锁文件 请参考：库的锁文件. 更新文档，请参考：http://www.phpcomposer.com/]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>PHP面试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL基本操作汇总]]></title>
    <url>%2F2017%2F06%2F15%2FMySQL%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[数据类型 对于整型来说TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT等 指定长度是没有意义的，它不会限制值的范围，只会影响显示字符的个数，比如int(10) 如果不足十位的会补足0（通过zerofill属性来显示）。值得注意的是，如果字符少于位数则会补足0，如果超过了宽度则不受影响。 对于实数类型来说FLOAT（四个字节）、DOUBLE（八个字节）、DECIMAL（可以存储比BIGINT还要大的整型，可以存储精确的小数，16字节，精度较高的运算一般用decimal） 字符串类型VARCHAR、CHAR、TEXT、BLOB char的长度是固定的，最大为255个字符，而char的最大长度是255每个字符占用的字节数，比如utf8编码就是255\3,gbk就是255*2 varchar代表变长，最大的长度为65535个字节，如果采用varchar的话需要用1-2个字节来存储长度信息（255个字节一下需要1个字节，超过255需要两个字节）。而varchar最大字符数也是根据编码来决定（注意：char和varchar后面跟的数字都是指字符数），如果是utf8编码，varchar最大字符数=65535/3约为21845个字符，如果是gbkvarchar最大字符数=65533/2约为32766个字符 text和blob类型查询的时候会产生临时表，尽量不要使用 日期和时间类型（尽量使用TIMESTAMP，比DATETIME空间效率高） 常用命令 veresion(); //显示当前服务版本 now(); //显示当前时间 user(); //显示当前用户 concat(‘a’, ‘b’); //字符链接 concat_ws(‘-‘, ‘a’, ‘b’); //使用指定分隔符连接 lower(‘MYSQL’) upper(‘mysql’) //大小写转换 left(‘mysql’, 2) //左截取 right(‘mysql’, 2) //右截取 length(‘mysql’) //获取字符串长度 replace(‘-my-sql’, ‘-‘, ‘+’) //替换字符 substring(‘mysql’, 1 ,2) //截取字符 date_format(‘2017-9-11’, ‘%Y-%m-%d’); //日期格式化 avg(); //平均值 count(); //总数 max(); min() //最大值，最小值 sum(); //求和 常用数据库操作1.创建数据库12create &#123;database|schema&#125; [if not exists] db_name [default] character set [=] charset_name例：CREATE DATABASE test; 2.修改数据库12alter &#123;database|schema&#125; db_name [default] character set [=] charset_name例：ALTER DATABASE test CHARACTER SET utf8; 3.删除数据库12drop &#123;database|schema&#125; [if exists] db_name例：DROP DATABASE test; 常用数据表操作1.创建表1234567891011create table [if not exists] tbl_name( age tinyint(2) unsigned not null auto_increment primary key);例：CREATE TABLE user( id INT(10) UNSIGNED AUTO_INCREMENT PRIMARY KEY,//主键自增 name VARCHAR(20) NOT NULL UNIQUE KEY,//唯一 price DECIMAL(8,2) UNSIGNED DEFAULT 0.00,//默认 cid INT(10) UNSIGNED, KEY cid(cid), FOREIGN KEY (cid) REFERENCES cate (id) ON DELETE CASCADE//外键 （删除时执行CASCADE）) ENGINE=InnoDB DEFAULT CHARSET=utf8; 2.查看表结构12show colums from tbl_name;例：SHOW COLUMUNS FROM user 3. 修改表结构1234alter table tbl_name op[add|drop|modify] [column] (col_name column_definition,..);例：ALTER TABLE user ADD num INT(10) UNSIGNED, time INT(10) UNSIGNED; // 添加字段ALTER TABLE user DROP num,DROP time; // 删除字段 4.插入12（1）insert [into] tbl_name [(col_name,..)] &#123;values|value&#125; (&#123;expr|default&#125;,...),(...),...;例：INSERT user (id,name,price) VALUES (DEFAULT,tom&apos;,20); 5.更新12update tbl_name set col_name1=&#123;expr1|default&#125; [,col_name2=&#123;expr2|default&#125;].. [where where_condition]例：UPDATE user SET num = num + id; 6.删除12delete from tbl_name [where where_condition]例：DELETE FROM user WHERE id=3; 约束性(1)主键约束：primary key 每个表只存在一个 保证记录的唯一性 自动为not null 添加了主键约束 (2)唯一约束： unique key 每个表可以存在多个 保证记录的唯一性 可以存一个null 添加了唯一约束 (3)默认约束：default 给列添加了默认值 123例如：ALTER TABLE user ALTER num SET DEFAULT 0;ALTER TABLE user ALTER num DROP DEFAULT; (4)非空约束(5)外键约束 保证了数据的一致性，实现了1对1,1对多的关系 cascade：从父表中删除或更新且自动删除或更新子表中的匹配行 set nul： 从父表删除或更新并设置子表中的外键列为null。如果使用该选项，必须保证子表没有指定not null restrict：拒绝对父表的删除或更新操作 123456添加外键约束： alter table tbl_name add [constraint [symbol]] foreign key [index_name] (index_col_name,...) reference_definition 例：ALTER TABLE user ADD FOREIGN KEY (cid) REFERENCES cate (id) 删除外键约束： alter table tbl_name drop foreign key symbol 例：ALTER TABLE user DROP FOREIGN KEY cid; 子查询 嵌套在内部，始终出现在括号内; 可以包含多个关键字或条件，如distinct，group by，order by，limit，函数等;外层可以是：select，insert，update，set 1.比较运算符:=,&gt;,&lt;,&lt;=,&gt;=,&lt;&gt;123select * from t1 where col_name1 &gt;= ANY (select col_name2 from t2);(1)any:符合任意一个(2)all:符合所有 2.（not）in/exists1select * from t1 where col_name1 NOT IN ALL (select col_name2 from t2); 连接查询内连接（inner join），左连接（left join）， 右连接（right join）,全连接（full join）， 交叉连接（across join）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788现有两张表A、B表Aid name 1 张2 李3 王表Bid address A_id1 北京 12 上海 33 南京 10**************left join**********SELECT A.name, B.addressFROM ALEFT JOIN B ON A.id = B.A_id 结果是：name address张 北京李 NULL王 上海可以看到A表（左边的表）的所有行都显示出来了，B表中没有匹配到的行是NULL值************right join***********SELECT A.name, B.addressFROM ARIGHT JOIN B ON A.id = B.A_id结果是：name address张 北京王 上海NULL 南京与left join相反，B表（右边的表）中的行全显示出来，A表中匹配不到的行显示NULL**********inner join************select A.name,B.address from A inner join Bon A.id = B.A_id结果是：name address张 北京王 上海内连接等价于：SELECT A.name, B.addressFROM A, BWHERE A.id = B.A_id内连接只返回A、B两表都有的行，相当于A、B的交集*********full join**********全外连接返回参与连接的两个数据集合中的全部数据，无论它们是否具有与之相匹配的行。在功能上，它等价于对这两个数据集合分别进行左外连接和右外连接，然后再使用消去重复行的并操作将上述两个结果集合并为一个结果集select * from A full join B结果是：id name id address A_id1 张 1 北京 12 李 1 北京 13 王 1 北京 11 张 2 上海 32 李 2 上海 33 王 2 上海 31 张 3 南京 102 李 3 南京 103 王 3 南京 10*********across join***********返回笛卡尔积，A*BSELECT * FROM ACROSS JOIN B结果是：id name id address A_id1 张 1 北京 12 李 1 北京 13 王 1 北京 11 张 2 上海 32 李 2 上海 33 王 2 上海 31 张 3 南京 102 李 3 南京 103 王 3 南京 10等价于sql：select * from A,B 联合查询（union与union all） 把多个结果集集中在一起]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL索引优化策略与笔记]]></title>
    <url>%2F2017%2F05%2F29%2FMySQL%E7%B4%A2%E5%BC%95%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%E4%B8%8E%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[面试知识，数据库索引优化 什么问题？索引有什么代价？哪些场景下你需要建索引？或者有时候反过来问，哪些场景下不推荐建索引。 建好索引之后，怎么才能最高效地利用索引？或者反过来问，请说出一个无法有效利用已建索引的案例。 索引的好处？ 快速查询数据。 代价是什么？索引需要占硬盘空间，这是空间方面的代价。 一旦插入新的数据，就需要重新建索引，这是时间上的代价。 不同场景，不同对待。场景一 数据表规模不大，就几千行，即使不建索引，查询语句的返回时间也不长，这时建索引的意义就不大。当然，若就几千行，索引所占的空间也不多，所以这种情况下，顶多属于“性价比”不高。 场景二 某个商品表里有几百万条商品信息，同时每天会在一个时间点，往其中更新大概十万条左右的商品信息，现在用where语句查询特定商品时（比如where name = ‘XXX’）速度很慢。为了提升查询效率可以建索引，但当每天更新数据时，又会重建索引，这是要耗费时间的。这时就需要综合考虑，甚至可以在更新前删除索引，更新后再重建。 场景三 因为在数据表里ID值都不相同，所以索引能发挥出比较大的作用。相反，如果某个字段重复率很高，如性别字段，或者某个字段大多数值是空（null），那么不建议对该字段建索引。 建立索引原则一定是有业务需求了才会建索引。比如在一个商品表里，我们经常要根据name做查询，如果没有索引，查询速度会很慢，这时就需要建索引。但在项目开发中，如果不经常根据商品编号查询，那么就没必要对编号建索引。 最后再强调一次，建索引是要付出代价的，没事别乱建着玩，同时在一个表上也不能建太多的索引。具体的例子来看索引的正确用法 语句一：select name from 商品表。不会用到索引，因为没有where语句。 语句二：select * from 商品表 where name = ‘Java书’，会用到索引，如果项目里经常用到name来查询，且商品表的数据量很大，而name值的重复率又不高，那么建议建索引。 语句三：select * from 商品表 where name like ‘Java%’ 这是个模糊查询，会用到索引，请大家记住，用like进行模糊查询时，如果第一个就是模糊的匹配符，比如where name like ‘%java’，那么在查询时不会走索引。在其他情况下，不论用了多少个%，也不论%的位置，只要不出现在第一个位置，那么都能用到索引。 学生成绩表里有两个字段：姓名和成绩。现在对成绩这个整数类型的字段建索引。 第一种情况，当数字型字段遇到非等值操作符时，无法用到索引。比如： ​ select name from 学生成绩表 where 成绩&gt;95 , 一旦出现大于符号，就不能用到索引，为了用到索引，我们应该改一下SQL语句里的where从句：where 成绩 in (96,97,98,99,100) 第二种情况，如果对索引字段进行了某种左值操作，那么无法用到索引。 ​ 能用到索引的写法：select name from 学生成绩表 where 成绩 = 60 ​ 不能用到索引的写法：select name from 学生成绩表 where 成绩+40 = 100 第三种情况，如果对索引字段进行了函数操作，那么无法用到索引。 ​ 比如SQL语句：select * from 商品表 where substr(name) = ‘J’，我们希望查询商品名首字母是J的记录，可一旦针对name使用函数，即使name字段上有索引，也无法用到。​ 非聚集索引和聚集索引的区别在于， 通过聚集索引可以查到需要查找的数据， 而通过非聚集索引可以查到记录对应的主键值 ， 再使用主键的值通过聚集索引查找到需要的数据。 不管以任何方式查询表， 最终都会利用主键通过聚集索引来定位到数据， 聚集索引（主键）是通往真实数据所在的唯一路径。]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL拓展(extend)]]></title>
    <url>%2F2017%2F05%2F28%2FMySQL%E6%8B%93%E5%B1%95(extend)%2F</url>
    <content type="text"><![CDATA[一、分区表的原理 对用户来说，分区表是一个独立的逻辑表，但是底层mysql将其分成了多个物理的子表，这对用户来说是透明的，每一个分区表都会使用一个独立的表文件。 原理： 创建表的时候通过partition by子句定义每个分区存放的数据，执行查询的时候，优化器会根据分区定义过滤那些没有我们需要数据的分区，这样查询只查询所需的数据所在分区 分区的主要目的是将数据按照一个比较粗的粒度分在不同的表中，这样可以将相关的数据存放在一起，而且要一次删除整个分区也十分方便 适用场景 表非常大，无法全部存在内存，或者只在表的最后有热点数据，其他都是历史数据 分区表的数据更易维护，可以对独立的分区进行独立的操作 分区表的数据可以分布在不同的机器上，从而高效的使用资源 二、分库分表的原理 通过一些HASH算法或者工具将一张数据表垂直或者水平的进行物理切分 适用场景 单表数据达到百万甚至千万的级别 解决表锁的问题 水平分割此时表很大，分割后可以降低在查询时需要读的数据和索引的页数，同时也降低了索引的层数，提高查询效率 适用场景： 表中的数据有独立性，例如表中记录各个地区或各个时间段的数据，有些数据常用，有些数据不常用 需要把数据存放在多个介质上 缺点： 给应用增加复杂度，通常查询需要多个表名，查询所有的数据都需要union操作 垂直分表将主键和一些列放在一张表，然后把主键和其他列放在另外一张表 使用场景： 表中一些列常用，一些列不常用 可以使数据行变小，一个数据页能存储更多的数据，减少查询I/O次数 缺点： 管理冗余列，查询所有的数据都需要join操作]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MYSQL 事务处理常见有两种方法]]></title>
    <url>%2F2017%2F05%2F26%2FMYSQL%20%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86%E5%B8%B8%E8%A7%81%E6%9C%89%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[MYSQL在操作大量的数据或者比较重要的数据的时候，事务处理很重要，比如银行的转账，支付，等等，作为开发人员事务是必须的一步。 1、用 BEGIN, ROLLBACK, COMMIT来实现 BEGIN 开始一个事务 ROLLBACK 事务回滚 COMMIT 事务确认 2、直接用 SET 来改变 MySQL 的自动提交模式: SET AUTOCOMMIT=0 禁止自动提交 SET AUTOCOMMIT=1 开启自动提交 注意的是，在 MySQL 中只有使用了 Innodb 数据库引擎的数据库或表才支持事务。 MySQL 事务主要用于处理操作量大，复杂度高的数据。比如说，在人员管理系统中，你删除一个人员，你即需要删除人员的基本资料，也要删除和该人员相关的信息，如信箱，文章等等，这样，这些数据库操作语句就构成一个事务！ 一、php事务处理概述： 事务:是若干事件的集合 事务处理:当所有事件执行成功,事务才执行;若有任何一个事件不能成功执行,事务的其它事件也不被执行。 只要你的MySQL版本支持BDB或InnoDB表类型，那么你的MySQL就具有事务处理的能力。这里面，又以InnoDB表类型用的最多，虽然后来发生了诸如Oracle收购InnoDB等令MySQL不爽的事情，但是这类商业事件与技术无关，下面就以InnoDB表类型为例简单说一下MySQL中的事务处理。 二、php事务处理代码：&lt;?php try{ $pdo=new PDO(&quot;mysql:host=localhost;dbname=psp&quot;,&quot;root&quot;,&quot;&quot;); $pdo-&gt;exec(&quot;set names utf8&quot;); $pdo-&gt;setAttribute(PDO::ATTR_ERRMODE,PDO::ERRMODE_EXCEPTION);//设置异常处理模式 $pdo-&gt;setAttribute(PDO::ATTR_AUTOCOMMIT,0);//关闭自动提交 }catch(PDOException $e){ echo &quot;数据库连接失败&quot;; exit; } try{ $age=10; $pdo-&gt;beginTransaction();//开始事务 $affected_rows1=$pdo-&gt;exec(&quot;update kfry set k_age=k_age+{$age} where k_name=&apos;user1&apos;&quot;); $affected_rows2=$pdo-&gt;exec(&quot;update kfry set k_age=k_age-{$age} where k_name=&apos;user2&apos;&quot;);//随意更改使之执行成功或失败 /* if($affected_rows1&amp;&amp;$affected_rows2) { $pdo-&gt;commit(); echo &quot;操作成功&quot;; }else{ $pdo-&gt;rollback(); } */ if(!$affected_rows1) throw new PDOException(&quot;加入错误&quot;); if(!$affected_rows2) throw new PDOException(&quot;减少错误&quot;); echo &quot;操作成功&quot;; $pdo-&gt;commit();//如果执行到此处前面两个更新sql语句执行成功，整个事务执行成功 }catch(PDOException $e){ echo &quot;操作失败：&quot;.$e-&gt;getMessage(); $pdo-&gt;rollback();//执行事务中的语句出了问题，整个事务全部撤销 } $pdo-&gt;setAttribute(PDO::ATTR_AUTOCOMMIT,1); //测试是否成功 echo &quot;\n操作结果为:\n&quot;; $sql=&quot;select * from kfry&quot;; $result=$pdo-&gt;query($sql); foreach($result as $v) { echo $v[&apos;k_name&apos;].&quot; &quot;.$v[&apos;k_age&apos;].&quot;\n&quot;; } ?&gt; 以上就是php 事务处理详解的详细内容，更多请关注php中文网其它相关文章！]]></content>
      <categories>
        <category>PHP</category>
      </categories>
      <tags>
        <tag>PHP</tag>
        <tag>实战</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS游戏——关于马甲包与审核]]></title>
    <url>%2F2017%2F05%2F26%2FiOS%E6%B8%B8%E6%88%8F%E2%80%94%E2%80%94%E5%85%B3%E4%BA%8E%E9%A9%AC%E7%94%B2%E5%8C%85%E4%B8%8E%E5%AE%A1%E6%A0%B8%2F</url>
    <content type="text"><![CDATA[最近因为工作的原因，博客与公众号停了近一个月，网站也没有及时更新，实在抱歉！当初也不知道哪里来的冲动，脑子一热就离开了多年的城市@广州，去了一个鸟都不拉屎的地方@东莞（有点夸张了，老铁！）。 本以为是一个新的开始，本以为可以进入一个新的台阶，没料想，半年之久就遍体鳞伤的回到的这个梦想最开始的地方，其中的心酸就不便多说了，当然也让我学到了不少东西，认识了不少朋友和牛人……今天我们来聊的话题是：甲包与审核…… 引言：离开东莞之后，来到广州的一家新公司（创业公司），开始从事小说类游戏开发，说来也惭愧，以前不怎么玩游戏，最多偶尔玩一下近期比较火的游戏，也没有从事过任何游戏开发的工作，有几次有朋友看到我的英文名@iCocos都会问我，你之前事做游戏开发吗？当时我也只能无奈的解释一下： iCocos的含义是 i+ Coco + s = 苹果 + Cocoa Touch + s 取这个名字的原因是，一直以来都是从事iOS开发，而且对这一块比较喜欢，也很喜欢苹果的产品！ 接触iOS行业也有近四年多，第一次有幸进入游戏这个行业，以前很多同行（非游戏行业）朋友，包括我，一听到游戏行业和游戏开发就觉得很可怕。工资高，加班多——这是是很多人对游戏的行业的最初认识。 背景 项目启动不久，就接到公司内部关于游戏马甲包的整个对接与提审任务，后续也会一直负责这个任务，因为之前是由总部的一个多年Android开发并且有过一些iOS开发经验的同事负责，后面我进来之后iOS这一块也就有我对接处理了，当然其实有很多，我处理的只是其中一两个！ 因为之前上架过不少次，当时还是挺有自信的，但是经过了解和沟通之后发现其实并不是相信中的那么简单。 我还专门花时间在网上，群里，各大论坛和学习网站寻找相关的资料，希望能有一点帮忙！ 其中提到最多的就是关于审核的问题，而且这边同事也说了关于马甲包的审核，通过与否基本上靠运气，而且概率非常低！ 关于马甲包那么说了这么多，什么是马甲包呢？ 马甲APP指的是为了让认识你的人猜不到,在常用的用户名外再注册的其他名字的APP。 马甲APP与真实APP的区别是什么?相同的地方是什么? 应用名称不一样。 关键词不一样。 应用图标。不一样。 应用截图。可以一样,也可以不一样,不做要求。 开屏图片。最好不一样。 其余的,比如主APP的一些品牌因素,最好去掉。因为马甲是要用来做坏事的,当然不能让人察觉咯。 以上出自网络：关于为什么要做马甲APP? 马甲APP怎么做? 马甲APP需要事项? 可以查看下面网络来源说明: 什么是马甲APP？怎么用马甲APP导流 对接事项一直以来我们做的APP一般都是这几种情况： 从零开始发一个完整的项目，然后提审上线。 接手一个已经开发到一部分或者已经上线的迭代项目，然后提审上线。 接手一个审核被拒，根据苹果给出的条款修改内容，然后提审上线 有一些外包或者项目和公司比较特殊的情况 但是正因为是马甲包，所以比较例外，例外在哪里呢？ 我拿不到源码，改不了App内部的任何界面与效果 我只要修改提审内容信息，然后向渠道回到进度 审核几率非常低，而且经常要切换Apple账户处理 …… 操作流程所以在处理之前我这边其实大概根据之前的上线经验整理了一下具体的对接事项和具体步骤： 使用（或新）apple账号 apple developer生成并下载证书（开发与发布） apple developer创建一个Bundle ID apple developer添加设备ID apple developer创建并下载描述文件（开发与发布） itnues connect 创建一个对应ID的app 将证书导入到电脑钥匙串，右健对应证书到处P12 修改IAP支付信息（这种一般都是根据ID，不然会很麻烦） 提交或者修改App Icon，宣传图，应用名称,关键词,应用图标,文件等App和公司信息 将证书，描述文件，P12（+密码）和ID对应版本号ipa提供商 让他们根据重新出包，并提供新的ipa包 确认测试通过，并确认提审信息后，重新上传ipa 提交审核（使用手动发布模式）…… 跟渠道那边及时反馈进度 审核没通过则重新以上步骤（不用换账号还好，换账号是最虐心的！！！） 审核通过则先完成以下两个步骤再点击：发布： 1、通知CP切换到正式区服 2、通知我这边让后端把支付方式切换到第三方支付 最后注意一点： 有时候可能会涉及到SDK的处理，这个就要母包提前就准备好，并且根据实际需求替换对应的参数就可以由于这里已经涉及到SDK开发与集成的基础，已经不属于马甲包的范围 提审信息以下是我提供的数据证书与App提审信息证书与ipa信息 App提审信息 其实整个流程差不多就这么写，这里就不每一步网上都有对应的教程，而且很简单（傻瓜式），当然在整个过程也有可能遇到不少问题，这个就要根据个人经验和学习能力临时应变处理了，所以我就不一一介绍了。 补充：Android流程与注意事项以下是Android同事整理的流程与步骤，可能比较简洁，但是实际并没有这么简单，毕竟Android不像苹果那么多限制，仅供参考，具体流程还是要根据实际执行 反编译 母包 并且 集成了渠道sdk的demo 打开两者 反编译之后的目录进行资源替换 lib里面的so文件 smali源码 res的图片、value里面的资源 manifest的activity、权限等等，包名后面添加对应渠道的名字 注意：(除了3011,其他的渠道包还要修改appId、clientKey、clientId) 在eclipse新建一个同包名的项目， 拷贝修改后母包的res覆盖进去， 修改冲突ids(游戏母包也要修改)， 编译得到apk， 再反编译这个apk， 拿到包名对应下面的R$xxxx.smail覆盖到母包里面 后面就是提交apk或者发包的内容了 最后 那么说了这么多，好像里面学不到什么东西，我为什么还要写这篇文章呢？ 其实一开始不太想写，也确实没有太多有用的东西，但是我觉得写了还是会有一些用的！ 太久没有写博客和公众号了，有点手痒（哈哈！！！） 个人习惯，几乎接触第一次的领域，项目，技术都会简单记录一下 方便后面打算或者会负责此类任务的人，了解整个流程，也欢迎交流学习 最后一点其实也是最重要的，拒审多次，需求有经验的前辈，指点迷津！]]></content>
      <categories>
        <category>Games</category>
      </categories>
      <tags>
        <tag>iOS游戏——关于马甲包与审核</tag>
        <tag>Games</tag>
        <tag>关于马甲包与审核</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017@Swift——后感]]></title>
    <url>%2F2017%2F05%2F15%2F2017-Swift%E2%80%94%E2%80%94%E5%90%8E%E6%84%9F%2F</url>
    <content type="text"><![CDATA[2017@Swift——中国开发者大会（第二站） 会议时间：2017年5月13日-5月14日， 地址：深圳凯宾斯基酒店， 人数：400人 嘉宾：18人，其中8位国外，8位国内。 第一天所有外国嘉宾的演讲时间，第二天是国内大牛的演讲时间！ 记得第一站是去年在北京举行，当时项目太赶，就没有去，今年有幸能参加此次大会，并且收获真心不少。因此这里稍微整理了一下这两天最大的感触，和所学到的东西！ 当然关于技术的分享，后期会有专门的文章与结合简单的实战整理一并分享出来，敬请期待……. 更多信息请点2017@Swift——中国开发者大会,同时后面也会抽空整理一下会议中讲到的内容，底层，细节与实现，敬请期待！ 2017@Swift——后感 这种收获真不是一两句话或者一两篇文章就能说清楚的，如果你有事吗疑问或者想了解先关内容也可以直接关注我，联系我！ 第二届 @Swift 中国开发者大会 英语能力 交友 技术 英语为什么这里我把英语能力放在最前面，肯定是有原因的。 1. 开会第一天所有都是老外，所有人都直接用英语讲，几乎听不太懂，所以导致很多东西根本没有听到重点与细节 2. 会后交流，与提问，人家都能听懂，甚至都能提出自己的问题与观点，甚至还能通嘉宾进行更多更加升入的交流与讨论，而我却只能翻译或者发呆 3. 技术的学习，几乎所有的最好，最新的技术资料都是从英文开始的，当你开始看中文翻译资料或者开始研究的时候，人家已经换了一种技术 4. 后期的规划问题，出国旅游，出国发展（有点长远，当然也不是没有可能的），或者有机会接触外国朋友。 总结：针对程序员，哪怕只有一点基础的，技术这种东西都可以慢慢学，或者在实际项目中提高，如果你英文不好很多事情真的很难进行下去，除非你没有什么长远的目标或者没有自己个人的规划。 交友经过这次会议之后，看到了很多牛人，也认识了不少牛人之后才知道自己的渺小，才知道自己原来离期望中太远太远。 1. 最有名的Swift框架RXSwift的作者 2. Google工程师 3. 腾讯，美团，礼物说，阿里等一些非常厉害的架构师，同时也是技术迷 4. 不少技术书籍的作者，其中有一位是我非常喜欢的巧哥@唐巧 5. 不同公司的技术主管，开发人员，初入门的程序员 6. 两天的会议中认识到了不少来自去全国各地参会者，也结下了不少好友！ 总结：永远不要觉得自己多牛逼，当你真的看到牛人之后你就会发现，其实自己真的很low，哪怕你做过再多项目，写过再多代码，不思考，不提升依然是个菜鸟。 技术1. 主题就是Swift：iOS，后台肯定是重心 2. Swift实现AI，智能，机器学习，树莓派的结合，后期的转行。 3. 实现属于自己的东西，小的方向比如小框架，静态或者动态库，大的话其中有人自己写了一个Swift转H5的编译器 4. App性能的提升，多线程，锁，MVVM, 自动化（测试，脚本） 5. 包括开发，集成，测试，发布，维护需要有一套完成的规范与流程。 总之，程序员是一个需要时刻保持学习的职业，不学你就将被淘汰，不管学习技术，管理，社交，英语或者学习别人成功或失败的经验 未来的路还很长，希望一路上的坎坷能让我足够强大。 说说我接下来的规划（我只谈短期1-2年）： 1. 学习英语，不管是自学还是报班这都是现阶段最最最重要的任务,同时也会长期的坚持下去。 2. 着手Swift进行项目实战，对PHP实战后台开发进一步的提升，同时去了解Swift实现后台开发 3. iOS深挖：深入解析Max OS X &amp; iOS操作系统 4. 学习Python，了解机器学习等技术]]></content>
      <categories>
        <category>2017@Swift</category>
      </categories>
      <tags>
        <tag>Swift</tag>
        <tag>2017@Swift——后感</tag>
        <tag>大会后感</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017@Swift——中国开发者大会（第二天）]]></title>
    <url>%2F2017%2F05%2F15%2F2017-Swift%E2%80%94%E2%80%94%E4%B8%AD%E5%9B%BD%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E4%BC%9A%EF%BC%88%E7%AC%AC%E4%BA%8C%E5%A4%A9%EF%BC%89%2F</url>
    <content type="text"><![CDATA[首先介绍一下会议信息 会议时间：2017年5月13日-5月14日， 地址：深圳凯宾斯基酒店， 人数：400人 嘉宾：18人，其中8位国外，8位国内。 第一天所有外国嘉宾的演讲时间，第二天是国内大牛的演讲时间！ 更多信息请点2017@Swift——中国开发者大会,同时后面也会抽空整理一下会议中讲到的内容，底层，细节与实现，敬请期待！ ============================================================================= 唐巧（小猿搜题产品技术负责人）Richards 和 Deltablue 是衡量语言运算速度的两个主流的评测代码。Swift 在这两个评测中，显示出远超 Objective-C 的性能。特别是 Richards 评测，Swift 比 Objective-C 快了 4 倍。那么，为什么 Swift 这么快呢？本次分享，我将从编译器优化，内存分配优化，引用计数优化，方法调用优化，面向协议编程的实现细节等方面来介绍 Swift 在性能上所做的努力。如果可能，我们也会一起看看编译器处理后的源码，加深我们对于这些优化的理解。 小猿搜题产品技术负责人，资深 iOS 开发者，曾开发过网易邮箱、网易微博、有道云笔记、猿题库、小猿搜题。喜欢写作和分享，维护着中国关注者最多的iOS领域的微信公众号「iOS开发by唐巧」，产出了超过 100 篇原创的 iOS 技术文章，出版了一本 iOS 进阶读物《iOS开发进阶》。 Swift 为什么这么快1. 第二天中，唯一一个使用英文演讲的国内大牛，挑战自我。 2. 从不同方便说明为什么swift比OC好：引用数，struct的内存布局，性能优化，编译器、内存、消息转发机制、引用数和protocol。 ============================================================================= 戴铭（滴滴出行技术专家） github.com/ming1016 结合实例介绍如何用 LLVM IR 中间语言和编译前端的语法解析来设计一门简洁易用支持多平台的 DSL 语言，从而提高开发效率。介绍如何使用 Swift 开发一个类似 Clang 并且支持插件的简版编译前端，从而优化代码，提高工程质量。此外还会简单介绍一些有趣的实践。 微博@戴铭。滴滴出行技术专家，技术上主要负责滴滴出行 iOS 相关的开发工作。时常会将对新技术的深入研究和工作的经验总结发在微博上。对 ReactiveCocoa，RxSwift，软件架构，性能优化和算法有着浓厚的兴趣。最近正在研究iOS编译相关底层技术，用来解决工程优化问题，到时会将成果分享出来。 学习 iOS 编译原理能做哪些有意思的事情1. 这哥们可谓是有着一双被编程耽误了的画手，不但主题，技术吸引人，所画出来的作品更是让人回味。 2. 先讲了一个关于flexbox布局的web代码，自己使用swift写了一个解析器。 3. 结合上面的实现个性化的讲解了一下编译器前后端，及编译的整个过程 ============================================================================= 柯灵杰（腾讯公司iOS开发）图片组件可以说是app开发中使用最多的组件之一，它既简单也不简单，如何设计和开发一个具有高扩展性，高性能的图片组件呢？本次分享将会从架构设计到性能优化等多方面，全面解析一个优秀图片组件的设计和开发原理，以及在性能优化和架构设计方面的一些经验和探索。 柯灵杰（lingtonke），腾讯公司 iOS 开发。腾讯学院认证讲师，主要讲授课程《设计模式》。曾参加过 QQ、QZone、微云、企鹅 MV、闪咖等的开发。他主导开发的图片组件，在腾讯内被多个项目使用，获得公司内多项优秀组件奖，目前已着手对业内开源。同时他还是数项技术发明专利的发明人，曾参加过程序员 LiveShow《有码的开发哥无码的直播》。目前是腾讯 QZone 团队 iOS 开发。 打造易扩展的高性能图片组件1. 如何设计高性能易拓展的图片组件，一步一步的优化。 2. 综合分析的不同框架，不同实现方案，不同技术的优缺点。 3. 当时我会中问到是否开源，回答是会，并且预计年底会开源。 ============================================================================= 王文槿（UC资深开发工程师）来自 UC 浏览器的 iPhone 组，参与了 UC 浏览器，UC 头条和夸克浏览器相关产品的开发，工作中主要使用 OC 和 Weex 。不过业余是不折不扣的 Swift 爱好者，自诩 Swift 的函数式编程的布道师。曾经先后通过文章&amp;演讲的形式分享了 Swift 异步串行/并行编程以及函数式的设计模式等话题。 一个轻量级 FRP 框架的诞生记1. 之前UC面试过一次，可惜挂了 2. 主要结合实际以一个很简单的Demo演示了MVVM，并且很清晰的解答了众多为止疑惑的参会者。 ============================================================================= 赵恩生（美团点评高级工程师）随着业务拆分和组件化的完成，美团 iOS 客户端在集成和交付的道路上越发艰难。在业务代码下放后，如何能保证客户端的安全稳定；面对千万用户，如何能快速优雅地组建并交付一个完整的客户端，这一切的一切都值得思考和深究。 希望通过本次分享，和各位一起探讨如何玩转大业务体量下的众多组件、如何搭建整个客户端的发布流程。 美团点评高级工程师，曾维护美团 iOS 客户端，现负责 iOS 发布流程相关工作，专业打杂，通过对复杂业务下组件发布集成的踩坑总结，对 CI CD 有一定的认识和理解。闲暇时喜欢捣鼓乱七八糟的东西，自学习得包括疏通下水道，手机贴膜，设备维修在内的一些奇怪技能。 组件 + 组建 = 美团 iOS 客户端1. 美团开发，集成，打包，测试，发布整个流程的工作与注意点。 2. 组件式的开发方式，规范性的流程与测试形成一个闭环 3. 校验，避错，后期的修复的考虑及解决方案 4. CI的基本流程介绍 ============================================================================= 傅若愚（ThoughtWorks 高级咨询师）是的，你一定听说过 Metal，或者你还写过一些 Metal 的 Shader。不过，说实话，喝了两杯啤酒之后，面对 Xcode，我们能做点儿更有趣的东西么？要不要来试试？ 来自 ThoughtWorks，刚睡醒的移动开发者，喝高了的 Tech Lead，SwiftyJSON 的作者（但这家伙已经弃坑……Oh，别担心，其他人还在维护）。业余喜读书，文史哲无所禁忌，爱美食与啤酒（最近爱上了 American Pale Ale），今年大概 17 岁的样子。 一些跟 Metal 有关系，肯定有趣但多半没用的东西1. 主要是机器学习，高逼格式的讲解了神经网络 2. 输入图片，视频染色然后渲染并输出对应的效果。 3. 主要是一些基础的概念，理论知识。 ============================================================================= 尹航（Google 工程师）深度学习总是让人联想到成吨的数据、笨重的服务器。但在移动端，我们能不能利用深度学习做一点有趣的事情呢？本次分享，让我们看看如何在iOS上运行起工业级的深度学习框架TensorFlow吧。 一个技能树歪掉的开发者。iOS 首个游戏辅助“叉叉助手”作者，也曾经编写《Cocos2d-x高级开发教程》，目前兴趣有移动安全和机器学习，Google 工程师，从事 Gmail 语义理解相关开发。 TensorFlow+iOS=❤️: 造一个颜文字输入法1. 机器学习，实战了一个emoji表情的输出。 2. 介绍了TensorFlow，介绍了他的一些特性与简单的应用 ============================================================================= 唐晓轩（礼物说联合创始人）Live Coding 礼物说联合创始人，全栈打杂工程师，公众号糖炒小虾，热衷于折腾各种黑技术。 当 Swift 遇上树莓派1. swift和树莓派之间的交互。 2. Swift+ARM，展示了一个watch跑的超级玛丽。 3. 展示数个小Demo，演示怎么与实际相结合，并提到为了对小米设备的实际应用 ============================================================================= 第二天尾声…到此整个会议已经全部结束，这一天在技术，底层，架构等方面收获就完全不一样了，同时后期会有文章介绍相关内容！ 随后，大家都纷纷离开准备回家或者会工作的城市！]]></content>
      <categories>
        <category>2017@Swift</category>
      </categories>
      <tags>
        <tag>Swift</tag>
        <tag>大会</tag>
        <tag>2017@Swift——中国开发者大会（第二天）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[2017@Swift——中国开发者大会（第一天）]]></title>
    <url>%2F2017%2F05%2F15%2F2017-Swift%E2%80%94%E2%80%94%E4%B8%AD%E5%9B%BD%E5%BC%80%E5%8F%91%E8%80%85%E5%A4%A7%E4%BC%9A%EF%BC%88%E7%AC%AC%E4%B8%80%E5%A4%A9%EF%BC%89%2F</url>
    <content type="text"><![CDATA[首先介绍一下会议信息 会议时间：2017年5月13日-5月14日， 地址：深圳凯宾斯基酒店， 人数：400人 嘉宾：18人，其中8位国外，8位国内。 第一天所有外国嘉宾的演讲时间，第二天是国内大牛的演讲时间！ 更多信息请点2017@Swift——中国开发者大会,同时后面也会抽空整理一下会议中讲到的内容，底层，细节与实现，敬请期待！ ================================================================================================================================= Tuomas Artman（Uber 移动架构和框架组负责人）Uber 使用 Swift 重写了 app，在这个分享中，Tuomas 会介绍如何在上百人的团队中使用 Swift 进行开发，并介绍 Uber 重写过程中遇到的各种问题和解决方法。Tuomas 是 Uber 移动架构和框架组的负责人。在来湾区之前，他创立了一家芬兰知名的数字资讯公司，还在上海的一家公司工作过一年，开发游戏和多用户技术平台。 Swift with hundreds of engineers1. 使用swift重构Uber的过程中good，bad，ugly。 2. 对架构重新设计了，并且将他成为router-&gt;interaction-&gt;builder简称RIBs。适用于Android和iOS双平台。 3. 重构后Uber的崩溃率降到了0.01%，这确实是一个非常理想的数据。 4. binary size关于Uber包的大小。 5. 关于如何缩短Uber的编译时间 6. 最后聊了一下unit test。 ================================================================================================================================= Sam Davies（RayWenderlich CTO）从基础的 NSOperation 一直到 Grand Central Dispatch，彻底讲明白 iOS 并发。此外还会涉及一些高级内容，比如 dispatch group 和线程安全。 Concurrency on iOS1. 异步并发。 2. 提出了开发中经常遇到的一些问题和解决方案 3. 提到了反转优先级 ================================================================================================================================= Igor Jerkovic（前 Facebook 资深开发者）Igor 在 Facebook 的视频团队工作了五年，非常熟悉国外大公司的那一套理论。他会在这次分享中介绍如何提高开发效率和代码质量。 我很喜欢编程，也喜欢学习新东西，尝试不同的方法。2011-2013 年我在 Facebook 实习，负责开发 iOS 项目。2014 年我正式加入 Facebook，一直在视频团队工作到 2016年。现在我在 Bellabeat 工作，这是一家和健康相关的创业公司，在旧金山、深圳和萨格勒布都有办公室。 iOS development efficiency at Facebook1. Facebook的开发流程，包括各种开发中的规范 2. 介绍了很多实用的工具Buck... ================================================================================================================================= Sommer Panage（Chorus Fitness 核心 iOS）在这个分享中，我会介绍无障碍和 VoiceOver。首先说明为什么无障碍支持很重要，然后通过一个权威的 iOS 应用了解你可以实现哪些无障碍功能。接着我会介绍 iOS 无障碍 API 以及如何对接 VoiceOver。最后，我会补充一些有趣的无障碍知识，除了支持 VoiceOver，我们还能做得更好。 Sommer Panage 目前是 Chorus Fitness 的核心 iOS 开发。在这之前，她做了两年 iOS freelancer，与此同时她还成为了一名马戏表演者和教练。更早的时候，她在 Twitter 和 Apple 的无障碍团队工作。Sommer 对心理学和计算机科学两个领域都有涉猎。在编写代码之余，她还会做绳索训练、高空秋千、倒立、跑步和抱石。你可以在 Twitter 上关注她，@sommer。 From Zero to Hero: Making your iOS App Accessible to VoiceOver and Beyond1. 主要是无障碍应用开发 2. 讲了iOS中的accessibility programing，并演示了对失明者的交互与使用。 3. 通过code演示了一些使用方式 ================================================================================================================================= Krunoslav Zaher（RxSwift 框架作者）我会介绍开发 Rx 的初衷、我对 Rx 的看法、Rx 的特点以及 Rx 和传统编程方式的区别。我还会介绍 Rx 的性能、不同设计模式的实现方式以及如何关联 Rx 和状态机。 在行业内摸爬滚打了 16 年。做过很多东西，增强现实引擎、BPM 系统、手机应用、机器人……最近在研究函数式编程和链式编程。白天我会帮准妈妈们听婴儿的心跳，晚上我会哄自己的宝贝女儿睡觉。 Thinking in Rx way1. 主要是对rx的用法介绍。 2. 提到了他写RX的初衷 ================================================================================================================================= Marius Rackwitz（CocoaPods、Realm 核心开发者）Marius 从大学毕业开始就从事移动端和 web 应用开发。最近他开始专注移动端开发，尤其是 iOS 和 Objective-C，以及后起之秀 Swift。当然，作为 CocoaPods 核心成员，他也没有抛弃 Ruby。加入 Realm 之后，他在社区中更加活跃，参与了很多大会。 JavaScript for Swift Developer1. 这哥们整个演讲的过程中很是激情，但是却让在场大部分参会者归为叛徒的了（哈哈，开个玩笑）。 2. JavaScript for Swift Developer主要是两者之间的不同。 ================================================================================================================================= Kyle Jessup（Perfect 框架作者，CTO）Perfect 作者，Lasso 编程语言（被苹果的子公司 Claris 收购）的服务端核心开发者。Kyle 自学能力很强，对 Swift、Java 和 C++ 都有深入了解。 小时候，Kyle 的父亲——一名曾在美国海军潜艇上服役的核工程师——送给他一台雅达利 800 个人电脑。从那时起，他就对技术产生兴趣，一直延续到现在。 Kyle 在德克萨斯的达拉斯出生，2015 年搬到加拿大，加入 PerfectlySoft 团队。 Swift作为后端开发相关1. 演示并分享了Swift实现后端开发的利弊 2. 介绍了了Perfect ================================================================================================================================= Saul Mora（流利说 iOS 工程师，MagicalRecord 作者）从上古时代开始接触 iOS，熟悉手动内存管理、编译器宏、pthread 和 头文件。Saul Mora 为了表达对编程前辈的尊重，在 Swift 中使用带可选变量的 Nib 编写 UI。掌握了 Objective C 之后，Saul 开始环游世界，进行历练。为了应对前方的算法挑战，Saul 积极拥抱 Swift。最近，Saul 生活在现代中国的核心城市——上海。他在流利说寺修行，开发这款优秀的 app，帮助中国用户学习英语。 Building Confidence: Testing iOS applications1. 以一个很滑稽很有乔布斯风格的开场吸引住了在场的参会者 2. 分析并深入讲解了测试的重要性和原则 ============================================================================= 第一天尾声…第一天的会议也就这样结束了，整个会议中大家谈论最多的问题就是关于英语的能力，这个后面会有相关的文章详细说明！]]></content>
      <categories>
        <category>2017@Swift</category>
      </categories>
      <tags>
        <tag>2017@Swift——中国开发者大会（第一天）</tag>
        <tag>Swift</tag>
        <tag>大会</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS——App启动优化分析与总结]]></title>
    <url>%2F2017%2F05%2F03%2FiOS%E2%80%94%E2%80%94App%E5%90%AF%E5%8A%A8%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E4%B8%8E%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[随着人们对App性能与实用要求越来越高，也随着大量iOS开发中的涌入，现如今已经不再是几年前会做简单App，然后开始开发，能写出一个可用功能的产品就可以了，于是，很多公司和开发者也开始关注和实战启动优化，因为App除了桌面Log，启动是也是夺得用户喜好最首要的条件，如果一个App每次启动都要几十秒甚至几分钟，你觉得你还会继续使用吗…… 如果你对iOS开发中App的整个启动过程干兴趣，那么可以先看我之前分享的一个主题iOS——App整个启动过程分析 当然关于启动优化，上面只是简单的提了一下，本文结合那篇文章进行详细分析 启动优化准备APP启动时间： t(App总启动时间) = t1(main()之前的加载时间) + t2(main()之后的加载时间)。 t1 = 系统dylib(动态链接库)和自身App可执行文件的加载； t2 = main方法执行之后到AppDelegate类中的didFinishLaunchingWithOptions方法执行结束前这段时间，主要是构建第一个界面，并完成渲染展示。 前面我们分析了，Main之前和之后的加载过程这里我们先简单介绍一下，之前没有详细说明的一些其中关联的技术： App开始启动后，系统首先加载可执行文件（自身App的所有.o文件的集合），然后加载动态链接库dyld，dyld是一个专门用来加载动态链接库的库。执行从dyld开始，dyld从可执行文件的依赖开始, 递归加载所有的依赖动态链接库。 其实无论对于系统的动态链接库还是对于App本身的可执行文件而言，他们都算是image（镜像），而每个App都是以image(镜像)为单位进行加载的 什么是image 1.executable可执行文件 比如.o文件。 2.dylib 动态链接库 framework就是动态链接库和相应资源包含在一起的一个文件夹结构。 3.bundle 资源文件 只能用dlopen加载，不推荐使用这种方式加载。 注意：除了我们App本身的可行性文件，系统中所有的framework比如UIKit、Foundation等都是以动态链接库的方式集成进App中的。 不同进程之间共用系统dylib的_TEXT区，但是各自维护对应的_DATA区。 所有动态链接库和我们App中的静态库.a和所有类文件编译后的.o文件最终都是由dyld(the dynamic link editor)，Apple的动态链接器来加载到内存中。每个image都是由一个叫做ImageLoader的类来负责加载（一一对应） 是ImageLoader image 表示一个二进制文件(可执行文件或 so 文件)，里面是被编译过的符号、代码等， ImageLoader 作用是将这些文件加载进内存，且每一个文件对应一个ImageLoader实例来负责加载。 ImageLoader加载步骤分两步走： 在程序运行时它先将动态链接的 image 递归加载 (也就是上面测试栈中一串的递归调用的时刻)。 再从可执行文件 image 递归加载所有符号。 真正的启动优化：Main之前：检测方式：Apple提供了一种测量方法，在 Xcode 中 Edit scheme -&gt; Run -&gt; Auguments 将环境变量DYLD_PRINT_STATISTICS 设为1 pre-main阶段 1.1. 加载应用的可执行文件 1.2. 加载动态链接库加载器dyld（dynamic loader） 1.3. dyld递归加载应用所有依赖的dylib（dynamic library 动态链接库） 动态链接库的加载步骤具体分为5步： load dylibs image 读取库镜像文件 Rebase image Bind image Objc setup initializers load dylibs image在每个动态库的加载过程中， dyld需要： 分析所依赖的动态库 找到动态库的mach-o文件 打开文件 验证文件 在系统核心注册文件签名 对动态库的每一个segment调用mmap() 通常的，一个App需要加载100到400个dylibs， 但是其中的系统库被优化，可以很快的加载。 ######&gt; 针对这一步骤的优化有： 减少非系统库的依赖 尽量不使用内嵌（embedded）的dylib，加载内嵌dylib性能开销较大 合并已有的dylib和使用静态库（static archives），减少dylib的使用个数 使用静态资源，比如把代码加入主程序 懒加载dylib，但是要注意dlopen()可能造成一些问题，且实际上懒加载做的工作会更多 rebase/bind由于ASLR(address space layout randomization)的存在，可执行文件和动态链接库在虚拟内存中的加载地址每次启动都不固定，所以需要这2步来修复镜像中的资源指针，来指向正确的地址。 rebase修复的是指向当前镜像内部的资源指针； 而bind指向的是镜像外部的资源指针。rebase步骤先进行，需要把镜像读入内存，并以page为单位进行加密验证，保证不会被篡改，所以这一步的瓶颈在IO。bind在其后进行，由于要查询符号表，来指向跨镜像的资源，加上在rebase阶段，镜像已被读入和加密验证，所以这一步的瓶颈在于CPU计算。 通过命令行可以查看相关的资源指针: xcrun dyldinfo -rebase -bind -lazy_bind myApp.App/myApp 优化该阶段的关键在于减少__DATA segment中的指针数量。 ######&gt; 我们可以优化的点有： 减少ObjC类（class）、方法（selector）、分类（category）的数量 减少C++虚函数数量（创建虚函数表有开销） 转而使用swift stuct（其实本质上就是为了减少符号的数量） Objc setup这一步主要工作是: 注册Objc类 (class registration) 把category的定义插入方法列表 (category registration) 保证每一个selector唯一 (selctor uniquing) 由于之前2步骤的优化，这一步实际上没有什么可做的。 initializers以上三步属于静态调整(fix-up)，都是在修改__DATA segment中的内容，而这里则开始动态调整，开始在堆和堆栈中写入内容。 在这里的工作有： Objc的+load()函数 C++的构造函数属性函数 形如attribute((constructor)) void DoSomeInitializationWork() 非基本类型的C++静态全局变量的创建(通常是类或结构体)(non-trivial initializer) 比如一个全局静态结构体的构建，如果在构造函数中有繁重的工作，那么会拖慢启动速度 Objc的load函数和C++的静态构造函数采用由底向上的方式执行，来保证每个执行的方法，都可以找到所依赖的动态库。 +load方法断点的调用堆栈和顺序： dyld 开始将程序二进制文件初始化 交由 ImageLoader 读取 image，其中包含了我们的类、方法等各种符号 由于 runtime 向 dyld 绑定了回调，当 image 加载到内存后，dyld 会通知 runtime 进行处理 runtime 接手后调用 mapimages 做解析和处理，接下来 loadimages 中调用 callloadmethods 方法，遍历所有加载进来的 Class，按继承层级依次调用 Class 的 +load 方法和其 Category 的 +load 方法 ######&gt; 我们可以做的优化有： 少在类的+load方法里做事情，尽量把这些事情推迟到+initiailize 减少构造器函数个数，在构造器函数里少做些事情 减少C++静态全局变量的个数 至此，可执行文件中和动态库所有的符号(Class，Protocol，Selector，IMP，…)都已经按格式成功加载到内存中，被 runtime 所管理，再这之后，runtime 的那些方法(动态添加 Class、swizzle 等等才能生效)。 到这里整个过程： 整个事件由 dyld 主导，完成运行环境的初始化后，配合 ImageLoader 将二进制文件按格式加载到内存， 动态链接依赖库，并由 runtime 负责加载成 objc 定义的结构，所有初始化工作结束后，dyld 调用真正的 main 函数。 总结优化点： 减少不必要的framework，因为动态链接比较耗时 check framework应当设为optional和required，如果该framework在当前App支持的所有iOS系统版本都存在，那么就设为required，否则就设为optional，因为optional会有些额外的检查 合并或者删减一些OC类，关于清理项目中没用到的类，使用工具AppCode代码检查功能，查到当前项目中没有用到的类如下： 删减一些无用的静态变量 删减没有被调用到或者已经废弃的方法 将不必须在+load方法中做的事情延迟到+initialize中 尽量不要用C++虚函数(创建虚函数表有开销) Main之后：检测方式：测量main()函数开始执行到didFinishLaunchingWithOptions执行结束的耗时，自己插入代码到工程。 main()阶段 2.1. dyld调用main() 2.2. 调用UIApplicationMain() 2.3. 调用applicationWillFinishLaunching 2.4. 调用didFinishLaunchingWithOptions 在main()被调用之后，App的主要工作就是初始化必要的服务，显示首页内容等。而我们的优化也是围绕如何能够快速展现首页来开展。App通常在AppDelegate类中的didFinishLaunchingWithOptions方法中创建首页需要展示的view，然后在当前runloop的末尾，主动调用CA::Transaction::commit完成视图的渲染。 而视图的渲染主要涉及三个阶段： 准备阶段 这里主要是图片的解码 布局阶段 首页所有UIView的- (void)layoutSubViews()运行 绘制阶段 首页所有UIView的- (void)drawRect:(CGRect)rect运行 再加上启动之后必要服务的启动、必要数据的创建和读取，这些就是我们可以尝试优化的地方 因此，对于main()函数调用之前我们可以优化的点有： 不使用xib，直接视用代码加载首页视图 NSUserDefaults实际上是在Library文件夹下会生产一个plist文件，如果文件太大的话一次能读取到内存中可能很耗时，这个影响需要评估，如果耗时很大的话需要拆分(需考虑老版本覆盖安装兼容问题) 每次用NSLog方式打印会隐式的创建一个Calendar，因此需要删减启动时各业务方打的log，或者仅仅针对内测版输出log 梳理应用启动时发送的所有网络请求，是否可以统一在异步线程请求 梳理各个二方/三方库，找到可以延迟加载的库，做延迟加载处理，比如放到首页控制器的viewDidAppear方法里。 梳理业务逻辑，把可以延迟执行的逻辑，做延迟执行处理。比如检查新版本、注册推送通知等逻辑。 避免复杂/多余的计算。 避免在首页控制器的viewDidLoad和viewWillAppear做太多事情，这2个方法执行完，首页控制器才能显示，部分可以延迟创建的视图应做延迟创建/懒加载处理。 采用性能更好的API。 首页控制器用纯代码方式来构建。 总结：具体优化点 纯代码方式而不是storyboard加载首页UI。 对didFinishLaunching里的函数考虑能否挖掘可以延迟加载或者懒加载，需要与各个业务方pm和rd共同check 对于一些已经下线的业务，删减冗余代码。 对于一些与UI展示无关的业务，如微博认证过期检查、图片最大缓存空间设置等做延迟加载 对实现了+load()方法的类进行分析，尽量将load里的代码延后调用。 上面统计数据显示展示feed的导航控制器页面(NewsListViewController)比较耗时，对于viewDidLoad以及viewWillAppear方法中尽量去尝试少做，晚做，不做 到这里之后其实已经差不多了，相信你应该有哪么写成就与收获。 除了这些，我们还可以喂项目做一些缓存优化 ccache 等缓存方案 优化 Xcode 配置 加钱堆硬件 以上优化方案出发点都是基础优化编译耗时来解决的。 哪有没有一个办法可以做到不编译就执行修改后的代码呢？ 答案肯定是：有的 基于 Objective-C 的动态特性，是完全可以做到这一点的，这也是各种热修复框架的支撑原理之一。那么如果需要做到不编译就执行修改后的代码，我们可以这样做： 获取本地修改后代码 -&gt; 转 JavaScript 或 Lua -&gt; 模拟器执行修改后的脚本。 获取本地修改代码 这里也有许多方法，可以手动复制，也可以自动获取。这里我是选择利用 Xcode Editor Extension 来获取到你选中的修改代码的。 Objective-C 转 JavaScript 由于整个流程我是基于 JSPatch 来开发的，所以是需要转为 JS 的脚本。这里我是写了个 node.js 的脚本来实现，转换算法是利用 https://github.com/bang590/JSPatchConvertor 中的开源代码。 模拟器执行修改后的脚本 由于已经有 JSPatch 完整的框架做支撑，这里只需要利用其中的方法 -[JPEngine evaluateScriptWithPath:] 去执行修改后的脚本即可。 当然这种有一定的缺陷，不管是针对苹果审核，还是学习成本，或者是其他意向不到的问题 使用之前还是慎重考虑，可以适当的使用作为部分模块的优化，或者作为学习。 至于后续的步骤就是，写好代码，逻辑，界面，优化…… 性能优化推荐：iOS应用性能调优的25个建议和技巧]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>iOS——App启动优化分析与总结</tag>
        <tag>App启动优化分析与总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS——App整个启动过程分析]]></title>
    <url>%2F2017%2F04%2F28%2FiOS%E2%80%94%E2%80%94App%E6%95%B4%E4%B8%AA%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[你想知道App是怎么显示到我们眼前的吗？你想知道点击Run之后发生了什么吗？你想知道在Main函数之前都还有那些你不知道操作吗？你想知道在Objective-C和Swift混编的时候这么处理AppDelegate吗？你想知道UIApplication都干了撒吗？你想知道怎么优化App启动过程吗？你想知道…..你撒都想知道，那还等什么？ 前言 本文的起因： 前段时间在研究App启动相关内容，还有启动优化相关的东西，网上寻找了一下相关的资料并试着在整理，也自己试着去验证了其中的一些步骤。 公司项目是Objective-C版本的，大部分相关类都已经使用Swift重写了，后期新增的功能都使用的Swift混合开发。但是由于项目庞大又复杂，没有时间也没有精力去完全使用Swift重写。 因为从Swift一出来我就在学习，但是并没有在实际项目中应用到Swift，从1.0到2.0，再到3.0都有了解和学习相关的语法。但是近四年来一直都在使用OC和学习各种技术，以致于Swift其实该忘的也忘得差不多了。网上有句话：Swift从入门到入门。相信你既然都能看到这里，肯定明白其中的原因。所以我又开始重写学了一遍Swift，这一次是Swift3.1开始学，其实Swift4已经出来了，但是和3.1并没有太大的变化。 因为公司每个星期都有一个技术分享，所以我觉得去试一次，希望能把我知道的东西通过口述的方式分享出来。（不过分享的结果很差，毕竟是第一次，所以导致最后连我自己都不知道自己在说什么），同时这也是我在公司内部做的第一次技术分享的主题。 这一次，我希望能彻底从OC转到Swift进行实际开发。 资料地址： iOS——App整个启动过程分析。如果你觉得有用希望能给个star，或者有什么疑问欢迎issuse也欢迎联系我，谢谢！首先申明，本文大部分会以理论和实际分析，不会涉及的太多的代码实战，也不会涉及到太多相关术语的解释，当然部分会给出链接地址，但是力求从下面几个问题并结合实际App的启动过程，让你了解代码之前所干的事情和处理实际开发中遇到的相关问题…. 点击Run之后发生了什么,以致App能够显示在我们的眼前？ Main函数之前苹果还为我们的App做了哪些操作？ OC项目中怎么使用Swift重写AppDelagate？ UIApplication&amp;UIApplicationMain背后做了什么？ 如何优化App启动过程？…… 本文篇幅会比较长，但是如果你能完全掌握里面的内容和应用，足以让你有一个质的提升，当然这是在你不了解上面我提到的情况下。 好了，废话说太多没有什么意思……开干！ 2016 WWDC（苹果提供的启动优化方案） https://developer.apple.com/videos/play/wwdc2016/406/ 优化 App 的启动时间（各个阶段优化与处理） http://ios.jobbole.com/90331/ iOS 程序 main 函数之前发生了什么 http://blog.sunnyxx.com/2014/08/30/objc-pre-main/ 点击 Run 之后发生了什么？（Build类似） http://www.jianshu.com/p/d5cf01424e92 Xcode编译性能优化（各个阶段优化实战与对比） http://blog.csdn.net/qq_25131687/article/details/52194034]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>iOS——App整个启动过程分析</tag>
        <tag>App整个启动过程分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS——CocoaPods私有化组件]]></title>
    <url>%2F2017%2F04%2F25%2FiOS%E2%80%94%E2%80%94CocoaPods%E7%A7%81%E6%9C%89%E5%8C%96%E7%BB%84%E4%BB%B6%2F</url>
    <content type="text"><![CDATA[相信你能看到这里，就一定用过cocoapods，cocoapods是什么呢？怎么用？算了吧，都是本文的范围，这里只介绍怎么将自己的项目组件进行私有化，其实严格来说不算什么私有化，只是，想自己写好的框架放到github或者其他平台上让他支持cocoapods管理。这类文章网上已经随处可见，但是由于打算开始写自己的框架，就记录了一下….. 首推荐 用CocoaPods做iOS程序的依赖管理 深入理解 CocoaPods CocoaPods 都做了什么? 如果你还不知道sm是CocoaPods，或者想对CocoaPods了解更深入一点的，可以先看看上面的推荐 1. 在github创建仓库完成，然后将仓库检出到本地注意创建事项： LICENSE(许可证)文件不可缺少，否则检测spec文件时，会有警告（选择MIT就OK） 不要使用中文，最好和库名字直接对应 其实也可以在其他平台，不一定是github 2. 将写好开源框架/库的Demo或者Example放到Git仓库下，还要将要开源的文件夹也放入到git仓库中（该文件夹在后面会被用到） 3. 项目发布到github后，我们在工程根目录中初始化一个Podspec文件：创建自己项目的Podspec描述文件: pod spec create iCocos 4. 编辑修改iCocos.podspec对应信息 可以直接参照我的内容进行修改 具体内容不介绍 s.name：名称，pod search 搜索的关键词,注意这里一定要和.podspec的名称一样,否则报错 s.version：版本号 s.ios.deployment_target:支持的pod最低版本 s.summary: 简介 s.homepage:项目主页地址 s.license:许可证 s.author:作者 s.social_media_url:社交网址,这里我写的微博默认是Twitter,如果你写Twitter的话,你的podspec发布成功后会@你 s.source:项目的地址 s.source = { :git =&gt; “https://github.com/al1020119/iCocos.git&quot;, :commit =&gt; “68defea” } s.source = { :git =&gt; “https://github.com/al1020119/iCocos.git&quot;, :tag =&gt; 1.0.0 } s.source = { :git =&gt; “https://github.com/al1020119/iCocos.git&quot;, :tag =&gt; s.version } commit =&gt; “68defea” 表示将这个Pod版本与Git仓库中某个commit绑定 tag =&gt; 1.0.0 表示将这个Pod版本与Git仓库中某个版本的comit绑定 tag =&gt; s.version 表示将这个Pod版本与Git仓库中相同版本的comit绑定 s.source_files:需要包含的源文件 s.resources: 资源文件 s.requires_arc: 是否支持ARC s.dependency：依赖库，不能依赖未发布的库，如 s.dependency = ‘AFNetworking’ s.dependency：依赖库，如有多个可以这样写。我这里是托管在github上,所以这里将地址copy过来就行了。 source_files: 核心代码的文件地址。 这里是经常出错的地方！—如果使用的是这种方式来显示核心代码地址而不是下面的模块化的话， 需要将代码文件必须以仓库名命名（也就意味着最多只有两个文件.h和.m文件了，可以有一个.h文件，把它作为头文件就行），这种方式检验podspec文件有点严格，否则就会报错。 source_files:写法及含义建议大家写第一种或者第二种 &quot;YJSettingTableView/* &quot;&quot;YJSettingTableView/YJSettingTableView/*.{h,m}&quot; &quot;YJSettingTableView/**/*.h&quot; “*” 表示匹配所有文件 “*.{h,m}” 表示匹配所有以.h和.m结尾的文件 “**” 表示匹配所有子目录 5. 设置tag号，提交修改（注：只要spec文件被修改，就必须重新执行如下命令）因为cocoapods是依赖tag版本的,所以必须打tag,以后再次更新只需要把你的项目打一个tag然后修改.podspec文件中的版本接着提交到cocoapods官方就可以了,提交命令请看下面 git commit -m “Release 1.0.0” (先提交当前修改) git tag “v1.0.0” (添加tag) //为git打tag, 第一次需要在前面加一个v git push –tags (推送tag到远程) git push origin master (推送到远程到代码仓库) 或许可能有些没有加入的（执行 git add . 就可以） 6. 提交之前先验证.podspec文件是否合法 pod spec lint iCocos.podspec –verbose pod spec lint iCocos.podspec –allow-warnings (忽略警告) 我这边在验证的时候出现了下面错误 LiudeMacBook:iCocos a115$ pod spec lint iCocos.podspec –verbose -&gt; iCocos (1.0.0) - ERROR | license: Sample license type. - ERROR | description: The description is empty. - ERROR | [iOS] unknown: Encountered an unknown error (The `iCocos` pod failed to validate due to 2 errors. [!] The validator for Swift projects uses Swift 3.0 by default, if you are using a different version of swift you can use a `.swift-version` file to set the version for your Pod. For example to use Swift 2.3, run: `echo &quot;2.3&quot; &gt; .swift-version`: - ERROR | license: Sample license type. - ERROR | description: The description is empty. ) during validation. Analyzed 1 podspec. [!] The spec did not pass validation, due to 3 errors. [!] The validator for Swift projects uses Swift 3.0 by default, if you are using a different version of swift you can use a `.swift-version` file to set the version for your Pod. For example to use Swift 2.3, run: `echo &quot;2.3&quot; &gt; .swift-version`. 根据上面提示了三个错误（due to 3 errors），其实是两个，1，3是一个 1. description is empty这里原始description是这样的 `s.description = &lt;&lt;-DESC` `DESC` 需要改成一段属于自己的描述，其他信息类型修改 关于swift-version 这里直接执行echo &quot;2.3&quot; &gt; .swift-version就正常的 但是据需验证又出现如下错误： LiudeMacBook:iCocos a115$ pod spec lint iCocos.podspec -&gt; iCocos (1.0.0) - ERROR | [iOS] unknown: Encountered an unknown error ([!] /usr/bin/git clone https://github.com/al1020119/iCocos.git /var/folders/4z/d12mnyfx7c37vg91t8h0941h0000gp/T/d20171025-8857-1oau87 --template= --single-branch --depth 1 --branch 1.0.0 Cloning into &apos;/var/folders/4z/d12mnyfx7c37vg91t8h0941h0000gp/T/d20171025-8857-1oau87&apos;... warning: Could not find remote branch 1.0.0 to clone. fatal: Remote branch 1.0.0 not found in upstream origin ) during validation. Analyzed 1 podspec. [!] The spec did not pass validation, due to 1 error. 这里其实不是错误，是没有更新处理，前面说了： （注：只要spec文件被修改，就必须重新执行如下命令） git commit -m &quot;Release 1.0.0&quot; (先提交当前修改) git tag 1.0.0 (添加tag) git push --tags (推送tag到远程) git push origin master (推送到远程到代码仓库) 或许可能有些没有加入的（执行 git add . 就可以） 7. 然后再次验证就会成功 LiudeMacBook:iCocos a115$ pod spec lint iCocos.podspec –verbose -&gt; iCocos (1.0.0) Analyzed 1 podspec. iCocos.podspec passed validation. 或许你还可能遇到下面的错误，不过不要慌，直接按照下面操作就可以 先删除tag // 删除本地tag git tag -d 1.0.0 // 删除远程tag git push origin -d tag 1.0.0 修改spec文件（必须修改相应的version和source）重新执行–&gt;设置tag号，提交修改的步骤 8. trunk推送podspec文件podspec文件验证成功，通过trunk推送podspec文件 pod trunk push iCocos.podspec 如果你是第一次，并且没有帐号你会看到下面一段 LiudeMacBook:iCocos a115$ pod trunk push iCocos.podspec [!] You need to register a session first. Usage: $ pod trunk push [PATH] Publish the podspec at `PATH` to make it available to all users of the ‘master’ spec-repo. If `PATH` is not provided, defaults to the current directory. Before pushing the podspec to cocoapods.org, this will perform a local lint of the podspec, including a build of the library. However, it remains *your* responsibility to ensure that the published podspec will actually work for your users. Thus it is recommended that you *first* try to use the podspec to integrate the library into your demo and/or real application. If this is the first time you publish a spec for this pod, you will automatically be registered as the ‘owner’ of this pod. (Note that ‘owner’ in this case implies a person that is allowed to publish new versions and add other ‘owners’, not necessarily the library author.) Options: --allow-warnings Allows push even if there are lint warnings --use-libraries Linter uses static libraries to install the spec --swift-version=VERSION The SWIFT_VERSION that should be used to lint the spec. This takes precedence over a .swift-version file. --skip-import-validation Lint skips validating that the pod can be imported --skip-tests Lint skips building and running tests during validation --silent Show nothing --verbose Show more debugging information --no-ansi Show output without ANSI codes --help Show help banner of specified command 根据上面的提示是告诉你：需要你用邮箱注册一个trunk 9. 我们直接使用终端注册pod trunk register al10201119@163.com &quot;iCocos&quot; --description=&quot;iCocos&quot; 之后会有一封带有验证链接的邮件发送到你输入的邮箱，点击验证后就可以回来终端继续提交操作了。 已经注册过的不需要注册,怎么看自己有没有注册 pod trunk me 10. 发布代码到cocoapodspod trunk push iCocos.podspec --verbose pod trunk push iCocos.podspec --allow-warnings 发布时会验证 Pod 的有效性，如果你在手动验证 Pod 时使用了 –use-libraries 或 –allow-warnings 等修饰符，那么发布的时候也应该使用相同的字段修饰，否则出现相同的报错。 pod trunk push iCocos.podspec –verbose 一段很长的描述，然后你会看到下面的成功提示 11. 如果提交到cocoapods还有可能遇到下面错误：Updating spec repo &apos;master&apos; warning: inexact rename detection was skipped due to too many files. warning: you may want yo set your diff.renameLimit variable to at least 3080 an retry the command ... [!] There was an error pushing a new version to trunk: execution expired 这里有解决方案：http://www.sw33tcode.com/?p=31 git config merge.renameLimit 999999 git config --unset merge.renameLimit 12. 然后开始去搜索我的库了但是发现既然没有，各种排查，最后发现由于延时的问题，不过如果还是不出现的，网上找到了相应的方案： 删除~/Library/Caches/CocoaPods目录下的search_index.json文件 pod setup成功后会生成~/Library/Caches/CocoaPods/search_index.json文件。 终端输入rm ~/Library/Caches/CocoaPods/search_index.json 删除成功后再执行pod search 稍等片刻，然后pod search就会出现你所要搜的类库了。]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>iOS——CocoaPods私有化组件</tag>
        <tag>CocoaPods私有化组件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS——Objective-C与Swift优缺点对比]]></title>
    <url>%2F2017%2F04%2F19%2FiOS%E2%80%94%E2%80%94Objective-C%E4%B8%8ESwift%E4%BC%98%E7%BC%BA%E7%82%B9%E5%AF%B9%E6%AF%94%2F</url>
    <content type="text"><![CDATA[最近有个朋友（同事）在找工作，一起讨论关于面试过程中的面试题，有不少关于Objective-C的技术问题，尤其是关于一些技术的底层实现，当然这个在之前的文章中有整理. 其中有一个问题就是关于Objective-C和Swift区别，与优缺点的对比，由于我刚使用Swift实战开发并不久，过去三年多都都是搞Objective-C，所以对Swift和Objective-C优缺点还真没有多少简介，虽然网上也有不少相关的资料和回答，但是想想这个问题以后再很多打算或者还没有开始使用Swift的公司会被经常问到，所以就花了点时间整理一下………. 简介Swift，苹果于2014年WWDC（苹果开发者大会）发布的新开发语言（已开源），可与Objective-C*共同运行于Mac OS和iOS平台，用于搭建基于苹果平台的应用程序。 动态性Swift 是一种强类型语言。类型静态，也就是说 Swift 的默认类型是非常安全的。 Swift 当中存在有这两个修饰符@objc和@dynamic，此外我们同样还可以访问NSObject。 @objc将您的 Swift API 暴露给 Objective-C 运行时，但是它仍然不能保证编译器会尝试对其进行优化。 如果您真的想使用动态功能的话，就需要使用@dynamic。一旦您使用了@dynamic修饰符之后，就不需要添加@objc了，因为它已经隐含在其中。 Swift比Objective-C有什么优势？ Swift容易阅读，语法和文件结构简易化。 Swift更易于维护，文件分离后结构更清晰。 Swift更加安全，它是类型安全的语言。 Swift代码更少，简洁的语法，可以省去大量冗余代码 Swift速度更快，运算性能更高。 Swift目前存在的缺点 版本不稳定，之前升级Swift3大动刀，苦了好多人，swift4目前还未知 使用人数比例偏低，目前还是OC的天下 社区的开源项目偏少，毕竟OC独大好多年，很多优秀的类库都不支持Swift，不过这种状况正在改变，现在有好多优秀的Swift的开源类库了 公司使用的比例不高，很多公司以稳为主，还是在使用OC开发，很少一些在进行混合开发，更少一些是纯Swift开发。 偶尔开发中遇到的一些问题，很难查找到相关资料，这是一个弊端。 纯Swift的运行时和OC有本质区别，一些OC中运行时的强大功能，在纯Swift中变无效了。 对于不支持Swift的一些第三方类库，如果非得使用，只能混合编程，利用桥接文件实现。 整体总结 String： Swift中String操作已经甩OC三百万条街 泛型： Swift泛用性还是不够强，如果项目不止涉及常用的http啊xmpp啊之类的协议，而是要做一些SIP啊FFMPEG啊之类的干活，那明显是OC成熟得多 Discriminated Union swift里的enum. 是静态语言独有的特性. 安全： 由于swift的strong static type system，编译器可帮你检查出更多问题，而不是在运行时突然boom，还有一个很牛逼的安全特性就是OptionalType。 快速： 静态相对来说语言本身速度更快，swift编译期就能生成vtable，确定具体要调用的方法，比起oc的动态派发自然是更快，当然处理到与oc之间桥接部分，可能不一定比oc快 细节使用区别 在 Swift 中没有了 main.m，@UIApplicationMain 是程序入口 swift不分.h和.m文件 ，一个类只有.swift一个文件，所以整体的文件数量比起OC有一定减少。 swift句尾不需要分号 ，除非你想在一行中写三行代码就加分号隔开。 在 Swift 中，一个类就是用一对 { } 括起的，没有 @implementation 和 @end swift数据类型都会自动判断 ， 只区分变量var 和常量let 强制类型转换格式不同 OC强转：(int)a Swift强转：Int(a) 关于BOOL类型更加严格 ，Swift不再是OC的非0就是真，而是true才是真false才是假 swift的 循环语句中必须加{}就算只有一行代码也必须要加 swift的switch语句后面可以跟各种数据类型了 ，如Int、字符串都行，并且里面不用写break（OC好像不能字符串） swift if后的括号可以省略: if a&gt;b {}，而OC里 if后面必须写括号。 swift打印 用print(&quot;&quot;) 打印变量时可以 print(&quot;\(value)&quot;)，不用像OC那样记很多%@，d%等。 Swift3的【Any】可以代表任何类型的值，无论是类、枚举、结构体还是任何其他Swift类型，这个对应OC中的【id】类型。 在 OC 中 alloc / init 对应( ) 在 OC 中 alloc / initWithXXX 对应 (XXX: ) 在 OC 中的类函数调用，在 Swift 中，直接使用 . 在 Swift 中，绝大多数可以省略 self.，建议一般不写，可以提高对语境的理解（闭包时会体会到） 在 OC 中的 枚举类型使用 UIButtonTypeContactAdd，而 Swift 中分开了，操作热键：回车-&gt; 向右 -&gt;. Swift 中，枚举类型的前缀可以省略，如：.ContactAdd，但是：很多时候没有智能提示 监听方法，直接使用字符串引起 循环引用问题Objective-C中循环引用也是遇到比较多的，一不小心就会导致循环引用，甚至导致内存问题 Swift [weak self] self是可选项，如果self已经被释放，则为nil [unowned self] self不是可选项，如果self已经被释放，则出现野指针访问 Objective-C __weak typeof(self) weakSelf; 如果self已经被释放，则为nil __unsafe_unretained typeof(self) weakSelf; 如果self已经被释放，则出现野指针访问 参考：推荐几个Objective-C的框架 FBRetainCycleDetector PLeakSniffer MLeaksFinder]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>iOS——Objective-C与Swift优缺点对比</tag>
        <tag>Objective-C与Swift优缺点对比</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Xcode Server完整教程]]></title>
    <url>%2F2017%2F04%2F11%2FXcode-Server%E5%AE%8C%E6%95%B4%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[由于公司项目的需要，以及同事和测试人员的反馈，我们的打包服务器挂了，貌似之前经常挂（因为我桌旁的就是装有打包服务器的垃圾桶，在领用Mac之前，我都是用那个开发的）。其实之前我也一直想找个机会搞一下Xcode Server，记得还专门写过一篇总结，但是由于工作的原因，也由于帐号的原因就没去搞了，这一次刚好有这个机会，于是我专门放下手上所有的活，打算把这个东西弄好来……… 如果你对CI和CD已经足够了解，仅仅想知道怎么搞好Xcode Server，那么你可以直接从准备开始… 背景：之前项目使用的是Jenkins，但是由于Jenkins的各种问题，配置，后面被换到了Xcode Server（当然除了他们我还可以选择Fantlane）。自从老大走了以后Xcode Server经常挂，导致测试人员不发打包测试，从而影响整体进度。而且这一次好像是彻底挂了，根本都进不去了….. 常识：这里简单介绍一下Xcode Server和常用的持续交付与继承工具（CI&amp;CD）。 xcodebuild - 由Apple开发，主要用于Xcode的构建和测试，有时可能难以想起，但可配置程度很高。 fastlane - 实际上并不是一个工具，而是一组可用于构建、测试、上传至iTunes Connect、供应配置文件管理、屏幕截图创建、dsym上传/下载至主要崩溃报告平台的一系列工具。 xctool和其他 - “其他”是指诸如nomad tools等工具，这些工具或者被弃用，或者逐渐缺少支持，或者即将被废弃。尽管Facebook在使用某种工具，但并不意味着这个工具依然可以得到妥善的维护。 服务器方面主要的选择包括： TravisCI/CircleCI - 托管式服务器，可免费用于开源项目，可随处访问，极为强大。相比Jenkins可配置的选项较少，仅支持与Github集成。用于私有代码库的价格高昂。 Xcode Server - 能与Xcode高度集成，实际上也是唯一可用于Xcode的服务器，由Apple开发，最有可能只需要少量配置即可投入使用。 Jenkins - CI服务器领域曾经的王者，有大量插件可用，可与各种其他产品集成，需要一定的配置和维护，但是非常强大。 需求一般项目到了一定程度，或者公司和成员到了一定规模都会考虑自动化，当然也包括那些喜欢导致或者懒惰想一键打包发布的程序员。因此结合公司，项目或者个人的情况我们可以将整个过程进行拆分。 构建并签署我们的所有不同特性的应用； 将我们的应用商店首选项上传至iTunes Connect； 将IPA、dSYM，以及变更日志上传至HockeyApp； 针对发布和开发分支持续不断地运行单元测试和UI测试； 构建每次合并请求（MR）并汇报测试结果； 进行持续不断地构建和签署，以确保没有引入新的问题。 由于时间的原因，也由于目前需求的原因，我们目前只需要进行打包发布相关操作，至于，分析，测试… 看完上面之后，相信你知道我接下来要说的是什么， 准备OS X工具下载 下载 OS X Server（付费开发者免费使用） 下载 Xcode 这里就不多废话了，直接到苹果商店搜索就可以 帐号与代码仓库地址 开发者帐号（相关证书与描述文件） git源代码仓库地址 装备完了上面所需要的东西之后我们就可以开始配置和使用OS X Server了 配置Xcode打开OS X Server，根据提示点击一步一步操作即可 1 选择服务器主机：一般选自身，也可以指定IP 2 选择Xcode服务，开启服务（右上角） 老版配置页面 这里需要注意，之前的配置方案是直接有 权限和版本号：自己考虑，我这里默认设置不改了； 开发团队：使用自己的apple ID添加； 开发设备：需要用设备连接服务器后才能搜索到，初次连接，可能还需要在Status选项里面点击【Add to Teams】（比如设备不是你上面apple ID 的就需要） 查看Bot：会打开网页（Bot管理页面，默认127.0.01）,可以直接静态分析测试打包等，当然需要后面配置成功才能使用。 新版配置页面 最新版本不知道为什么没有了对应的信息，根据提示信息应该的Xcode兼容性问题（因为：点击选择Xcode，选择指定Xcode之后提示是说版本不兼容） 此时界面只有一个选择Xcode的按钮，点击进去就会直接进入到Server &amp; Bots 你会看到下面的界面 这里也可以直接在Xcode Preferences中打开， 解锁后点击OFF&amp;ON进行开启服务，然后会提示Select Integration User（选择集成用户） 这里我们一般都会新建一个用户，而不是直接使用服务器账户，然后填写对应的信息（帐号密码），这里要记住后面会有用 点击创建之后就会执行一系列操作，对Xcode进行配置 具体执行操作有 Saveing version information Enableing developer model Configuring SSL certificates Starting Redis initializing database Starting API Server Starting Apache Starting control daemon Starting Builder Upgrading Xcode Server Data Saveing version information 完成后你会看到，一个提示新用户登录的提示，为了更好的进行测试和验证，我们都会先登录用户 直接登录就可以，并且一步一步确认进入到子账户，子账户会提示： 这里先不用管，切回到服务器账户就可以，这个时候，你就可以看到刚刚创建的账户已经出现了，并且处于登录状态，我们可以点击直接切换到对应账户，或者选择和重新创建多个账户，控制超时时间。 登录账户在Xcode Preferences中选择Account，点击右下角添加账户，分别登录Apple ID，GitHub，Xcode Server Apple ID：对应开发者帐号，要和本地证书与描述文件对应 GitHub 注意:Source Control 需要打开才能使用！具体下面的各种自动自己考虑勾选； Xcode Server（如果使用Xcode 9.1最新版本，这一步已经不需要了，也就是说Xcode 9.1已经自带了Bots功能） 注意：Accounts 添加 Servers，一般默认自己主机，也可以选择指定IP地址，按服务器配置时对应选择；需要填写用户名，密码，自己搞定； 完成帐号登录以后，可以适当下载更新一下证书文件 配置Xcode Server（如果使用Xcode 9.1最新版本，这一步已经不需要了，也就是说Xcode 9.1已经自带了Bots功能）再次打开Xcode Server，进入Xcode服务页面，点击打开Xcode，会直接打开Xcode Preferences中的Server &amp; Bots并开启对应账户，这里和前面操作一样，如果没有就根据提示直接确定或者输入对应的账号信息就可以（如果没有登录帐号的话） 创建Bots 注意：自己的项目必须是个git文件夹，比如github上clone下来的项目，或者是本地服务器git来的，总之需要git文件夹；（官方文档使用Xcode的Source Control 方式搞定，具体可以看官方指南。我直接用了第三方软件SourceTree管理的） 这里由于我们是直接使用GitLab的，所以我直接从GitLab克隆了一分项目源码到本地文件夹。 打开Xcode，保证项目编译正常的情况下创建Bots（这里可以直接点击Products-&gt;Create Bots） 1 Bot命名与服务器选择，基本默认不改；2 选择项目内容，后面的master，可选其他分支，比如develop；3 Bot 操作配置Scheme：默认项目本身Actions：1分析；2单元测试（测试项目没开单元测试所有没的选）；3打包；Cleaning：可以选择，因为是自己的电脑做服务器的，所以选择了一周清理一次；Configuation：项目配置；4 环境变量？没用过，sorry；5 添加脚本，比如完成后发送个邮件通知；6 create！注意 需要一次验证 git 的用户名和密码！不是之前设置的用户名与密码。7 成功；各种信息以及完成的打包等；通过IP地址也可以实时整合并下载； 当你看到一个类型这样的界面，说明已经配置并且创建Bots完成，具体是否正确，需要进行打包验证或者根据日志进行查找具体问题 注意： 如果在使用Xcode Server对应的分析，测试功能，还需要额外进行一些配置，由于时间的问题，这里我们暂时只使用打包功能。 打包配置需要选择InHouse 打包验证输入网址或者点击Xcode Perferences中的Account，点击Xcode Server对应的地址链接，进入网址（这里是icocos.local对应ip是10.0.6.7），这里可以在当前服务器进行操作，也可以在配置Xcode的时候创建的那个帐号进行操作，同意可以在内网的其他端系统或者浏览器进行操作 然后就会有一打包的进度…… 错误总结错误问题一： 问题原因： 因为没有登录对应的Git帐号或者没有打包对应项目的workspace 错误问题二： 问题原因： 同样是因为没有登录对应的Git帐号 错误问题三： 问题原因： 同样是因为没有登录对应的Git帐号 错误问题四： 问题原因： 这里是由于创建Bots进行配置的时候，只需要打包功能就行了，那些什么分析,测试不需要,还有打包配置要选InHouse，不然打不了的 错误问题五： 问题原因： 由于升级了Xcode 9.1，和对应的Xcode Server不兼容，这里需要输入xcsd密码（也不知道是sm鬼），根据2017@swift大会的朋友经验： 备份/Library/Developer/XcodeServer，然后删除XcodeServer文件夹内容，重新前面的步骤就正常了。 错误问题六： 问题原因： 这其实不是sm错误 ，只是一个安全警告，解决方案：点击显示详细信息-&gt;继续访问网址，就可以了 错误问题七： 问题原因： 此问题困扰了很久，由于加入了GZIP库，但是…. 错误问题八： 问题原因： 此问题困扰了很久，由于加入了GZIP库，但是…. 其他问题 这些问题目前没有找到好的解决方案，根据提示查找发现，证书，前面都没有问题…… 总结一最后根据最新经验总结，最新版本Xcode 9.1配置其实非常简单了 配置Xcode Perferences中Server &amp; Bots 创建并登录帐号 Xcode Perferences登录git，app，server帐号 Clone远程Git源代码，并打开保证正常编译 Create Bots并配置 重启相关服务，Server，Bots，Xcode，开始打包 注意： 关于代码提交，不管使用Xcode还是其他方式，是没有关系的，Xcode服务指向的是master或者其他分支的服务器资源；关于打包，项目配置里就做好证书等配置文件的选择；内测的ipa包，可以结合蒲公英等平台作分发也是很方便的；考虑是不是可以搞个脚本上传。 打包过程中 通过 Xcode 中的 Integrate 查看进度我们不难发现，其实整个过程和我们手动打包的过程是一样的 打包完成 但是这样可能享受不到OS X Server那样多而且好用的服务，不过如果只是需要简单的进行打包，配合测试进行验证，简单的发布引用还是可以满足的…….. 推荐想了解更过关于Xcode Server配置与使用的，可以参考与学习下面的文章，笔者也是从下面的文章中一点一点学习的，并结合了朋友指点进行实战。 3个官方文档： 官方配置教程 苹果开发指南 OS X Server 帮助 简书教程： OS X Server 之 Xcode服务 手动打包流程 iOS App打包上架超详细流程]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>Xcode Server</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS——重整iOS技术（Fastlane完整打包流程）]]></title>
    <url>%2F2017%2F04%2F09%2FiOS%E2%80%94%E2%80%94%E9%87%8D%E6%95%B4iOS%E6%8A%80%E6%9C%AF%EF%BC%88Fastlane%E5%AE%8C%E6%95%B4%E6%89%93%E5%8C%85%E6%B5%81%E7%A8%8B%EF%BC%89%2F</url>
    <content type="text"><![CDATA[自从去年中旬电脑被我整挂了之后，大部分插件和脚本配置都失效了，重新安装也试了好几次，最近上班要上线了，不得不重新整理和配置一下，还好没有遇到什么坑…… 前言 项目即将进入阶段，每次都要点击那么多，频繁的打包脑子都要晕了，习惯了之前一行命令就搞定的我，只能花一个晚上加班整回之前的Fastlane。 本文采用的方案是：Fastlane + 蒲公英 + ……。 关于具体发布状态可以在这里（app-store, package, ad-hoc, enterprise, development）改，当然后面会结合App store发布最近本文内容，同时支持jenkins或者其他持续集成系统. Fastlane安装Fastlane是一套使用Ruby写的自动化工具集，用于iOS和Android的自动化打包、发布等工作，可以节省大量的时间。 安装过程如下： 1.检查Ruby版本，需要2.0及以上版本。在终端输入以下命令确认：ruby -v 需要注意的是需要将gem的source改为https://gems.ruby-china.org/。如何检查?在终端输入以下命令: gem sources 结果应为： *** CURRENT SOURCES *** https://gems.ruby-china.org/ 2.检查Xcode命令行工具是否安装。在终端输入以下命令：xcode-select --install 如果没有安装会进行安装。如果已经安装了则会提示： xcode-select: error: command line tools are already installed, use &quot;Software Update&quot; to install updates 3.安装Fastlanesudo gem install fastlane --verbose 如果出现以下错误： ERROR: While executing gem ... (Errno::EPERM) Operation not permitted - /usr/bin/rougify 则输入以下命令： sudo gem install -n /usr/local/bin fastlane 4.检查Fastlane是否正确安装。输入以下命令：fastlane --version 可以看到Fastlane版本信息，我的是2.85.0。 蒲公英的Fastlane插件安装打开终端，进入你的项目工程的根目录，输入以下命令： fastlane add_plugin pgyer 出现 Plugin &apos;fastlane-plugin-pgyer&apos; was added to &apos;./fastlane/Pluginfile&apos; It looks like fastlane plugins are not yet set up for this project. fastlane will create a new Gemfile at path &apos;Gemfile&apos; This change is necessary for fastlane plugins to work Should fastlane modify the Gemfile at path &apos;Gemfile&apos; for you? (y/n) 输入y按回车，出现 Installing plugin dependencies... Successfully installed plugins 便是安装成功了。 Fastlane配置1.打开终端，进入你的项目工程的根目录，输入以下命令：fastlane init 中间会让你输入苹果开发者账号的账号和密码，之后会在你项目工程的目录下生成一个fastlane文件夹，里面有Fastlane的配置文件，一个是Appfile文件，一个是Fastfile文件(如果要上传AppStore的话还有Deliverfile文件)。Appfile保存苹果开发者的相关信息、项目的相关信息等。Fastfile是运行脚本。 2.编辑Fastfile文件有时候一天需要打好几个包，为了区分，我们这里实现一个递增build号的功能。 (1)修改项目工程配置 修改Build Settings中的Versioning配置，Current Project Version随便填一个，Versioning System选择Apple Generic。 修改Info.plist File路径 (2)定义一个递增build号的函数，添加到Fastfile中def updateProjectBuildNumber currentTime = Time.new.strftime(&quot;%Y%m%d&quot;) build = get_build_number() if build.include?&quot;#{currentTime}.&quot; # =&gt; 为当天版本 计算迭代版本号 lastStr = build[build.length-2..build.length-1] lastNum = lastStr.to_i lastNum = lastNum + 1 lastStr = lastNum.to_s if lastNum &lt; 10 lastStr = lastStr.insert(0,&quot;0&quot;) end build = &quot;#{currentTime}.#{lastStr}&quot; else # =&gt; 非当天版本 build 号重置 build = &quot;#{currentTime}.01&quot; end puts(&quot;*************| 更新build #{build} |*************&quot;) # =&gt; 更改项目 build 号 increment_build_number( build_number: &quot;#{build}&quot; ) end 实现自动打包的完整Fastfile如下：可以直接拷贝修改# 定义fastlane版本号 ---- 修改 fastlane_version &quot;2.85.0&quot; # 定义打包平台 default_platform :ios def updateProjectBuildNumber currentTime = Time.new.strftime(&quot;%Y%m%d&quot;) build = get_build_number() if build.include?&quot;#{currentTime}.&quot; # =&gt; 为当天版本 计算迭代版本号 lastStr = build[build.length-2..build.length-1] lastNum = lastStr.to_i lastNum = lastNum + 1 lastStr = lastNum.to_s if lastNum &lt; 10 lastStr = lastStr.insert(0,&quot;0&quot;) end build = &quot;#{currentTime}.#{lastStr}&quot; else # =&gt; 非当天版本 build 号重置 build = &quot;#{currentTime}.01&quot; end puts(&quot;*************| 更新build #{build} |*************&quot;) # =&gt; 更改项目 build 号 increment_build_number( build_number: &quot;#{build}&quot; ) end #指定项目的scheme名称 ---- 修改 scheme=&quot;Fiction_iOS&quot; #蒲公英api_key和user_key ---- 修改 api_key=&quot;264c007c340157969a5e4da77637e60f&quot; user_key=&quot;3fdffa475f545097333473b980765ce1&quot; # 任务脚本 platform :ios do lane :development_build do|options| branch = options[:branch] puts &quot;开始打development ipa&quot; updateProjectBuildNumber #更改项目build号 # 开始打包 gym( #输出的ipa名称 output_name:&quot;#{scheme}_#{get_build_number()}&quot;, # 是否清空以前的编译信息 true：是 clean:true, # 指定打包方式，Release 或者 Debug configuration:&quot;Release&quot;, # 指定打包所使用的输出方式，目前支持app-store, package, ad-hoc, enterprise, development export_method:&quot;development&quot;, # 指定输出文件夹 output_directory:&quot;./fastlane/build&quot;, ) puts &quot;开始上传蒲公英&quot; # 开始上传蒲公英 pgyer(api_key: &quot;#{api_key}&quot;, user_key: &quot;#{user_key}&quot;) end end 注意：蒲公英的 api_key 和 user_key，开发者在自己账号下的 账号设置-API信息 中可以找到。打其它类型的包的方法与development类似，可自定义一个新的lane实现。 打包发布 在终端输入 fastlane development_build 便会进行自动打包并上传蒲公英了。 下面以执行流程 下面以执行结果 再来三张：入口，选项与结果]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>iOS——重整iOS技术（Fastlane完整打包流程）</tag>
        <tag>重整iOS技术（Fastlane完整打包流程）</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS游戏——渠道越狱打包流程]]></title>
    <url>%2F2017%2F04%2F09%2FiOS%E6%B8%B8%E6%88%8F%E2%80%94%E2%80%94%E6%B8%A0%E9%81%93%E8%B6%8A%E7%8B%B1%E6%89%93%E5%8C%85%E6%B5%81%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[关于iOS普通ipa打包这里就不说了，只要注意苹果的一些协议条款，基本上步骤都是傻瓜式的，如果不知道如何打包iOS普通ipa包，请出门左转，度娘在哪里等你！ 那么，如何将app打包成越狱包，越狱iphone在cydia源上下载安装，这样的需求在互联网游戏行业是非常普遍的，尤其是各种马甲包横行的时代！ 关于越狱相关内容，请看这里 iOS越狱解除工具 ios越狱原理详解 常见deb包打包方式1、第一种方式：自定义目录下 如desktop创建文件夹，起名test，test内新建文件夹Payload把xcode Build好的.app（Products目录中）拷到Payload目录中打开终端，cd指令 到test文件目录下，执行 zip -r “xxx.ipa” * 注意里面的空格：(zip -r “xxx.ipa”[空格]* ) ipa包就打好了，可以安装到越狱手机上试试看 注意事项：Debug或Release的Any iOS SDK都设置为正式发布证书，经测试该越狱包可正常接收推送。 2、第二种方式：在xcode中点击Product-&gt;Archive,完成后会弹出Organizer，点右边的Distribute，弹出一个向导对话框，点击“Export as Xcode Archive”，选择位置，会在那个位置生成后缀名是.xcarchive的文件右键“显示包内容”-&gt;”Products”-&gt;”applications” 然后找到那个应用程序， 将其拖到iTunes里面，在itunes的【应用程序】里找到这个文件，然后右键“在Finder 中显示”，便可找到ipa文件了…. ipa包就打好了，可以安装到越狱手机上试试看 最近发现了一种更好的方式致谢： 胖梁的技术笔记 1.修改编译选项重新打包在工程的Build Settings -&gt; Code Signing -&gt; Code Signing Identity选项, 将 Debug 和 Release 下的 Any iOS SDK都设置为 Don’t Code Sign然后在重新Archive 2.准备目录创建一个目录用来打包,如tmp,tmp下建DEBIAN和Applications两个目录, DEBIAN下建一个文本文件control tmp目录结构如下: -DEBIAN ---control -Applications control文件就是打包时的配置文件,它也会作为deb包的配置被打包到包中, 文件例子: Package: com.sharedream.game Name: 游戏测试 Version: 0.1-1 Description: 游戏测试游戏,开发中... Section: 游戏 Depends: firmware (&gt;= 4.3) Priority: optional Architecture: iphoneos-arm Author: liangwei &lt;http://weibo.com/iamliangwei&gt; Homepage: http://weibo.com/iamliangwei Icon: file:///Applications/game.app/Icon.png Maintainer: liangwei &lt;http://weibo.com/iamliangwei&gt; 然后将xcode打包出来的.app文件整个拷贝到Applications目录下, 结构如下: -DEBIAN ---control -Applications ---game.app 3.打包退出至tmp的上层目录 dpkg-deb -b tmp game.deb 看到如下几行就是打包完成了 warning, `com.sull.sample/DEBIAN/control&apos; contains user-defined field `Name&apos; warning, `com.sull.sample/DEBIAN/control&apos; contains user-defined field `Author&apos; warning, `com.sull.sample/DEBIAN/control&apos; contains user-defined field `Sponsor&apos; dpkg-deb: ignoring 3 warnings about the control file(s) 拷贝到cydia源中, 重新扫描包生成Packages列表文件, 并压缩成Packages.bz2就可以啦 dpkg-scanpackages -m debs &gt;Packages bzip2 -zkf Packages contains ununderstood data member data.tar.xz” 的安装错误 是因为自从1.17.0版本的dpkg-deb开始, 默认使用xz格式来压缩data.tar文件但是,cydia在ios提供的dpkg是1.14版本, 还没有支持xz这种压缩格式所以我们需要设置”-Zgzip”参数给dpkg-deb 进行打包, 类似命令: dpkg-deb -Zgzip -b tmp game.deb 关于iOS普通包打包可以参考这里： https://blog.csdn.net/zhanghow/article/details/60603461?utm_source=blogxgwz8]]></content>
      <categories>
        <category>Games</category>
      </categories>
      <tags>
        <tag>iOS游戏——渠道越狱打包流程</tag>
        <tag>游戏</tag>
        <tag>越狱</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[iOS——自动打包上传]]></title>
    <url>%2F2017%2F04%2F04%2FiOS%E2%80%94%E2%80%94%E8%87%AA%E5%8A%A8%E6%89%93%E5%8C%85%E4%B8%8A%E4%BC%A0%2F</url>
    <content type="text"><![CDATA[为了不去每次都做这些繁琐的操作，我们只能做一个懒得程序狗，也就是将整个过程进行脚本化…… 之前写过一篇关于自动打包上传代码的教程，最近又开始需要上线APP，发现新版的Mac对之前的教程不支持了，就换了一种方式，这次试用fir. http://al1020119.github.io/blog/2016/12/26/ios-daobao/ 工作中一般两种情况，1.打包Ad-Hoc给测试人员，或者内部人员测试 2.打包product发布到App Store 所以，为了不去每次都做这些繁琐的操作，我们只能做一个懒得程序狗，也就是将整个过程进行脚本化 大概的步骤是写个bash脚本，执行自动打包iOS版本，到指定的目录 （有条件的公司，可以自己搭个小服务器，这样谁都可以随时随地的打包） 将打包好的文件上传到fir.im (当然上传到自己公司的服务器或者任何地方都行，只是fir.im我一直用，觉得比较方便) 开发一个内部使用的类似APPStore，上面放着自己公司的所有APP,每次有更新的时候，测试童鞋直接通过这个自己下载新APP就可以了 主要命令编译workspacexcodebuild -workspace workspacename -scheme schemename -configuration [-configuration configurationname] clean build SYMROOT=(SYMROOT) 编译projectxcodebuild -target targetname -configuration [-configuration configurationname] clean build SYMROOT=(SYMROOT) 查看配置信息xcodebuild -list xcrun打包ipaxcrun -sdk iphoneos PackageApplication -v projectName.app -o ipaName.ipa 其他在终端输入：xcodebuild –help 或 –h查看具体的选项 显示xcodebuildversion：xcodebuild –version 显示当前系统安装的sdk：xcodebuild –showsdks 显示当前目录下project Information：xcodebuild –list xcodebuild&amp;xcrunxcodebuild-&gt;Build xcrun-&gt;Api 终端查看对应版本iCocosdeiMac:115科技 iCocos$ xcrun --version xcrun version 31. iCocosdeiMac:115科技 iCocos$ xcodebuild -version Xcode 8.2.1 Build version 8C1002 xcodebuild 是苹果提供的打包项目或者工程的命令需要在包含 name.xcodeproj 的目录下执行 xcodebuild 命令，且如果该目录下有多个 projects，那么需要使用 -project 指定需要 build 的项目。 在不指定 build 的 target 的时候，默认情况下会 build project 下的第一个 target 当 build workspace 时，需要同时指定 -workspace 和 -scheme 参数，scheme 参数控制了哪些 targets 会被 build 以及以怎样的方式 build。 有一些诸如 -list, -showBuildSettings, -showsdks 的参数可以查看项目或者工程的信息，不会对 build action 造成任何影响，放心使用。 使用xcodebuild和xcrun打包签名我这里就使用公司项目作为测试，方便查找问题和后续打包上传 1. 终端输入xcodebuild -project T 115科技.xcodeproj -target 115科技 -configuration Release xcodebuild -project T 曹理鹏(iCocos)-梦工厂.xcodeproj -target 曹理鹏(iCocos)-梦工厂 -configuration Debug 这是骚等所有走完之后就能看到对应的信息，表示build成功Signing Identity: &quot;iPhone Developer: xxx(59xxxxxx)&quot; Provisioning Profile: &quot;iOS Team Provisioning Profile: *&quot; 且在该目录下会多出一个 build 目录，该目录下有 Release-iphoneos 和 曹理鹏(iCocos)-梦工厂.build 文件，根据我们 build -configuration 配置的参数不同，Release-iphoneos 的文件名会不同。在 Release-iphoneos 文件夹下，有我们需要的曹理鹏(iCocos)-梦工厂.app文件，但是要安装到真机上，我们需要将该文件导出为ipa文件，这里使用 xcrun 命令。 xcrun -sdk iphoneos -v PackageApplication ./build/Release-iphoneos/曹理鹏(iCocos)-梦工厂.app -o ~/Desktop/曹理鹏(iCocos)-梦工厂.ipa 这个时候桌面上就会出现一个曹理鹏(iCocos)-梦工厂.ipa文件，这就是我们平时Archive之后的问题，也正是我们所需要的ipa包 但是xcodebuild期间我出现了一个这样的错误ld: library not found for -lAFNetworking clang: error: linker command failed with exit code 1 (use -v to see invocation) ** BUILD FAILED ** The following build commands failed: Ld build/曹理鹏(iCocos)-梦工厂.build/Release-iphoneos/曹理鹏(iCocos)-梦工厂.build/Objects-normal/armv7/曹理鹏(iCocos)-梦工厂 normal armv7 Ld build/曹理鹏(iCocos)-梦工厂.build/Release-iphoneos/曹理鹏(iCocos)-梦工厂.build/Objects-normal/arm64/曹理鹏(iCocos)-梦工厂 normal arm64 (2 failures) 貌似是linker command failed with exit code 1经典错误，但是然并卵，于是看了一下pingpong从零开始写个自动打包IPA脚本中的build方式，也有类似的问题。多谢pingpong帮我解决了这个问题。iCocosdeiMac ios (develop) $ xcodebuild 2016-05-02 13:05:04.623 xcodebuild[1015:16272] [MT] PluginLoading: Required plug-in compatibility UUID ACA8656B-FEA8-4B6D-8E4A-93F4C95C362C for plug-in at path &apos;~/Library/Application Support/Developer/Shared/Xcode/Plug-ins/XcodeColors.xcplugin&apos; not present in DVTPlugInCompatibilityUUIDs 2016-05-02 13:05:04.625 xcodebuild[1015:16272] [MT] PluginLoading: Required plug-in compatibility UUID ACA8656B-FEA8-4B6D-8E4A-93F4C95C362C for plug-in at path &apos;~/Library/Application Support/Developer/Shared/Xcode/Plug-ins/OMColorSense.xcplugin&apos; not present in DVTPlugInCompatibilityUUIDs === BUILD TARGET xxx OF PROJECT xxx WITH THE DEFAULT CONFIGURATION (Release) === Check dependencies Write auxiliary files write-file /Users/iCocos/Documents/code/xxx/ios/build/xxx.build/Release-iphoneos/xxx.build/xxx.hmap write-file /Users/iCocos/Documents/code/xxx/ios/build/xxx.build/Release-iphoneos/xxx.build/xxx-own-target-headers.hmap write-file /Users/iCocos/Documents/code/xxx/ios/build/xxx.build/Release-iphoneos/xxx.build/Script-492B764475E022A63FB67F55.sh 解决方案是：执行xcodebuild需要指定你所需要对应的workspace和schemexcodebuild -workspace /Users/iCocos/Desktop/MBA/最新Git源代码/曹理鹏(iCocos)-梦工厂/曹理鹏(iCocos)-梦工厂.xcworkspace -scheme 曹理鹏(iCocos)-梦工厂 执行前，先查看下-list,这个可以知道xcodebuild命令下对应的参数需要填写的内容iCocosdeiMac ios (develop) $ xcodebuild -list 2016-05-02 15:24:26.656 xcodebuild[16535:154176] [MT] PluginLoading: Required plug-in compatibility UUID ACA8656B-FEA8-4B6D-8E4A-93F4C95C362C for plug-in at path &apos;~/Library/Application Support/Developer/Shared/Xcode/Plug-ins/XcodeColors.xcplugin&apos; not present in DVTPlugInCompatibilityUUIDs 2016-05-02 15:24:26.661 xcodebuild[16535:154176] [MT] PluginLoading: Required plug-in compatibility UUID ACA8656B-FEA8-4B6D-8E4A-93F4C95C362C for plug-in at path &apos;~/Library/Application Support/Developer/Shared/Xcode/Plug-ins/OMColorSense.xcplugin&apos; not present in DVTPlugInCompatibilityUUIDs Information about project &quot;xxx&quot;: Targets: xxx xxxTests Build Configurations: Debug Release If no build configuration is specified and -scheme is not passed then &quot;Release&quot; is used. Schemes: xxx 然后，成功了,如下：Entitlements: { &quot;application-identifier&quot; = &quot;L64TE3S9T9.com.曹理鹏(iCocos)-梦工厂.shaoshang&quot;; &quot;aps-environment&quot; = development; &quot;com.apple.developer.pass-type-identifiers&quot; = ( &quot;L64TE3S9T9.*&quot; ); &quot;com.apple.developer.team-identifier&quot; = L64TE3S9T9; &quot;get-task-allow&quot; = 1; } builtin-productPackagingUtility -entitlements -format xml -o /Users/iCocos/Library/Developer/Xcode/DerivedData/曹理鹏(iCocos)-梦工厂-aqiwhxodbfeztuebgnzgbgicurgl/Build/Intermediates/曹理鹏(iCocos)-梦工厂.build/Debug-iphoneos/曹理鹏(iCocos)-梦工厂.build/曹理鹏(iCocos)-梦工厂.app.xcent CodeSign /Users/iCocos/Library/Developer/Xcode/DerivedData/曹理鹏(iCocos)-梦工厂-aqiwhxodbfeztuebgnzgbgicurgl/Build/Products/Debug-iphoneos/曹理鹏(iCocos)-梦工厂.app cd /Users/iCocos/Desktop/MBA/最新Git源代码/曹理鹏(iCocos)-梦工厂 export CODESIGN_ALLOCATE=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/codesign_allocate export PATH=&quot;/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/usr/bin:/Users/iCocos/.rvm/gems/ruby-2.3.0/bin:/Users/iCocos/.rvm/gems/ruby-2.3.0@global/bin:/Users/iCocos/.rvm/rubies/ruby-2.3.0/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/iCocos/.rvm/bin:/usr/local/mysql/bin/mysql&quot; Signing Identity: &quot;iPhone Developer: Songbai He (73N6HPPJDP)&quot; Provisioning Profile: &quot;iOS Team Provisioning Profile: com.曹理鹏(iCocos)-梦工厂.shaoshang&quot; (34c1d23b-ade5-4d0f-9329-7b16009b30c2) /usr/bin/codesign --force --sign F8BAED0C84DB84AAA84769FED9FEAA9E80825C29 --entitlements /Users/iCocos/Library/Developer/Xcode/DerivedData/曹理鹏(iCocos)-梦工厂-aqiwhxodbfeztuebgnzgbgicurgl/Build/Intermediates/曹理鹏(iCocos)-梦工厂.build/Debug-iphoneos/曹理鹏(iCocos)-梦工厂.build/曹理鹏(iCocos)-梦工厂.app.xcent --timestamp=none /Users/iCocos/Library/Developer/Xcode/DerivedData/曹理鹏(iCocos)-梦工厂-aqiwhxodbfeztuebgnzgbgicurgl/Build/Products/Debug-iphoneos/曹理鹏(iCocos)-梦工厂.app Validate /Users/iCocos/Library/Developer/Xcode/DerivedData/曹理鹏(iCocos)-梦工厂-aqiwhxodbfeztuebgnzgbgicurgl/Build/Products/Debug-iphoneos/曹理鹏(iCocos)-梦工厂.app cd /Users/iCocos/Desktop/MBA/最新Git源代码/曹理鹏(iCocos)-梦工厂 export PATH=&quot;/Applications/Xcode.app/Contents/Developer/Platforms/iPhoneOS.platform/Developer/usr/bin:/Applications/Xcode.app/Contents/Developer/usr/bin:/Users/iCocos/.rvm/gems/ruby-2.3.0/bin:/Users/iCocos/.rvm/gems/ruby-2.3.0@global/bin:/Users/iCocos/.rvm/rubies/ruby-2.3.0/bin:/usr/local/bin:/usr/bin:/bin:/usr/sbin:/sbin:/Users/iCocos/.rvm/bin:/usr/local/mysql/bin/mysql&quot; export PRODUCT_TYPE=com.apple.product-type.application builtin-validationUtility /Users/iCocos/Library/Developer/Xcode/DerivedData/曹理鹏(iCocos)-梦工厂-aqiwhxodbfeztuebgnzgbgicurgl/Build/Products/Debug-iphoneos/曹理鹏(iCocos)-梦工厂.app ** BUILD SUCCEEDED ** 下面就是验证你全栈的时候到了，其实也没有那么难，就是一点脚本而已先git 指令，pull到最新的分支# git update git checkout $BRANCHNAME if [ $? -ne 0 ]; then exit 1 fi git pull #pod update --verbose --no-repo-update if [ $? -ne 0 ]; then exit 1 fi 成功之后你同意会看到提示信息logout Saving session... ...copying shared history... ...saving history...truncating history files... ...completed. Deleting expired sessions...118 completed. xcodebuild进行编译xcodebuild \ -workspace $SORCEPATH/曹理鹏(iCocos)-梦工厂.xcworkspace \ -scheme $SCHEMENAMEPLQ \ -configuration Debug \ CODE_SIGN_IDENTITY=&quot;iPhone Developer: Songbai He(73N6HPPJDP)” \ PROVISIONING_PROFILE=&quot;com.曹理鹏(iCocos)-梦工厂.shaoshang&quot; \ clean \ build \ -derivedDataPath $IPAPATH/$BRANCHNAME/$DATE 测试与发布只需要更改对应的参数即可-configuration Debug -configuration Release 用xcrun打包成ipa包xcrun -sdk iphoneos PackageApplication \ -v $IPAPATH/Build/Products/Debug-iphoneos/$SCHEMENAME.app \ -o $IPAPATH/$IPANAME 生成ipa包，上传到fir.im一：1.注册fir.拿到tokenfir.im官网：https://fir.im 注册号账号，点击右上角个人信息，进入API token 拿到token，并保存 二：安装fir-clifir-cli 使用 Ruby 构建, 无需编译, 只要安装相应 gem 即可. $ ruby -v # &gt; 1.9.3 $ gem install fir-cli 三：Mac新版之后可能由于源的问题导致无法安装（由于10.11引入了 rootless, 无法直接安装 fir-cli）一般都是这么安装的：使用 Homebrew 及 RVM 安装 Ruby, 再安装 fir-cli Install Homebrew: $ ruby -e &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)&quot; Install RVM: $ \curl -sSL https://get.rvm.io | bash -s stable --ruby Install fir-cli: $ gem install fir-cli 四：在终端登录并查看信息 $ fir login 按照提示输入token,然后 iCocosdeiMac:曹理鹏(iCocos)-梦工厂 iCocos$ fir login Please enter your fir.im API Token: 849ad212b94683b4c3087248d422d124 I, [2017-04-18T13:58:01.184043 #14531] INFO -- : Login succeed, previous user&apos;s email: al10201119@163.com I, [2017-04-18T13:58:01.185131 #14531] INFO -- : Login succeed, current user&apos;s email: al10201119@163.com I, [2017-04-18T13:58:01.185223 #14531] INFO -- : $fir me 登录成功,就会显示用户信息 I, [2017-04-18T13:58:21.876164 #14545] INFO – : Login succeed, current user’s email: al10201119@163.com I, [2017-04-18T13:58:21.876246 #14545] INFO – : Login succeed, current user’s name: al10201119 I, [2017-04-18T13:58:21.876261 #14545] INFO – : 使用fir指令，上传我们的ipa包 fir login -T c525718a775b954882xxxxxxxx # fir.im token fir publish $IPAPATH/Develop/xxx.ipa 最后你会看到 恭喜！！！上传fir.im成功！ 也可以直接使用执行脚本#author iCocos #注意：脚本目录和xxxx.xcodeproj要在同一个目录，如果放到其他目录，请自行修改脚本。 #工程名字(Target名字) Project_Name=&quot;Target名字，系统默认和工程名字一样&quot; #配置环境，Release或者Debug Configuration=&quot;Release&quot; #AdHoc版本的Bundle ID AdHocBundleID=&quot;com.xxx&quot; #AppStore版本的Bundle ID AppStoreBundleID=&quot;com.xxx&quot; #enterprise的Bundle ID EnterpriseBundleID=&quot;com.xxx&quot; # ADHOC #证书名#描述文件 ADHOCCODE_SIGN_IDENTITY=&quot;iPhone Distribution: xxxx&quot; ADHOCPROVISIONING_PROFILE_NAME=&quot;xxxx-xxxx-xxxx-xxxx&quot; #AppStore证书名#描述文件 APPSTORECODE_SIGN_IDENTITY=&quot;iPhone Distribution: xxxx&quot; APPSTOREROVISIONING_PROFILE_NAME=&quot;xxxx-xxxx-xxxx-xxxx&quot; #企业(enterprise)证书名#描述文件 ENTERPRISECODE_SIGN_IDENTITY=&quot;iPhone Distribution: xxxxx&quot; ENTERPRISEROVISIONING_PROFILE_NAME=&quot;xxxx-xxxx-xxxx-xxxx&quot; #加载各个版本的plist文件 ADHOCExportOptionsPlist=./ADHOCExportOptionsPlist.plist AppStoreExportOptionsPlist=./AppStoreExportOptionsPlist.plist EnterpriseExportOptionsPlist=./EnterpriseExportOptionsPlist.plist ADHOCExportOptionsPlist=${ADHOCExportOptionsPlist} AppStoreExportOptionsPlist=${AppStoreExportOptionsPlist} EnterpriseExportOptionsPlist=${EnterpriseExportOptionsPlist} echo &quot;~~~~~~~~~~~~选择打包方式(输入序号)~~~~~~~~~~~~~~~&quot; echo &quot; 1 appstore&quot; echo &quot; 2 adhoc&quot; echo &quot; 3 enterprise&quot; # 读取用户输入并存到变量里 read parameter sleep 0.5 method=&quot;$parameter&quot; # 判读用户是否有输入 if [ -n &quot;$method&quot; ] then #clean下 xcodebuild clean -xcodeproj ./$Project_Name/$Project_Name.xcodeproj -configuration $Configuration -alltargets if [ &quot;$method&quot; = &quot;1&quot; ] then #appstore脚本 xcodebuild -project $Project_Name.xcodeproj -scheme $Project_Name -configuration $Configuration -archivePath build/$Project_Name-appstore.xcarchive clean archive build CODE_SIGN_IDENTITY=&quot;${APPSTORECODE_SIGN_IDENTITY}&quot; PROVISIONING_PROFILE=&quot;${APPSTOREROVISIONING_PROFILE_NAME}&quot; PRODUCT_BUNDLE_IDENTIFIER=&quot;${AppStoreBundleID}&quot; xcodebuild -exportArchive -archivePath build/$Project_Name-appstore.xcarchive -exportOptionsPlist $AppStoreExportOptionsPlist -exportPath ~/Desktop/$Project_Name-appstore.ipa elif [ &quot;$method&quot; = &quot;2&quot; ] then #adhoc脚本 xcodebuild -project $Project_Name.xcodeproj -scheme $Project_Name -configuration $Configuration -archivePath build/$Project_Name-adhoc.xcarchive clean archive build CODE_SIGN_IDENTITY=&quot;${ADHOCCODE_SIGN_IDENTITY}&quot; PROVISIONING_PROFILE=&quot;${ADHOCPROVISIONING_PROFILE_NAME}&quot; PRODUCT_BUNDLE_IDENTIFIER=&quot;${AdHocBundleID}&quot; xcodebuild -exportArchive -archivePath build/$Project_Name-adhoc.xcarchive -exportOptionsPlist $ADHOCExportOptionsPlist -exportPath ~/Desktop/$Project_Name-adhoc.ipa elif [ &quot;$method&quot; = &quot;3&quot; ] then #企业打包脚本 xcodebuild -project $Project_Name.xcodeproj -scheme $Project_Name -configuration $Configuration -archivePath build/$Project_Name-enterprise.xcarchive clean archive build CODE_SIGN_IDENTITY=&quot;${ENTERPRISECODE_SIGN_IDENTITY}&quot; PROVISIONING_PROFILE=&quot;${ENTERPRISEROVISIONING_PROFILE_NAME}&quot; PRODUCT_BUNDLE_IDENTIFIER=&quot;${EnterpriseBundleID}&quot; xcodebuild -exportArchive -archivePath build/$Project_Name-enterprise.xcarchive -exportOptionsPlist $EnterpriseExportOptionsPlist -exportPath ~/Desktop/$Project_Name-enterprise.ipa else echo &quot;参数无效....&quot; exit 1 fi fi 这里还有一个大神使用PHP干了一票，虽然还在学习PHP中，但是有机会也要试一下 这里目前只是实现了Ad-Hoc打包上传，关于App Store其实也就是更改参数，和对应的地址，后续会退出相应的文章界面具体实战……]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>iOS——自动打包上传</tag>
        <tag>自动打包上传</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带你简单理解block和__block的实现？]]></title>
    <url>%2F2017%2F03%2F25%2F%E5%B8%A6%E4%BD%A0%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3block%E5%92%8C-block%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[其实类似block和__block的实现网上随便一搜就能找到无数篇，有些分析的确实非常不错，但是或许有些人刚好就没有那个耐心去看完，有些人就算最后看完也没有理解，甚至还是没能理解其中的核心,所以这里我就根据个人的理解和网上资料的整理，简单的总结一下…… block1.Block其实是闭包 2.Block是基于C语言的拓展 3.Block是基于指针和函数指针实现的， 4.同时他也是一种匿名函数，而且你会发现他和函数其实有很多相似的地方 5.通过打印我们可以知道他其实是一种的结构体 block的实现Block是被设为_NSConcreteStackBlock，在栈上生成。当我们把Block作为全局变量使用时，对应生成的Block将被设为_NSConcreteGlobalBlock Block属性这里还有一点关于block类型的ARC属性。上文也说明了，ARC会自动帮strong类型且捕获外部变量的block进行copy，所以在定义block类型的属性时也可以使用strong，不一定使用copy。也就是以下代码： /** 假如有栈block赋给以下两个属性 **/ // 这里因为ARC，当栈block中会捕获外部变量时，这个block会被copy进堆中 // 如果没有捕获外部变量，这个block会变为全局类型 // 不管怎么样，它都脱离了栈生命周期的约束 @property (strong, nonatomic) Block *strongBlock; // 这里都会被copy进堆中 @property (copy, nonatomic) Block *copyBlock; __blockBlock不允许修改外部变量的值，这里所说的外部变量的值，指的是栈中指针的内存地址。__block 所起到的作用就是只要观察到该变量被 block 所持有，就将“外部变量”在栈中的内存地址放到了堆中。进而在block内部也可以修改外部变量的值。 总结就是：__block对象在block中是可以被修改、重新赋值的。 __block的实现__block其实是堆栈的拷贝， 首先block修饰的变量会变成block_bref_val_0的结构体，它包含实例变量本身__forwarding(用于访问局部变量val)。 block拷贝到堆上的时候： _val_0也会拷贝到堆上，局部变量销毁，block任然能对堆上的局部变量操作 __forwarding替换为堆上的__block变量的地址 栈上的_val_0结构体中的__forwarding指针也会指向堆上的结构体 main函数或者blcok释放的时候，只是释放了栈上的东西，所有对局部变量的操作都已经移到了对上。]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>底层</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带你简单理解weak和__weak的实现？]]></title>
    <url>%2F2017%2F03%2F21%2F%E5%B8%A6%E4%BD%A0%E7%AE%80%E5%8D%95%E7%90%86%E8%A7%A3weak%E5%92%8C-weak%E7%9A%84%E5%AE%9E%E7%8E%B0%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[其实类似weak和__weak的实现网上随便一搜就能找到无数篇，有些分析的确实非常不错，但是或许有些人刚好就没有那个耐心去看完，有些人就算最后看完也没有理解，甚至还是没能理解其中的核心,所以这里我就根据个人的理解和网上资料的整理，简单的总结一下…… weak字面含义就是弱引用,Objective-C中默认都是强引用的（strong） weak的实现Runtime维护了一个weak表，用于存储指向某个对象的所有weak指针。weak表其实是一个hash（哈希）表，Key是所指对象的地址，Value是weak指针的地址（这个地址的值是所指对象的地址）数组。 weak 的实现原理可以概括一下三步：1、初始化时：runtime会调用objc_initWeak函数，初始化一个新的weak指针指向对象的地址。 2、添加引用时：objc_initWeak函数会调用 objc_storeWeak() 函数， objc_storeWeak() 的作用是更新指针指向，创建对应的弱引用表。 3、释放时，调用clearDeallocating函数。clearDeallocating函数首先根据对象地址获取所有weak指针地址的数组，然后遍历这个数组把其中的数据设为nil，最后把这个entry从weak表中删除，最后清理对象的记录。 __weak__weak修饰符的对象，作用等同于定义为weak的property。他并不会导致循环引用问题（通过苹果文档我们可以得出这样的结论），当原对象没有任何强引用的时候，弱引用指针也会被设置为nil。 __weak的实现简单来说，系统有一个全局的 CFMutableDictionary 实例，来保存每个对象的 weak 指针列表，因为每个对象可能有多个 weak 指针，所以这个实例的值是 CFMutableSet（Array） 类型。 剩下我们要做的，就是在引用计数变成 0 的时候，去这个全局的字典里面，找到所有的 weak 指针，将其值设置成 nil。如何做到这一点呢？Friday QA 上介绍了一种类似 KVO 实现的方式。当对象存在 weak 指针时，我们可以将这个实例指向一个新创建的子类，然后修改这个子类的 release 方法，在 release 方法中，去从全局的 CFMutableDictionary 字典中找到所有的 weak 对象，并且设置成 nil。我摘抄了 Friday QA 上的实现的核心代码，如下： Class subclass = objc_allocateClassPair(class, newNameC, 0); Method release = class_getInstanceMethod(class, @selector(release)); Method dealloc = class_getInstanceMethod(class, @selector(dealloc)); class_addMethod(subclass, @selector(release), (IMP)CustomSubclassRelease, method_getTypeEncoding(release)); class_addMethod(subclass, @selector(dealloc), (IMP)CustomSubclassDealloc, method_getTypeEncoding(dealloc)); objc_registerClassPair(subclass); 总结一句就是：一个通俗的解释就是，在Objective-C的运行时环境中，维护了一种weak表，这张哈希表用对象的首地址作为键，将由若干个weak修饰的指针自身的地址组成的数组作为值。当一个Objective-C对象被释放后，通过这个对象的起始地址来找到所有指向它的weak指针，并将它们指向nil。 __weak的作用在Objective-C中，用__weak修饰的指针，会在所指向的那个Objective-C对象被释放后，自动指向nil。 使用__weak来修饰指针，相比于__unsafe_unretained，可以帮助程序员减小访问野指针的风险，方便了程序员对内存的管理。 block和weak的区别前面提到了block，也大概说了一下其简单实现，所以这里总结一下block和__weak修饰符的区别： 1.__block不管是ARC还是MRC模式下都可以使用，可以修饰对象，还可以修饰基本数据类型。 2.__weak只能在ARC模式下使用，也只能修饰对象（NSString），不能修饰基本数据类型（int）。 3.__block对象可以在block中被重新赋值，__weak不可以。 4.__block对象在ARC下可能会导致循环引用，非ARC下会避免循环引用，__weak只在ARC下使用，可以避免循环引用。]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>带你简单理解weak和__weak的实现？</tag>
        <tag>底层分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[分类能添加属性吗？成员变量呢？为什么？]]></title>
    <url>%2F2017%2F03%2F18%2F%E5%88%86%E7%B1%BB%E8%83%BD%E6%B7%BB%E5%8A%A0%E5%B1%9E%E6%80%A7%E5%90%97%EF%BC%9F%E6%88%90%E5%91%98%E5%8F%98%E9%87%8F%E5%91%A2%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[在iOS开发中，如果你要在不改变原来的类内容的基础上，为类增加一些方法，那么苹果提供了一个很好机制，那就是分类，当然我们平时可能比较多的是直接给他添加方法，并且我们清楚一点的是，我们还能给一个分类添加属性（使用关联对象），但是并不能添加成员变量，可是为什么刚好成员变量就不能连接呢…… 分类概念： Category 分类是OC特有的语言，依赖于类。 分类的作用： 在不改变原来的类内容的基础上，为类增加一些方法。 分类增加属性和方法属性方法 这里其实就不用做过多介绍了，只需要知道分类增加方法其实就和一个类增加一个方法一样。 属性 至于分类增加属性，只要使用到的就是Objective-C里面比较底层的一个技术Runtime，Runtime里面有个里面有个关联对象的概念，具体请查看官方或者相关资料，很简单。 成员变量NO，NO，NO，分类是不能增加属性的，一定没办法，如果你有可以来找我，哈哈…. 为什么不能增加成员变量呢？首先我们不能混淆了成员变量和属性的概念.Property是Property，Ivar是Ivar。 分类里面不能添加Ivar是因为分类本身并不是一个真正的类（Objective-C中真正的类是有一个isa存在的），但是分类并没有自己的ISA . 类最开始生成了很多基本属性，比如IvarList，MethodList，分类只会将自己的method attach到主类，并不会影响到主类的IvarList。 这就是为什么分类里面不能增加成员变量的原因” 类和分类的初始化1.当程序启动时，就会加载项目中所有的类和分类，而且加载后会调用每个类和分类的+load方法，只会调用一次； 2.当第一次使用某个类时，就会调用当前类的+initialize方法； 3.先加载父类，再加载子类（先调用父类的+load方法，再调用子类的+load方法，最后调用分类的+load方法），先初始化父类，再初始化子类（先调用父类的+initialize方法，再调用子类的+initialize方法）。 4.注意：在初始化的时候，如果在分类中重写了+initialize方法，则会覆盖掉父类的。 5.重写+initialize方法可以监听类的使用情况。]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>底层</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[为什么调用nil的任何方法都不会崩溃？]]></title>
    <url>%2F2017%2F03%2F15%2F%E4%B8%BA%E4%BB%80%E4%B9%88%E8%B0%83%E7%94%A8nil%E7%9A%84%E4%BB%BB%E4%BD%95%E6%96%B9%E6%B3%95%E9%83%BD%E4%B8%8D%E4%BC%9A%E5%B4%A9%E6%BA%83%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[我们知道，在Objective-C消息和转发机制的背后有这样一个说法：调用没有实现或者不存在的方法，会导致App崩溃，但是如果你比较细心或者专门研究过消息和转发机制那么你应该听过这样的结论： Objective-C中调用nil的任何方法都不会崩溃，但是，为什么就不会崩溃呢？这里就简单分析一下具体的原因和底层的实现方式…… 访问了一个已经被释放的对象我们知道在不使用 ARC 的时候，内存要自己管理，这时重复或过早释放都有可能导致 Crash。 NSObject * aObj = [[NSObject alloc] init]; [aObj release]; NSLog(@&quot;%@&quot;, aObj); 原因aObj 这个对象已经被释放，但是指针没有置空，这时访问这个指针指向的内存就会 Crash。 解决办法使用前要判断非空，释放后要置空。正确的释放应该是: [aObj release]; aObj = nil; 由于ObjC的特性，调用 nil 指针的任何方法相当于无作用，所以即使有人在使用这个指针时没有判断至少还不会挂掉。 那么这里就有一个问题？为什么调用nil的任何方法都不会崩溃呢？ 首先在Objective-C里，nil对象被设计来跟NULL空指针关联的。他们的区别就是nil是一个对象，而NULL只是一个值。而且我们对于nil调用方法，不会产生crash或者抛出异常。这个技术被framework通过多种不同的方式使用。 最主要的就是我们现在在调用方法之前根本无须去检查这个对象是否是nil。假如我们调了nil对象的一个有返回值的方法，那么我们会得到一个nil返回值。 我们先来看看这断代码： - (void) dealloc { self.caption = nil; self.photographer = nil; [super dealloc]; } 具体原因 之所以可以这么做是因为我们给把nil对象设给了一个成员变量，setter就会retain nil对象(当然了这个时候nil对象啥事情也不会做)然后release旧的对象。这个方式来释放对象其实更好，因为这样做的话，成员变量连指向随机数据的机会都没有，而通过别的方式，出现指向随机数据的情形机会不可避免。 注意到我们调用的self.VAR这样的语法，这表示我们正在用setter，而且不会引起任何内存问题。假如我们直接去设值的话，就会有内存溢出： // incorrect. causes a memory leak. // use self.caption to Go through setter caption = nil; 这里进入提到了崩溃，那么就大概整理一下开发中常见的崩溃问题和类型，方便提前预防和部分相关处理一、访问了一个已经被释放的对象：nil，autorelease 二、访问数组类对象越界或插入了空对象：分类或者runtime替换 三、访问了不存在的方法：判断是否有，runtime修改 四、字节对齐：使用 memcpy 来作内存拷贝，而不是直接对指针赋值 五、堆栈溢出（过多的递归会导致栈溢出，过多的 alloc 变量会导致堆溢出。） 六、多线程并发操作：加锁 ，原子，Operation Objects, GCD, Idle-time notifications, Asynchronous functions, Timers, Separate processes。 七、Repeating NSTimer：写了个宏用来释放Timer]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>底层</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[生命之源-RunLoop]]></title>
    <url>%2F2017%2F03%2F09%2F%E7%94%9F%E5%91%BD%E4%B9%8B%E6%BA%90-RunLoop%2F</url>
    <content type="text"><![CDATA[iOS一切的生命之源都需要RunLoop的支持，关于RunLoop的界面和相关知识，网上想你一一搜一天也看不完，所以这里只为总结Runloop相关技术，适用于底层，面试，简单实战了解！ 前言：1+ 一个线程只能执行一个任务，任务执行完之后，线程就会退出，但是主线程不会退出，因为我们需要让主线程等待接收事件 介绍123456+ 运行循环（do-while）：不断处理各种事件+ 一个线程（唯一）对应一个RunLoop（可以嵌套子runloops），主线程默认启动，子线程手动启动（run）：获取RunLoop对象的时候，就会创建RunLoop+ RunLoop有多个Model，Model有多个timer（array）/source（set）/observer（array）。+ 每次启动只能启动一个，切换需要先推出在指定（分隔不同time/source/observer）+ 当前Model没有任何timer/source/observer就会推出（mode只能添加不能删除）+ 第一次获取创建，线程结束销毁 runloop退出的条件：1app退出；线程关闭；设置最大时间到期；modeItem为空； Ref12345+ CFRunLoopTimerRef，基于时间触发（NSTimer），受Model影响，GCD不受Model影响+ CFRunLoopSourceRef，事件源，source0（非Port），source1（Port），跟Port密切联系source0：event事件，只含有回调，需要标记待处理（signal），然后手动将runloop唤醒（wakeup）；source1 ：包含一个 mach_port 和一个回调，被用于通过内核和其他线程发送的消息，能主动唤醒runloop。 Runloop本质：mach port和mach_msg()。123Mach是XNU的内核，进程、线程和虚拟内存等对象通过端口发消息进行通信，Runloop通过mach_msg()函数发送消息，如果没有port 消息，内核会将线程置于等待状态 mach_msg_trap() 。如果有消息，判断消息类型处理事件，并通过modeItem的callback回调(处理事件的具体执行是在DoBlock里还是在回调里目前我还不太明白？？？)。Runloop有两个关键判断点，一个是通过msg决定Runloop是否等待，一个是通过判断退出条件来决定Runloop是否循环 定时源，输入源12+ 定时源，同步消息，特定或者一定时间间隔发生+ 输入源，来自起来线程或者程序 应用1+ NSTimer，ImageView显示，PerformSelector，常驻线程，自动释放池，界面刷新，手势识别，GCD任务，timer：（与CADisplayLink），网络请求： autorrelease释放时机：12+ 手动干预释放：指定autorreleasepool,当前作用域大括号结束立即释放+ 系统自动释放：不指定，aut对象在当前RunLoop迭代结束释放 自动释放池12自动释放池寄生于Runloop：程序启动后，主线程注册了两个Observer监听runloop的进出与睡觉。一个最高优先级OB监测Entry状态；一个最低优先级OB监听BeforeWaiting状态和Exit状态。线程(创建)--&gt;runloop将进入--&gt;最高优先级OB创建释放池--&gt;runloop将睡--&gt;最低优先级OB销毁旧池创建新池--&gt;runloop将退出--&gt;最低优先级OB销毁新池--&gt;线程(销毁) Timer注意121、如果是在主线程中运行timer，想要timer在某界面有视图滚动时，依然能正常运转，那么将timer添加到RunLoop中时，就需要设置mode 为NSRunLoopCommonModes。2、如果是在子线程中运行timer,那么将timer添加到RunLoop中后，Mode设置为NSDefaultRunLoopMode或NSRunLoopCommonModes均可，但是需要保证RunLoop在运行，且其中有任务。 runloop:121、（要让马儿跑）通过do-while死循环让程序持续运行：接收用户输入，调度处理事件时间。2、（要让马儿少吃草）通过mach_msg()让runloop没事时进入trap状态，节省CPU资源。]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>生命之源-RunLoop</tag>
        <tag>RunLoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[前世-内存管理]]></title>
    <url>%2F2017%2F03%2F03%2F%E5%89%8D%E4%B8%96-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86%2F</url>
    <content type="text"><![CDATA[最近越来越发觉，自己做了这么久iOS开发，似乎都是在瞎玩，回望这一路写的App，写的代码，以及上线的App，又有几个自己真的非常满意的？ 说道这里，不得不提的是：我相信不止我一个人有这种感触，代码写久了，自己都不知道自己在写撒，就连上线都是个意外。 这一路走来我一直在总结，也在回望，到底自己哪里出了问题，终于发现：终究是自己不敢面对而已，底层不了解，只知道敲。 因为换了电脑，换了工作，换了新的环境，也换了一个不一样的心情重新开始探索之路，所以之前的博客地址也停止更新了，如果你对iOS相关知识感兴趣或者有什么疑问和建议可以联系我，或者直接在下面评论， 本来打算这个博客中不会再写iOS的东西，但是最近一直上班，也找了一段时间公司，总之感触不少，现在这个行情，大家都懂的。 前段时间也整理了一些东西，我相信值得一看。 1原则：没有强指针指向对象，对象就会被释放。 MRC-ARC123ARC：LLVM3.0（iOS5，Xcode4）前段编译器：方法内创建对象，末尾自动插入release销毁。类拥有对象，在dealloc内释放。更底层的C语言实现。objc_release,objc_retain优化调用过程ARC优化器：负责移除多余的插入，和一些引用的优化。包括运行期组件。 关于循环12345weak：循环引用，自身强引用，IBOutlet。非拥有，不保留也不释放，置nil，weak必须用于OC对象，assign非OC对象for循环：只有当自动释放器被release，池中的表示autorrelease对象才会被释放===内存耗尽，没有释放-&gt;内存泄露1.i比较大：使用@autorreleasepool&#123;&#125;，在for外面，循环结束，销毁创建对象，解决占据栈内存问题。2.i玩命大：一次循环都会自动释放池满，@autorreleasepool&#123;&#125;放在for里面，每次循环前将上一次对象release。 关于内存12内存布局：没有多继承，所以布局简单最前面isa，指向类。父类实例变量在子类实例变量之前。 关于线程123456789界面线程维护自己的线程池。自己创建的线程数据，需要创建线程的内存池。autorreleasepool实现：objc_autorreleasepool=Push,Pop,objc_autorrelease每次RunLoop完成一个循环的时候，都会检测对象的retainCount，为0则没有使用，释放。内存管理的范围：集成自NSObject对象，基本数据类型无效。因为存储空间不同，基本数据存在栈区。对象在堆中，代码块结束，涉及局部变量弹栈清空，指向对象指针回收，对象没有指针指向，但是还在堆中，所以内存泄露了。unowned（unsafe_unretained）：对象销毁不会为空，但是更快，因为weak需要unwrap。 常见状态管理1234567野指针:指针变量没有初始化，指向的空间被释放。调用方法报异常，崩溃。release后，地址nil，OC中没有空指针异常内存泄露：对象提前赋值nil，导致release不起作用。没有配对释放或者清空。栈区释放了，堆区没有释放。最终导致内存溢出内存溢出：容量超出使用限制僵尸对象：堆中已经被释放的对象count=0空指针：指针赋值为nil判断对象销毁：dealloc（需要super一下），已经释放的对象无法复活 对象关系123集成：组合：（包含关系），确保成员连边不被提前释放，重写set方法，retain一下。成员变量在dealloc中配对释放。内存泄露：1.set没有retain对象，2.没有release旧对象，3.没有判断set方法传入是否是同一对象依赖：（对象作为方法参数传递） autorrelease（pool）/垃圾回收机制1234autorrelease：把该对象放入自动释放池，自动释放池释放时，内部对象引用计数-1。NSAutorreleasePool：通过接受对象向他发送的release消息，记录该对象的release消息，自动释放池销毁时，向池中记录release的对象发送release消息。 垃圾回收机制：autorrelease只是延迟释放，GC是每隔一段时间询问程序，是否有无指针指向的对象，没有就释放 自动释放池123456自动释放池： 1.存储多想对象类型的指针变量（可以嵌套） 2.作用：将对象与自动释放池建议关系，池子内调用autorrelease，在自动释放池销毁时销毁对象，延迟release销毁时间 3.对池内对象作用：存入池中的对象，池销毁，全部对象release一次 4.调用autorrelease将对象加入自动释放池，多次调用导致野指针异常 5.释放时机：简单：autorrelease的&#125;执行完后。实际：Autorrelease对象是在当前RunLoop迭代结束时释放，原因是：系统在每个RunLoop迭代中加入了自动释放池Push，Pop 关键字12345block中多次使用weakSelf（延迟操作，导致取不到弱指针），可以block种先使用strongSelf，防止执行是weakSelf意外释放，对于非ARCweak改为block就可以release和drain：ARC中一样，GC中release无效操作，所以无论是否为GC使用drain没有问题。copy：OC对象类型如果有mutable，深拷贝，新对象为count=1，没有为浅拷贝，count+1. 其他总结1234567891011通过Observer监听RunLoop状态，一旦监听到RunLoop即将进入休眠等状态，就释放自动释放池。FIFO：新访问的数据插入队列尾部，数据在队列中移动，淘汰头部数据。LRU（FIFO相反），LFU循环引用：定时器（timer作为类的成员变量，self-target,不使用记得invalidate），Blcok（block在copy时对内部对象强引用（ARC）或者引用计数+1（MRC）），代理：（assign（MRC），weak（ARC））通知：多对多，主要跨层传值。对象加入到通知中心后，对象被销毁前没有将对象从通知中心移除，当再次发送通知的时候，会崩溃。默认关键字：基本数据类型（atomic，readwrite，assign），OC对象类型（atomic，readwrite，strong）TableView代理用assign：控制器对内部的View进行了一次retain，TableView对代理控制器也retain一次就会循环引用。 其实知道了这个并不证明就能写出好的代码，还需要时间的沉淀，不断的尝试，不断的思考与总结。感谢你能看到最后，希望对你有用，我们下次再见！]]></content>
      <categories>
        <category>iOS</category>
      </categories>
      <tags>
        <tag>iOS</tag>
        <tag>内存管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[温馨提示]]></title>
    <url>%2F2017%2F03%2F01%2F%E6%B8%A9%E9%A6%A8%E6%8F%90%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[嗨，欢迎来到梦工厂，我是曹理鹏 (@iCocos)，一名 大数据开发工程师，前iOS(+游戏)/ 前端 开发者，希望做点有意义的事情。现居广州，正在修行，探求创意之源。。 1234微信：clpaial10201119 / QQ：2211523682博客(前后端)：https://icocos.github.io/github：https://github.com/al1020119个人网站：http://www.icocos.cn/ 由于学习，技术方向的原因，本人之前github博客将会停止更新，之前所有文章偏向技术，并且主要针对iOS基础，底层，面试与实战，此后将会停止更新此博客，并将所有新发布文章，更新至当前博客地址，届时会不定时在公众号，和微博发布相关动态。 此博客将会保函以下内容： 大数据开发相关技术 大数据实战/架构/优化 数据仓库设计与建模 少量iOS高级/实战经验(+游戏) 读书笔记与后感 个人业余爱好 其他原创杂文 如果你想了解更多关于作者，或者关于大数据开发，请关注我的个人公众号，也可以通过微信，QQ，Github联系我，也可以直接在这里评论留言。 同时，如果对我的文章有一些意见或者建议，也欢迎联系我，我们一起共进退！]]></content>
      <categories>
        <category>Others</category>
      </categories>
      <tags>
        <tag>温馨提示</tag>
        <tag>iCocos</tag>
        <tag>提示</tag>
      </tags>
  </entry>
</search>
